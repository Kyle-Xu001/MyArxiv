<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-12T00:00:00Z">2023-10-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">37</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Octopus: Embodied Vision-Language Programmer from Environmental Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (VLMs) have achieved substantial progress in
multimodal perception and reasoning. Furthermore, when seamlessly integrated
into an embodied agent, it signifies a crucial stride towards the creation of
autonomous and context-aware systems capable of formulating plans and executing
commands with precision. In this paper, we introduce Octopus, a novel VLM
designed to proficiently decipher an agent's vision and textual task objectives
and to formulate intricate action sequences and generate executable code. Our
design allows the agent to adeptly handle a wide spectrum of tasks, ranging
from mundane daily chores in simulators to sophisticated interactions in
complex video games. Octopus is trained by leveraging GPT-4 to control an
explorative agent to generate training data, i.e., action blueprints and the
corresponding executable code, within our experimental environment called
OctoVerse. We also collect the feedback that allows the enhanced training
scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a
series of experiments, we illuminate Octopus's functionality and present
compelling results, and the proposed RLEF turns out to refine the agent's
decision-making. By open-sourcing our model architecture, simulator, and
dataset, we aspire to ignite further innovation and foster collaborative
applications within the broader embodied AI community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://choiszt.github.io/Octopus/, Codebase:
  https://github.com/dongyh20/Octopus</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Fatigued Movements for Virtual Character Animation <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noshaba Cheema, Rui Xu, Nam Hee Kim, Perttu Hämäläinen, Vladislav Golyanik, Marc Habermann, Christian Theobalt, Philipp Slusallek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual character animation and movement synthesis have advanced rapidly
during recent years, especially through a combination of extensive motion
capture datasets and machine learning. A remaining challenge is interactively
simulating characters that fatigue when performing extended motions, which is
indispensable for the realism of generated animations. However, capturing such
movements is problematic, as performing movements like backflips with fatigued
variations up to exhaustion raises capture cost and risk of injury.
Surprisingly, little research has been done on faithful fatigue modeling. To
address this, we propose a deep reinforcement learning-based approach, which --
for the first time in literature -- generates control policies for full-body
physically simulated agents aware of cumulative fatigue. For this, we first
leverage Generative Adversarial Imitation Learning (GAIL) to learn an expert
policy for the skill; Second, we learn a fatigue policy by limiting the
generated constant torque bounds based on endurance time to non-linear, state-
and time-dependent limits in the joint-actuation space using a
Three-Compartment Controller (3CC) model. Our results demonstrate that agents
can adapt to different fatigue and rest rates interactively, and discover
realistic recovery strategies without the need for any captured data of
fatigued movement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 22 figures. To be published in ACM SIGGRAPH Asia Conference
  Papers 2023. ACM ISBN 979-8-4007-0315-7/23/12</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tree-Planner: Efficient Close-loop Task Planning with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies close-loop task planning, which refers to the process of
generating a sequence of skills (a plan) to accomplish a specific goal while
adapting the plan based on real-time observations. Recently, prompting Large
Language Models (LLMs) to generate actions iteratively has become a prevalent
paradigm due to its superior performance and user-friendliness. However, this
paradigm is plagued by two inefficiencies: high token consumption and redundant
error correction, both of which hinder its scalability for large-scale testing
and applications. To address these issues, we propose Tree-Planner, which
reframes task planning with LLMs into three distinct phases: plan sampling,
action tree construction, and grounded deciding. Tree-Planner starts by using
an LLM to sample a set of potential plans before execution, followed by the
aggregation of them to form an action tree. Finally, the LLM performs a
top-down decision-making process on the tree, taking into account real-time
environmental information. Experiments show that Tree-Planner achieves
state-of-the-art performance while maintaining high efficiency. By decomposing
LLM queries into a single plan-sampling call and multiple grounded-deciding
calls, a considerable part of the prompt are less likely to be repeatedly
consumed. As a result, token consumption is reduced by 92.2% compared to the
previously best-performing model. Additionally, by enabling backtracking on the
action tree as needed, the correction process becomes more flexible, leading to
a 40.5% decrease in error corrections. Project page:
https://tree-planner.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Visual Decomposer: Long-Horizon Manipulation Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Zhang, Yunshuang Li, Osbert Bastani, Abhishek Gupta, Dinesh Jayaraman, Yecheng Jason Ma, Luca Weihs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world robotic tasks stretch over extended horizons and encompass
multiple stages. Learning long-horizon manipulation tasks, however, is a
long-standing challenge, and demands decomposing the overarching task into
several manageable subtasks to facilitate policy learning and generalization to
unseen tasks. Prior task decomposition methods require task-specific knowledge,
are computationally intensive, and cannot readily be applied to new tasks. To
address these shortcomings, we propose Universal Visual Decomposer (UVD), an
off-the-shelf task decomposition method for visual long horizon manipulation
using pre-trained visual representations designed for robotic control. At a
high level, UVD discovers subgoals by detecting phase shifts in the embedding
space of the pre-trained representation. Operating purely on visual
demonstrations without auxiliary information, UVD can effectively extract
visual subgoals embedded in the videos, while incurring zero additional
training cost on top of standard visuomotor policy training. Goal-conditioned
policies learned with UVD-discovered subgoals exhibit significantly improved
compositional generalization at test time to unseen tasks. Furthermore,
UVD-discovered subgoals can be used to construct goal-based reward shaping that
jump-starts temporally extended exploration for reinforcement learning. We
extensively evaluate UVD on both simulation and real-world tasks, and in all
cases, UVD substantially outperforms baselines across imitation and
reinforcement learning settings on in-domain and out-of-domain task sequences
alike, validating the clear advantage of automated visual task decomposition
within the simple, compact UVD framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Act from Actionless Videos through Dense Correspondences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, Joshua B. Tenenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present an approach to construct a video-based robot policy
capable of reliably executing diverse tasks across different robots and
environments from few video demonstrations without using any action
annotations. Our method leverages images as a task-agnostic representation,
encoding both the state and action information, and text as a general
representation for specifying robot goals. By synthesizing videos that
``hallucinate'' robot executing actions and in combination with dense
correspondences between frames, our approach can infer the closed-formed action
to execute to an environment without the need of any explicit action labels.
This unique capability allows us to train the policy solely based on RGB videos
and deploy learned policies to various robotic tasks. We demonstrate the
efficacy of our approach in learning policies on table-top manipulation and
navigation tasks. Additionally, we contribute an open-source framework for
efficient video modeling, enabling the training of high-fidelity policy models
with four GPUs within a single day.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://flow-diffusion.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PolyTask: Learning Unified Policies through Behavior Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhant Haldar, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unified models capable of solving a wide variety of tasks have gained
traction in vision and NLP due to their ability to share regularities and
structures across tasks, which improves individual task performance and reduces
computational footprint. However, the impact of such models remains limited in
embodied learning problems, which present unique challenges due to
interactivity, sample inefficiency, and sequential task presentation. In this
work, we present PolyTask, a novel method for learning a single unified model
that can solve various embodied tasks through a 'learn then distill' mechanism.
In the 'learn' step, PolyTask leverages a few demonstrations for each task to
train task-specific policies. Then, in the 'distill' step, task-specific
policies are distilled into a single policy using a new distillation method
called Behavior Distillation. Given a unified policy, individual task behavior
can be extracted through conditioning variables. PolyTask is designed to be
conceptually simple while being able to leverage well-established algorithms in
RL to enable interactivity, a handful of expert demonstrations to allow for
sample efficiency, and preventing interactive access to tasks during
distillation to enable lifelong learning. Experiments across three simulated
environment suites and a real-robot suite show that PolyTask outperforms prior
state-of-the-art approaches in multi-task and lifelong learning settings by
significant margins.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Security Considerations in AI-Robotics: A Survey of Current Methods,
  Challenges, and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subash Neupane, Shaswata Mitra, Ivan A. Fernandez, Swayamjit Saha, Sudip Mittal, Jingdao Chen, Nisha Pillai, Shahram Rahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotics and Artificial Intelligence (AI) have been inextricably intertwined
since their inception. Today, AI-Robotics systems have become an integral part
of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These
systems are built upon three fundamental architectural elements: perception,
navigation and planning, and control. However, while the integration of
AI-Robotics systems has enhanced the quality our lives, it has also presented a
serious problem - these systems are vulnerable to security attacks. The
physical components, algorithms, and data that make up AI-Robotics systems can
be exploited by malicious actors, potentially leading to dire consequences.
Motivated by the need to address the security concerns in AI-Robotics systems,
this paper presents a comprehensive survey and taxonomy across three
dimensions: attack surfaces, ethical and legal concerns, and Human-Robot
Interaction (HRI) security. Our goal is to provide users, developers and other
stakeholders with a holistic understanding of these areas to enhance the
overall AI-Robotics system security. We begin by surveying potential attack
surfaces and provide mitigating defensive strategies. We then delve into
ethical issues, such as dependency and psychological impact, as well as the
legal concerns regarding accountability for these systems. Besides, emerging
trends such as HRI are discussed, considering privacy, integrity, safety,
trustworthiness, and explainability concerns. Finally, we present our vision
for future research directions in this dynamic and promising field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate
  Exploration Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Sobol Mark, Archit Sharma, Fahim Tajwar, Rafael Rafailov, Sergey Levine, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is desirable for policies to optimistically explore new states and
behaviors during online reinforcement learning (RL) or fine-tuning, especially
when prior offline data does not provide enough state coverage. However,
exploration bonuses can bias the learned policy, and our experiments find that
naive, yet standard use of such bonuses can fail to recover a performant
policy. Concurrently, pessimistic training in offline RL has enabled recovery
of performant policies from static datasets. Can we leverage offline RL to
recover better policies from online interaction? We make a simple observation
that a policy can be trained from scratch on all interaction data with
pessimistic objectives, thereby decoupling the policies used for data
collection and for evaluation. Specifically, we propose offline retraining, a
policy extraction step at the end of online fine-tuning in our
Offline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL).
An optimistic (exploration) policy is used to interact with the environment,
and a separate pessimistic (exploitation) policy is trained on all the observed
data for evaluation. Such decoupling can reduce any bias from online
interaction (intrinsic rewards, primacy bias) in the evaluation policy, and can
allow more exploratory behaviors during online interaction which in turn can
generate better data for exploitation. OOO is complementary to several
offline-to-online RL and online RL methods, and improves their average
performance by 14% to 26% in our fine-tuning experiments, achieves
state-of-the-art performance on several environments in the D4RL benchmarks,
and improves online RL performance by 165% on two OpenAI gym environments.
Further, OOO can enable fine-tuning from incomplete offline datasets where
prior methods can fail to recover a performant policy. Implementation:
https://github.com/MaxSobolMark/OOO
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Episodic Curriculum for Transformer Agents <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucy Xiaoyang Shi, Yunfan Jiang, Jake Grigsby, Linxi "Jim" Fan, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the
learning efficiency and generalization of Transformer agents. Central to CEC is
the placement of cross-episodic experiences into a Transformer's context, which
forms the basis of a curriculum. By sequentially structuring online learning
trials and mixed-quality demonstrations, CEC constructs curricula that
encapsulate learning progression and proficiency increase across episodes. Such
synergy combined with the potent pattern recognition capabilities of
Transformer models delivers a powerful cross-episodic attention mechanism. The
effectiveness of CEC is demonstrated under two representative scenarios: one
involving multi-task reinforcement learning with discrete control, such as in
DeepMind Lab, where the curriculum captures the learning progression in both
individual and progressively complex settings; and the other involving
imitation learning with mixed-quality data for continuous control, as seen in
RoboMimic, where the curriculum captures the improvement in demonstrators'
expertise. In all instances, policies resulting from CEC exhibit superior
performance and strong generalization. Code is open-sourced at
https://cec-agent.github.io/ to facilitate research on Transformer agent
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in NeurIPS 2023; The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Experience-based TAMP Framework for Foliated Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Hu, Shrutheesh R. Iyer, Henrik I. Christensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their complexity, foliated structure problems often pose intricate
challenges to task and motion planning in robotics manipulation. To counter
this, our study presents the ``Foliated Repetition Roadmap.'' This roadmap
assists task and motion planners by transforming the complex foliated structure
problem into a more accessible graph format. By leveraging query experiences
from different foliated manifolds, our framework can dynamically and
efficiently update this graph. The refined graph can generate distribution
sets, optimizing motion planning performance in foliated structure problems. In
our paper, we lay down the theoretical groundwork and illustrate its practical
applications through real-world examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUN-FRL: A Visual Inertial LiDAR Dataset for Aerial Autonomous
  Navigation and Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravindu G. Thalagala, Sahan M. Gunawardena, Oscar De Silva, Awantha Jayasiri, Arthur Gubbels, George K. I Mann, Raymond G. Gosine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a unique outdoor aerial visual-inertial-LiDAR dataset
captured using a multi-sensor payload to promote the global navigation
satellite system (GNSS)-denied navigation research. The dataset features flight
distances ranging from 300m to 5km, collected using a DJI M600 hexacopter drone
and the National Research Council (NRC) Bell 412 Advanced Systems Research
Aircraft (ASRA). The dataset consists of hardware synchronized monocular
images, IMU measurements, 3D LiDAR point-clouds, and high-precision real-time
kinematic (RTK)-GNSS based ground truth. Ten datasets were collected as ROS
bags over 100 mins of outdoor environment footage ranging from urban areas,
highways, hillsides, prairies, and waterfronts. The datasets were collected to
facilitate the development of visual-inertial-LiDAR odometry and mapping
algorithms, visual-inertial navigation algorithms, object detection,
segmentation, and landing zone detection algorithms based upon real-world drone
and full-scale helicopter data. All the datasets contain raw sensor
measurements, hardware timestamps, and spatio-temporally aligned ground truth.
The intrinsic and extrinsic calibrations of the sensors are also provided along
with raw calibration datasets. A performance summary of state-of-the-art
methods applied on the datasets is also provided.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Design and Development of an ArUco Markers-Based Quantitative
  Surface Tactile Sensor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ozdemir Can Kara, Charles Everson, Farshid Alambeigi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, with the goal of quantifying the qualitative image outputs of
a Vision-based Tactile Sensor (VTS), we present the design, fabrication, and
characterization of a novel Quantitative Surface Tactile Sensor (called QS-TS).
QS-TS directly estimates the sensor's gel layer deformation in real-time
enabling safe and autonomous tactile manipulation and servoing of delicate
objects using robotic manipulators. The core of the proposed sensor is the
utilization of miniature 1.5 mm x 1.5 mm synthetic square markers with inner
binary patterns and a broad black border, called ArUco Markers. Each ArUco
marker can provide real-time camera pose estimation that, in our design, is
used as a quantitative measure for obtaining deformation of the QS-TS gel
layer. Moreover, thanks to the use of ArUco markers, we propose a unique
fabrication procedure that mitigates various challenges associated with the
fabrication of the existing marker-based VTSs and offers an intuitive and
less-arduous method for the construction of the VTS. Remarkably, the proposed
fabrication facilitates the integration and adherence of markers with the gel
layer to robustly and reliably obtain a quantitative measure of deformation in
real-time regardless of the orientation of ArUco Markers. The performance and
efficacy of the proposed QS-TS in estimating the deformation of the sensor's
gel layer were experimentally evaluated and verified. Results demonstrate the
phenomenal performance of the QS-TS in estimating the deformation of the gel
layer with a relative error of <5%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Planning for Heterogeneous Robot Teams using Dynamic
  Topological Graphs and Mixed-Integer Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cora A. Dimmig, Kevin C. Wolfe, Marin Kobilarov, Joseph Moore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning under uncertainty is a fundamental challenge in robotics. For
multi-robot teams, the challenge is further exacerbated, since the planning
problem can quickly become computationally intractable as the number of robots
increase. In this paper, we propose a novel approach for planning under
uncertainty using heterogeneous multi-robot teams. In particular, we leverage
the notion of a dynamic topological graph and mixed-integer programming to
generate multi-robot plans that deploy fast scout team members to reduce
uncertainty about the environment. We test our approach in a number of
representative scenarios where the robot team must move through an environment
while minimizing detection in the presence of uncertain observer positions. We
demonstrate that our approach is sufficiently computationally tractable for
real-time re-planning in changing environments, can improve performance in the
presence of imperfect information, and can be adjusted to accommodate different
risk profiles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALPHA: Attention-based Long-horizon Pathfinding in Highly-structured
  Areas <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyang He, Tianze Yang, Tanishq Duhan, Yutong Wang, Guillaume Sartoretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multi-agent pathfinding (MAPF) problem seeks collision-free paths for a
team of agents from their current positions to their pre-set goals in a known
environment, and is an essential problem found at the core of many logistics,
transportation, and general robotics applications. Existing learning-based MAPF
approaches typically only let each agent make decisions based on a limited
field-of-view (FOV) around its position, as a natural means to fix the input
dimensions of its policy network. However, this often makes policies
short-sighted, since agents lack the ability to perceive and plan for
obstacles/agents beyond their FOV. To address this challenge, we propose ALPHA,
a new framework combining the use of ground truth proximal (local) information
and fuzzy distal (global) information to let agents sequence local decisions
based on the full current state of the system, and avoid such myopicity. We
further allow agents to make short-term predictions about each others' paths,
as a means to reason about each others' path intentions, thereby enhancing the
level of cooperation among agents at the whole system level. Our neural
structure relies on a Graph Transformer architecture to allow agents to
selectively combine these different sources of information and reason about
their inter-dependencies at different spatial scales. Our simulation
experiments demonstrate that ALPHA outperforms both globally-guided MAPF
solvers and communication-learning based ones, showcasing its potential towards
scalability in realistic deployments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE International Conference on Robotics and
  Automation (ICRA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multicriteria Optimization of Lower Limb Exoskeleton Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayat Ibrayev, Arman Ibrayeva, Ayaulym Rakhmatullina, Aizhan Ibrayeva, Bekzat Amanov, Nurbibi Imanbayeva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Typical leg exoskeletons employ open-loop kinematic chains with motors placed
directly on movable joints; while this design offers flexibility, it leads to
increased costs and heightened control complexity due to the high number of
degrees of freedom. The use of heavy servo-motors to handle torque in active
joints results in complex and bulky designs, as highlighted in existing
literature. In this study, we introduced a novel synthesis method with
analytical solutions provided for synthesizing lower-limb exoskeleton.
Additionally, we have incorporated multicriteria optimization by six designing
criteria. As a result, we offer several mechanisms, comprising only six links,
well-suited to the human anatomical structure, exhibit superior trajectory
accuracy, efficient force transmission, satisfactory step height, and having
internal transfer segment of the foot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hilbert Space Embedding-based Trajectory Optimization for Multi-Modal
  Uncertain Obstacle Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Basant Sharma, Aditya Sharma, K. Madhava Krishna, Arun Kumar Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe autonomous driving critically depends on how well the ego-vehicle can
predict the trajectories of neighboring vehicles. To this end, several
trajectory prediction algorithms have been presented in the existing
literature. Many of these approaches output a multi-modal distribution of
obstacle trajectories instead of a single deterministic prediction to account
for the underlying uncertainty. However, existing planners cannot handle the
multi-modality based on just sample-level information of the predictions. With
this motivation, this paper proposes a trajectory optimizer that can leverage
the distributional aspects of the prediction in a computationally tractable and
sample-efficient manner. Our optimizer can work with arbitrarily complex
distributions and thus can be used with output distribution represented as a
deep neural network. The core of our approach is built on embedding
distribution in Reproducing Kernel Hilbert Space (RKHS), which we leverage in
two ways. First, we propose an RKHS embedding approach to select probable
samples from the obstacle trajectory distribution. Second, we rephrase
chance-constrained optimization as distribution matching in RKHS and propose a
novel sampling-based optimizer for its solution. We validate our approach with
hand-crafted and neural network-based predictors trained on real-world datasets
and show improvement over the existing stochastic optimization approaches in
safety metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Time Step Frequency on the Realism of Robotic Manipulation
  Simulation for Objects of Different Scales <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Q. Ta, Holly Dinkel, Hameed Abdul-Rashid, Yangfei Dai, Jessica Myers, Tan Chen, Junyi Geng, Timothy Bretl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work evaluates the impact of time step frequency and component scale on
robotic manipulation simulation accuracy. Increasing the time step frequency
for small-scale objects is shown to improve simulation accuracy. This
simulation, demonstrating pre-assembly part picking for two object geometries,
serves as a starting point for discussing how to improve Sim2Real transfer in
robotic assembly processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 3 figures, Best Poster Finalist at the 2023 Robotics and AI
  in Future Factory Workshop at the IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS). Video presentation
  [https://www.youtube.com/watch?v=JOXrBpMmI0A]. Robotics and AI in Future
  Factory workshop [https://sites.google.com/view/robot-ai-future-factory/]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Slip Detection and Surface Prediction Through Bio-Inspired Tactile
  Feedback <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dexter R. Shepherd, Phil Husbands, Andy Philippides, Chris Johnson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High resolution tactile sensing has great potential in autonomous mobile
robotics, particularly for legged robots. One particular area where it has
significant promise is the traversal of challenging, varied terrain. Depending
on whether an environment is slippery, soft, hard or dry, a robot must adapt
its method of locomotion accordingly. Currently many multi-legged robots, such
as Boston Dynamic's Spot robot, have preset gaits for different surface types,
but struggle over terrains where the surface type changes frequently. Being
able to automatically detect changes within an environment would allow a robot
to autonomously adjust its method of locomotion to better suit conditions,
without requiring a human user to manually set the change in surface type. In
this paper we report on the first detailed investigation of the properties of a
particular bio-inspired tactile sensor, the TacTip, to test its suitability for
this kind of automatic detection of surface conditions. We explored different
processing techniques and a regression model, using a custom made rig for data
collection to determine how a robot could sense directional and general force
on the sensor in a variety of conditions. This allowed us to successfully
demonstrate how the sensor can be used to distinguish between soft, hard, dry
and (wet) slippery surfaces. We further explored a neural model to classify
specific surface textures. Pin movement (the movement of optical markers within
the sensor) was key to sensing this information, and all models relied on some
form of temporal information. Our final trained models could successfully
determine the direction the sensor is heading in, the amount of force acting on
it, and determine differences in the surface texture such as Lego vs smooth
hard surface, or concrete vs smooth hard surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages + references, under review for ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Active Measurement for Human Mesh Recovery in Close Proximity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takahiro Maeda, Keisuke Takeshita, Kazuhito Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For safe and sophisticated physical human-robot interactions (pHRI), a robot
needs to estimate the accurate body pose or mesh of the target person. However,
in these pHRI scenarios, the robot cannot fully observe the target person's
body with equipped cameras because the target person is usually close to the
robot. This leads to severe truncation and occlusions, and results in poor
accuracy of human pose estimation. For better accuracy of human pose estimation
or mesh recovery on this limited information from cameras, we propose an active
measurement and sensor fusion framework of the equipped cameras and other
sensors such as touch sensors and 2D LiDAR. These touch and LiDAR sensing are
obtained attendantly through pHRI without additional costs. These sensor
measurements are sparse but reliable and informative cues for human mesh
recovery. In our active measurement process, camera viewpoints and sensor
placements are optimized based on the uncertainty of the estimated pose, which
is closely related to the truncated or occluded areas. In our sensor fusion
process, we fuse the sensor measurements to the camera-based estimated pose by
minimizing the distance between the estimated mesh and measured positions. Our
method is agnostic to robot configurations. Experiments were conducted using
the Toyota Human Support Robot, which has a camera, 2D LiDAR, and a touch
sensor on the robot arm. Our proposed method demonstrated the superiority in
the human pose estimation accuracy on the quantitative comparison. Furthermore,
our proposed method reliably estimated the pose of the target person in
practical settings such as target people occluded by a blanket and standing aid
with the robot arm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Modal Sensor Fusion and Object Tracking for Autonomous Racing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Karle, Felix Fent, Sebastian Huch, Florian Sauerbeck, Markus Lienkamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable detection and tracking of surrounding objects are indispensable for
comprehensive motion prediction and planning of autonomous vehicles. Due to the
limitations of individual sensors, the fusion of multiple sensor modalities is
required to improve the overall detection capabilities. Additionally, robust
motion tracking is essential for reducing the effect of sensor noise and
improving state estimation accuracy. The reliability of the autonomous vehicle
software becomes even more relevant in complex, adversarial high-speed
scenarios at the vehicle handling limits in autonomous racing. In this paper,
we present a modular multi-modal sensor fusion and tracking method for
high-speed applications. The method is based on the Extended Kalman Filter
(EKF) and is capable of fusing heterogeneous detection inputs to track
surrounding objects consistently. A novel delay compensation approach enables
to reduce the influence of the perception software latency and to output an
updated object list. It is the first fusion and tracking method validated in
high-speed real-world scenarios at the Indy Autonomous Challenge 2021 and the
Autonomous Challenge at CES (AC@CES) 2022, proving its robustness and
computational efficiency on embedded systems. It does not require any labeled
data and achieves position tracking residuals below 0.1 m. The related code is
available as open-source software at https://github.com/TUMFTM/FusionTracking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Predictive Inferential Control of Neural State-Space Models for
  Autonomous Vehicle Motion Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iman Askari, Xumein Tu, Shen Zeng, Huazhen Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model predictive control (MPC) has proven useful in enabling safe and optimal
motion planning for autonomous vehicles. In this paper, we investigate how to
achieve MPC-based motion planning when a neural state-space model represents
the vehicle dynamics. As the neural state-space model will lead to highly
complex, nonlinear and nonconvex optimization landscapes, mainstream
gradient-based MPC methods will be computationally too heavy to be a viable
solution. In a departure, we propose the idea of model predictive inferential
control (MPIC), which seeks to infer the best control decisions from the
control objectives and constraints. Following the idea, we convert the MPC
problem for motion planning into a Bayesian state estimation problem. Then, we
develop a new particle filtering/smoothing approach to perform the estimation.
This approach is implemented as banks of unscented Kalman filters/smoothers and
offers high sampling efficiency, fast computation, and estimation accuracy. We
evaluate the MPIC approach through a simulation study of autonomous driving in
different scenarios, along with an exhaustive comparison with gradient-based
MPC. The results show that the MPIC approach has considerable computational
efficiency, regardless of complex neural network architectures, and shows the
capability to solve large-scale MPC problems for neural state-space models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Receive, Reason, and React: Drive as You Say with Large Language Models
  in Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Ziran Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fusion of human-centric design and artificial intelligence (AI)
capabilities has opened up new possibilities for next-generation autonomous
vehicles that go beyond transportation. These vehicles can dynamically interact
with passengers and adapt to their preferences. This paper proposes a novel
framework that leverages Large Language Models (LLMs) to enhance the
decision-making process in autonomous vehicles. By utilizing LLMs' linguistic
and contextual understanding abilities with specialized tools, we aim to
integrate the language and reasoning capabilities of LLMs into autonomous
vehicles. Our research includes experiments in HighwayEnv, a collection of
environments for autonomous driving and tactical decision-making tasks, to
explore LLMs' interpretation, interaction, and reasoning in various scenarios.
We also examine real-time personalization, demonstrating how LLMs can influence
driving behaviors based on verbal commands. Our empirical results highlight the
substantial advantages of utilizing chain-of-thought prompting, leading to
improved driving decisions, and showing the potential for LLMs to enhance
personalized driving experiences through ongoing verbal feedback. The proposed
framework aims to transform autonomous vehicle operations, offering
personalized support, transparent decision-making, and continuous learning to
enhance safety and effectiveness. We achieve user-centric, transparent, and
adaptive autonomous driving ecosystems supported by the integration of LLMs
into autonomous vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2309.10228</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning of Display Transfer Robots in Glass Flow Control
  Systems: A Physical Simulation-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hwajong Lee, Chan Kim, Seong-Woo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A flow control system is a critical concept for increasing the production
capacity of manufacturing systems. To solve the scheduling optimization problem
related to the flow control with the aim of improving productivity, existing
methods depend on a heuristic design by domain human experts. Therefore, the
methods require correction, monitoring, and verification by using real
equipment. As system designs increase in complexity, the monitoring time
increases, which decreases the probability of arriving at the optimal design.
As an alternative approach to the heuristic design of flow control systems, the
use of deep reinforcement learning to solve the scheduling optimization problem
has been considered. Although the existing research on reinforcement learning
has yielded excellent performance in some areas, the applicability of the
results to actual FAB such as display and semiconductor manufacturing processes
is not evident so far. To this end, we propose a method to implement a physical
simulation environment and devise a feasible flow control system design using a
transfer robot in display manufacturing through reinforcement learning. We
present a model and parameter setting to build a virtual environment for
different display transfer robots, and training methods of reinforcement
learning on the environment to obtain an optimal scheduling of glass flow
control systems. Its feasibility was verified by using different types of
robots used in the actual process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think, Act, and Ask: Open-World Interactive Personalized Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinpei Dai, Run Peng, Sikai Li, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-Shot Object Navigation (ZSON) enables agents to navigate towards
open-vocabulary objects in unknown environments. The existing works of ZSON
mainly focus on following individual instructions to find generic object
classes, neglecting the utilization of natural language interaction and the
complexities of identifying user-specific objects. To address these
limitations, we introduce Zero-shot Interactive Personalized Object Navigation
(ZIPON), where robots need to navigate to personalized goal objects while
engaging in conversations with users. To solve ZIPON, we propose a new
framework termed Open-woRld Interactive persOnalized Navigation (ORION), which
uses Large Language Models (LLMs) to make sequential decisions to manipulate
different modules for perception, navigation and communication. Experimental
results show that the performance of interactive agents that can leverage user
feedback exhibits significant improvement. However, obtaining a good balance
between task completion and the efficiency of navigation and interaction
remains challenging for all methods. We further provide more findings on the
impact of diverse user feedback forms on the agents' performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Video available at https://www.youtube.com/watch?v=QW6rMHVpxUY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David DeFazio, Eisuke Hirota, Shiqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seeing-eye robots are very useful tools for guiding visually impaired people,
potentially producing a huge societal impact given the low availability and
high cost of real guide dogs. Although a few seeing-eye robot systems have
already been demonstrated, none considered external tugs from humans, which
frequently occur in a real guide dog setting. In this paper, we simultaneously
train a locomotion controller that is robust to external tugging forces via
Reinforcement Learning (RL), and an external force estimator via supervised
learning. The controller ensures stable walking, and the force estimator
enables the robot to respond to the external forces from the human. These
forces are used to guide the robot to the global goal, which is unknown to the
robot, while the robot guides the human around nearby obstacles via a local
planner. Experimental results in simulation and on hardware show that our
controller is robust to external forces, and our seeing-eye system can
accurately detect force direction. We demonstrate our full seeing-eye robot
system on a real quadruped robot with a blindfolded human. The video can be
seen at our project page: https://bu-air-lab.github.io/guide_dog/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoRL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eclares: Energy-Aware Clarity-Driven Ergodic Search <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaleb Ben Naveed, Devansh Agrawal, Christopher Vermillion, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning informative trajectories while considering the spatial distribution
of the information over the environment, as well as constraints such as the
robot's limited battery capacity, makes the long-time horizon persistent
coverage problem complex. Ergodic search methods consider the spatial
distribution of environmental information while optimizing robot trajectories;
however, current methods lack the ability to construct the target information
spatial distribution for environments that vary stochastically across space and
time. Moreover, current coverage methods dealing with battery capacity
constraints either assume simple robot and battery models, or are
computationally expensive. To address these problems, we propose a framework
called Eclares, in which our contribution is two-fold. 1) First, we propose a
method to construct the target information spatial distribution for ergodic
trajectory optimization using clarity, an information measure bounded between
[0,1]. The clarity dynamics allows us to capture information decay due to lack
of measurements and to quantify the maximum attainable information in
stochastic spatiotemporal environments. 2) Second, instead of directly tracking
the ergodic trajectory, we introduce the energy-aware (eware) filter, which
iteratively validates the ergodic trajectory to ensure that the robot has
enough energy to return to the charging station when needed. The proposed eware
filter is applicable to nonlinear robot models and is computationally
lightweight. We demonstrate the working of the framework through a simulation
case study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Conference of Robotics and Automation
  (ICRA) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AirIMU: Learning Uncertainty Propagation for Inertial Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04874v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04874v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Qiu, Chen Wang, Xunfei Zhou, Youjie Xia, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty estimation for inertial odometry is the foundation to
achieve optimal fusion in multi-sensor systems, such as visual or LiDAR
inertial odometry. Prior studies often simplify the assumptions regarding the
uncertainty of inertial measurements, presuming fixed covariance parameters and
empirical IMU sensor models. However, the inherent physical limitations and
non-linear characteristics of sensors are difficult to capture. Moreover,
uncertainty may fluctuate based on sensor rates and motion modalities, leading
to variations across different IMUs. To address these challenges, we formulate
a learning-based method that not only encapsulate the non-linearities inherent
to IMUs but also ensure the accurate propagation of covariance in a data-driven
manner. We extend the PyPose library to enable differentiable batched IMU
integration with covariance propagation on manifolds, leading to significant
runtime speedup. To demonstrate our method's adaptability, we evaluate it on
several benchmarks as well as a large-scale helicopter dataset spanning over
262 kilometers. The drift rate of the inertial odometry on these datasets is
reduced by a factor of between 2.2 and 4 times. Our method lays the groundwork
for advanced developments in inertial odometry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-based NLOS Detection and Uncertainty Prediction of GNSS
  Observations with Transformer-Enhanced LSTM Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoming Zhang, Zhanxin Wang, Heike Vallery
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global navigation satellite systems (GNSS) play a vital role in transport
systems for accurate and consistent vehicle localization. However, GNSS
observations can be distorted due to multipath effects and non-line-of-sight
(NLOS) receptions in challenging environments such as urban canyons. In such
cases, traditional methods to classify and exclude faulty GNSS observations may
fail, leading to unreliable state estimation and unsafe system operations. This
work proposes a deep-learning-based method to detect NLOS receptions and
predict GNSS pseudorange errors by analyzing GNSS observations as a
spatio-temporal modeling problem. Compared to previous works, we construct a
transformer-like attention mechanism to enhance the long short-term memory
(LSTM) networks, improving model performance and generalization. For the
training and evaluation of the proposed network, we used labeled datasets from
the cities of Hong Kong and Aachen. We also introduce a dataset generation
process to label the GNSS observations using lidar maps. In experimental
studies, we compare the proposed network with a deep-learning-based model and
classical machine-learning models. Furthermore, we conduct ablation studies of
our network components and integrate the NLOS detection with data
out-of-distribution in a state estimator. As a result, our network presents
improved precision and recall ratios compared to other models. Additionally, we
show that the proposed method avoids trajectory divergence in real-world
vehicle localization by classifying and excluding NLOS observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the IEEE ITSC2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16292v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16292v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in autonomous driving have relied on data-driven
approaches, which are widely adopted but face challenges including dataset
bias, overfitting, and uninterpretability. Drawing inspiration from the
knowledge-driven nature of human driving, we explore the question of how to
instill similar capabilities into autonomous driving systems and summarize a
paradigm that integrates an interactive environment, a driver agent, as well as
a memory component to address this question. Leveraging large language models
with emergent abilities, we propose the DiLu framework, which combines a
Reasoning and a Reflection module to enable the system to perform
decision-making based on common-sense knowledge and evolve continuously.
Extensive experiments prove DiLu's capability to accumulate experience and
demonstrate a significant advantage in generalization ability over
reinforcement learning-based methods. Moreover, DiLu is able to directly
acquire experiences from real-world datasets which highlights its potential to
be deployed on practical autonomous driving systems. To the best of our
knowledge, we are the first to instill knowledge-driven capability into
autonomous driving systems from the perspective of how humans drive.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing LiDAR performance: Robust De-skewing Exclusively Relying on
  Range Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Salem, Emanuele Giacomini, Leonardo Brizi, Luca Di Giammarino, Giorgio Grisetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most commercially available Light Detection and Ranging (LiDAR)s measure the
distances along a 2D section of the environment by sequentially sampling the
free range along directions centered at the sensor's origin. When the sensor
moves during the acquisition, the measured ranges are affected by a phenomenon
known as "skewing", which appears as a distortion in the acquired scan. Skewing
potentially affects all systems that rely on LiDAR data, however, it could be
compensated if the position of the sensor were known each time a single range
is measured. Most methods to de-skew a LiDAR are based on external sensors such
as IMU or wheel odometry, to estimate these intermediate LiDAR positions. In
this paper, we present a method that relies exclusively on range measurements
to effectively estimate the robot velocities which are then used for
de-skewing. Our approach is suitable for low-frequency LiDAR where the skewing
is more evident. It can be seamlessly integrated into existing pipelines,
enhancing their performance at a negligible computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages , 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeSteps: Learning Safer Footstep Planning Policies for Legged Robots
  via Model-Based Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shafeef Omar, Lorenzo Amatucci, Victor Barasuol, Giulio Turrisi, Claudio Semini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a footstep planning policy for quadrupedal locomotion that is able
to directly take into consideration a-priori safety information in its
decisions. At its core, a learning process analyzes terrain patches,
classifying each landing location by its kinematic feasibility, shin collision,
and terrain roughness. This information is then encoded into a small vector
representation and passed as an additional state to the footstep planning
policy, which furthermore proposes only safe footstep location by applying a
masked variant of the Proximal Policy Optimization algorithm. The performance
of the proposed approach is shown by comparative simulations and experiments on
an electric quadruped robot walking in different rough terrain scenarios. We
show that violations of the above safety conditions are greatly reduced both
during training and the successive deployment of the policy, resulting in an
inherently safer footstep planner. Furthermore, we show how, as a byproduct,
fewer reward terms are needed to shape the behavior of the policy, which in
return is able to achieve both better final performances and sample efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 2023 IEEE-RAS International
  Conference on Humanoid Robots (Humanoids)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GP-net: Flexible Viewpoint Grasp Proposal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10404v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10404v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Konrad, John McDonald, Rudi Villing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Grasp Proposal Network (GP-net), a Convolutional Neural
Network model which can generate 6-DoF grasps from flexible viewpoints, e.g. as
experienced by mobile manipulators. To train GP-net, we synthetically generate
a dataset containing depth-images and ground-truth grasp information. In
real-world experiments, we use the EGAD evaluation benchmark to evaluate GP-net
against two commonly used algorithms, the Volumetric Grasping Network (VGN) and
the Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In
contrast to the state-of-the-art methods in robotic grasping, GP-net can be
used for grasping objects from flexible, unknown viewpoints without the need to
define the workspace and achieves a grasp success of 54.4% compared to 51.6%
for VGN and 44.2% for GPD. We provide a ROS package along with our code and
pre-trained models at https://aucoroboticsmu.github.io/GP-net/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICAR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> GRADE: Generating Realistic Animated <span class="highlight-title">Dynamic Environment</span>s for Robotics
  Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Bonetto, Chenghao Xu, <span class="highlight-author">Aamir Ahmad</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, computer vision tasks like target tracking and human pose
estimation have immensely benefited from synthetic data generation and novel
rendering techniques. On the other hand, methods in robotics, especially for
robot perception, have been slow to leverage these techniques. This is because
state-of-the-art simulation frameworks for robotics lack either complete
control, integration with the Robot Operating System (ROS), realistic physics
or photorealism. To solve this, we present a fully customizable framework for
generating realistic animated dynamic environments (GRADE) for robotics
research, focused primarily at robot perception. The framework can be used
either to generate ground truth data for robotic vision-related tasks and
offline processing, or to experiment with robots online in dynamic
environments. We build upon the Nvidia Isaac Sim to allow control of custom
robots. We provide methods to include assets, populate and control the
simulation, and process the data. Using autonomous robots in GRADE, we generate
video datasets of an indoor dynamic environment. First, we use it to
demonstrate the framework's visual realism by evaluating the sim-to-real gap
through experiments with YOLO and Mask R-CNN. Second, we benchmark dynamic SLAM
algorithms with this dataset. This not only shows that GRADE can significantly
improve training performance and generalization to real sequences, but also
highlights how current dynamic SLAM methods over-rely on known benchmarks,
failing to generalize. We also introduce a method to precisely repeat a
previously recorded experiment, while allowing changes in the surroundings of
the robot. Code and data are provided as open-source at
https://grade.is.tue.mpg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 10 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tightly-Coupled LiDAR-Visual <span class="highlight-title">SLAM</span> Based on Geometric Features for Mobile
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Cao, Ruiping Liu, Ze Wang, Kunyu Peng, Jiaming Zhang, Junwei Zheng, Zhifeng Teng, Kailun Yang, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to
provide autonomous navigation and task execution in complex and unknown
environments. However, it is hard to develop a dedicated algorithm for mobile
robots due to dynamic and challenging situations, such as poor lighting
conditions and motion blur. To tackle this issue, we propose a tightly-coupled
LiDAR-visual SLAM based on geometric features, which includes two sub-systems
(LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework
associates the depth and semantics of the multi-modal geometric features to
complement the visual line landmarks and to add direction optimization in
Bundle Adjustment (BA). This further constrains visual odometry. On the other
hand, the entire line segment detected by the visual subsystem overcomes the
limitation of the LiDAR subsystem, which can only perform the local calculation
for geometric features. It adjusts the direction of linear feature points and
filters out outliers, leading to a higher accurate odometry system. Finally, we
employ a module to detect the subsystem's operation, providing the LiDAR
subsystem's output as a complementary trajectory to our system while visual
subsystem tracking fails. The evaluation results on the public dataset M2DGR,
gathered from ground robots across various indoor and outdoor scenarios, show
that our system achieves more accurate and robust pose estimation compared to
current state-of-the-art multi-modal methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ROBIO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Simulate Tree-Branch Dynamics for Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayadeep Jacob, Tirthankar Bandyopadhyay, Jason Williams, Paulo Borges, Fabio Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to use a simulation driven inverse inference approach to model the
dynamics of tree branches under manipulation. Learning branch dynamics and
gaining the ability to manipulate deformable vegetation can help with
occlusion-prone tasks, such as fruit picking in dense foliage, as well as
moving overhanging vines and branches for navigation in dense vegetation. The
underlying deformable tree geometry is encapsulated as coarse spring
abstractions executed on parallel, non-differentiable simulators. The implicit
statistical model defined by the simulator, reference trajectories obtained by
actively probing the ground truth, and the Bayesian formalism, together guide
the spring parameter posterior density estimation. Our non-parametric inference
algorithm, based on Stein Variational Gradient Descent, incorporates
biologically motivated assumptions into the inference process as neural network
driven learnt joint priors; moreover, it leverages the finite difference scheme
for gradient approximations. Real and simulated experiments confirm that our
model can predict deformation trajectories, quantify the estimation
uncertainty, and it can perform better when base-lined against other inference
algorithms, particularly from the Monte Carlo family. The model displays strong
robustness properties in the presence of heteroscedastic sensor noise;
furthermore, it can generalise to unseen grasp locations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LF-VI<span class="highlight-title">SLAM</span>: A <span class="highlight-title">SLAM</span> Framework for Large Field-of-View Cameras with
  Negative Imaging Plane on Mobile Agents <span class="chip">IROS2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05167v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05167v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ze Wang, Kailun Yang, Hao Shi, Peng Li, Fei Gao, Jian Bai, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization And Mapping (SLAM) has become a crucial aspect in
the fields of autonomous driving and robotics. One crucial component of visual
SLAM is the Field-of-View (FoV) of the camera, as a larger FoV allows for a
wider range of surrounding elements and features to be perceived. However, when
the FoV of the camera reaches the negative half-plane, traditional methods for
representing image feature points using [u,v,1]^T become ineffective. While the
panoramic FoV is advantageous for loop closure, its benefits are not easily
realized under large-attitude-angle differences where loop-closure frames
cannot be easily matched by existing methods. As loop closure on wide-FoV
panoramic data further comes with a large number of outliers, traditional
outlier rejection methods are not directly applicable. To address these issues,
we propose LF-VISLAM, a Visual Inertial SLAM framework for cameras with
extremely Large FoV with loop closure. A three-dimensional vector with unit
length is introduced to effectively represent feature points even on the
negative half-plane. The attitude information of the SLAM system is leveraged
to guide the feature point detection of the loop closure. Additionally, a new
outlier rejection method based on the unit length representation is integrated
into the loop closure module. We collect the PALVIO dataset using a Panoramic
Annular Lens (PAL) system with an entire FoV of 360{\deg}x(40{\deg}~120{\deg})
and an Inertial Measurement Unit (IMU) for Visual Inertial Odometry (VIO) to
address the lack of panoramic SLAM datasets. Experiments on the established
PALVIO and public datasets show that the proposed LF-VISLAM outperforms
state-of-the-art SLAM methods. Our code will be open-sourced at
https://github.com/flysoaryun/LF-VISLAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Automation Science and Engineering
  (T-ASE). Extended version of IROS2022 paper arXiv:2202.12613. Code and
  dataset will be open-sourced at https://github.com/flysoaryun/LF-SLAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitation Learning from Observation with Automatic Discount Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07433v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07433v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Liu, Weijun Dong, Yingdong Hu, Chuan Wen, Zhao-Heng Yin, Chongjie Zhang, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans often acquire new skills through observation and imitation. For
robotic agents, learning from the plethora of unlabeled video demonstration
data available on the Internet necessitates imitating the expert without access
to its action, presenting a challenge known as Imitation Learning from
Observations (ILfO). A common approach to tackle ILfO problems is to convert
them into inverse reinforcement learning problems, utilizing a proxy reward
computed from the agent's and the expert's observations. Nonetheless, we
identify that tasks characterized by a progress dependency property pose
significant challenges for such approaches; in these tasks, the agent needs to
initially learn the expert's preceding behaviors before mastering the
subsequent ones. Our investigation reveals that the main cause is that the
reward signals assigned to later steps hinder the learning of initial
behaviors. To address this challenge, we present a novel ILfO framework that
enables the agent to master earlier behaviors before advancing to later ones.
We introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively
alters the discount factor in reinforcement learning during the training phase,
prioritizing earlier rewards initially and gradually engaging later rewards
only when the earlier behaviors have been mastered. Our experiments, conducted
on nine Meta-World tasks, demonstrate that our method significantly outperforms
state-of-the-art methods across all tasks, including those that are unsolvable
by them.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">134</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Generalized Dynamic Novel View Synthesis from Monocular Videos
  Possible Today? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoming Zhao, Alex Colburn, Fangchang Ma, Miguel Angel Bautista, Joshua M. Susskind, Alexander G. Schwing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rendering scenes observed in a monocular video from novel viewpoints is a
challenging problem. For static scenes the community has studied both
scene-specific optimization techniques, which optimize on every test scene, and
generalized techniques, which only run a deep net forward pass on a test scene.
In contrast, for dynamic scenes, scene-specific optimization techniques exist,
but, to our best knowledge, there is currently no generalized method for
dynamic novel view synthesis from a given monocular video. To answer whether
generalized dynamic novel view synthesis from monocular videos is possible
today, we establish an analysis framework based on existing techniques and work
toward the generalized approach. We find a pseudo-generalized process without
scene-specific appearance optimization is possible, but geometrically and
temporally consistent depth estimates are needed. Despite no scene-specific
appearance optimization, the pseudo-generalized approach improves upon some
scene-specific methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://xiaoming-zhao.github.io/projects/pgdvs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Octopus: Embodied Vision-Language Programmer from Environmental Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (VLMs) have achieved substantial progress in
multimodal perception and reasoning. Furthermore, when seamlessly integrated
into an embodied agent, it signifies a crucial stride towards the creation of
autonomous and context-aware systems capable of formulating plans and executing
commands with precision. In this paper, we introduce Octopus, a novel VLM
designed to proficiently decipher an agent's vision and textual task objectives
and to formulate intricate action sequences and generate executable code. Our
design allows the agent to adeptly handle a wide spectrum of tasks, ranging
from mundane daily chores in simulators to sophisticated interactions in
complex video games. Octopus is trained by leveraging GPT-4 to control an
explorative agent to generate training data, i.e., action blueprints and the
corresponding executable code, within our experimental environment called
OctoVerse. We also collect the feedback that allows the enhanced training
scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a
series of experiments, we illuminate Octopus's functionality and present
compelling results, and the proposed RLEF turns out to refine the agent's
decision-making. By open-sourcing our model architecture, simulator, and
dataset, we aspire to ignite further innovation and foster collaborative
applications within the broader embodied AI community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://choiszt.github.io/Octopus/, Codebase:
  https://github.com/dongyh20/Octopus</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic
  Scenes <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun Bao, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to tackle the challenge of dynamic view synthesis from
multi-view videos. The key observation is that while previous grid-based
methods offer consistent rendering, they fall short in capturing appearance
details of a complex dynamic scene, a domain where multi-view image-based
rendering methods demonstrate the opposite properties. To combine the best of
two worlds, we introduce Im4D, a hybrid scene representation that consists of a
grid-based geometry representation and a multi-view image-based appearance
representation. Specifically, the dynamic geometry is encoded as a 4D density
function composed of spatiotemporal feature planes and a small MLP network,
which globally models the scene structure and facilitates the rendering
consistency. We represent the scene appearance by the original multi-view
videos and a network that learns to predict the color of a 3D point from image
features, instead of memorizing detailed appearance totally with networks,
thereby naturally making the learning of networks easier. Our method is
evaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap,
NHR, DNA-Rendering and ENeRF-Outdoor datasets. The results show that Im4D
exhibits state-of-the-art performance in rendering quality and can be trained
efficiently, while realizing real-time rendering with a speed of 79.8 FPS for
512x512 images, on a single RTX 3090 GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2023; Project page: https://zju3dv.github.io/im4d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PonderV2: Pave the Way for 3D Foundataion Model with A Universal
  Pre-training Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha Zhang, Xianglong He, Tong He, Hengshuang Zhao, Chunhua Shen, Yu Qiao, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to numerous NLP and 2D computer vision foundational models, the
learning of a robust and highly generalized 3D foundational model poses
considerably greater challenges. This is primarily due to the inherent data
variability and the diversity of downstream tasks. In this paper, we introduce
a comprehensive 3D pre-training framework designed to facilitate the
acquisition of efficient 3D representations, thereby establishing a pathway to
3D foundational models. Motivated by the fact that informative 3D features
should be able to encode rich geometry and appearance cues that can be utilized
to render realistic images, we propose a novel universal paradigm to learn
point cloud representations by differentiable neural rendering, serving as a
bridge between 3D and 2D worlds. We train a point cloud encoder within a
devised volumetric neural renderer by comparing the rendered images with the
real images. Notably, our approach demonstrates the seamless integration of the
learned 3D encoder into diverse downstream tasks. These tasks encompass not
only high-level challenges such as 3D detection and segmentation but also
low-level objectives like 3D reconstruction and image synthesis, spanning both
indoor and outdoor scenarios. Besides, we also illustrate the capability of
pre-training a 2D backbone using the proposed universal methodology, surpassing
conventional pre-training methods by a large margin. For the first time,
\sexyname achieves state-of-the-art performance on 11 indoor and outdoor
benchmarks. The consistent improvements in various settings imply the
effectiveness of the proposed method. Code and models will be made available at
https://github.com/Pointcept/Pointcept.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2301.00157</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is ImageNet worth 1 video? Learning strong image encoders from 1 long
  unlabelled video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashanka Venkataramanan, Mamshad Nayeem Rizve, João Carreira, Yuki M. Asano, Yannis Avrithis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has unlocked the potential of scaling up pretraining
to billions of images, since annotation is unnecessary. But are we making the
best use of data? How more economical can we be? In this work, we attempt to
answer this question by making two contributions. First, we investigate
first-person videos and introduce a "Walking Tours" dataset. These videos are
high-resolution, hours-long, captured in a single uninterrupted take, depicting
a large number of objects and actions with natural scene transitions. They are
unlabeled and uncurated, thus realistic for self-supervision and comparable
with human learning.
  Second, we introduce a novel self-supervised image pretraining method
tailored for learning from continuous videos. Existing methods typically adapt
image-based pretraining approaches to incorporate more frames. Instead, we
advocate a "tracking to learn to recognize" approach. Our method called DoRA,
leads to attention maps that Discover and tRAck objects over time in an
end-to-end manner, using transformer cross-attention. We derive multiple views
from the tracks and use them in a classical self-supervised distillation loss.
Using our novel approach, a single Walking Tours video remarkably becomes a
strong competitor to ImageNet for several image and video downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Visual Decomposer: Long-Horizon Manipulation Made Easy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Zhang, Yunshuang Li, Osbert Bastani, Abhishek Gupta, Dinesh Jayaraman, Yecheng Jason Ma, Luca Weihs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world robotic tasks stretch over extended horizons and encompass
multiple stages. Learning long-horizon manipulation tasks, however, is a
long-standing challenge, and demands decomposing the overarching task into
several manageable subtasks to facilitate policy learning and generalization to
unseen tasks. Prior task decomposition methods require task-specific knowledge,
are computationally intensive, and cannot readily be applied to new tasks. To
address these shortcomings, we propose Universal Visual Decomposer (UVD), an
off-the-shelf task decomposition method for visual long horizon manipulation
using pre-trained visual representations designed for robotic control. At a
high level, UVD discovers subgoals by detecting phase shifts in the embedding
space of the pre-trained representation. Operating purely on visual
demonstrations without auxiliary information, UVD can effectively extract
visual subgoals embedded in the videos, while incurring zero additional
training cost on top of standard visuomotor policy training. Goal-conditioned
policies learned with UVD-discovered subgoals exhibit significantly improved
compositional generalization at test time to unseen tasks. Furthermore,
UVD-discovered subgoals can be used to construct goal-based reward shaping that
jump-starts temporally extended exploration for reinforcement learning. We
extensively evaluate UVD on both simulation and real-world tasks, and in all
cases, UVD substantially outperforms baselines across imitation and
reinforcement learning settings on in-domain and out-of-domain task sequences
alike, validating the clear advantage of automated visual task decomposition
within the simple, compact UVD framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniControl: Control Any Joint at Any Time for Human Motion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, Huaizu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach named OmniControl for incorporating flexible
spatial control signals into a text-conditioned human motion generation model
based on the diffusion process. Unlike previous methods that can only control
the pelvis trajectory, OmniControl can incorporate flexible spatial control
signals over different joints at different times with only one model.
Specifically, we propose analytic spatial guidance that ensures the generated
motion can tightly conform to the input control signals. At the same time,
realism guidance is introduced to refine all the joints to generate more
coherent motion. Both the spatial and realism guidance are essential and they
are highly complementary for balancing control accuracy and motion realism. By
combining them, OmniControl generates motions that are realistic, coherent, and
consistent with the spatial constraints. Experiments on HumanML3D and KIT-ML
datasets show that OmniControl not only achieves significant improvement over
state-of-the-art methods on pelvis control but also shows promising results
when incorporating the constraints over other joints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://neu-vi.github.io/omnicontrol/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperHuman: Hyper-Realistic Human Generation with Latent Structural
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, Sergey Tulyakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advances in large-scale text-to-image models, achieving
hyper-realistic human image generation remains a desirable yet unsolved task.
Existing models like Stable Diffusion and DALL-E 2 tend to generate human
images with incoherent parts or unnatural poses. To tackle these challenges,
our key insight is that human image is inherently structural over multiple
granularities, from the coarse-level body skeleton to fine-grained spatial
geometry. Therefore, capturing such correlations between the explicit
appearance and latent structure in one model is essential to generate coherent
and natural human images. To this end, we propose a unified framework,
HyperHuman, that generates in-the-wild human images of high realism and diverse
layouts. Specifically, 1) we first build a large-scale human-centric dataset,
named HumanVerse, which consists of 340M images with comprehensive annotations
like human pose, depth, and surface normal. 2) Next, we propose a Latent
Structural Diffusion Model that simultaneously denoises the depth and surface
normal along with the synthesized RGB image. Our model enforces the joint
learning of image appearance, spatial relationship, and geometry in a unified
network, where each branch in the model complements to each other with both
structural awareness and textural richness. 3) Finally, to further boost the
visual quality, we propose a Structure-Guided Refiner to compose the predicted
conditions for more detailed generation of higher resolution. Extensive
experiments demonstrate that our framework yields the state-of-the-art
performance, generating hyper-realistic human images under diverse scenarios.
Project Page: https://snap-research.github.io/HyperHuman/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://snap-research.github.io/HyperHuman/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Data-Type Understanding does not emerge from Scaling
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishaal Udandarao, Max F. Burg, Samuel Albanie, Matthias Bethge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in the development of vision-language models (VLMs) are
yielding remarkable success in recognizing visual semantic content, including
impressive instances of compositional image understanding. Here, we introduce
the novel task of \textit{Visual Data-Type Identification}, a basic perceptual
skill with implications for data curation (e.g., noisy data-removal from large
datasets, domain-specific retrieval) and autonomous vision (e.g.,
distinguishing changing weather conditions from camera lens staining). We
develop two datasets consisting of animal images altered across a diverse set
of 27 visual \textit{data-types}, spanning four broad categories. An extensive
zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a
nuanced performance landscape. While VLMs are reasonably good at identifying
certain stylistic \textit{data-types}, such as cartoons and sketches, they
struggle with simpler \textit{data-types} arising from basic manipulations like
image rotations or additive noise. Our findings reveal that (i) model scaling
alone yields marginal gains for contrastively-trained models like CLIP, and
(ii) there is a pronounced drop in performance for the largest
auto-regressively trained VLMs like OpenFlamingo. This finding points to a
blind spot in current frontier VLMs: they excel in recognizing semantic content
but fail to acquire an understanding of visual \textit{data-types} through
scaling. By analyzing the pre-training distributions of these models and
incorporating \textit{data-type} information into the captions during
fine-tuning, we achieve a significant enhancement in performance. By exploring
this previously uncharted task, we aim to set the stage for further advancing
VLMs to equip them with visual data-type understanding. Code and datasets are
released \href{https://github.com/bethgelab/DataTypeIdentification}{here}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Act from Actionless Videos through Dense Correspondences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, Joshua B. Tenenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present an approach to construct a video-based robot policy
capable of reliably executing diverse tasks across different robots and
environments from few video demonstrations without using any action
annotations. Our method leverages images as a task-agnostic representation,
encoding both the state and action information, and text as a general
representation for specifying robot goals. By synthesizing videos that
``hallucinate'' robot executing actions and in combination with dense
correspondences between frames, our approach can infer the closed-formed action
to execute to an environment without the need of any explicit action labels.
This unique capability allows us to train the policy solely based on RGB videos
and deploy learned policies to various robotic tasks. We demonstrate the
efficacy of our approach in learning policies on table-top manipulation and
navigation tasks. Additionally, we contribute an open-source framework for
efficient video modeling, enabling the training of high-fidelity policy models
with four GPUs within a single day.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://flow-diffusion.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic
  Image Design and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ``Idea to Image,'' a system that enables multimodal iterative
self-refinement with GPT-4V(ision) for automatic image design and generation.
Humans can quickly identify the characteristics of different text-to-image
(T2I) models via iterative explorations. This enables them to efficiently
convert their high-level generation ideas into effective T2I prompts that can
produce good images. We investigate if systems based on large multimodal models
(LMMs) can develop analogous multimodal self-refinement abilities that enable
exploring unknown models or environments via self-refining tries. Idea2Img
cyclically generates revised T2I prompts to synthesize draft images, and
provides directional feedback for prompt revision, both conditioned on its
memory of the probed T2I model's characteristics. The iterative self-refinement
brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img
can process input ideas with interleaved image-text sequences, follow ideas
with design instructions, and generate images of better semantic and visual
qualities. The user preference study validates the efficacy of multimodal
iterative self-refinement on automatic image design and generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page at https://idea2img.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image2PCI -- A Multitask Learning Framework for Estimating Pavement
  Condition Indices Directly from Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neema Jakisa Owor, Hang Du, Abdulateef Daud, Armstrong Aboah, Yaw Adu-Gyamfi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Pavement Condition Index (PCI) is a widely used metric for evaluating
pavement performance based on the type, extent and severity of distresses
detected on a pavement surface. In recent times, significant progress has been
made in utilizing deep-learning approaches to automate PCI estimation process.
However, the current approaches rely on at least two separate models to
estimate PCI values -- one model dedicated to determining the type and extent
and another for estimating their severity. This approach presents several
challenges, including complexities, high computational resource demands, and
maintenance burdens that necessitate careful consideration and resolution. To
overcome these challenges, the current study develops a unified multi-tasking
model that predicts the PCI directly from a top-down pavement image. The
proposed architecture is a multi-task model composed of one encoder for feature
extraction and four decoders to handle specific tasks: two detection heads, one
segmentation head and one PCI estimation head. By multitasking, we are able to
extract features from the detection and segmentation heads for automatically
estimating the PCI directly from the images. The model performs very well on
our benchmarked and open pavement distress dataset that is annotated for
multitask learning (the first of its kind). To our best knowledge, this is the
first work that can estimate PCI directly from an image at real time speeds
while maintaining excellent accuracy on all related tasks for crack detection
and segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XAI Benchmark for Visual Explanation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Zhang, Siyi Gu, James Song, Bo Pan, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of deep learning algorithms has led to significant advancements in
computer vision tasks, but their "black box" nature has raised concerns
regarding interpretability. Explainable AI (XAI) has emerged as a critical area
of research aiming to open this "black box", and shed light on the
decision-making process of AI models. Visual explanations, as a subset of
Explainable Artificial Intelligence (XAI), provide intuitive insights into the
decision-making processes of AI models handling visual data by highlighting
influential areas in an input image. Despite extensive research conducted on
visual explanations, most evaluations are model-centered since the availability
of corresponding real-world datasets with ground truth explanations is scarce
in the context of image data. To bridge this gap, we introduce an XAI Benchmark
comprising a dataset collection from diverse topics that provide both class
labels and corresponding explanation annotations for images. We have processed
data from diverse domains to align with our unified visual explanation
framework. We introduce a comprehensive Visual Explanation pipeline, which
integrates data loading, preprocessing, experimental setup, and model
evaluation processes. This structure enables researchers to conduct fair
comparisons of various visual explanation techniques. In addition, we provide a
comprehensive review of over 10 evaluation methods for visual explanation to
assist researchers in effectively utilizing our dataset collection. To further
assess the performance of existing visual explanation methods, we conduct
experiments on selected datasets using various model-centered and ground
truth-centered evaluation metrics. We envision this benchmark could facilitate
the advancement of visual explanation models. The XAI dataset collection and
easy-to-use code for evaluation are publicly accessible at
https://xaidataset.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Animating Street View <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyi Shan, Brian Curless, Ira Kemelmacher-Shlizerman, Steve Seitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a system that automatically brings street view imagery to life by
populating it with naturally behaving, animated pedestrians and vehicles. Our
approach is to remove existing people and vehicles from the input image, insert
moving objects with proper scale, angle, motion, and appearance, plan paths and
traffic behavior, as well as render the scene with plausible occlusion and
shadowing effects. The system achieves these by reconstructing the still image
street scene, simulating crowd behavior, and rendering with consistent
lighting, visibility, occlusions, and shadows. We demonstrate results on a
diverse range of street scenes including regular still images and panoramas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2023 Conference Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniPose: Detecting Any Keypoints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Yang, Ailing Zeng, Ruimao Zhang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a unified framework called UniPose to detect keypoints of
any articulated (e.g., human and animal), rigid, and soft objects via visual or
textual prompts for fine-grained vision understanding and manipulation.
Keypoint is a structure-aware, pixel-level, and compact representation of any
object, especially articulated objects. Existing fine-grained promptable tasks
mainly focus on object instance detection and segmentation but often fail to
identify fine-grained granularity and structured information of image and
instance, such as eyes, leg, paw, etc. Meanwhile, prompt-based keypoint
detection is still under-explored. To bridge the gap, we make the first attempt
to develop an end-to-end prompt-based keypoint detection framework called
UniPose to detect keypoints of any objects. As keypoint detection tasks are
unified in this framework, we can leverage 13 keypoint detection datasets with
338 keypoints across 1,237 categories over 400K instances to train a generic
keypoint detection model. UniPose can effectively align text-to-keypoint and
image-to-keypoint due to the mutual enhancement of textual and visual prompts
based on the cross-modality contrastive learning optimization objectives. Our
experimental results show that UniPose has strong fine-grained localization and
generalization abilities across image styles, categories, and poses. Based on
UniPose as a generalist keypoint detector, we hope it could serve fine-grained
visual perception, understanding, and generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with
  Point Cloud Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, the generation of 3D assets from text prompts has shown
impressive results. Both 2D and 3D diffusion models can generate decent 3D
objects based on prompts. 3D diffusion models have good 3D consistency, but
their quality and generalization are limited as trainable 3D data is expensive
and hard to obtain. 2D diffusion models enjoy strong abilities of
generalization and fine generation, but the 3D consistency is hard to
guarantee. This paper attempts to bridge the power from the two types of
diffusion models via the recent explicit and efficient 3D Gaussian splatting
representation. A fast 3D generation framework, named as \name, is proposed,
where the 3D diffusion model provides point cloud priors for initialization and
the 2D diffusion model enriches the geometry and appearance. Operations of
noisy point growing and color perturbation are introduced to enhance the
initialized Gaussians. Our \name can generate a high-quality 3D instance within
25 minutes on one GPU, much faster than previous methods, while the generated
instances can be directly rendered in real time. Demos and code are available
at https://taoranyi.com/gaussiandreamer/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. Project page: https://taoranyi.com/gaussiandreamer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representing and rendering dynamic scenes has been an important but
challenging task. Especially, to accurately model complex motions, high
efficiency is usually hard to maintain. We introduce the 4D Gaussian Splatting
(4D-GS) to achieve real-time dynamic scene rendering while also enjoying high
training and storage efficiency. An efficient deformation field is constructed
to model both Gaussian motions and shape deformations. Different adjacent
Gaussians are connected via a HexPlane to produce more accurate position and
shape deformations. Our 4D-GS method achieves real-time rendering under high
resolutions, 70 FPS at a 800$\times$800 resolution on an RTX 3090 GPU, while
maintaining comparable or higher quality than previous state-of-the-art
methods. More demos and code are available at
https://guanjunwu.github.io/4dgs/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. Project page: https://guanjunwu.github.io/4dgs/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Learning of Object-Centric Embeddings for Cell Instance
  Segmentation in Microscopy Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steffen Wolf, Manan Lalit, Henry Westmacott, Katie McDole, Jan Funke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of objects in microscopy images is required for many biomedical
applications. We introduce object-centric embeddings (OCEs), which embed image
patches such that the spatial offsets between patches cropped from the same
object are preserved. Those learnt embeddings can be used to delineate
individual objects and thus obtain instance segmentations. Here, we show
theoretically that, under assumptions commonly found in microscopy images, OCEs
can be learnt through a self-supervised task that predicts the spatial offset
between image patches. Together, this forms an unsupervised cell instance
segmentation method which we evaluate on nine diverse large-scale microscopy
datasets. Segmentations obtained with our method lead to substantially improved
results, compared to state-of-the-art baselines on six out of nine datasets,
and perform on par on the remaining three datasets. If ground-truth annotations
are available, our method serves as an excellent starting point for supervised
training, reducing the required amount of ground-truth needed by one order of
magnitude, thus substantially increasing the practical applicability of our
method. Source code is available at https://github.com/funkelab/cellulus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Edit Multimodal Large Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on editing Multimodal Large Language Models (MLLMs).
Compared to editing single-modal LLMs, multimodal model editing is more
challenging, which demands a higher level of scrutiny and careful consideration
in the editing process. To facilitate research in this area, we construct a new
benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite
of innovative metrics for evaluation. We conduct comprehensive experiments
involving various model editing baselines and analyze the impact of editing
different components for multimodal LLMs. Empirically, we notice that previous
baselines can implement editing multimodal LLMs to some extent, but the effect
is still barely satisfactory, indicating the potential difficulty of this task.
We hope that our work can provide the NLP community with insights\footnote{Code
and dataset are available in https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MotionDirector: Motion Customization of Text-to-Video Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained diffusion models have exhibited remarkable
capabilities in diverse video generations. Given a set of video clips of the
same motion concept, the task of Motion Customization is to adapt existing
text-to-video diffusion models to generate videos with this motion. For
example, generating a video with a car moving in a prescribed manner under
specific camera movements to make a movie, or a video illustrating how a bear
would lift weights to inspire creators. Adaptation methods have been developed
for customizing appearance like subject or style, yet unexplored for motion. It
is straightforward to extend mainstream adaption methods for motion
customization, including full model tuning, parameter-efficient tuning of
additional layers, and Low-Rank Adaptions (LoRAs). However, the motion concept
learned by these methods is often coupled with the limited appearances in the
training videos, making it difficult to generalize the customized motion to
other appearances. To overcome this challenge, we propose MotionDirector, with
a dual-path LoRAs architecture to decouple the learning of appearance and
motion. Further, we design a novel appearance-debiased temporal loss to
mitigate the influence of appearance on the temporal training objective.
Experimental results show the proposed method can generate videos of diverse
appearances for the customized motions. Our method also supports various
downstream applications, such as the mixing of different videos with their
appearance and motion respectively, and animating a single image with
customized motions. Our code and model weights will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://showlab.github.io/MotionDirector/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proving the Potential of Skeleton Based Action Recognition to Automate
  the Analysis of Manual Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlin Berger, Frederik Cloppenburg, Jens Eufinger, Thomas Gries
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In manufacturing sectors such as textiles and electronics, manual processes
are a fundamental part of production. The analysis and monitoring of the
processes is necessary for efficient production design. Traditional methods for
analyzing manual processes are complex, expensive, and inflexible. Compared to
established approaches such as Methods-Time-Measurement (MTM), machine learning
(ML) methods promise: Higher flexibility, self-sufficient & permanent use,
lower costs. In this work, based on a video stream, the current motion class in
a manual assembly process is detected. With information on the current motion,
Key-Performance-Indicators (KPIs) can be derived easily. A skeleton-based
action recognition approach is taken, as this field recently shows major
success in machine vision tasks. For skeleton-based action recognition in
manual assembly, no sufficient pre-work could be found. Therefore, a ML
pipeline is developed, to enable extensive research on different (pre-)
processing methods and neural nets. Suitable well generalizing approaches are
found, proving the potential of ML to enhance analyzation of manual processes.
Models detect the current motion, performed by an operator in manual assembly,
but the results can be transferred to all kinds of manual processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures. Find peer-reviewed version in Proceedings of
  IntelliSys 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debias the Training of Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Yu, Li Shen, Jie Huang, Man Zhou, Hongsheng Li, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated compelling generation quality by
optimizing the variational lower bound through a simple denoising score
matching loss. In this paper, we provide theoretical evidence that the
prevailing practice of using a constant loss weight strategy in diffusion
models leads to biased estimation during the training phase. Simply optimizing
the denoising network to predict Gaussian noise with constant weighting may
hinder precise estimations of original images. To address the issue, we propose
an elegant and effective weighting strategy grounded in the theoretically
unbiased principle. Moreover, we conduct a comprehensive and systematic
exploration to dissect the inherent bias problem deriving from constant
weighting loss from the perspectives of its existence, impact and reasons.
These analyses are expected to advance our understanding and demystify the
inner workings of diffusion models. Through empirical evaluation, we
demonstrate that our proposed debiased estimation method significantly enhances
sample quality without the reliance on complex techniques, and exhibits
improved efficiency compared to the baseline method both in training and
sampling processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>University of Science and Technology of China, Alibaba Group, The
  Chinese University of Hong Kong</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing of Soil Erosion Risk Through Geoinformation Sciences and
  Remote Sensing -- A Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lachezar Filchev, Vasil Kolev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During past decades a marked manifestation of widespread erosion phenomena
was studied worldwide. Global conservation community has launched campaigns at
local, regional and continental level in developing countries for preservation
of soil resources in order not only to stop or mitigate human impact on nature
but also to improve life in rural areas introducing new approaches for soil
cultivation. After the adoption of Sustainable Development Goals of UNs and
launching several world initiatives such as the Land Degradation Neutrality
(LDN) the world came to realize the very importance of the soil resources on
which the biosphere relies for its existence. The main goal of the chapter is
to review different types and structures erosion models as well as their
applications. Several methods using spatial analysis capabilities of geographic
information systems (GIS) are in operation for soil erosion risk assessment,
such as Universal Soil Loss Equation (USLE), Revised Universal Soil Loss
Equation (RUSLE) in operation worldwide and in the USA and MESALES model. These
and more models are being discussed in the present work alongside more
experimental models and methods for assessing soil erosion risk such as
Artificial Intelligence (AI), Machine and Deep Learning, etc. At the end of
this work, a prospectus for the future development of soil erosion risk
assessment is drawn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Chapter 21 (pages 54)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Data Augmentation for Rotational Invariance in Convolutional
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Facundo Manuel Quiroga, Franco Ronchetti, Laura Lanzarini, Aurelio Fernandez-Bariviera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNN) offer state of the art performance in
various computer vision tasks. Many of those tasks require different subtypes
of affine invariances (scale, rotational, translational) to image
transformations. Convolutional layers are translation equivariant by design,
but in their basic form lack invariances. In this work we investigate how best
to include rotational invariance in a CNN for image classification. Our
experiments show that networks trained with data augmentation alone can
classify rotated images nearly as well as in the normal unrotated case; this
increase in representational power comes only at the cost of training time. We
also compare data augmentation versus two modified CNN models for achieving
rotational invariance or equivariance, Spatial Transformer Networks and Group
Equivariant CNNs, finding no significant accuracy increase with these
specialized methods. In the case of data augmented networks, we also analyze
which layers help the network to encode the rotational invariance, which is
important for understanding its limitations and how to best retrain a network
with data augmentation to achieve invariance to rotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "SegLoc": Study on Novel Visual Self-supervised Learning Scheme (Segment
  Localization) Tailored for Dense Prediction Tasks of Security Inspection
  X-ray Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shervin Halat, Mohammad Rahmati, Ehsan Nazerfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lately, remarkable advancements of artificial intelligence have been
attributed to the integration of self-supervised learning scheme. Despite
impressive achievements within NLP, yet SSL in computer vision has not been
able to stay on track comparatively. Recently, integration of contrastive
learning on top of existing SSL models has established considerable progress in
computer vision through which visual SSL models have outperformed their
supervised counterparts. Nevertheless, most of these improvements were limited
to classification tasks, and also, few works have been dedicated to evaluation
of SSL models in real-world scenarios of computer vision, while the majority of
works are centered around datasets containing class-wise portrait images, most
notably, ImageNet. Consequently, in this work, we have considered dense
prediction task of semantic segmentation in security inspection x-ray images to
evaluate our proposed model Segmentation Localization. Based upon the model
Instance Localization, our model SegLoc has managed to address one of the most
challenging downsides of contrastive learning, i.e., false negative pairs of
query embeddings. In order to do so, in contrast to baseline model InsLoc, our
pretraining dataset is synthesized by cropping, transforming, then pasting
already labeled segments from an available labeled dataset, foregrounds, onto
instances of an unlabeled dataset, backgrounds. In our case, PIDray and SIXray
datasets are considered as labeled and unlabeled datasets, respectively.
Moreover, we fully harness labels by avoiding false negative pairs through
implementing the idea, one queue per class, in MoCo-v2 whereby negative pairs
corresponding to each query are extracted from its corresponding queue within
the memory bank. Our approach has outperformed random initialization by 3% to
6%, while having underperformed supervised initialization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Attention-Prompted Prediction and Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Zhang, Siyi Gu, Bo Pan, Guangji Bai, Xiaofeng Yang, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanation(attention)-guided learning is a method that enhances a model's
predictive power by incorporating human understanding during the training
phase. While attention-guided learning has shown promising results, it often
involves time-consuming and computationally expensive model retraining. To
address this issue, we introduce the attention-prompted prediction technique,
which enables direct prediction guided by the attention prompt without the need
for model retraining. However, this approach presents several challenges,
including: 1) How to incorporate the visual attention prompt into the model's
decision-making process and leverage it for future predictions even in the
absence of a prompt? and 2) How to handle the incomplete information from the
visual attention prompt? To tackle these challenges, we propose a novel
framework called Visual Attention-Prompted Prediction and Learning, which
seamlessly integrates visual attention prompts into the model's decision-making
process and adapts to images both with and without attention prompts for
prediction. To address the incomplete information of the visual attention
prompt, we introduce a perturbation-based attention map modification method.
Additionally, we propose an optimization-based mask aggregation method with a
new weight learning function for adaptive perturbed annotation aggregation in
the attention map modification process. Our overall framework is designed to
learn in an attention-prompt guided multi-task manner to enhance future
predictions even for samples without attention prompts and trained in an
alternating manner for better convergence. Extensive experiments conducted on
two datasets demonstrate the effectiveness of our proposed framework in
enhancing predictions for samples, both with and without provided prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Design and Development of an ArUco Markers-Based Quantitative
  Surface Tactile Sensor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ozdemir Can Kara, Charles Everson, Farshid Alambeigi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, with the goal of quantifying the qualitative image outputs of
a Vision-based Tactile Sensor (VTS), we present the design, fabrication, and
characterization of a novel Quantitative Surface Tactile Sensor (called QS-TS).
QS-TS directly estimates the sensor's gel layer deformation in real-time
enabling safe and autonomous tactile manipulation and servoing of delicate
objects using robotic manipulators. The core of the proposed sensor is the
utilization of miniature 1.5 mm x 1.5 mm synthetic square markers with inner
binary patterns and a broad black border, called ArUco Markers. Each ArUco
marker can provide real-time camera pose estimation that, in our design, is
used as a quantitative measure for obtaining deformation of the QS-TS gel
layer. Moreover, thanks to the use of ArUco markers, we propose a unique
fabrication procedure that mitigates various challenges associated with the
fabrication of the existing marker-based VTSs and offers an intuitive and
less-arduous method for the construction of the VTS. Remarkably, the proposed
fabrication facilitates the integration and adherence of markers with the gel
layer to robustly and reliably obtain a quantitative measure of deformation in
real-time regardless of the orientation of ArUco Markers. The performance and
efficacy of the proposed QS-TS in estimating the deformation of the sensor's
gel layer were experimentally evaluated and verified. Results demonstrate the
phenomenal performance of the QS-TS in estimating the deformation of the gel
layer with a relative error of <5%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyp-UML: Hyperbolic Image Retrieval with Uncertainty-aware Metric
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyang Yan, Zongxuan Liu, Lin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning plays a critical role in training image retrieval and
classification. It is also a key algorithm in representation learning, e.g.,
for feature learning and its alignment in metric space. Hyperbolic embedding
has been recently developed, compared to the conventional Euclidean embedding
in most of the previously developed models, and can be more effective in
representing the hierarchical data structure. Second, uncertainty
estimation/measurement is a long-lasting challenge in artificial intelligence.
Successful uncertainty estimation can improve a machine learning model's
performance, robustness, and security. In Hyperbolic space, uncertainty
measurement is at least with equivalent, if not more, critical importance. In
this paper, we develop a Hyperbolic image embedding with uncertainty-aware
metric learning for image retrieval. We call our method Hyp-UML: Hyperbolic
Uncertainty-aware Metric Learning. Our contribution are threefold: we propose
an image embedding algorithm based on Hyperbolic space, with their
corresponding uncertainty value; we propose two types of uncertainty-aware
metric learning, for the popular Contrastive learning and conventional
margin-based metric learning, respectively. We perform extensive experimental
validations to prove that the proposed algorithm can achieve state-of-the-art
results among related methods. The comprehensive ablation study validates the
effectiveness of each component of the proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeanAP-Guided Reinforced Active Learning for Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Liang, Xingyu Zeng, Rui Zhao, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning presents a promising avenue for training high-performance
models with minimal labeled data, achieved by judiciously selecting the most
informative instances to label and incorporating them into the task learner.
Despite notable advancements in active learning for image recognition, metrics
devised or learned to gauge the information gain of data, crucial for query
strategy design, do not consistently align with task model performance metrics,
such as Mean Average Precision (MeanAP) in object detection tasks. This paper
introduces MeanAP-Guided Reinforced Active Learning for Object Detection
(MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task
model to devise a sampling strategy employing a reinforcement learning-based
sampling agent. Built upon LSTM architecture, the agent efficiently explores
and selects subsequent training instances, and optimizes the process through
policy gradient with MeanAP serving as reward. Recognizing the time-intensive
nature of MeanAP computation at each step, we propose fast look-up tables to
expedite agent training. We assess MAGRAL's efficacy across popular benchmarks,
PASCAL VOC and MS COCO, utilizing different backbone architectures. Empirical
findings substantiate MAGRAL's superiority over recent state-of-the-art
methods, showcasing substantial performance gains. MAGRAL establishes a robust
baseline for reinforced active object detection, signifying its potential in
advancing the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoVP: An Automated Visual Prompting Framework and Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsi-Ai Tsao, Lei Hsiung, Pin-Yu Chen, Sijia Liu, Tsung-Yi Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach
to adapting pre-trained vision models to solve various downstream
image-classification tasks. However, there has hitherto been little systematic
study of the design space of VP and no clear benchmark for evaluating its
performance. To bridge this gap, we propose AutoVP, an end-to-end expandable
framework for automating VP design choices, along with 12 downstream
image-classification tasks that can serve as a holistic VP-performance
benchmark. Our design space covers 1) the joint optimization of the prompts; 2)
the selection of pre-trained models, including image classifiers and text-image
encoders; and 3) model output mapping strategies, including nonparametric and
trainable label mapping. Our extensive experimental results show that AutoVP
outperforms the best-known current VP methods by a substantial margin, having
up to 6.7% improvement in accuracy; and attains a maximum performance increase
of 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold
contribution: serving both as an efficient tool for hyperparameter tuning on VP
design choices, and as a comprehensive benchmark that can reasonably be
expected to accelerate VP's development. The source code is available at
https://github.com/IBM/AutoVP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. The code is available at https://github.com/IBM/AutoVP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Worst-Case Morphs using Wasserstein ALI and Improved MIPGAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Una M. Kelly, Meike Nauta, Lu Liu, Luuk J. Spreeuwers, Raymond N. J. Veldhuis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A lot of progress has been made in the last years on using Generative
Adversarial Networks (GAN) to create realistic images. However, to be able
reconstruct images or to generate images using real data as input, an Encoder
is needed that reverses the mapping from the GAN's latent space to image space.
This means that three networks are needed: an Encoder, a Decoder (called
Generator in a normal GAN) and a Discriminator. These three networks can be
trained from scratch simultaneously (Adversarially Learned Inference), or
alternatively an Encoder network can be trained that maps images into the
latent space of a \textit{pretrained} GAN model (Inverse GAN). In the latter
case, the networks are trained consecutively, so the Encoder has to make do
with whatever model the Decoder learned during GAN training. Training three
networks simultaneously is more unstable and therefore more challenging, but it
is possible that the Encoder and Decoder benefit from interacting with each
other during training. We compare the two different approaches and discuss
whether it is worth the extra effort to train all three networks
simultaneously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniPAD: A Universal Pre-training Paradigm for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honghui Yang, Sha Zhang, Di Huang, Xiaoyang Wu, Haoyi Zhu, Tong He, Shixiang Tang, Hengshuang Zhao, Qibo Qiu, Binbin Lin, Xiaofei He, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of autonomous driving, the significance of effective feature
learning is widely acknowledged. While conventional 3D self-supervised
pre-training methods have shown widespread success, most methods follow the
ideas originally designed for 2D images. In this paper, we present UniPAD, a
novel self-supervised learning paradigm applying 3D volumetric differentiable
rendering. UniPAD implicitly encodes 3D space, facilitating the reconstruction
of continuous 3D shape structures and the intricate appearance characteristics
of their 2D projections. The flexibility of our method enables seamless
integration into both 2D and 3D frameworks, enabling a more holistic
comprehension of the scenes. We manifest the feasibility and effectiveness of
UniPAD by conducting extensive experiments on various downstream 3D tasks. Our
method significantly improves lidar-, camera-, and lidar-camera-based baseline
by 9.1, 7.7, and 6.9 NDS, respectively. Notably, our pre-training pipeline
achieves 73.2 NDS for 3D object detection and 79.4 mIoU for 3D semantic
segmentation on the nuScenes validation set, achieving state-of-the-art results
in comparison with previous methods. The code will be available at
https://github.com/Nightmare-n/UniPAD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping Memes to Words for Multimodal Hateful Meme Classification <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Burbi, Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, Alberto Del Bimbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal image-text memes are prevalent on the internet, serving as a
unique form of communication that combines visual and textual elements to
convey humor, ideas, or emotions. However, some memes take a malicious turn,
promoting hateful content and perpetuating discrimination. Detecting hateful
memes within this multimodal context is a challenging task that requires
understanding the intertwined meaning of text and images. In this work, we
address this issue by proposing a novel approach named ISSUES for multimodal
hateful meme classification. ISSUES leverages a pre-trained CLIP
vision-language model and the textual inversion technique to effectively
capture the multimodal semantic content of the memes. The experiments show that
our method achieves state-of-the-art results on the Hateful Memes Challenge and
HarMeme datasets. The code and the pre-trained models are publicly available at
https://github.com/miccunifi/ISSUES.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV2023 CLVL Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCU: A Task-centric Framework for Open-ended Agent Evaluation in
  Minecraft 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Lin, Zihao Wang, Jianzhu Ma, Yitao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To pursue the goal of creating an open-ended agent in Minecraft, an
open-ended game environment with unlimited possibilities, this paper introduces
a task-centric framework named MCU for Minecraft agent evaluation. The MCU
framework leverages the concept of atom tasks as fundamental building blocks,
enabling the generation of diverse or even arbitrary tasks. Within the MCU
framework, each task is measured with six distinct difficulty scores (time
consumption, operational effort, planning complexity, intricacy, creativity,
novelty). These scores offer a multi-dimensional assessment of a task from
different angles, and thus can reveal an agent's capability on specific facets.
The difficulty scores also serve as the feature of each task, which creates a
meaningful task space and unveils the relationship between tasks. For efficient
evaluation of Minecraft agents employing the MCU framework, we maintain a
unified benchmark, namely SkillForge, which comprises representative tasks with
diverse categories and difficulty distribution. We also provide convenient
filters for users to select tasks to assess specific capabilities of agents. We
show that MCU has the high expressivity to cover all tasks used in recent
literature on Minecraft agent, and underscores the need for advancements in
areas such as creativity, precise control, and out-of-distribution
generalization under the goal of open-ended Minecraft agent development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Generic Software Framework for Distributed Topological Analysis
  Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eve Le Guillou, Michael Will, Pierre Guillou, Jonas Lukasczyk, Pierre Fortin, Christoph Garth, Julien Tierny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This system paper presents a software framework for the support of
topological analysis pipelines in a distributed-memory model. While several
recent papers introduced topology-based approaches for distributed-memory
environments, these were reporting experiments obtained with tailored,
mono-algorithm implementations. In contrast, we describe in this paper a
general-purpose, generic framework for topological analysis pipelines, i.e. a
sequence of topological algorithms interacting together, possibly on distinct
numbers of processes. Specifically, we instantiated our framework with the MPI
model, within the Topology ToolKit (TTK). While developing this framework, we
faced several algorithmic and software engineering challenges, which we
document in this paper. We provide a taxonomy for the distributed-memory
topological algorithms supported by TTK, depending on their communication needs
and provide examples of hybrid MPI+thread parallelizations. Detailed
performance analyses show that parallel efficiencies range from $20\%$ to
$80\%$ (depending on the algorithms), and that the MPI-specific preconditioning
introduced by our framework induces a negligible computation time overhead. We
illustrate the new distributed-memory capabilities of TTK with an example of
advanced analysis pipeline, combining multiple algorithms, run on the largest
publicly available dataset we have found (120 billion vertices) on a standard
cluster with 64 nodes (for a total of 1,536 cores). Finally, we provide a
roadmap for the completion of TTK's MPI extension, along with generic
recommendations for each algorithm communication category.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Neural BRDF with Spherically Distributed Primitives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yishun Dou, Zhong Zheng, Qiaoqiao Jin, Bingbing Ni, Yugang Chen, Junxiang Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel compact and efficient neural BRDF offering highly
versatile material representation, yet with very-light memory and neural
computation consumption towards achieving real-time rendering. The results in
Figure 1, rendered at full HD resolution on a current desktop machine, show
that our system achieves real-time rendering with a wide variety of
appearances, which is approached by the following two designs. On the one hand,
noting that bidirectional reflectance is distributed in a very sparse
high-dimensional subspace, we propose to project the BRDF into two
low-dimensional components, i.e., two hemisphere feature-grids for incoming and
outgoing directions, respectively. On the other hand, learnable neural
reflectance primitives are distributed on our highly-tailored spherical surface
grid, which offer informative features for each component and alleviate the
conventional heavy feature learning network to a much smaller one, leading to
very fast evaluation. These primitives are centrally stored in a codebook and
can be shared across multiple grids and even across materials, based on the
low-cost indices stored in material-specific spherical surface grids. Our
neural BRDF, which is agnostic to the material, provides a unified framework
that can represent a variety of materials in consistent manner. Comprehensive
experimental results on measured BRDF compression, Monte Carlo simulated BRDF
acceleration, and extension to spatially varying effect demonstrate the
superior quality and generalizability achieved by the proposed scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NSM4D: Neural Scene Model Based Online 4D Point Cloud Sequence
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Dong, Zhuoyang Zhang, Yunze Liu, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding 4D point cloud sequences online is of significant practical
value in various scenarios such as VR/AR, robotics, and autonomous driving. The
key goal is to continuously analyze the geometry and dynamics of a 3D scene as
unstructured and redundant point cloud sequences arrive. And the main challenge
is to effectively model the long-term history while keeping computational costs
manageable. To tackle these challenges, we introduce a generic online 4D
perception paradigm called NSM4D. NSM4D serves as a plug-and-play strategy that
can be adapted to existing 4D backbones, significantly enhancing their online
perception capabilities for both indoor and outdoor scenarios. To efficiently
capture the redundant 4D history, we propose a neural scene model that
factorizes geometry and motion information by constructing geometry tokens
separately storing geometry and motion features. Exploiting the history becomes
as straightforward as querying the neural scene model. As the sequence
progresses, the neural scene model dynamically deforms to align with new
observations, effectively providing the historical context and updating itself
with the new observations. By employing token representation, NSM4D also
exhibits robustness to low-level sensor noise and maintains a compact size
through a geometric sampling scheme. We integrate NSM4D with state-of-the-art
4D perception backbones, demonstrating significant improvements on various
online perception benchmarks in indoor and outdoor settings. Notably, we
achieve a 9.6% accuracy improvement for HOI4D online action segmentation and a
3.4% mIoU improvement for SemanticKITTI online semantic segmentation.
Furthermore, we show that NSM4D inherently offers excellent scalability to
longer sequences beyond the training set, which is crucial for real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defending Our Privacy With Backdoors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of large AI models trained on uncurated, often sensitive
web-scraped data has raised significant privacy concerns. One of the concerns
is that adversaries can extract information about the training data using
privacy attacks. Unfortunately, the task of removing specific information from
the models without sacrificing performance is not straightforward and has
proven to be challenging. We propose a rather easy yet effective defense based
on backdoor attacks to remove private information such as names of individuals
from models, and focus in this work on text encoders. Specifically, through
strategic insertion of backdoors, we align the embeddings of sensitive phrases
with those of neutral terms-"a person" instead of the person's name. Our
empirical results demonstrate the effectiveness of our backdoor-based defense
on CLIP by assessing its performance using a specialized privacy attack for
zero-shot classifiers. Our approach provides not only a new "dual-use"
perspective on backdoor attacks, but also presents a promising avenue to
enhance the privacy of individuals within models trained on uncurated
web-scraped data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extended target tracking utilizing machine-learning software -- with
  applications to animal classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magnus Malmström, Anton Kullberg, Isaac Skog, Daniel Axehill, Fredrik Gustafsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of detecting and tracking objects in a
sequence of images. The problem is formulated in a filtering framework, using
the output of object-detection algorithms as measurements. An extension to the
filtering formulation is proposed that incorporates class information from the
previous frame to robustify the classification, even if the object-detection
algorithm outputs an incorrect prediction. Further, the properties of the
object-detection algorithm are exploited to quantify the uncertainty of the
bounding box detection in each frame. The complete filtering method is
evaluated on camera trap images of the four large Swedish carnivores, bear,
lynx, wolf, and wolverine. The experiments show that the class tracking
formulation leads to a more robust classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GePSAn: Generative Procedure Step Anticipation in Cooking Videos <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Ashraf Abdelsalam, Samrudhdhi B. Rangrej, Isma Hadji, Nikita Dvornik, Konstantinos G. Derpanis, Afsaneh Fazly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of future step anticipation in procedural videos. Given
a video of an ongoing procedural activity, we predict a plausible next
procedure step described in rich natural language. While most previous work
focus on the problem of data scarcity in procedural video datasets, another
core challenge of future anticipation is how to account for multiple plausible
future realizations in natural settings. This problem has been largely
overlooked in previous work. To address this challenge, we frame future step
prediction as modelling the distribution of all possible candidates for the
next step. Specifically, we design a generative model that takes a series of
video clips as input, and generates multiple plausible and diverse candidates
(in natural language) for the next step. Following previous work, we side-step
the video annotation scarcity by pretraining our model on a large text-based
corpus of procedural activities, and then transfer the model to the video
domain. Our experiments, both in textual and video domains, show that our model
captures diversity in the next step prediction and generates multiple plausible
future predictions. Moreover, our model establishes new state-of-the-art
results on YouCookII, where it outperforms existing baselines on the next step
anticipation. Finally, we also show that our model can successfully transfer
from text to the video domain zero-shot, ie, without fine-tuning or adaptation,
and produces good-quality future step predictions from video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHIP: Contrastive Hierarchical Image Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arpit Mittal, Harshil Jhaveri, Swapnil Mallick, Abhishek Ajmera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot object classification is the task of classifying objects in an image
with limited number of examples as supervision. We propose a one-shot/few-shot
classification model that can classify an object of any unseen class into a
relatively general category in an hierarchically based classification. Our
model uses a three-level hierarchical contrastive loss based ResNet152
classifier for classifying an object based on its features extracted from Image
embedding, not used during the training phase. For our experimentation, we have
used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal
classes for training our model and created our own dataset of unseen classes
for evaluating our trained model. Our model provides satisfactory results in
classifying the unknown objects into a generic category which has been later
discussed in greater detail.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Variational Auto-encoder based Audio-Visual Segmentation <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Mao, Jing Zhang, Mochu Xiang, Yiran Zhong, Yuchao Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an Explicit Conditional Multimodal Variational Auto-Encoder
(ECMVAE) for audio-visual segmentation (AVS), aiming to segment sound sources
in the video sequence. Existing AVS methods focus on implicit feature fusion
strategies, where models are trained to fit the discrete samples in the
dataset. With a limited and less diverse dataset, the resulting performance is
usually unsatisfactory. In contrast, we address this problem from an effective
representation learning perspective, aiming to model the contribution of each
modality explicitly. Specifically, we find that audio contains critical
category information of the sound producers, and visual data provides candidate
sound producer(s). Their shared information corresponds to the target sound
producer(s) shown in the visual data. In this case, cross-modal shared
representation learning is especially important for AVS. To achieve this, our
ECMVAE factorizes the representations of each modality with a modality-shared
representation and a modality-specific representation. An orthogonality
constraint is applied between the shared and specific representations to
maintain the exclusive attribute of the factorized latent code. Further, a
mutual information maximization regularizer is introduced to achieve extensive
exploration of each modality. Quantitative and qualitative evaluations on the
AVSBench demonstrate the effectiveness of our approach, leading to a new
state-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging
MS3 subset for multiple sound source segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2023,Project
  page(https://npucvr.github.io/MMVAE-AVS),Code(https://github.com/OpenNLPLab/MMVAE-AVS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direction-Oriented Visual-semantic Embedding Model for Remote Sensing
  Image-text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Ma, Jiancheng Pan, Cong Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-text retrieval has developed rapidly in recent years. However, it is
still a challenge in remote sensing due to visual-semantic imbalance, which
leads to incorrect matching of non-semantic visual and textual features. To
solve this problem, we propose a novel Direction-Oriented Visual-semantic
Embedding Model (DOVE) to mine the relationship between vision and language.
Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the
distance between the final visual and textual embeddings in the latent semantic
space, oriented by regional visual features. Meanwhile, a lightweight Digging
Text Genome Assistant (DTGA) is designed to expand the range of tractable
textual representation and enhance global word-level semantic connections using
less attention operations. Ultimately, we exploit a global visual-semantic
constraint to reduce single visual dependency and serve as an external
constraint for the final visual and textual representations. The effectiveness
and superiority of our method are verified by extensive experiments including
parameter evaluation, quantitative comparison, ablation studies and visual
analysis, on two benchmark datasets, RSICD and RSITMD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for
  Multi-Modal 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziying Song, Haiyue Wei, Lin Bai, Lei Yang, Caiyan Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR and cameras are complementary sensors for 3D object detection in
autonomous driving. However, it is challenging to explore the unnatural
interaction between point clouds and images, and the critical factor is how to
conduct feature alignment of heterogeneous modalities. Currently, many methods
achieve feature alignment by projection calibration only, without considering
the problem of coordinate conversion accuracy errors between sensors, leading
to sub-optimal performance. In this paper, we present GraphAlign, a more
accurate feature alignment strategy for 3D object detection by graph matching.
Specifically, we fuse image features from a semantic segmentation encoder in
the image branch and point cloud features from a 3D Sparse CNN in the LiDAR
branch. To save computation, we construct the nearest neighbor relationship by
calculating Euclidean distance within the subspaces that are divided into the
point cloud features. Through the projection calibration between the image and
point cloud, we project the nearest neighbors of point cloud features onto the
image features. Then by matching the nearest neighbors with a single point
cloud to multiple images, we search for a more appropriate feature alignment.
In addition, we provide a self-attention module to enhance the weights of
significant relations to fine-tune the feature alignment between heterogeneous
modalities. Extensive experiments on nuScenes benchmark demonstrate the
effectiveness and efficiency of our GraphAlign.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invisible Threats: Backdoor Attack in OCR Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mauro Conti, Nicola Farronato, Stefanos Koffas, Luca Pajola, Stjepan Picek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical Character Recognition (OCR) is a widely used tool to extract text
from scanned documents. Today, the state-of-the-art is achieved by exploiting
deep neural networks. However, the cost of this performance is paid at the
price of system vulnerability. For instance, in backdoor attacks, attackers
compromise the training phase by inserting a backdoor in the victim's model
that will be activated at testing time by specific patterns while leaving the
overall model performance intact. This work proposes a backdoor attack for OCR
resulting in the injection of non-readable characters from malicious input
images. This simple but effective attack exposes the state-of-the-art OCR
weakness, making the extracted text correct to human eyes but simultaneously
unusable for the NLP application that uses OCR as a preprocessing step.
Experimental results show that the attacked models successfully output
non-readable characters for around 90% of the poisoned instances without
harming their performance for the remaining instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling from Vision-Language Models for Improved OOD Generalization
  in Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sravanti Addepalli, Ashish Ramayee Asokan, Lakshay Sharma, R. Venkatesh Babu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) such as CLIP are trained on large amounts of
image-text pairs, resulting in remarkable generalization across several data
distributions. The prohibitively expensive training and data
collection/curation costs of these models make them valuable Intellectual
Property (IP) for organizations. This motivates a vendor-client paradigm, where
a vendor trains a large-scale VLM and grants only input-output access to
clients on a pay-per-query basis in a black-box setting. The client aims to
minimize inference cost by distilling the VLM to a student model using the
limited available task-specific data, and further deploying this student model
in the downstream application. While naive distillation largely improves the
In-Domain (ID) accuracy of the student, it fails to transfer the superior
out-of-distribution (OOD) generalization of the VLM teacher using the limited
available labeled images. To mitigate this, we propose Vision-Language to
Vision-Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and
language modalities of the teacher model with the vision modality of a
pre-trained student model, and further distills the aligned VLM embeddings to
the student. This maximally retains the pre-trained features of the student,
while also incorporating the rich representations of the VLM image encoder and
the superior generalization of the text embeddings. The proposed approach
achieves state-of-the-art results on the standard Domain Generalization
benchmarks in a black-box teacher setting, and also when weights of the VLM are
accessible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/val-iisc/VL2V-ADiP.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Discrete Optimisation for Geometrically Consistent 3D Shape
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Roetzer, Ahmed Abbas, Dongliang Cao, Florian Bernard, Paul Swoboda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose to combine the advantages of learning-based and
combinatorial formalisms for 3D shape matching. While learning-based shape
matching solutions lead to state-of-the-art matching performance, they do not
ensure geometric consistency, so that obtained matchings are locally unsmooth.
On the contrary, axiomatic methods allow to take geometric consistency into
account by explicitly constraining the space of valid matchings. However,
existing axiomatic formalisms are impractical since they do not scale to
practically relevant problem sizes, or they require user input for the
initialisation of non-convex optimisation problems. In this work we aim to
close this gap by proposing a novel combinatorial solver that combines a unique
set of favourable properties: our approach is (i) initialisation free, (ii)
massively parallelisable powered by a quasi-Newton method, (iii) provides
optimality gaps, and (iv) delivers decreased runtime and globally optimal
results for many instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paul Roetzer and Ahmed Abbas contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structural analysis of Hindi online handwritten characters for character
  recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Sharma, A. G. Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direction properties of online strokes are used to analyze them in terms of
homogeneous regions or sub-strokes with points satisfying common geometric
properties. Such sub-strokes are called sub-units. These properties are used to
extract sub-units from Hindi ideal online characters. These properties along
with some heuristics are used to extract sub-units from Hindi online
handwritten characters.\\ A method is developed to extract point stroke,
clockwise curve stroke, counter-clockwise curve stroke and loop stroke segments
as sub-units from Hindi online handwritten characters. These extracted
sub-units are close in structure to the sub-units of the corresponding Hindi
online ideal characters.\\ Importance of local representation of online
handwritten characters in terms of sub-units is assessed by training a
classifier with sub-unit level local and character level global features
extracted from characters for character recognition. The classifier has the
recognition accuracy of 93.5\% on the testing set. This accuracy is the highest
when compared with that of the classifiers trained only with global features
extracted from characters in the same training set and evaluated on the same
testing set.\\ Sub-unit extraction algorithm and the sub-unit based character
classifier are tested on Hindi online handwritten character dataset. This
dataset consists of samples from 96 different characters. There are 12832 and
2821 samples in the training and testing sets, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 36 jpg figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge
  Retention and Promotion <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Preetha Vijayan, Prashant Bhat, Elahe Arani, Bahram Zonooz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) has remained a persistent challenge for deep neural
networks due to catastrophic forgetting (CF) of previously learned tasks.
Several techniques such as weight regularization, experience rehearsal, and
parameter isolation have been proposed to alleviate CF. Despite their relative
success, these research directions have predominantly remained orthogonal and
suffer from several shortcomings, while missing out on the advantages of
competing strategies. On the contrary, the brain continually learns,
accommodates, and transfers knowledge across tasks by simultaneously leveraging
several neurophysiological processes, including neurogenesis, active
forgetting, neuromodulation, metaplasticity, experience rehearsal, and
context-dependent gating, rarely resulting in CF. Inspired by how the brain
exploits multiple mechanisms concurrently, we propose TriRE, a novel CL
paradigm that encompasses retaining the most prominent neurons for each task,
revising and solidifying the extracted knowledge of current and past tasks, and
actively promoting less active neurons for subsequent tasks through rewinding
and relearning. Across CL settings, TriRE significantly reduces task
interference and surpasses different CL approaches considered in isolation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Tailed Classification Based on Coarse-Grained Leading Forest and
  Multi-Center Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinye Yang, Ji Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-tailed(LT) classification is an unavoidable and challenging problem in
the real world. Most of the existing long-tailed classification methods focus
only on solving the inter-class imbalance in which there are more samples in
the head class than in the tail class, while ignoring the intra-lass imbalance
in which the number of samples of the head attribute within the same class is
much larger than the number of samples of the tail attribute. The deviation in
the model is caused by both of these factors, and due to the fact that
attributes are implicit in most datasets and the combination of attributes is
very complex, the intra-class imbalance is more difficult to handle. For this
purpose, we proposed a long-tailed classification framework, known as
\textbf{\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest
(CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint
solution model by means of invariant feature learning. In this method, we
designed an unsupervised learning method, i.e., CLF, to better characterize the
distribution of attributes within a class. Depending on the distribution of
attributes, we can flexibly construct sampling strategies suitable for
different environments. In addition, we introduce a new metric learning loss
(MCL), which aims to gradually eliminate confusing attributes during the
feature learning process. More importantly, this approach does not depend on a
specific model structure and can be integrated with existing LT methods as an
independent component. We have conducted extensive experiments and our approach
has state-of-the-art performance in both existing benchmarks ImageNet-GLT and
MSCOCO-GLT, and can improve the performance of existing LT methods. Our codes
are available on GitHub: \url{https://github.com/jinyery/cognisance}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is another research work to apply leading tree structure along
  with deep learning architecture</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lifelong Audio-video Masked Autoencoder with Forget-robust Localized
  Alignments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoo Lee, Jaehong Yoon, Wonjae Kim, Yunji Kim, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a lifelong audio-video masked autoencoder that continually learns
the multimodal representations from a video stream containing audio-video
pairs, while its distribution continually shifts over time. Specifically, we
propose two novel ideas to tackle the problem: (1) Localized Alignment: We
introduce a small trainable multimodal encoder that predicts the audio and
video tokens that are well-aligned with each other. This allows the model to
learn only the highly correlated audiovisual patches with accurate multimodal
relationships. (2) Forget-robust multimodal patch selection: We compare the
relative importance of each audio-video patch between the current and past data
pair to mitigate unintended drift of the previously learned audio-video
representations. Our proposed method, FLAVA (Forget-robust Localized
Audio-Video Alignment), therefore, captures the complex relationships between
the audio and video modalities during training on a sequence of pre-training
tasks while alleviating the forgetting of learned audiovisual correlations. Our
experiments validate that FLAVA outperforms the state-of-the-art continual
learning methods on several benchmark datasets under continual audio-video
representation learning scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, project page: https://g-jwlee.github.io/FLAVA/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Li, Dan Zhang, Shengzhao Lei, Xun Zhao, Shuyan Li, Porawit Kamnoedboon, WeiWei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of standardized robustness metrics and the widespread reliance on
numerous unrelated benchmark datasets for testing have created a gap between
academically validated robust models and their often problematic practical
adoption. To address this, we introduce XIMAGENET-12, an explainable benchmark
dataset with over 200K images and 15,600 manual semantic annotations. Covering
12 categories from ImageNet to represent objects commonly encountered in
practical life and simulating six diverse scenarios, including overexposure,
blurring, color changing, etc., we further propose a novel robustness criterion
that extends beyond model generation ability assessment. This benchmark
dataset, along with related code, is available at
https://sites.google.com/view/ximagenet-12/home. Researchers and practitioners
can leverage this resource to evaluate the robustness of their visual models
under challenging conditions and ultimately benefit from the demands of
practical computer vision systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>UnderSubmission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Floris, Raffaele Mura, Luca Scionis, Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the adversarial robustness of machine learning models using
gradient-based attacks is challenging. In this work, we show that
hyperparameter optimization can improve fast minimum-norm attacks by automating
the selection of the loss function, the optimizer and the step-size scheduler,
along with the corresponding hyperparameters. Our extensive evaluation
involving several robust models demonstrates the improved efficacy of fast
minimum-norm attacks when hyper-up with hyperparameter optimization. We release
our open-source code at https://github.com/pralab/HO-FMN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ESANN23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COVID-19 Detection Using Swin Transformer Approach from Computed
  Tomography Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenan Morani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate and efficient diagnosis of COVID-19 is of paramount importance,
particularly in the context of large-scale medical imaging datasets. In this
preprint paper, we propose a novel approach for COVID-19 diagnosis using CT
images that leverages the power of Swin Transformer models, state-of-the-art
solutions in computer vision tasks. Our method includes a systematic approach
for patient-level predictions, where individual CT slices are classified as
COVID-19 or non-COVID, and the patient's overall diagnosis is determined
through majority voting. The application of the Swin Transformer in this
context results in patient-level predictions that demonstrate exceptional
diagnostic accuracy. In terms of evaluation metrics, our approach consistently
outperforms the baseline, as well as numerous competing methods, showcasing its
effectiveness in COVID-19 diagnosis. The macro F1 score achieved by our model
exceeds the baseline and offers a robust solution for accurate diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Learning Framework for Spatiotemporal Ultrasound Localization
  Microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Léo Milecki, Jonathan Porée, Hatim Belgharbi, Chloé Bourquin, Rafat Damseh, Patrick Delafontaine-Martel, Frédéric Lesage, Maxime Gasse, Jean Provost
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound Localization Microscopy can resolve the microvascular bed down to
a few micrometers. To achieve such performance microbubble contrast agents must
perfuse the entire microvascular network. Microbubbles are then located
individually and tracked over time to sample individual vessels, typically over
hundreds of thousands of images. To overcome the fundamental limit of
diffraction and achieve a dense reconstruction of the network, low microbubble
concentrations must be used, which lead to acquisitions lasting several
minutes. Conventional processing pipelines are currently unable to deal with
interference from multiple nearby microbubbles, further reducing achievable
concentrations. This work overcomes this problem by proposing a Deep Learning
approach to recover dense vascular networks from ultrasound acquisitions with
high microbubble concentrations. A realistic mouse brain microvascular network,
segmented from 2-photon microscopy, was used to train a three-dimensional
convolutional neural network based on a V-net architecture. Ultrasound data
sets from multiple microbubbles flowing through the microvascular network were
simulated and used as ground truth to train the 3D CNN to track microbubbles.
The 3D-CNN approach was validated in silico using a subset of the data and in
vivo on a rat brain acquisition. In silico, the CNN reconstructed vascular
networks with higher precision (81%) than a conventional ULM framework (70%).
In vivo, the CNN could resolve micro vessels as small as 10 $\mu$m with an
increase in resolution when compared against a conventional approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2021 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Annotation for Face Anti-Spoofing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Chen, Yunde Jia, Yuwei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face anti-spoofing plays a critical role in safeguarding facial recognition
systems against presentation attacks. While existing deep learning methods show
promising results, they still suffer from the lack of fine-grained annotations,
which lead models to learn task-irrelevant or unfaithful features. In this
paper, we propose a fine-grained annotation method for face anti-spoofing.
Specifically, we first leverage the Segment Anything Model (SAM) to obtain
pixel-wise segmentation masks by utilizing face landmarks as point prompts. The
face landmarks provide segmentation semantics, which segments the face into
regions. We then adopt these regions as masks and assemble them into three
separate annotation maps: spoof, living, and background maps. Finally, we
combine three separate maps into a three-channel map as annotations for model
training. Furthermore, we introduce the Multi-Channel Region Exchange
Augmentation (MCREA) to diversify training data and reduce overfitting.
Experimental results demonstrate that our method outperforms existing
state-of-the-art approaches in both intra-dataset and cross-dataset
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DualAug: Exploiting Additional Heavy Augmentation with OOD Data
  Rejection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehao Wang, Yiwen Guo, Qizhang Li, Guanglei Yang, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is a dominant method for reducing model overfitting and
improving generalization. Most existing data augmentation methods tend to find
a compromise in augmenting the data, \textit{i.e.}, increasing the amplitude of
augmentation carefully to avoid degrading some data too much and doing harm to
the model performance. We delve into the relationship between data augmentation
and model performance, revealing that the performance drop with heavy
augmentation comes from the presence of out-of-distribution (OOD) data.
Nonetheless, as the same data transformation has different effects for
different training samples, even for heavy augmentation, there remains part of
in-distribution data which is beneficial to model training. Based on the
observation, we propose a novel data augmentation method, named
\textbf{DualAug}, to keep the augmentation in distribution as much as possible
at a reasonable time and computational cost. We design a data mixing strategy
to fuse augmented data from both the basic- and the heavy-augmentation
branches. Extensive experiments on supervised image classification benchmarks
show that DualAug improve various automated data augmentation method. Moreover,
the experiments on semi-supervised learning and contrastive self-supervised
learning demonstrate that our DualAug can also improve related method. Code is
available at
\href{https://github.com/shuguang99/DualAug}{https://github.com/shuguang99/DualAug}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tailored Visions: Enhancing Text-to-Image Generation with Personalized
  Prompt Rewriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Chen, Lichao Zhang, Fangsheng Weng, Lili Pan, Zhenzhong Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel perspective of viewing large pretrained models as search
engines, thereby enabling the repurposing of techniques previously used to
enhance search engine performance. As an illustration, we employ a personalized
query rewriting technique in the realm of text-to-image generation. Despite
significant progress in the field, it is still challenging to create
personalized visual representations that align closely with the desires and
preferences of individual users. This process requires users to articulate
their ideas in words that are both comprehensible to the models and accurately
capture their vision, posing difficulties for many users. In this paper, we
tackle this challenge by leveraging historical user interactions with the
system to enhance user prompts. We propose a novel approach that involves
rewriting user prompts based a new large-scale text-to-image dataset with over
300k prompts from 3115 users. Our rewriting model enhances the expressiveness
and alignment of user prompts with their intended visual outputs. Experimental
results demonstrate the superiority of our methods over baseline approaches, as
evidenced in our new offline evaluation method and online tests. Our approach
opens up exciting possibilities of applying more search engine techniques to
build truly personalized large pretrained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DUSA: Decoupled Unsupervised Sim2Real Adaptation for
  Vehicle-to-Everything Collaborative Perception <span class="chip">ACM MM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghao Kong, Wentao Jiang, Jinrang Jia, Yifeng Shi, Runsheng Xu, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle-to-Everything (V2X) collaborative perception is crucial for
autonomous driving. However, achieving high-precision V2X perception requires a
significant amount of annotated real-world data, which can always be expensive
and hard to acquire. Simulated data have raised much attention since they can
be massively produced at an extremely low cost. Nevertheless, the significant
domain gap between simulated and real-world data, including differences in
sensor type, reflectance patterns, and road surroundings, often leads to poor
performance of models trained on simulated data when evaluated on real-world
data. In addition, there remains a domain gap between real-world collaborative
agents, e.g. different types of sensors may be installed on autonomous vehicles
and roadside infrastructures with different extrinsics, further increasing the
difficulty of sim2real generalization. To take full advantage of simulated
data, we present a new unsupervised sim2real domain adaptation method for V2X
collaborative detection named Decoupled Unsupervised Sim2Real Adaptation
(DUSA). Our new method decouples the V2X collaborative sim2real domain
adaptation problem into two sub-problems: sim2real adaptation and inter-agent
adaptation. For sim2real adaptation, we design a Location-adaptive Sim2Real
Adapter (LSA) module to adaptively aggregate features from critical locations
of the feature map and align the features between simulated data and real-world
data via a sim/real discriminator on the aggregated global feature. For
inter-agent adaptation, we further devise a Confidence-aware Inter-agent
Adapter (CIA) module to align the fine-grained features from heterogeneous
agents under the guidance of agent-wise confidence maps. Experiments
demonstrate the effectiveness of the proposed DUSA approach on unsupervised
sim2real adaptation from the simulated V2XSet dataset to the real-world
DAIR-V2X-C dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM MM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Active Measurement for Human Mesh Recovery in Close Proximity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takahiro Maeda, Keisuke Takeshita, Kazuhito Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For safe and sophisticated physical human-robot interactions (pHRI), a robot
needs to estimate the accurate body pose or mesh of the target person. However,
in these pHRI scenarios, the robot cannot fully observe the target person's
body with equipped cameras because the target person is usually close to the
robot. This leads to severe truncation and occlusions, and results in poor
accuracy of human pose estimation. For better accuracy of human pose estimation
or mesh recovery on this limited information from cameras, we propose an active
measurement and sensor fusion framework of the equipped cameras and other
sensors such as touch sensors and 2D LiDAR. These touch and LiDAR sensing are
obtained attendantly through pHRI without additional costs. These sensor
measurements are sparse but reliable and informative cues for human mesh
recovery. In our active measurement process, camera viewpoints and sensor
placements are optimized based on the uncertainty of the estimated pose, which
is closely related to the truncated or occluded areas. In our sensor fusion
process, we fuse the sensor measurements to the camera-based estimated pose by
minimizing the distance between the estimated mesh and measured positions. Our
method is agnostic to robot configurations. Experiments were conducted using
the Toyota Human Support Robot, which has a camera, 2D LiDAR, and a touch
sensor on the robot arm. Our proposed method demonstrated the superiority in
the human pose estimation accuracy on the quantitative comparison. Furthermore,
our proposed method reliably estimated the pose of the target person in
practical settings such as target people occluded by a blanket and standing aid
with the robot arm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing
  Label Bias in Foundation Models <span class="chip">NeurIPS2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beier Zhu, Kaihua Tang, Qianru Sun, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models like CLIP allow zero-shot transfer on various tasks without
additional training data. Yet, the zero-shot performance is less competitive
than a fully supervised one. Thus, to enhance the performance, fine-tuning and
ensembling are also commonly adopted to better fit the downstream tasks.
However, we argue that such prior work has overlooked the inherent biases in
foundation models. Due to the highly imbalanced Web-scale training set, these
foundation models are inevitably skewed toward frequent semantics, and thus the
subsequent fine-tuning or ensembling is still biased. In this study, we
systematically examine the biases in foundation models and demonstrate the
efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that
bias estimation in foundation models is challenging, as most pre-train data
cannot be explicitly accessed like in traditional long-tailed classification
tasks. To this end, GLA has an optimization-based bias estimation approach for
debiasing foundation models. As our work resolves a fundamental flaw in the
pre-training, the proposed GLA demonstrates significant improvements across a
diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large
average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on
long-tailed classification. Codes are in \url{https://github.com/BeierZhu/GLA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SingleInsert: Inserting New Concepts from a Single Image into
  Text-to-Image Models for Flexible Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Wu, Chaohui Yu, Zhen Zhu, Fan Wang, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in text-to-image (T2I) models enables high-quality image
generation with flexible textual control. To utilize the abundant visual priors
in the off-the-shelf T2I models, a series of methods try to invert an image to
proper embedding that aligns with the semantic space of the T2I model. However,
these image-to-text (I2T) inversion methods typically need multiple source
images containing the same concept or struggle with the imbalance between
editing flexibility and visual fidelity. In this work, we point out that the
critical problem lies in the foreground-background entanglement when learning
an intended concept, and propose a simple and effective baseline for
single-image I2T inversion, named SingleInsert. SingleInsert adopts a two-stage
scheme. In the first stage, we regulate the learned embedding to concentrate on
the foreground area without being associated with the irrelevant background. In
the second stage, we finetune the T2I model for better visual resemblance and
devise a semantic loss to prevent the language drift problem. With the proposed
techniques, SingleInsert excels in single concept generation with high visual
fidelity while allowing flexible editing. Additionally, SingleInsert can
perform single-image novel view synthesis and multiple concepts composition
without requiring joint training. To facilitate evaluation, we design an
editing prompt list and introduce a metric named Editing Success Rate (ESR) for
quantitative assessment of editing flexibility. Our project page is:
https://jarrentwu1031.github.io/SingleInsert-web/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jarrentwu1031.github.io/SingleInsert-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistent123: Improve Consistency for One Image to 3D Object Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang, C. L. Philip Chen, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large image diffusion models enable novel view synthesis with high quality
and excellent zero-shot capability. However, such models based on
image-to-image translation have no guarantee of view consistency, limiting the
performance for downstream tasks like 3D reconstruction and image-to-3D
generation. To empower consistency, we propose Consistent123 to synthesize
novel views simultaneously by incorporating additional cross-view attention
layers and the shared self-attention mechanism. The proposed attention
mechanism improves the interaction across all synthesized views, as well as the
alignment between the condition view and novel views. In the sampling stage,
such architecture supports simultaneously generating an arbitrary number of
views while training at a fixed length. We also introduce a progressive
classifier-free guidance strategy to achieve the trade-off between texture and
geometry for synthesized object views. Qualitative and quantitative experiments
show that Consistent123 outperforms baselines in view consistency by a large
margin. Furthermore, we demonstrate a significant improvement of Consistent123
on varying downstream tasks, showing its great potential in the 3D generation
field. The project page is available at consistent-123.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For more qualitative results, please see
  https://consistent-123.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Volumetric Medical Image Segmentation via Scribble Annotations and Shape
  Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhui Chen, Haiying Lyu, Xinyue Hu, Yong Lu, Yi Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, weakly-supervised image segmentation using weak annotations like
scribbles has gained great attention in computer vision and medical image
analysis, since such annotations are much easier to obtain compared to
time-consuming and labor-intensive labeling at the pixel/voxel level. However,
due to a lack of structure supervision on regions of interest (ROIs), existing
scribble-based methods suffer from poor boundary localization. Furthermore,
most current methods are designed for 2D image segmentation, which do not fully
leverage the volumetric information if directly applied to each image slice. In
this paper, we propose a scribble-based volumetric image segmentation,
Scribble2D5, which tackles 3D anisotropic image segmentation and aims to its
improve boundary prediction. To achieve this, we augment a 2.5D attention UNet
with a proposed label propagation module to extend semantic information from
scribbles and use a combination of static and active boundary prediction to
learn ROI's boundary and regularize its shape. Also, we propose an optional
add-on component, which incorporates the shape prior information from unpaired
segmentation masks to further improve model accuracy. Extensive experiments on
three public datasets and one private dataset demonstrate our Scribble2D5
achieves state-of-the-art performance on volumetric image segmentation using
scribbles and shape prior if available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2205.06779</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jointly Optimized Global-Local Visual Localization of UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoling Li, Jiuniu Wang, Zhiwei Wei, Wenjia Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigation and localization of UAVs present a challenge when global
navigation satellite systems (GNSS) are disrupted and unreliable. Traditional
techniques, such as simultaneous localization and mapping (SLAM) and visual
odometry (VO), exhibit certain limitations in furnishing absolute coordinates
and mitigating error accumulation. Existing visual localization methods achieve
autonomous visual localization without error accumulation by matching with
ortho satellite images. However, doing so cannot guarantee real-time
performance due to the complex matching process. To address these challenges,
we propose a novel Global-Local Visual Localization (GLVL) network. Our GLVL
network is a two-stage visual localization approach, combining a large-scale
retrieval module that finds similar regions with the UAV flight scene, and a
fine-grained matching module that localizes the precise UAV coordinate,
enabling real-time and precise localization. The training process is jointly
optimized in an end-to-end manner to further enhance the model capability.
Experiments on six UAV flight scenes encompassing both texture-rich and
texture-sparse regions demonstrate the ability of our model to achieve the
real-time precise localization requirements of UAVs. Particularly, our method
achieves a localization error of only 2.39 meters in 0.48 seconds in a village
scene with sparse texture features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RT-SRTS: Angle-Agnostic Real-Time Simultaneous 3D Reconstruction and
  Tumor Segmentation from Single X-Ray Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Zhu, Qiming Fu, Bo Liu, Mengxi Zhang, Bojian Li, Xiaoyan Luo, Fugen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiotherapy is one of the primary treatment methods for tumors, but the
organ movement caused by respiratory motion limits its accuracy. Recently, 3D
imaging from single X-ray projection receives extensive attentions as a
promising way to address this issue. However, current methods can only
reconstruct 3D image without direct location of the tumor and are only
validated for fixed-angle imaging, which fails to fully meet the requirement of
motion control in radiotherapy. In this study, we propose a novel imaging
method RT-SRTS which integrates 3D imaging and tumor segmentation into one
network based on the multi-task learning (MTL) and achieves real-time
simultaneous 3D reconstruction and tumor segmentation from single X-ray
projection at any angle. Futhermore, we propose the attention enhanced
calibrator (AEC) and uncertain-region elaboration (URE) modules to aid feature
extraction and improve segmentation accuracy. We evaluated the proposed method
on ten patient cases and compared it with two state-of-the-art methods. Our
approach not only delivered superior 3D reconstruction but also demonstrated
commendable tumor segmentation results. The simultaneous reconstruction and
segmentation could be completed in approximately 70 ms, significantly faster
than the required time threshold for real-time tumor tracking. The efficacy of
both AEC and URE was also validated through ablation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network pruning has shown to be an effective technique for reducing
the network size, trading desirable properties like generalization and
robustness to adversarial attacks for higher sparsity. Recent work has claimed
that adversarial pruning methods can produce sparse networks while also
preserving robustness to adversarial examples. In this work, we first
re-evaluate three state-of-the-art adversarial pruning methods, showing that
their robustness was indeed overestimated. We then compare pruned and dense
versions of the same models, discovering that samples on thin ice, i.e., closer
to the unpruned model's decision boundary, are typically misclassified after
pruning. We conclude by discussing how this intuition may lead to designing
more effective adversarial pruning methods in future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Transferable Conceptual Prototypes for Interpretable
  Unsupervised Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Gao, Xinhong Ma, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the great progress of unsupervised domain adaptation (UDA) with the
deep neural networks, current UDA models are opaque and cannot provide
promising explanations, limiting their applications in the scenarios that
require safe and controllable model decisions. At present, a surge of work
focuses on designing deep interpretable methods with adequate data annotations
and only a few methods consider the distributional shift problem. Most existing
interpretable UDA methods are post-hoc ones, which cannot facilitate the model
learning process for performance enhancement. In this paper, we propose an
inherently interpretable method, named Transferable Conceptual Prototype
Learning (TCPL), which could simultaneously interpret and improve the processes
of knowledge transfer and decision-making in UDA. To achieve this goal, we
design a hierarchically prototypical module that transfers categorical basic
concepts from the source domain to the target domain and learns domain-shared
prototypes for explaining the underlying reasoning process. With the learned
transferable prototypes, a self-predictive consistent pseudo-label strategy
that fuses confidence, predictions, and prototype information, is designed for
selecting suitable target samples for pseudo annotations and gradually
narrowing down the domain gap. Comprehensive experiments show that the proposed
method can not only provide effective and intuitive explanations but also
outperform previous state-of-the-arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE TIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency-Aware Re-Parameterization for Over-Fitting Based Image
  Compression <span class="chip">ICIP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Ye, Yanjie Pan, Qually Jiang, Ming Lu, Xiaoran Fang, Beryl Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-fitting-based image compression requires weights compactness for
compression and fast convergence for practical use, posing challenges for deep
convolutional neural networks (CNNs) based methods. This paper presents a
simple re-parameterization method to train CNNs with reduced weights storage
and accelerated convergence. The convolution kernels are re-parameterized as a
weighted sum of discrete cosine transform (DCT) kernels enabling direct
optimization in the frequency domain. Combined with L1 regularization, the
proposed method surpasses vanilla convolutions by achieving a significantly
improved rate-distortion with low computational cost. The proposed method is
verified with extensive experiments of over-fitting-based image restoration on
various datasets, achieving up to -46.12% BD-rate on top of HEIF with only 200
iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published at ICIP 2023, this version fixed a mistake in Eq. (1)
  in the proceeding version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Age Estimation Based on Graph Convolutional Networks and Multi-head
  Attention Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miaomiao Yang, Changwei Yao, Shijin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Age estimation technology is a part of facial recognition and has been
applied to identity authentication. This technology achieves the development
and application of a juvenile anti-addiction system by authenticating users in
the game. Convolutional Neural Network (CNN) and Transformer algorithms are
widely used in this application scenario. However, these two models cannot
flexibly extract and model features of faces with irregular shapes, and they
are ineffective in capturing key information. Furthermore, the above methods
will contain a lot of background information while extracting features, which
will interfere with the model. In consequence, it is easy to extract redundant
information from images. In this paper, a new modeling idea is proposed to
solve this problem, which can flexibly model irregular objects. The Graph
Convolutional Network (GCN) is used to extract features from irregular face
images effectively, and multi-head attention mechanisms are added to avoid
redundant features and capture key region information in the image. This model
can effectively improve the accuracy of age estimation and reduce the MAE error
value to about 3.64, which is better than the effect of today's age estimation
model, to improve the accuracy of face recognition and identity authentication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EC-Depth: Exploring the consistency of self-supervised monocular depth
  estimation under challenging scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruijie Zhu, Ziyang Song, Chuxin Wang, Jianfeng He, Tianzhu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised monocular depth estimation holds significant importance in
the fields of autonomous driving and robotics. However, existing methods are
typically designed to train and test on clear and pristine datasets,
overlooking the impact of various adverse conditions prevalent in real-world
scenarios. As a result, it is commonly observed that most self-supervised
monocular depth estimation methods struggle to perform adequately under
challenging conditions. To address this issue, we present EC-Depth, a novel
self-supervised two-stage training framework to achieve a robust depth
estimation, starting from the foundation of depth prediction consistency under
different perturbations. Leveraging the proposed perturbation-invariant depth
consistency constraint module and the consistency-based pseudo-label selection
module, our model attains accurate and consistent depth predictions in both
standard and challenging scenarios. Extensive experiments substantiate the
effectiveness of the proposed method. Moreover, our method surpasses existing
state-of-the-art methods on KITTI, KITTI-C and DrivingStereo benchmarks,
demonstrating its potential for enhancing the reliability of self-supervised
monocular depth estimation models in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-HRNet: Towards Lightweight Human Pose Estimation with Spatially
  Unidimensional Self-Attention <span class="chip">ICME 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Zhou, Xuanhan Wang, Xing Xu, Lei Zhao, Jingkuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-resolution representation is necessary for human pose estimation to
achieve high performance, and the ensuing problem is high computational
complexity. In particular, predominant pose estimation methods estimate human
joints by 2D single-peak heatmaps. Each 2D heatmap can be horizontally and
vertically projected to and reconstructed by a pair of 1D heat vectors.
Inspired by this observation, we introduce a lightweight and powerful
alternative, Spatially Unidimensional Self-Attention (SUSA), to the pointwise
(1x1) convolution that is the main computational bottleneck in the depthwise
separable 3c3 convolution. Our SUSA reduces the computational complexity of the
pointwise (1x1) convolution by 96% without sacrificing accuracy. Furthermore,
we use the SUSA as the main module to build our lightweight pose estimation
backbone X-HRNet, where `X' represents the estimated cross-shape attention
vectors. Extensive experiments on the COCO benchmark demonstrate the
superiority of our X-HRNet, and comprehensive ablation studies show the
effectiveness of the SUSA modules. The code is publicly available at
https://github.com/cool-xuan/x-hrnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning via Manifold Expansion Replay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Xu, Xuan Tang, Yufei Shi, Jianfeng Zhang, Jian Yang, Mingsong Chen, Xian Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In continual learning, the learner learns multiple tasks in sequence, with
data being acquired only once for each task. Catastrophic forgetting is a major
challenge to continual learning. To reduce forgetting, some existing
rehearsal-based methods use episodic memory to replay samples of previous
tasks. However, in the process of knowledge integration when learning a new
task, this strategy also suffers from catastrophic forgetting due to an
imbalance between old and new knowledge. To address this problem, we propose a
novel replay strategy called Manifold Expansion Replay (MaER). We argue that
expanding the implicit manifold of the knowledge representation in the episodic
memory helps to improve the robustness and expressiveness of the model. To this
end, we propose a greedy strategy to keep increasing the diameter of the
implicit manifold represented by the knowledge in the buffer during memory
management. In addition, we introduce Wasserstein distance instead of cross
entropy as distillation loss to preserve previous knowledge. With extensive
experimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show
that the proposed method significantly improves the accuracy in continual
learning setup, outperforming the state of the arts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BaSAL: Size Balanced Warm Start Active Learning for LiDAR Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarong Wei, Yancong Lin, Holger Caesar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning strives to reduce the need for costly data annotation, by
repeatedly querying an annotator to label the most informative samples from a
pool of unlabeled data and retraining a model from these samples. We identify
two problems with existing active learning methods for LiDAR semantic
segmentation. First, they ignore the severe class imbalance inherent in LiDAR
semantic segmentation datasets. Second, to bootstrap the active learning loop,
they train their initial model from randomly selected data samples, which leads
to low performance and is referred to as the cold start problem. To address
these problems we propose BaSAL, a size-balanced warm start active learning
model, based on the observation that each object class has a characteristic
size. By sampling object clusters according to their size, we can thus create a
size-balanced dataset that is also more class-balanced. Furthermore, in
contrast to existing information measures like entropy or CoreSet, size-based
sampling does not require an already trained model and thus can be used to
address the cold start problem. Results show that we are able to improve the
performance of the initial model by a large margin. Combining size-balanced
sampling and warm start with established information measures, our approach
achieves a comparable performance to training on the entire SemanticKITTI
dataset, despite using only 5% of the annotations, which outperforms existing
active learning methods. We also match the existing state-of-the-art in active
learning on nuScenes. Our code will be made available upon paper acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Large Language Models for Multi-Modal Out-of-Distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Dai, Hao Lang, Kaisheng Zeng, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is essential for reliable and trustworthy
machine learning. Recent multi-modal OOD detection leverages textual
information from in-distribution (ID) class names for visual OOD detection, yet
it currently neglects the rich contextual information of ID classes. Large
language models (LLMs) encode a wealth of world knowledge and can be prompted
to generate descriptive features for each class. Indiscriminately using such
knowledge causes catastrophic damage to OOD detection due to LLMs'
hallucinations, as is observed by our analysis. In this paper, we propose to
apply world knowledge to enhance OOD detection performance through selective
generation from LLMs. Specifically, we introduce a consistency-based
uncertainty calibration method to estimate the confidence score of each
generation. We further extract visual objects from each image to fully
capitalize on the aforementioned world knowledge. Extensive experiments
demonstrate that our method consistently outperforms the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP2023 Findings Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Sharing Weights in Decoupling Feature Learning Network for UAV
  RGB-Infrared Vehicle Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyue Liu, Jiahao Qi, Chen Chen, Kangcheng Bin, Ping Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to the capacity of performing full-time target search, cross-modality
vehicle re-identification (Re-ID) based on unmanned aerial vehicle (UAV) is
gaining more attention in both video surveillance and public security. However,
this promising and innovative research has not been studied sufficiently due to
the data inadequacy issue. Meanwhile, the cross-modality discrepancy and
orientation discrepancy challenges further aggravate the difficulty of this
task. To this end, we pioneer a cross-modality vehicle Re-ID benchmark named
UAV Cross-Modality Vehicle Re-ID (UCM-VeID), containing 753 identities with
16015 RGB and 13913 infrared images. Moreover, to meet cross-modality
discrepancy and orientation discrepancy challenges, we present a hybrid weights
decoupling network (HWDNet) to learn the shared discriminative
orientation-invariant features. For the first challenge, we proposed a hybrid
weights siamese network with a well-designed weight restrainer and its
corresponding objective function to learn both modality-specific and modality
shared information. In terms of the second challenge, three effective
decoupling structures with two pretext tasks are investigated to learn
orientation-invariant feature. Comprehensive experiments are carried out to
validate the effectiveness of the proposed method. The dataset and codes will
be released at https://github.com/moonstarL/UAV-CM-VeID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, 64 citations, submitted to TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video
  Retrieval <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pandeng Li, Hongtao Xie, Jiannan Ge, Lei Zhang, Shaobo Min, Yongdong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised video hashing usually optimizes binary codes by learning to
reconstruct input videos. Such reconstruction constraint spends much effort on
frame-level temporal context changes without focusing on video-level global
semantics that are more useful for retrieval. Hence, we address this problem by
decomposing video information into reconstruction-dependent and
semantic-dependent information, which disentangles the semantic extraction from
reconstruction constraint. Specifically, we first design a simple dual-stream
structure, including a temporal layer and a hash layer. Then, with the help of
semantic similarity knowledge obtained from self-supervision, the hash layer
learns to capture information for semantic retrieval, while the temporal layer
learns to capture the information for reconstruction. In this way, the model
naturally preserves the disentangled semantics into binary codes. Validated by
comprehensive experiments, our method consistently outperforms the
state-of-the-arts on three video benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLP-AMDC: An MLP Architecture for Adaptive-Mask-based Dual-Camera
  snapshot hyperspectral imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Cai, Can Zhang, Xunhao Chen, Shanghuan Liu, Chengqian Jin, Feipeng Da
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coded Aperture Snapshot Spectral Imaging (CASSI) system has great advantages
over traditional methods in dynamically acquiring Hyper-Spectral Image (HSI),
but there are the following problems. 1) Traditional mask relies on random
patterns or analytical design, both of which limit the performance improvement
of CASSI. 2) Existing high-quality reconstruction algorithms are slow in
reconstruction and can only reconstruct scene information offline. To address
the above two problems, this paper designs the AMDC-CASSI system, introducing
RGB camera with CASSI based on Adaptive-Mask as multimodal input to improve the
reconstruction quality. The existing SOTA reconstruction schemes are based on
transformer, but the operation of self-attention pulls down the operation
efficiency of the network. In order to improve the inference speed of the
reconstruction network, this paper proposes An MLP Architecture for
Adaptive-Mask-based Dual-Camera (MLP-AMDC) to replace the transformer structure
of the network. Numerous experiments have shown that MLP performs no less well
than transformer-based structures for HSI reconstruction, while MLP greatly
improves the network inference speed and has less number of parameters and
operations, our method has a 8 db improvement over SOTA and at least a 5-fold
improvement in reconstruction speed. (https://github.com/caizeyu1992/MLP-AMDC.)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2308.01541</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by
  Volume Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhang, Wanjuan Su, Wenbing Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, learning neural implicit surface by volume rendering has been a
promising way for multi-view reconstruction. However, limited accuracy and
excessive time complexity remain bottlenecks that current methods urgently need
to overcome. To address these challenges, we propose a new method called
Point-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient
reconstruction. Point modeling is organically embedded into the volume
rendering to enhance and regularize the representation of implicit surface.
Specifically, to achieve precise point guidance and noise robustness, aleatoric
uncertainty of the point cloud is modeled to capture the distribution of noise
and estimate the reliability of points. Additionally, a Neural Projection
module connecting points and images is introduced to add geometric constraints
to the Signed Distance Function (SDF). To better compensate for geometric bias
between volume rendering and point modeling, high-fidelity points are filtered
into an Implicit Displacement Network to improve the representation of SDF.
Benefiting from our effective point guidance, lightweight networks are employed
to achieve an impressive 11x speedup compared to NeuS. Extensive experiments
show that our method yields high-quality surfaces, especially for fine-grained
details and smooth regions. Moreover, it exhibits strong robustness to both
noisy and sparse data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reset It and Forget It: Relearning Last-Layer Weights Improves Continual
  and Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lapo Frati, Neil Traft, Jeff Clune, Nick Cheney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work identifies a simple pre-training mechanism that leads to
representations exhibiting better continual and transfer learning. This
mechanism -- the repeated resetting of weights in the last layer, which we
nickname "zapping" -- was originally designed for a meta-continual-learning
procedure, yet we show it is surprisingly applicable in many settings beyond
both meta-learning and continual learning. In our experiments, we wish to
transfer a pre-trained image classifier to a new set of classes, in a few
shots. We show that our zapping procedure results in improved transfer accuracy
and/or more rapid adaptation in both standard fine-tuning and continual
learning settings, while being simple to implement and computationally
efficient. In many cases, we achieve performance on par with state of the art
meta-learning without needing the expensive higher-order gradients, by using a
combination of zapping and sequential learning. An intuitive explanation for
the effectiveness of this zapping procedure is that representations trained
with repeated zapping learn features that are capable of rapidly adapting to
newly initialized classifiers. Such an approach may be considered a
computationally cheaper type of, or alternative to, meta-learning rapidly
adaptable features with higher-order gradients. This adds to recent work on the
usefulness of resetting neural network parameters during training, and invites
further investigation of this mechanism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HeightFormer: A Multilevel Interaction and Image-adaptive
  Classification-regression Network for Monocular Height Estimation with Aerial
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Chen, Yidan Zhang, Xiyu Qi, Yongqiang Mao, Xin Zhou, Lulu Niu, Hui Wu, Lei Wang, Yunping Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Height estimation has long been a pivotal topic within measurement and remote
sensing disciplines, proving critical for endeavours such as 3D urban
modelling, MR and autonomous driving. Traditional methods utilise stereo
matching or multisensor fusion, both well-established techniques that typically
necessitate multiple images from varying perspectives and adjunct sensors like
SAR, leading to substantial deployment costs. Single image height estimation
has emerged as an attractive alternative, boasting a larger data source variety
and simpler deployment. However, current methods suffer from limitations such
as fixed receptive fields, a lack of global information interaction, leading to
noticeable instance-level height deviations. The inherent complexity of height
prediction can result in a blurry estimation of object edge depth when using
mainstream regression methods based on fixed height division. This paper
presents a comprehensive solution for monocular height estimation in remote
sensing, termed HeightFormer, combining multilevel interactions and
image-adaptive classification-regression. It features the Multilevel
Interaction Backbone (MIB) and Image-adaptive Classification-regression Height
Generator (ICG). MIB supplements the fixed sample grid in CNN of the
conventional backbone network with tokens of different interaction ranges. It
is complemented by a pixel-, patch-, and feature map-level hierarchical
interaction mechanism, designed to relay spatial geometry information across
different scales and introducing a global receptive field to enhance the
quality of instance-level height estimation. The ICG dynamically generates
height partition for each image and reframes the traditional regression task,
using a refinement from coarse to fine classification-regression that
significantly mitigates the innate ill-posedness issue and drastically improves
edge sharpness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-supervised visual learning for analyzing firearms trafficking
  activities on the Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sotirios Konstantakos, Despina Ioanna Chalkiadaki, Ioannis Mademlis, Adamantia Anna Rebolledo Chrysochoou, Georgios Th. Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated visual firearms classification from RGB images is an important
real-world task with applications in public space security, intelligence
gathering and law enforcement investigations. When applied to images massively
crawled from the World Wide Web (including social media and dark Web sites), it
can serve as an important component of systems that attempt to identify
criminal firearms trafficking networks, by analyzing Big Data from open-source
intelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology
for achieving this, with Convolutional Neural Networks (CNN) being typically
employed. The common transfer learning approach consists of pretraining on a
large-scale, generic annotated dataset for whole-image classification, such as
ImageNet-1k, and then finetuning the DNN on a smaller, annotated,
task-specific, downstream dataset for visual firearms classification. Neither
Visual Transformer (ViT) neural architectures nor Self-Supervised Learning
(SSL) approaches have been so far evaluated on this critical task. SSL
essentially consists of replacing the traditional supervised pretraining
objective with an unsupervised pretext task that does not require ground-truth
labels..
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CleftGAN: Adapting A Style-Based Generative Adversarial Network To
  Create Images Depicting Cleft Lip Deformity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Hayajneh, Erchin Serpedin, Mohammad Shaqfeh, Graeme Glass, Mitchell A. Stotland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major obstacle when attempting to train a machine learning system to
evaluate facial clefts is the scarcity of large datasets of high-quality,
ethics board-approved patient images. In response, we have built a deep
learning-based cleft lip generator designed to produce an almost unlimited
number of artificial images exhibiting high-fidelity facsimiles of cleft lip
with wide variation. We undertook a transfer learning protocol testing
different versions of StyleGAN-ADA (a generative adversarial network image
generator incorporating adaptive data augmentation (ADA)) as the base model.
Training images depicting a variety of cleft deformities were pre-processed to
adjust for rotation, scaling, color adjustment and background blurring. The ADA
modification of the primary algorithm permitted construction of our new
generative model while requiring input of a relatively small number of training
images. Adversarial training was carried out using 514 unique frontal
photographs of cleft-affected faces to adapt a pre-trained model based on
70,000 normal faces. The Frechet Inception Distance (FID) was used to measure
the similarity of the newly generated facial images to the cleft training
dataset, while Perceptual Path Length (PPL) and the novel Divergence Index of
Severity Histograms (DISH) measures were also used to assess the performance of
the image generator that we dub CleftGAN. We found that StyleGAN3 with
translation invariance (StyleGAN3-t) performed optimally as a base model.
Generated images achieved a low FID reflecting a close similarity to our
training input dataset of genuine cleft images. Low PPL and DISH measures
reflected a smooth and semantically valid interpolation of images through the
transfer learning process and a similar distribution of severity in the
training and generated images, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soundify: Matching Sound Effects to Video <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Chuan-En Lin, Anastasis Germanidis, Cristóbal Valenzuela, Yining Shi, Nikolas Martelaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the art of video editing, sound helps add character to an object and
immerse the viewer within a space. Through formative interviews with
professional editors (N=10), we found that the task of adding sounds to video
can be challenging. This paper presents Soundify, a system that assists editors
in matching sounds to video. Given a video, Soundify identifies matching
sounds, synchronizes the sounds to the video, and dynamically adjusts panning
and volume to create spatial audio. In a human evaluation study (N=889), we
show that Soundify is capable of matching sounds to video out-of-the-box for a
diverse range of audio categories. In a within-subjects expert study (N=12), we
demonstrate the usefulness of Soundify in helping video editors match sounds to
video with lighter workload, reduced task completion time, and improved
usability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;
  Online demo: https://soundify.cc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StoryBench: A Multifaceted Benchmark for Continuous Story Visualization <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Bugliarello, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan, Vittorio Ferrari, Pieter-Jan Kindermans, Paul Voigtlaender
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating video stories from text prompts is a complex task. In addition to
having high visual quality, videos need to realistically adhere to a sequence
of text prompts whilst being consistent throughout the frames. Creating a
benchmark for video generation requires data annotated over time, which
contrasts with the single caption used often in video datasets. To fill this
gap, we collect comprehensive human annotations on three existing datasets, and
introduce StoryBench: a new, challenging multi-task benchmark to reliably
evaluate forthcoming text-to-video models. Our benchmark includes three video
generation tasks of increasing difficulty: action execution, where the next
action must be generated starting from a conditioning video; story
continuation, where a sequence of actions must be executed starting from a
conditioning video; and story generation, where a video must be generated from
only text prompts. We evaluate small yet strong text-to-video baselines, and
show the benefits of training on story-like data algorithmically generated from
existing video captions. Finally, we establish guidelines for human evaluation
of video stories, and reaffirm the need of better automatic metrics for video
generation. StoryBench aims at encouraging future research efforts in this
exciting new area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS D&B 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxiang Shi, Aldo Lipani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt tuning (PT), where a small amount of trainable soft (continuous)
prompt vectors is affixed to the input of language models (LM), has shown
promising results across various tasks and models for parameter-efficient
fine-tuning (PEFT). PT stands out from other PEFT approaches because it
maintains competitive performance with fewer trainable parameters and does not
drastically scale up its parameters as the model size expands. However, PT
introduces additional soft prompt tokens, leading to longer input sequences,
which significantly impacts training and inference time and memory usage due to
the Transformer's quadratic complexity. Particularly concerning for Large
Language Models (LLMs) that face heavy daily querying. To address this issue,
we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt
into a shorter soft prompt and a pair of low-rank matrices that are then
optimised with two different learning rates. This allows DePT to achieve better
performance while saving over 20% memory and time costs compared to vanilla PT
and its variants, without changing trainable parameter sizes. Through extensive
experiments on 23 natural language processing (NLP) and vision-language (VL)
tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,
including the full fine-tuning baseline in some scenarios. Additionally, we
empirically show that DEPT grows more efficient as the model size increases.
Our further study reveals that DePT integrates seamlessly with
parameter-efficient transfer learning in the few-shot learning setting and
highlights its adaptability to various model architectures and sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/ZhengxiangShi/DePT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CheXmask: a large-scale dataset of anatomical segmentation masks for
  multi-center chest x-ray images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolás Gaggion, Candelaria Mosquera, Lucas Mansilla, Martina Aineseder, Diego H. Milone, Enzo Ferrante
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of successful artificial intelligence models for chest X-ray
analysis relies on large, diverse datasets with high-quality annotations. While
several databases of chest X-ray images have been released, most include
disease diagnosis labels but lack detailed pixel-level anatomical segmentation
labels. To address this gap, we introduce an extensive chest X-ray multi-center
segmentation dataset with uniform and fine-grain anatomical annotations for
images coming from six well-known publicly available databases: CANDID-PTX,
ChestX-ray8, Chexpert, MIMIC-CXR-JPG, Padchest, and VinDr-CXR, resulting in
676,803 segmentation masks. Our methodology utilizes the HybridGNet model to
ensure consistent and high-quality segmentations across all datasets. Rigorous
validation, including expert physician evaluation and automatic quality
control, was conducted to validate the resulting masks. Additionally, we
provide individualized quality indices per mask and an overall quality
estimation per dataset. This dataset serves as a valuable resource for the
broader scientific community, streamlining the development and assessment of
innovative methodologies in chest X-ray analysis. The CheXmask dataset is
publicly available at:
https://physionet.org/content/chexmask-cxr-segmentation-data/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The CheXmask dataset is publicly available at
  https://physionet.org/content/chexmask-cxr-segmentation-data/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NECO: NEural Collapse Based Out-of-distribution detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouïn Ben Ammar, Nacim Belkhir, Sebastian Popescu, Antoine Manzanera, Gianni Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting out-of-distribution (OOD) data is a critical challenge in machine
learning due to model overconfidence, often without awareness of their
epistemological limits. We hypothesize that ``neural collapse'', a phenomenon
affecting in-distribution data for models trained beyond loss convergence, also
influences OOD data. To benefit from this interplay, we introduce NECO, a novel
post-hoc method for OOD detection, which leverages the geometric properties of
``neural collapse'' and of principal component spaces to identify OOD data. Our
extensive experiments demonstrate that NECO achieves state-of-the-art results
on both small and large-scale OOD detection tasks while exhibiting strong
generalization capabilities across different network architectures.
Furthermore, we provide a theoretical explanation for the effectiveness of our
method in OOD detection. We plan to release the code after the anonymity
period.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Scale Space Radon Transform, Properties and Application in CT Image
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.05188v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.05188v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nafaa Nacereddine, Djemel Ziou, Aicha Baya Goumeidane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the Radon transform (RT) consists in a line integral function, some
modeling assumptions are made on Computed Tomography (CT) system, making image
reconstruction analytical methods, such as Filtered Backprojection (FBP),
sensitive to artifacts and noise. In the other hand, recently, a new integral
transform, called Scale Space Radon Transform (SSRT), is introduced where, RT
is a particular case. Thanks to its interesting properties, such as good scale
space behavior, the SSRT has known number of new applications. In this paper,
with the aim to improve the reconstructed image quality for these methods, we
propose to model the X-ray beam with the Scale Space Radon Transform (SSRT)
where, the assumptions done on the physical dimensions of the CT system
elements reflect better the reality. After depicting the basic properties and
the inversion of SSRT, the FBP algorithm is used to reconstruct the image from
the SSRT sinogram where the RT spectrum used in FBP is replaced by SSRT and the
Gaussian kernel, expressed in their frequency domain. PSNR and SSIM, as quality
measures, are used to compare RT and SSRT-based image reconstruction on
Shepp-Logan head and anthropomorphic abdominal phantoms. The first findings
show that the SSRT-based method outperforms the methods based on RT,
especially, when the number of projections is reduced, making it more
appropriate for applications requiring low-dose radiation, such as medical
X-ray CT. While SSRT-FBP and RT-FBP have utmost the same runtime, the
experiments show that SSRT-FBP is more robust to Poisson-Gaussian noise
corrupting CT data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Pre-trained Vision and Language Models Answer Visual
  Information-Seeking Questions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11713v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11713v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, Ming-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained vision and language models have demonstrated state-of-the-art
capabilities over existing tasks involving images and texts, including visual
question answering. However, it remains unclear whether these models possess
the capability to answer questions that are not only querying visual content
but knowledge-intensive and information-seeking. In this study, we introduce
InfoSeek, a visual question answering dataset tailored for information-seeking
questions that cannot be answered with only common sense knowledge. Using
InfoSeek, we analyze various pre-trained visual question answering models and
gain insights into their characteristics. Our findings reveal that
state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)
face challenges in answering visual information-seeking questions, but
fine-tuning on the InfoSeek dataset elicits models to use fine-grained
knowledge that was learned during their pre-training. Furthermore, we show that
accurate visual entity recognition can be used to improve performance on
InfoSeek by retrieving relevant documents, showing a significant space for
improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 (main conference); Our dataset and evaluation is available
  at https://open-vision-language.github.io/infoseek/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvi Jonnarth, Yushan Zhang, Michael Felsberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-level weakly-supervised semantic segmentation (WSSS) reduces the
usually vast data annotation cost by surrogate segmentation masks during
training. The typical approach involves training an image classification
network using global average pooling (GAP) on convolutional feature maps. This
enables the estimation of object locations based on class activation maps
(CAMs), which identify the importance of image regions. The CAMs are then used
to generate pseudo-labels, in the form of segmentation masks, to supervise a
segmentation model in the absence of pixel-level ground truth. Our work is
based on two techniques for improving CAMs; importance sampling, which is a
substitute for GAP, and the feature similarity loss, which utilizes a heuristic
that object contours almost always align with color edges in images. However,
both are based on the multinomial posterior with softmax, and implicitly assume
that classes are mutually exclusive, which turns out suboptimal in our
experiments. Thus, we reformulate both techniques based on binomial posteriors
of multiple independent binary problems. This has two benefits; their
performance is improved and they become more general, resulting in an add-on
method that can boost virtually any WSSS method. This is demonstrated on a wide
variety of baselines on the PASCAL VOC dataset, improving the region similarity
and contour quality of all implemented state-of-the-art methods. Experiments on
the MS COCO dataset show that our proposed add-on is well-suited for
large-scale settings. Our code is available at https://github.com/arvijj/hfpl.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04946v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04946v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking-head synthesis is a popular research topic for virtual
human-related applications. However, the inflexibility and inefficiency of
existing methods, which necessitate expensive end-to-end training to transfer
emotions from guidance videos to talking-head predictions, are significant
limitations. In this work, we propose the Emotional Adaptation for Audio-driven
Talking-head (EAT) method, which transforms emotion-agnostic talking-head
models into emotion-controllable ones in a cost-effective and efficient manner
through parameter-efficient adaptations. Our approach utilizes a pretrained
emotion-agnostic talking-head transformer and introduces three lightweight
adaptations (the Deep Emotional Prompts, Emotional Deformation Network, and
Emotional Adaptation Module) from different perspectives to enable precise and
realistic emotion controls. Our experiments demonstrate that our approach
achieves state-of-the-art performance on widely-used benchmarks, including LRW
and MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable
generalization ability, even in scenarios where emotional training videos are
scarce or nonexistent. Project website: https://yuangan.github.io/eat/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2023. Project page: https://yuangan.github.io/eat/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClusVPR: Efficient Visual Place Recognition with Clustering-based
  Weighted Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Xu, Pourya Shamsolmoali, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition (VPR) is a highly challenging task that has a wide
range of applications, including robot navigation and self-driving vehicles.
VPR is particularly difficult due to the presence of duplicate regions and the
lack of attention to small objects in complex scenes, resulting in recognition
deviations. In this paper, we present ClusVPR, a novel approach that tackles
the specific issues of redundant information in duplicate regions and
representations of small objects. Different from existing methods that rely on
Convolutional Neural Networks (CNNs) for feature map generation, ClusVPR
introduces a unique paradigm called Clustering-based Weighted Transformer
Network (CWTNet). CWTNet leverages the power of clustering-based weighted
feature maps and integrates global dependencies to effectively address visual
deviations encountered in large-scale VPR problems. We also introduce the
optimized-VLAD (OptLAD) layer that significantly reduces the number of
parameters and enhances model efficiency. This layer is specifically designed
to aggregate the information obtained from scale-wise image patches.
Additionally, our pyramid self-supervised strategy focuses on extracting
representative and diverse information from scale-wise image patches instead of
entire images, which is crucial for capturing representative and diverse
information in VPR. Extensive experiments on four VPR datasets show our model's
superior performance compared to existing models while being less complex.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDRNet: Enabling Real-time Document Localization on Mobile Devices <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02136v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02136v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wu, Holland Qian, Huaming Wu, Aad van Moorsel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Identity Document Verification (IDV) technology on mobile devices
becomes ubiquitous in modern business operations, the risk of identity theft
and fraud is increasing. The identity document holder is normally required to
participate in an online video interview to circumvent impostors. However, the
current IDV process depends on an additional human workforce to support online
step-by-step guidance which is inefficient and expensive. The performance of
existing AI-based approaches cannot meet the real-time and lightweight demands
of mobile devices. In this paper, we address those challenges by designing an
edge intelligence-assisted approach for real-time IDV. Aiming at improving the
responsiveness of the IDV process, we propose a new document localization model
for mobile devices, LDRNet, to Localize the identity Document in Real-time. On
the basis of a lightweight backbone network, we build three prediction branches
for LDRNet, the corner points prediction, the line borders prediction and the
document classification. We design novel supplementary targets, the
equal-division points, and use a new loss function named Line Loss, to improve
the speed and accuracy of our approach. In addition to the IDV process, LDRNet
is an efficient and reliable document localization alternative for all kinds of
mobile applications. As a matter of proof, we compare the performance of LDRNet
with other popular approaches on localizing general document datasets. The
experimental results show that LDRNet runs at a speed up to 790 FPS which is
47x faster, while still achieving comparable Jaccard Index(JI) in single-model
and single-scale tests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECML-PKDD 2022 https://doi.org/10.1007/978-3-031-23618-1_42</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Characterising representation dynamics in recurrent neural networks for
  object recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sushrut Thorat, Adrien Doerig, Tim C. Kietzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recurrent neural networks (RNNs) have yielded promising results for both
recognizing objects in challenging conditions and modeling aspects of primate
vision. However, the representational dynamics of recurrent computations remain
poorly understood, especially in large-scale visual models. Here, we studied
such dynamics in RNNs trained for object classification on MiniEcoset, a novel
subset of ecoset. We report two main insights. First, upon inference,
representations continued to evolve after correct classification, suggesting a
lack of the notion of being ``done with classification''. Second, focusing on
``readout zones'' as a way to characterize the activation trajectories, we
observe that misclassified representations exhibit activation patterns with
lower L2 norm, and are positioned more peripherally in the readout zones. Such
arrangements help the misclassified representations move into the correct zones
as time progresses. Our findings generalize to networks with lateral and
top-down connections, and include both additive and multiplicative interactions
with the bottom-up sweep. The results therefore contribute to a general
understanding of RNN dynamics in naturalistic tasks. We hope that the analysis
framework will aid future investigations of other types of RNNs, including
understanding of representational dynamics in primate vision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures; revision of our Conference on Cognitive
  Computational Neuroscience (CCN) 2023 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Machine Learning Paradigm for Studying Pictorial Realism: Are
  Constable's Clouds More Real than His Contemporaries? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.09348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.09348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuomin Zhang, Elizabeth C. Mansfield, Jia Li, John Russell, George S. Young, Catherine Adams, James Z. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The British landscape painter John Constable is considered foundational for
the Realist movement in 19th-century European painting. Constable's painted
skies, in particular, were seen as remarkably accurate by his contemporaries,
an impression shared by many viewers today. Yet, assessing the accuracy of
realist paintings like Constable's is subjective or intuitive, even for
professional art historians, making it difficult to say with certainty what set
Constable's skies apart from those of his contemporaries. Our goal is to
contribute to a more objective understanding of Constable's realism. We propose
a new machine-learning-based paradigm for studying pictorial realism in an
explainable way. Our framework assesses realism by measuring the similarity
between clouds painted by artists noted for their skies, like Constable, and
photographs of clouds. The experimental results of cloud classification show
that Constable approximates more consistently than his contemporaries the
formal features of actual clouds in his paintings. The study, as a novel
interdisciplinary approach that combines computer vision and machine learning,
meteorology, and art history, is a springboard for broader and deeper analyses
of pictorial realism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplementary materials are available from the authors or
  http://wang.ist.psu.edu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Convolutional Neural Networks for Chronic Obstructive
  Pulmonary Disease Detection in Clinical Computed Tomography Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07189v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07189v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tina Dorosti, Manuel Schultheiss, Felix Hofmann, Johannes Thalhammer, Luisa Kirchner, Theresa Urban, Franz Pfeiffer, Florian Schaff, Tobias Lasser, Daniela Pfeiffer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to optimize the binary detection of Chronic Obstructive Pulmonary
Disease (COPD) based on emphysema presence in the lung with convolutional
neural networks (CNN) by exploring manually adjusted versus automated
window-setting optimization (WSO) on computed tomography (CT) images. 7,194 CT
images (3,597 with COPD; 3,597 healthy controls) from 78 subjects (43 with
COPD; 35 healthy controls) were selected retrospectively (10.2018-12.2019) and
preprocessed. For each image, intensity values were manually clipped to the
emphysema window setting and a baseline 'full-range' window setting.
Class-balanced train, validation, and test sets contained 3,392, 1,114, and
2,688 images. The network backbone was optimized by comparing various CNN
architectures. Furthermore, automated WSO was implemented by adding a
customized layer to the model. The image-level area under the Receiver
Operating Characteristics curve (AUC) [lower, upper limit 95% confidence] was
utilized to compare model variations. Repeated inference (n=7) on the test set
showed that the DenseNet was the most efficient backbone and achieved a mean
AUC of 0.80 [0.76, 0.85] without WSO. Comparably, with input images manually
adjusted to the emphysema window, the DenseNet model predicted COPD with a mean
AUC of 0.86 [0.82, 0.89]. By adding a customized WSO layer to the DenseNet, an
optimal window in the proximity of the emphysema window setting was learned
automatically, and a mean AUC of 0.82 [0.78, 0.86] was achieved. Detection of
COPD with DenseNet models was improved by WSO of CT data to the emphysema
window setting range.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Primitive Simultaneous Optimization of Similarity Metrics for Image
  Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01601v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01601v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diana Waldmannstetter, Benedikt Wiestler, Julian Schwarting, Ivan Ezhov, Marie Metz, Spyridon Bakas, Bhakti Baheti, Satrajit Chakrabarty, Daniel Rueckert, Jan S. Kirschke, Rolf A. Heckemann, Marie Piraud, Bjoern H. Menze, Florian Kofler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even though simultaneous optimization of similarity metrics is a standard
procedure in the field of semantic segmentation, surprisingly, this is much
less established for image registration. To help closing this gap in the
literature, we investigate in a complex multi-modal 3D setting whether
simultaneous optimization of registration metrics, here implemented by means of
primitive summation, can benefit image registration. We evaluate two
challenging datasets containing collections of pre- to post-operative and pre-
to intra-operative MR images of glioma. Employing the proposed optimization, we
demonstrate improved registration accuracy in terms of TRE on expert
neuroradiologists' landmark annotations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparative study of multi-person tracking methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Mbey Akola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a study of two tracking algorithms (SORT~\cite{7533003}
and Tracktor++~\cite{2019}) that were ranked first positions on the MOT
Challenge leaderboard (The MOTChallenge web page: https://motchallenge.net ).
The purpose of this study is to discover the techniques used and to provide
useful insights about these algorithms in the tracking pipeline that could
improve the performance of MOT tracking algorithms. To this end, we adopted the
popular tracking-by-detection approach. We trained our own Pedestrian Detection
model using the MOT17Det dataset (MOT17Det :
https://motchallenge.net/data/MOT17Det/ ). We also used a re-identification
model trained on MOT17 dataset (MOT17 : https://motchallenge.net/data/MOT17/ )
for Tracktor++ to reduce the false re-identification alarms. We then present
experimental results which shows that Tracktor++ is a better multi-person
tracking algorithm than SORT. We also performed ablation studies to discover
the contribution of re-identification(RE-ID) network and motion to the results
of Tracktor++. We finally conclude by providing some recommendations for future
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to effectively train an ensemble of Faster R-CNN object detectors to
  quantify uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Mbey Akola, Gianni Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new approach for training two-stage object detection
ensemble models, more specifically, Faster R-CNN models to estimate
uncertainty. We propose training one Region Proposal
Network(RPN)~\cite{https://doi.org/10.48550/arxiv.1506.01497} and multiple Fast
R-CNN prediction heads is all you need to build a robust deep ensemble network
for estimating uncertainty in object detection. We present this approach and
provide experiments to show that this approach is much faster than the naive
method of fully training all $n$ models in an ensemble. We also estimate the
uncertainty by measuring this ensemble model's Expected Calibration Error
(ECE). We then further compare the performance of this model with that of
Gaussian YOLOv3, a variant of YOLOv3 that models uncertainty using predicted
bounding box coordinates. The source code is released at
\url{https://github.com/Akola-Mbey-Denis/EfficientEnsemble}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OWAdapt: An adaptive loss function for deep learning using OWA operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastián Maldonado, Carla Vairetti, Katherine Jara, Miguel Carrasco, Julio López
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a fuzzy adaptive loss function for enhancing deep
learning performance in classification tasks. Specifically, we redefine the
cross-entropy loss to effectively address class-level noise conditions,
including the challenging problem of class imbalance. Our approach introduces
aggregation operators, leveraging the power of fuzzy logic to improve
classification accuracy. The rationale behind our proposed method lies in the
iterative up-weighting of class-level components within the loss function,
focusing on those with larger errors. To achieve this, we employ the ordered
weighted average (OWA) operator and combine it with an adaptive scheme for
gradient-based learning. Through extensive experimentation, our method
outperforms other commonly used loss functions, such as the standard
cross-entropy or focal loss, across various binary and multiclass
classification tasks. Furthermore, we explore the influence of hyperparameters
associated with the OWA operators and present a default configuration that
performs well across different experimental settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 1 figure, published</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computer Vision Pipeline for Automated Antarctic Krill Analysis <span class="chip">BMVC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mazvydas Gudelis, Michal Mackiewicz, Julie Bremner, Sophie Fielding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  British Antarctic Survey (BAS) researchers launch annual expeditions to the
Antarctic in order to estimate Antarctic Krill biomass and assess the change
from previous years. These comparisons provide insight into the effects of the
current environment on this key component of the marine food chain. In this
work we have developed tools for automating the data collection and analysis
process, using web-based image annotation tools and deep learning image
classification and regression models. We achieve highly accurate krill instance
segmentation results with an average 77.28% AP score, as well as separate
maturity stage and length estimation of krill specimens with 62.99% accuracy
and a 1.98mm length error respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MVEO @ BMVC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-time Strawberry Detection Based on Improved YOLOv5s Architecture
  for Robotic Harvesting in open-field environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03998v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03998v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan He, Salik Ram Khanal, Xin Zhang, Manoj Karkee, Qin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposed a YOLOv5-based custom object detection model to detect
strawberries in an outdoor environment. The original architecture of the
YOLOv5s was modified by replacing the C3 module with the C2f module in the
backbone network, which provided a better feature gradient flow. Secondly, the
Spatial Pyramid Pooling Fast in the final layer of the backbone network of
YOLOv5s was combined with Cross Stage Partial Net to improve the generalization
ability over the strawberry dataset in this study. The proposed architecture
was named YOLOv5s-Straw. The RGB images dataset of the strawberry canopy with
three maturity classes (immature, nearly mature, and mature) was collected in
open-field environment and augmented through a series of operations including
brightness reduction, brightness increase, and noise adding. To verify the
superiority of the proposed method for strawberry detection in open-field
environment, four competitive detection models (YOLOv3-tiny, YOLOv5s,
YOLOv5s-C2f, and YOLOv8s) were trained, and tested under the same computational
environment and compared with YOLOv5s-Straw. The results showed that the
highest mean average precision of 80.3% was achieved using the proposed
architecture whereas the same was achieved with YOLOv3-tiny, YOLOv5s,
YOLOv5s-C2f, and YOLOv8s were 73.4%, 77.8%, 79.8%, 79.3%, respectively.
Specifically, the average precision of YOLOv5s-Straw was 82.1% in the immature
class, 73.5% in the nearly mature class, and 86.6% in the mature class, which
were 2.3% and 3.7%, respectively, higher than that of the latest YOLOv8s. The
model included 8.6*10^6 network parameters with an inference speed of 18ms per
image while the inference speed of YOLOv8s had a slower inference speed of
21.0ms and heavy parameters of 11.1*10^6, which indicates that the proposed
model is fast enough for real time strawberry detection and localization for
the robotic picking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages; 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazmus Sakib Ahmed, Saad Sakib Noor, Ashraful Islam Shanto Sikder, Abhijit Paul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using
the YOLOv8 model and innovative post-processing techniques. We tackle
challenges unique to the complex Bengali script by employing data augmentation
for model robustness. After meticulous validation set evaluation, we fine-tune
our approach on the complete dataset, leading to a two-stage prediction
strategy for accurate element segmentation. Our ensemble model, combined with
post-processing, outperforms individual base architectures, addressing issues
identified in the BaDLAD dataset. By leveraging this approach, we aim to
advance Bengali document analysis, contributing to improved OCR and document
comprehension and BaDLAD serves as a foundational resource for this endeavor,
aiding future research in the field. Furthermore, our experiments provided key
insights to incorporate new strategies into the established solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arief Purnama Muharram, Hollyana Puteri Haryono, Abassi Haji Juma, Ira Puspasari, Nugraha Priya Utama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reading and interpreting chest X-ray images is one of the most radiologist's
routines. However, it still can be challenging, even for the most experienced
ones. Therefore, we proposed a multi-model deep learning-based automated chest
X-ray report generator system designed to assist radiologists in their work.
The basic idea of the proposed system is by utilizing multi
binary-classification models for detecting multi abnormalities, with each model
responsible for detecting one abnormality, in a single image. In this study, we
limited the radiology abnormalities detection to only cardiomegaly, lung
effusion, and consolidation. The system generates a radiology report by
performing the following three steps: image pre-processing, utilizing deep
learning models to detect abnormalities, and producing a report. The aim of the
image pre-processing step is to standardize the input by scaling it to 128x128
pixels and slicing it into three segments, which covers the upper, lower, and
middle parts of the lung. After pre-processing, each corresponding model
classifies the image, resulting in a 0 (zero) for no abnormality detected and a
1 (one) for the presence of an abnormality. The prediction outputs of each
model are then concatenated to form a 'result code'. The 'result code' is used
to construct a report by selecting the appropriate pre-determined sentence for
each detected abnormality in the report generation step. The proposed system is
expected to reduce the workload of radiologists and increase the accuracy of
chest X-ray diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented in the 2023 IEEE International Conference on Data and
  Software Engineering (ICoDSE 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedDrive v2: an Analysis of the Impact of Label Skewness in Federated
  Semantic Segmentation for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eros Fanì, Marco Ciccone, Barbara Caputo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose FedDrive v2, an extension of the Federated Learning benchmark for
Semantic Segmentation in Autonomous Driving. While the first version aims at
studying the effect of domain shift of the visual features across clients, in
this work, we focus on the distribution skewness of the labels. We propose six
new federated scenarios to investigate how label skewness affects the
performance of segmentation models and compare it with the effect of domain
shift. Finally, we study the impact of using the domain information during
testing. Official website: https://feddrive.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5th Italian Conference on Robotics and Intelligent Machines (I-RIM)
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Wang Bian, Wenjing Bian, Victor Adrian Prisacariu, Philip Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural surface reconstruction is sensitive to the camera pose noise, even if
state-of-the-art pose estimators like COLMAP or ARKit are used. More
importantly, existing Pose-NeRF joint optimisation methods have struggled to
improve pose accuracy in challenging real-world scenarios. To overcome the
challenges, we introduce the pose residual field (\textbf{PoRF}), a novel
implicit representation that uses an MLP for regressing pose updates. This is
more robust than the conventional pose parameter optimisation due to parameter
sharing that leverages global information over the entire sequence.
Furthermore, we propose an epipolar geometry loss to enhance the supervision
that leverages the correspondences exported from COLMAP results without the
extra computational overhead. Our method yields promising results. On the DTU
dataset, we reduce the rotation error by 78\% for COLMAP poses, leading to the
decreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the
MobileBrick dataset that contains casually captured unbounded 360-degree
videos, our method refines ARKit poses and improves the reconstruction F1 score
from 69.18 to 75.67, outperforming that with the dataset provided ground-truth
pose (75.14). These achievements demonstrate the efficacy of our approach in
refining camera poses and improving the accuracy of neural surface
reconstruction in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spiking Denoising Diffusion Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17046v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17046v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahang Cao, Ziqing Wang, Hanzhong Guo, Hao Cheng, Qiang Zhang, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs) have ultra-low energy consumption and high
biological plausibility due to their binary and bio-driven nature compared with
artificial neural networks (ANNs). While previous research has primarily
focused on enhancing the performance of SNNs in classification tasks, the
generative potential of SNNs remains relatively unexplored. In our paper, we
put forward Spiking Denoising Diffusion Probabilistic Models (SDDPM), a new
class of SNN-based generative models that achieve high sample quality. To fully
exploit the energy efficiency of SNNs, we propose a purely Spiking U-Net
architecture, which achieves comparable performance to its ANN counterpart
using only 4 time steps, resulting in significantly reduced energy consumption.
Extensive experimental results reveal that our approach achieves
state-of-the-art on the generative tasks and substantially outperforms other
SNN-based generative models, achieving up to $12\times$ and $6\times$
improvement on the CIFAR-10 and the CelebA datasets, respectively. Moreover, we
propose a threshold-guided strategy that can further improve the performances
by 16.7% in a training-free manner. The SDDPM symbolizes a significant
advancement in the field of SNN generation, injecting new perspectives and
potential avenues of exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Transformers: From Semantic Segmentation to Dense Prediction <span class="chip">CVPR 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09339v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09339v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhang, Jiachen Lu, Sixiao Zheng, Xinxuan Zhao, Xiatian Zhu, Yanwei Fu, Tao Xiang, Jianfeng Feng, Philip H. S. Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of vision transformers (ViTs) in image classification has
shifted the methodologies for visual representation learning. In particular,
ViTs learn visual representation at full receptive field per layer across all
the image patches, in comparison to the increasing receptive fields of CNNs
across layers and other alternatives (e.g., large kernels and atrous
convolution). In this work, for the first time we explore the global context
learning potentials of ViTs for dense visual prediction (e.g., semantic
segmentation). Our motivation is that through learning global context at full
receptive field layer by layer, ViTs may capture stronger long-range dependency
information, critical for dense prediction tasks. We first demonstrate that
encoding an image as a sequence of patches, a vanilla ViT without local
convolution and resolution reduction can yield stronger visual representation
for semantic segmentation. For example, our model, termed as SEgmentation
TRansformer (SETR), excels on ADE20K (50.28% mIoU, the first position in the
test leaderboard on the day of submission) and Pascal Context (55.83% mIoU),
and performs competitively on Cityscapes. For tackling general dense visual
prediction tasks in a cost-effective manner, we further formulate a family of
Hierarchical Local-Global (HLG) Transformers, characterized by local attention
within windows and global-attention across windows in a pyramidal architecture.
Extensive experiments show that our methods achieve appealing performance on a
variety of dense prediction tasks (e.g., object detection and instance
segmentation and semantic segmentation) as well as image classification. Our
code and models are available at https://github.com/fudan-zvg/SETR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of CVPR 2021 paper arXiv:2012.15840</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pushing the Limits of Fewshot Anomaly Detection in Industry Vision:
  Graphcore 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12082v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12082v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyang Xie, Jinbao Wang, Jiaqi Liu, Feng Zheng, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the area of fewshot anomaly detection (FSAD), efficient visual feature
plays an essential role in memory bank M-based methods. However, these methods
do not account for the relationship between the visual feature and its rotated
visual feature, drastically limiting the anomaly detection performance. To push
the limits, we reveal that rotation-invariant feature property has a
significant impact in industrial-based FSAD. Specifically, we utilize graph
representation in FSAD and provide a novel visual isometric invariant feature
(VIIF) as anomaly measurement feature. As a result, VIIF can robustly improve
the anomaly discriminating ability and can further reduce the size of redundant
features stored in M by a large amount. Besides, we provide a novel model
GraphCore via VIIFs that can fast implement unsupervised FSAD training and can
improve the performance of anomaly detection. A comprehensive evaluation is
provided for comparing GraphCore and other SOTA anomaly detection models under
our proposed fewshot anomaly detection setting, which shows GraphCore can
increase average AUC by 5.8%, 4.1%, 3.4%, and 1.6% on MVTec AD and by 25.5%,
22.0%, 16.9%, and 14.1% on MPDD for 1, 2, 4, and 8-shot cases, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locate before Answering: Answer Guided Question Localization for Video
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.02081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.02081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianwen Qian, Ran Cui, Jingjing Chen, Pai Peng, Xiaowei Guo, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video question answering (VideoQA) is an essential task in vision-language
understanding, which has attracted numerous research attention recently.
Nevertheless, existing works mostly achieve promising performances on short
videos of duration within 15 seconds. For VideoQA on minute-level long-term
videos, those methods are likely to fail because of lacking the ability to deal
with noise and redundancy caused by scene changes and multiple actions in the
video. Considering the fact that the question often remains concentrated in a
short temporal range, we propose to first locate the question to a segment in
the video and then infer the answer using the located segment only. Under this
scheme, we propose "Locate before Answering" (LocAns), a novel approach that
integrates a question locator and an answer predictor into an end-to-end model.
During the training phase, the available answer label not only serves as the
supervision signal of the answer predictor, but also is used to generate pseudo
temporal labels for the question locator. Moreover, we design a decoupled
alternative training strategy to update the two modules separately. In the
experiments, LocAns achieves state-of-the-art performance on two modern
long-term VideoQA datasets NExT-QA and ActivityNet-QA, and its qualitative
examples show the reliable performance of the question localization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Industrial Image Anomaly Detection: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11514v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11514v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Liu, Guoyang Xie, Jinbao Wang, Shangnian Li, Chengjie Wang, Feng Zheng, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent rapid development of deep learning has laid a milestone in
industrial Image Anomaly Detection (IAD). In this paper, we provide a
comprehensive review of deep learning-based image anomaly detection techniques,
from the perspectives of neural network architectures, levels of supervision,
loss functions, metrics and datasets. In addition, we extract the new setting
from industrial manufacturing and review the current IAD approaches under our
proposed our new setting. Moreover, we highlight several opening challenges for
image anomaly detection. The merits and downsides of representative network
architectures under varying supervision are discussed. Finally, we summarize
the research findings and point out future research directions. More resources
are available at
https://github.com/M-3LAB/awesome-industrial-anomaly-detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13359v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13359v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyang Xie, Jinbao Wang, Jiaqi Liu, Jiayi Lyu, Yong Liu, Chengjie Wang, Feng Zheng, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image anomaly detection (IAD) is an urgent issue that needs to be addressed
in modern industrial manufacturing (IM). Recently, many advanced algorithms
have been released, but their performance varies greatly due to non-uniformed
settings. That is, researchers find it difficult to analyze because they are
designed for different or specific cases in IM. To eliminate this problem, we
first propose a uniform IAD setting to systematically assess the effectiveness
of these algorithms, mainly considering three aspects of supervision level
(unsupervised, fully supervised), learning paradigm (few-shot, continual, noisy
label), and efficiency (memory usage, inference speed). Then, we skillfully
construct a comprehensive image anomaly detection benchmark (IM-IAD), which
includes 19 algorithms on 7 major datasets with the same setting. Our extensive
experiments (17,017 total) provide new insights into the redesign or selection
of the IAD algorithm under uniform conditions. Importantly, the proposed IM-IAD
presents feasible challenges and future directions for further work. We believe
that this work can have a significant impact on the IAD field. To foster
reproducibility and accessibility, the source code of IM-IAD is uploaded on the
website, https://github.com/M-3LAB/IM-IAD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RFAConv: Innovating Spatial Attention and Standard Convolutional
  Operation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03198v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03198v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Chen Liu, Degang Yang, Tingting Song, Yichen Ye, Ke Li, Yingze Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial attention has been widely used to improve the performance of
convolutional neural networks. However, it has certain limitations. In this
paper, we propose a new perspective on the effectiveness of spatial attention,
which is that the spatial attention mechanism essentially solves the problem of
convolutional kernel parameter sharing. However, the information contained in
the attention map generated by spatial attention is not sufficient for
large-size convolutional kernels. Therefore, we propose a novel attention
mechanism called Receptive-Field Attention (RFA). Existing spatial attention,
such as Convolutional Block Attention Module (CBAM) and Coordinated Attention
(CA) focus only on spatial features, which does not fully address the problem
of convolutional kernel parameter sharing. In contrast, RFA not only focuses on
the receptive-field spatial feature but also provides effective attention
weights for large-size convolutional kernels. The Receptive-Field Attention
convolutional operation (RFAConv), developed by RFA, represents a new approach
to replace the standard convolution operation. It offers nearly negligible
increment of computational cost and parameters, while significantly improving
network performance. We conducted a series of experiments on ImageNet-1k, COCO,
and VOC datasets to demonstrate the superiority of our approach. Of particular
importance, we believe that it is time to shift focus from spatial features to
receptive-field spatial features for current spatial attention mechanisms. In
this way, we can further improve network performance and achieve even better
results. The code and pre-trained models for the relevant tasks can be found at
https://github.com/Liuchen1997/RFAConv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3DDesigner: Towards Photorealistic 3D Object Generation and Editing with
  Text-guided Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14108v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14108v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gang Li, Heliang Zheng, Chaoyue Wang, Chang Li, Changwen Zheng, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided diffusion models have shown superior performance in image/video
generation and editing. While few explorations have been performed in 3D
scenarios. In this paper, we discuss three fundamental and interesting problems
on this topic. First, we equip text-guided diffusion models to achieve
3D-consistent generation. Specifically, we integrate a NeRF-like neural field
to generate low-resolution coarse results for a given camera view. Such results
can provide 3D priors as condition information for the following diffusion
process. During denoising diffusion, we further enhance the 3D consistency by
modeling cross-view correspondences with a novel two-stream (corresponding to
two different views) asynchronous diffusion process. Second, we study 3D local
editing and propose a two-step solution that can generate 360-degree
manipulated results by editing an object from a single view. Step 1, we propose
to perform 2D local editing by blending the predicted noises. Step 2, we
conduct a noise-to-text inversion process that maps 2D blended noises into the
view-independent text embedding space. Once the corresponding text embedding is
obtained, 360-degree images can be generated. Last but not least, we extend our
model to perform one-shot novel view synthesis by fine-tuning on a single
image, firstly showing the potential of leveraging text guidance for novel view
synthesis. Extensive experiments and various applications show the prowess of
our 3DDesigner. The project page is available at
https://3ddesigner-diffusion.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IJCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Pre-trained Networks Detect Familiar Out-of-Distribution Data? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsuyuki Miyai, Qing Yu, Go Irie, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is critical for safety-sensitive machine
learning applications and has been extensively studied, yielding a plethora of
methods developed in the literature. However, most studies for OOD detection
did not use pre-trained models and trained a backbone from scratch. In recent
years, transferring knowledge from large pre-trained models to downstream tasks
by lightweight tuning has become mainstream for training in-distribution (ID)
classifiers. To bridge the gap between the practice of OOD detection and
current classifiers, the unique and crucial problem is that the samples whose
information networks know often come as OOD input. We consider that such data
may significantly affect the performance of large pre-trained networks because
the discriminability of these OOD data depends on the pre-training algorithm.
Here, we define such OOD data as PT-OOD (Pre-Trained OOD) data. In this paper,
we aim to reveal the effect of PT-OOD on the OOD detection performance of
pre-trained networks from the perspective of pre-training algorithms. To
achieve this, we explore the PT-OOD detection performance of supervised and
self-supervised pre-training algorithms with linear-probing tuning, the most
common efficient tuning method. Through our experiments and analysis, we find
that the low linear separability of PT-OOD in the feature space heavily
degrades the PT-OOD detection performance, and self-supervised models are more
vulnerable to PT-OOD than supervised pre-trained models, even with
state-of-the-art detection methods. To solve this vulnerability, we further
propose a unique solution to large-scale pre-trained models: Leveraging
powerful instance-by-instance discriminative representations of pre-trained
models and detecting OOD in the feature space independent of the ID decision
boundaries. The code will be available via https://github.com/AtsuMiyai/PT-OOD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S4C: Self-Supervised Semantic Scene Completion with Neural Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Hayler, Felix Wimbauer, Dominik Muhle, Christian Rupprecht, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D semantic scene understanding is a fundamental challenge in computer
vision. It enables mobile agents to autonomously plan and navigate arbitrary
environments. SSC formalizes this challenge as jointly estimating dense
geometry and semantic information from sparse observations of a scene. Current
methods for SSC are generally trained on 3D ground truth based on aggregated
LiDAR scans. This process relies on special sensors and annotation by hand
which are costly and do not scale well. To overcome this issue, our work
presents the first self-supervised approach to SSC called S4C that does not
rely on 3D ground truth data. Our proposed method can reconstruct a scene from
a single image and only relies on videos and pseudo segmentation ground truth
generated from off-the-shelf image segmentation network during training. Unlike
existing methods, which use discrete voxel grids, we represent scenes as
implicit semantic fields. This formulation allows querying any point within the
camera frustum for occupancy and semantic class. Our architecture is trained
through rendering-based self-supervised losses. Nonetheless, our method
achieves performance close to fully supervised state-of-the-art methods.
Additionally, our method demonstrates strong generalization capabilities and
can synthesize accurate segmentation maps for far away viewpoints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Object Detection with Large Vision Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09408v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09408v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Lin, Wenze Hu, Yaowei Wang, Yonghong Tian, Guangming Lu, Fanglin Chen, Yong Xu, Xiaoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past few years, there has been growing interest in developing a
broad, universal, and general-purpose computer vision system. Such systems have
the potential to address a wide range of vision tasks simultaneously, without
being limited to specific problems or data domains. This universality is
crucial for practical, real-world computer vision applications. In this study,
our focus is on a specific challenge: the large-scale, multi-domain universal
object detection problem, which contributes to the broader goal of achieving a
universal vision system. This problem presents several intricate challenges,
including cross-dataset category label duplication, label conflicts, and the
necessity to handle hierarchical taxonomies. To address these challenges, we
introduce our approach to label handling, hierarchy-aware loss design, and
resource-efficient model training utilizing a pre-trained large vision model.
Our method has demonstrated remarkable performance, securing a prestigious
second-place ranking in the object detection track of the Robust Vision
Challenge 2022 (RVC 2022) on a million-scale cross-dataset object detection
benchmark. We believe that our comprehensive study will serve as a valuable
reference and offer an alternative approach for addressing similar challenges
within the computer vision community. The source code for our work is openly
available at https://github.com/linfeng93/Large-UniDet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Journal of Computer Vision (IJCV). The 2nd
  place in the object detection track of the Robust Vision Challenge (RVC 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finite Scalar Quantization: VQ-VAE Made Simple 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Mentzer, David Minnen, Eirikur Agustsson, Michael Tschannen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to replace vector quantization (VQ) in the latent representation
of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where
we project the VAE representation down to a few dimensions (typically less than
10). Each dimension is quantized to a small set of fixed values, leading to an
(implicit) codebook given by the product of these sets. By appropriately
choosing the number of dimensions and values each dimension can take, we obtain
the same codebook size as in VQ. On top of such discrete representations, we
can train the same models that have been trained on VQ-VAE representations. For
example, autoregressive and masked transformer models for image generation,
multimodal generation, and dense prediction computer vision tasks. Concretely,
we employ FSQ with MaskGIT for image generation, and with UViM for depth
estimation, colorization, and panoptic segmentation. Despite the much simpler
design of FSQ, we obtain competitive performance in all these tasks. We
emphasize that FSQ does not suffer from codebook collapse and does not need the
complex machinery employed in VQ (commitment losses, codebook reseeding, code
splitting, entropy penalties, etc.) to learn expressive discrete
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code:
  https://github.com/google-research/google-research/tree/master/fsq</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Single-View Images for Unsupervised 3D Point Cloud Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lintai Wu, Qijian Zhang, Junhui Hou, Yong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point clouds captured by scanning devices are often incomplete due to
occlusion. To overcome this limitation, point cloud completion methods have
been developed to predict the complete shape of an object based on its partial
input. These methods can be broadly classified as supervised or unsupervised.
However, both categories require a large number of 3D complete point clouds,
which may be difficult to capture. In this paper, we propose Cross-PCC, an
unsupervised point cloud completion method without requiring any 3D complete
point clouds. We only utilize 2D images of the complete objects, which are
easier to capture than 3D complete and clean point clouds. Specifically, to
take advantage of the complementary information from 2D images, we use a
single-view RGB image to extract 2D features and design a fusion module to fuse
the 2D and 3D features extracted from the partial point cloud. To guide the
shape of predicted point clouds, we project the predicted points of the object
to the 2D plane and use the foreground pixels of its silhouette maps to
constrain the position of the projected points. To reduce the outliers of the
predicted point clouds, we propose a view calibrator to move the points
projected to the background into the foreground by the single-view silhouette
image. To the best of our knowledge, our approach is the first point cloud
completion method that does not require any 3D supervision. The experimental
results of our method are superior to those of the state-of-the-art
unsupervised methods by a large margin. Moreover, our method even achieves
comparable performance to some supervised methods. We will make the source code
publicly available at https://github.com/ltwu6/cross-pcc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Survey of Computer Vision Technologies In Urban and
  Controlled-environment Agriculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayun Luo, Boyang Li, Cyril Leung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the evolution of agriculture to its next stage, Agriculture 5.0,
artificial intelligence will play a central role. Controlled-environment
agriculture, or CEA, is a special form of urban and suburban agricultural
practice that offers numerous economic, environmental, and social benefits,
including shorter transportation routes to population centers, reduced
environmental impact, and increased productivity. Due to its ability to control
environmental factors, CEA couples well with computer vision (CV) in the
adoption of real-time monitoring of the plant conditions and autonomous
cultivation and harvesting. The objective of this paper is to familiarize CV
researchers with agricultural applications and agricultural practitioners with
the solutions offered by CV. We identify five major CV applications in CEA,
analyze their requirements and motivation, and survey the state of the art as
reflected in 68 technical papers using deep learning methods. In addition, we
discuss five key subareas of computer vision and how they related to these CEA
problems, as well as eleven vision-based CEA datasets. We hope the survey will
help researchers quickly gain a bird-eye view of the striving research area and
will spark inspiration for new research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1 overview figures, 37 pages, 8 tables, accepted by ACM Computing
  Surveys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tightly-Coupled LiDAR-Visual <span class="highlight-title">SLAM</span> Based on Geometric Features for Mobile
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Cao, Ruiping Liu, Ze Wang, Kunyu Peng, Jiaming Zhang, Junwei Zheng, Zhifeng Teng, Kailun Yang, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to
provide autonomous navigation and task execution in complex and unknown
environments. However, it is hard to develop a dedicated algorithm for mobile
robots due to dynamic and challenging situations, such as poor lighting
conditions and motion blur. To tackle this issue, we propose a tightly-coupled
LiDAR-visual SLAM based on geometric features, which includes two sub-systems
(LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework
associates the depth and semantics of the multi-modal geometric features to
complement the visual line landmarks and to add direction optimization in
Bundle Adjustment (BA). This further constrains visual odometry. On the other
hand, the entire line segment detected by the visual subsystem overcomes the
limitation of the LiDAR subsystem, which can only perform the local calculation
for geometric features. It adjusts the direction of linear feature points and
filters out outliers, leading to a higher accurate odometry system. Finally, we
employ a module to detect the subsystem's operation, providing the LiDAR
subsystem's output as a complementary trajectory to our system while visual
subsystem tracking fails. The evaluation results on the public dataset M2DGR,
gathered from ground robots across various indoor and outdoor scenarios, show
that our system achieves more accurate and robust pose estimation compared to
current state-of-the-art multi-modal methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ROBIO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Imbalanced Regression: Fair Uncertainty Quantification via
  Probabilistic Smoothing <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06599v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06599v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Wang, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing regression models tend to fall short in both accuracy and
uncertainty estimation when the label distribution is imbalanced. In this
paper, we propose a probabilistic deep learning model, dubbed variational
imbalanced regression (VIR), which not only performs well in imbalanced
regression but naturally produces reasonable uncertainty estimation as a
byproduct. Different from typical variational autoencoders assuming I.I.D.
representations (a data point's representation is not directly affected by
other data points), our VIR borrows data with similar regression labels to
compute the latent representation's variational distribution; furthermore,
different from deterministic regression models producing point estimates, VIR
predicts the entire normal-inverse-gamma distributions and modulates the
associated conjugate distributions to impose probabilistic reweighting on the
imbalanced data, thereby providing better uncertainty estimation. Experiments
in several real-world datasets show that our VIR can outperform
state-of-the-art imbalanced regression models in terms of both accuracy and
uncertainty estimation. Code will soon be available at
\url{https://github.com/Wang-ML-Lab/variational-imbalanced-regression}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point
  Cloud Semantic Segmentation <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08594v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08594v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Yu, Meida Chen, Zhikang Zhang, Suya You, Raghuveer Rao, Sanjeev Agarwal, Fengbo Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Common image-based LiDAR point cloud semantic segmentation (LiDAR PCSS)
approaches have bottlenecks resulting from the boundary-blurring problem of
convolution neural networks (CNNs) and quantitation loss of spherical
projection. In this work, we propose a transformer-based plug-and-play
uncertain point refiner, i.e., TransUPR, to refine selected uncertain points in
a learnable manner, which leads to an improved segmentation performance.
Uncertain points are sampled from coarse semantic segmentation results of 2D
image segmentation where uncertain points are located close to the object
boundaries in the 2D range image representation and 3D spherical projection
background points. Following that, the geometry and coarse semantic features of
uncertain points are aggregated by neighbor points in 3D space without adding
expensive computation and memory footprint. Finally, the transformer-based
refiner, which contains four stacked self-attention layers, along with an MLP
module, is utilized for uncertain point classification on the concatenated
features of self-attention layers. As the proposed refiner is independent of 2D
CNNs, our TransUPR can be easily integrated into any existing image-based LiDAR
PCSS approaches, e.g., CENet. Our TransUPR with the CENet achieves
state-of-the-art performance, i.e., 68.2% mean Intersection over Union (mIoU)
on the Semantic KITTI benchmark, which provides a performance improvement of
0.6% on the mIoU compared to the original CENet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages; Accepted by 2023 IROS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locality-Aware Generalizable Implicit Neural Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05624v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05624v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doyup Lee, Chiheon Kim, Minsu Cho, Wook-Shin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizable implicit neural representation (INR) enables a single
continuous function, i.e., a coordinate-based neural network, to represent
multiple data instances by modulating its weights or intermediate features
using latent codes. However, the expressive power of the state-of-the-art
modulation is limited due to its inability to localize and capture fine-grained
details of data entities such as specific pixels and rays. To address this
issue, we propose a novel framework for generalizable INR that combines a
transformer encoder with a locality-aware INR decoder. The transformer encoder
predicts a set of latent tokens from a data instance to encode local
information into each latent token. The locality-aware INR decoder extracts a
modulation vector by selectively aggregating the latent tokens via
cross-attention for a coordinate input and then predicts the output by
progressively decoding with coarse-to-fine modulation through multiple
frequency bandwidths. The selective token aggregation and the multi-band
feature modulation enable us to learn locality-aware representation in spatial
and spectral aspects, respectively. Our framework significantly outperforms
previous generalizable INRs and validates the usefulness of the locality-aware
latents for downstream tasks such as image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on
  Neuro-Symbolic Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15889v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15889v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenguan Wang, Yi Yang, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural-symbolic computing (NeSy), which pursues the integration of the
symbolic and statistical paradigms of cognition, has been an active research
area of Artificial Intelligence (AI) for many years. As NeSy shows promise of
reconciling the advantages of reasoning and interpretability of symbolic
representation and robust learning in neural networks, it may serve as a
catalyst for the next generation of AI. In the present paper, we provide a
systematic overview of the recent developments and important contributions of
NeSy research. Firstly, we introduce study history of this area, covering early
work and foundations. We further discuss background concepts and identify key
driving factors behind the development of NeSy. Afterward, we categorize recent
landmark approaches along several main characteristics that underline this
research paradigm, including neural-symbolic integration, knowledge
representation, knowledge embedding, and functionality. Next, we briefly
discuss the successful application of modern NeSy approaches in several
domains. Then, we benchmark several NeSy methods on three representative
application tasks. Finally, we identify the open problems together with
potential future research directions. This survey is expected to help new
researchers enter this rapidly evolving field and accelerate the progress
towards data-and knowledge-driven AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing project</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS
  Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohong Fan, Yin Yang, Ke Chen, Yujie Feng, Jianping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proximal gradient-based optimization is one of the most common strategies to
solve inverse problem of images, and it is easy to implement. However, these
techniques often generate heavy artifacts in image reconstruction. One of the
most popular refinement methods is to fine-tune the regularization parameter to
alleviate such artifacts, but it may not always be sufficient or applicable due
to increased computational costs. In this work, we propose a deep geometric
incremental learning framework based on the second Nesterov proximal gradient
optimization. The proposed end-to-end network not only has the powerful
learning ability for high-/low-frequency image features, but also can
theoretically guarantee that geometric texture details will be reconstructed
from preliminary linear reconstruction. Furthermore, it can avoid the risk of
intermediate reconstruction results falling outside the geometric decomposition
domains and achieve fast convergence. Our reconstruction framework is
decomposed into four modules including general linear reconstruction, cascade
geometric incremental restoration, Nesterov acceleration, and post-processing.
In the image restoration step, a cascade geometric incremental learning module
is designed to compensate for missing texture information from different
geometric spectral decomposition domains. Inspired by the overlap-tile
strategy, we also develop a post-processing module to remove the block effect
in patch-wise-based natural image reconstruction. All parameters in the
proposed model are learnable, an adaptive initialization technique of physical
parameters is also employed to make model flexibility and ensure converging
smoothly. We compare the reconstruction performance of the proposed method with
existing state-of-the-art methods to demonstrate its superiority. Our source
codes are available at https://github.com/fanxiaohong/Nest-DGIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,our source codes are available at
  https://github.com/fanxiaohong/Nest-DGIL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06488v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06488v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlong Li, Wenhao Liu, Changze Lv, Jianhan Xu, Cenyuan Zhang, Muling Wu, Xiaoqing Zheng, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs) have demonstrated the capability to achieve
comparable performance to deep neural networks (DNNs) in both visual and
linguistic domains while offering the advantages of improved energy efficiency
and adherence to biological plausibility. However, the extension of such
single-modality SNNs into the realm of multimodal scenarios remains an
unexplored territory. Drawing inspiration from the concept of contrastive
language-image pre-training (CLIP), we introduce a novel framework, named
SpikeCLIP, to address the gap between two modalities within the context of
spike-based computing through a two-step recipe involving ``Alignment
Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that
SNNs achieve comparable results to their DNN counterparts while significantly
reducing energy consumption across a variety of datasets commonly used for
multimodal model evaluation. Furthermore, SpikeCLIP maintains robust
performance in image classification tasks that involve class labels not
predefined within specific categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LF-VI<span class="highlight-title">SLAM</span>: A <span class="highlight-title">SLAM</span> Framework for Large Field-of-View Cameras with
  Negative Imaging Plane on Mobile Agents <span class="chip">IROS2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05167v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05167v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ze Wang, Kailun Yang, Hao Shi, Peng Li, Fei Gao, Jian Bai, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization And Mapping (SLAM) has become a crucial aspect in
the fields of autonomous driving and robotics. One crucial component of visual
SLAM is the Field-of-View (FoV) of the camera, as a larger FoV allows for a
wider range of surrounding elements and features to be perceived. However, when
the FoV of the camera reaches the negative half-plane, traditional methods for
representing image feature points using [u,v,1]^T become ineffective. While the
panoramic FoV is advantageous for loop closure, its benefits are not easily
realized under large-attitude-angle differences where loop-closure frames
cannot be easily matched by existing methods. As loop closure on wide-FoV
panoramic data further comes with a large number of outliers, traditional
outlier rejection methods are not directly applicable. To address these issues,
we propose LF-VISLAM, a Visual Inertial SLAM framework for cameras with
extremely Large FoV with loop closure. A three-dimensional vector with unit
length is introduced to effectively represent feature points even on the
negative half-plane. The attitude information of the SLAM system is leveraged
to guide the feature point detection of the loop closure. Additionally, a new
outlier rejection method based on the unit length representation is integrated
into the loop closure module. We collect the PALVIO dataset using a Panoramic
Annular Lens (PAL) system with an entire FoV of 360{\deg}x(40{\deg}~120{\deg})
and an Inertial Measurement Unit (IMU) for Visual Inertial Odometry (VIO) to
address the lack of panoramic SLAM datasets. Experiments on the established
PALVIO and public datasets show that the proposed LF-VISLAM outperforms
state-of-the-art SLAM methods. Our code will be open-sourced at
https://github.com/flysoaryun/LF-VISLAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Automation Science and Engineering
  (T-ASE). Extended version of IROS2022 paper arXiv:2202.12613. Code and
  dataset will be open-sourced at https://github.com/flysoaryun/LF-SLAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Effective Motion-Centric Paradigm for 3D Single Object Tracking in
  Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoda Zheng, Xu Yan, Haiming Zhang, Baoyuan Wang, Shenghui Cheng, Shuguang Cui, Zhen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D single object tracking in LiDAR point clouds (LiDAR SOT) plays a crucial
role in autonomous driving. Current approaches all follow the Siamese paradigm
based on appearance matching. However, LiDAR point clouds are usually
textureless and incomplete, which hinders effective appearance matching.
Besides, previous methods greatly overlook the critical motion clues among
targets. In this work, beyond 3D Siamese tracking, we introduce a
motion-centric paradigm to handle LiDAR SOT from a new perspective. Following
this paradigm, we propose a matching-free two-stage tracker M^2-Track. At the
1st-stage, M^2-Track localizes the target within successive frames via motion
transformation. Then it refines the target box through motion-assisted shape
completion at the 2nd-stage. Due to the motion-centric nature, our method shows
its impressive generalizability with limited training labels and provides good
differentiability for end-to-end cycle training. This inspires us to explore
semi-supervised LiDAR SOT by incorporating a pseudo-label-based motion
augmentation and a self-supervised loss term. Under the fully-supervised
setting, extensive experiments confirm that M^2-Track significantly outperforms
previous state-of-the-arts on three large-scale datasets while running at 57FPS
(~3%, ~11% and ~22% precision gains on KITTI, NuScenes, and Waymo Open Dataset
respectively). While under the semi-supervised setting, our method performs on
par with or even surpasses its fully-supervised counterpart using fewer than
half of the labels from KITTI. Further analysis verifies each component's
effectiveness and shows the motion-centric paradigm's promising potential for
auto-labeling and unsupervised domain adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted version of the journal extension of M^2-Track. Accepted by
  TPAMI. arXiv admin note: substantial text overlap with arXiv:2203.01730</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised
  Domain Adaptation <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarun Kalluri, Astuti Sharma, Manmohan Chandraker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practical real world datasets with plentiful categories introduce new
challenges for unsupervised domain adaptation like small inter-class
discriminability, that existing approaches relying on domain invariance alone
cannot handle sufficiently well. In this work we propose MemSAC, which exploits
sample level similarity across source and target domains to achieve
discriminative transfer, along with architectures that scale to a large number
of categories. For this purpose, we first introduce a memory augmented approach
to efficiently extract pairwise similarity relations between labeled source and
unlabeled target domain instances, suited to handle an arbitrary number of
classes. Next, we propose and theoretically justify a novel variant of the
contrastive loss to promote local consistency among within-class cross domain
samples while enforcing separation between classes, thus preserving
discriminative transfer from source to target. We validate the advantages of
MemSAC with significant improvements over previous state-of-the-art on multiple
challenging transfer tasks designed for large-scale adaptation, such as
DomainNet with 345 classes and fine-grained adaptation on Caltech-UCSD birds
dataset with 200 classes. We also provide in-depth analysis and insights into
the effectiveness of MemSAC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022. Project Webpage:
  https://tarun005.github.io/MemSAC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Event-based Temporally Dense Optical Flow Estimation with Sequential
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01244v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01244v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wachirawit Ponghiran, Chamika Mihiranga Liyanagedera, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras provide an advantage over traditional frame-based cameras when
capturing fast-moving objects without a motion blur. They achieve this by
recording changes in light intensity (known as events), thus allowing them to
operate at a much higher frequency and making them suitable for capturing
motions in a highly dynamic scene. Many recent studies have proposed methods to
train neural networks (NNs) for predicting optical flow from events. However,
they often rely on a spatio-temporal representation constructed from events
over a fixed interval, such as 10Hz used in training on the DSEC dataset. This
limitation restricts the flow prediction to the same interval (10Hz) whereas
the fast speed of event cameras, which can operate up to 3kHz, has not been
effectively utilized. In this work, we show that a temporally dense flow
estimation at 100Hz can be achieved by treating the flow estimation as a
sequential problem using two different variants of recurrent networks -
Long-short term memory (LSTM) and spiking neural network (SNN). First, We
utilize the NN model constructed similar to the popular EV-FlowNet but with
LSTM layers to demonstrate the efficiency of our training method. The model not
only produces 10x more frequent optical flow than the existing ones, but the
estimated flows also have 13% lower errors than predictions from the baseline
EV-FlowNet. Second, we construct an EV-FlowNet SNN but with leaky integrate and
fire neurons to efficiently capture the temporal dynamics. We found that simple
inherent recurrent dynamics of SNN lead to significant parameter reduction
compared to the LSTM model. In addition, because of its event-driven
computation, the spiking model is estimated to consume only 1.5% energy of the
LSTM model, highlighting the efficiency of SNN in processing events and the
potential for achieving temporally dense flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03270v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03270v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yefei He, Jing Liu, Weijia Wu, Hong Zhou, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable capabilities in image synthesis
and related generative tasks. Nevertheless, their practicality for low-latency
real-world applications is constrained by substantial computational costs and
latency issues. Quantization is a dominant way to compress and accelerate
diffusion models, where post-training quantization (PTQ) and quantization-aware
training (QAT) are two main approaches, each bearing its own properties. While
PTQ exhibits efficiency in terms of both time and data usage, it may lead to
diminished performance in low bit-width. On the other hand, QAT can alleviate
performance degradation but comes with substantial demands on computational and
data resources. To capitalize on the advantages while avoiding their respective
drawbacks, we introduce a data-free and parameter-efficient fine-tuning
framework for low-bit diffusion models, dubbed EfficientDM, to achieve
QAT-level performance with PTQ-like efficiency. Specifically, we propose a
quantization-aware variant of the low-rank adapter (QALoRA) that can be merged
with model weights and jointly quantized to low bit-width. The fine-tuning
process distills the denoising capabilities of the full-precision model into
its quantized counterpart, eliminating the requirement for training data. We
also introduce scale-aware optimization and employ temporal learned step-size
quantization to further enhance performance. Extensive experimental results
demonstrate that our method significantly outperforms previous PTQ-based
diffusion models while maintaining similar time and data efficiency.
Specifically, there is only a marginal 0.05 sFID increase when quantizing both
weights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to
QAT-based methods, our EfficientDM also boasts a 16.2x faster quantization
speed with comparable generation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and
  Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Wang, Giljoo Nam, Tuur Stuyck, Stephen Lombardi, Chen Cao, Jason Saragih, Michael Zollhoefer, Jessica Hodgins, Christoph Lassner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capture and animation of human hair are two of the major challenges in
the creation of realistic avatars for the virtual reality. Both problems are
highly challenging, because hair has complex geometry and appearance, as well
as exhibits challenging motion. In this paper, we present a two-stage approach
that models hair independently from the head to address these challenges in a
data-driven manner. The first stage, state compression, learns a
low-dimensional latent space of 3D hair states containing motion and
appearance, via a novel autoencoder-as-a-tracker strategy. To better
disentangle the hair and head in appearance learning, we employ multi-view hair
segmentation masks in combination with a differentiable volumetric renderer.
The second stage learns a novel hair dynamics model that performs temporal hair
transfer based on the discovered latent codes. To enforce higher stability
while driving our dynamics model, we employ the 3D point-cloud autoencoder from
the compression stage for de-noising of the hair state. Our model outperforms
the state of the art in novel view synthesis and is capable of creating novel
hair animations without having to rely on hair observations as a driving
signal. Project page is here https://ziyanw1.github.io/neuwigs/.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-11T00:00:00Z">2023-10-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">47</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel State Value Network for Combined Prediction and Planning in
  Interactive Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sascha Rosbach, Stefan M. Leupold, Simon Großjohann, Stefan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated vehicles operating in urban environments have to reliably interact
with other traffic participants. Planning algorithms often utilize separate
prediction modules forecasting probabilistic, multi-modal, and interactive
behaviors of objects. Designing prediction and planning as two separate modules
introduces significant challenges, particularly due to the interdependence of
these modules. This work proposes a deep learning methodology to combine
prediction and planning. A conditional GAN with the U-Net architecture is
trained to predict two high-resolution image sequences. The sequences represent
explicit motion predictions, mainly used to train context understanding, and
pixel state values suitable for planning encoding kinematic reachability,
object dynamics, safety, and driving comfort. The model can be trained offline
on target images rendered by a sampling-based model-predictive planner,
leveraging real-world driving data. Our results demonstrate intuitive behavior
in complex situations, such as lane changes amidst conflicting objectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Layout Design and Control of Robust Cooperative Grasped-Load
  Aerial Transportation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Bosio, Jerry Tang, Ting-Hao Wang, Mark W. Mueller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to cooperative aerial transportation through a
team of drones, using optimal control theory and a hierarchical control
strategy. We assume the drones are connected to the payload through rigid
attachments, essentially transforming the whole system into a larger flying
object with "thrust modules" at the attachment locations of the drones. We
investigate the optimal arrangement of the thrust modules around the payload,
so that the resulting system is robust to disturbances. We choose the
$\mathcal{H}_2$ norm as a measure of robustness, and propose an iterative
optimization routine to compute the optimal layout of the vehicles around the
object. We experimentally validate our approach using four drones and comparing
the disturbance rejection performances achieved by two different layouts (the
optimal one and a sub-optimal one), and observe that the results match our
predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AG-CVG: Coverage Planning with a Mobile Recharging UGV and an
  Energy-Constrained UAV <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nare Karapetyan, Ahmad Bilal Asghar, Amisha Bhaskar, Guangyao Shi, Dinesh Manocha, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an approach for coverage path planning for a team
of an energy-constrained Unmanned Aerial Vehicle (UAV) and an Unmanned Ground
Vehicle (UGV). Both the UAV and the UGV have predefined areas that they have to
cover. The goal is to perform complete coverage by both robots while minimizing
the coverage time. The UGV can also serve as a mobile recharging station. The
UAV and UGV need to occasionally rendezvous for recharging. We propose a
heuristic method to address this NP-Hard planning problem. Our approach
involves initially determining coverage paths without factoring in energy
constraints. Subsequently, we cluster segments of these paths and employ graph
matching to assign UAV clusters to UGV clusters for efficient recharging
management. We perform numerical analysis on real-world coverage applications
and show that compared with a greedy approach our method reduces rendezvous
overhead on average by 11.33\%. We demonstrate proof-of-concept with a team of
a VOXL m500 drone and a Clearpath Jackal ground vehicle, providing a complete
system from the offline algorithm to the field execution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA 2024 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leader-Follower Formation Control of Perturbed Nonholonomic Agents along
  Parametric Curves with Directed Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Zhang, Hui Zhi, Jose Guadalupe Romero, David Navarro-Alarcon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel formation controller for nonholonomic
agents to form general parametric curves. First, we derive a unified parametric
representation for both open and closed curves. Then, a leader-follower
formation controller is designed to form the parametric curves. We consider
directed communications and constant input disturbances rejection in the
controller design. Rigorous Lyapunov-based stability analysis proves the
asymptotic stability of the proposed controller. Detailed numerical simulations
and experimental studies are conducted to verify the performance of the
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViT-A*: Legged Robot Path Planning using Vision Transformer A* 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Liu, Shirui Lyu, Denis Hadjivelichkov, Valerio Modugno, Dimitrios Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legged robots, particularly quadrupeds, offer promising navigation
capabilities, especially in scenarios requiring traversal over diverse terrains
and obstacle avoidance. This paper addresses the challenge of enabling legged
robots to navigate complex environments effectively through the integration of
data-driven path-planning methods. We propose an approach that utilizes
differentiable planners, allowing the learning of end-to-end global plans via a
neural network for commanding quadruped robots. The approach leverages 2D maps
and obstacle specifications as inputs to generate a global path. To enhance the
functionality of the developed neural network-based path planner, we use Vision
Transformers (ViT) for map pre-processing, to enable the effective handling of
larger maps. Experimental evaluations on two real robotic quadrupeds (Boston
Dynamics Spot and Unitree Go1) demonstrate the effectiveness and versatility of
the proposed approach in generating reliable path plans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Sun, Peihao Chen, Jugang Fan, Thomas H. Li, Jian Chen, Mingkui Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning to navigate to an image-specified goal is an important but
challenging task for autonomous systems. The agent is required to reason the
goal location from where a picture is shot. Existing methods try to solve this
problem by learning a navigation policy, which captures semantic features of
the goal image and observation image independently and lastly fuses them for
predicting a sequence of navigation actions. However, these methods suffer from
two major limitations. 1) They may miss detailed information in the goal image,
and thus fail to reason the goal location. 2) More critically, it is hard to
focus on the goal-relevant regions in the observation image, because they
attempt to understand observation without goal conditioning. In this paper, we
aim to overcome these limitations by designing a Fine-grained Goal Prompting
(FGPrompt) method for image-goal navigation. In particular, we leverage
fine-grained and high-resolution feature maps in the goal image as prompts to
perform conditioned embedding, which preserves detailed information in the goal
image and guides the observation encoder to pay attention to goal-relevant
regions. Compared with existing methods on the image-goal navigation benchmark,
our method brings significant performance improvement on 3 benchmark datasets
(i.e., Gibson, MP3D, and HM3D). Especially on Gibson, we surpass the
state-of-the-art success rate by 8% with only 1/50 model size. Project page:
https://xinyusun.github.io/fgprompt-pages
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for
  Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rezaul Karim, Soheil Mohamad Alizadeh Shabestary, Amir Rasouli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting temporally consistent road users' trajectories in a multi-agent
setting is a challenging task due to unknown characteristics of agents and
their varying intentions. Besides using semantic map information and modeling
interactions, it is important to build an effective mechanism capable of
reasoning about behaviors at different levels of granularity. To this end, we
propose Dynamic goal quErieS with temporal Transductive alIgNmEnt (DESTINE)
method. Unlike past arts, our approach 1) dynamically predicts agents' goals
irrespective of particular road structures, such as lanes, allowing the method
to produce a more accurate estimation of destinations; 2) achieves map
compliant predictions by generating future trajectories in a coarse-to-fine
fashion, where the coarser predictions at a lower frame rate serve as
intermediate goals; and 3) uses an attention module designed to temporally
align predicted trajectories via masked attention. Using the common Argoverse
benchmark dataset, we show that our method achieves state-of-the-art
performance on various metrics, and further investigate the contributions of
proposed modules via comprehensive ablation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 tables 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator
  Walker Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivanna Kramer, Kevin Weirauch, Sabine Bauer, Mark Oliver Mints, Peer Neubert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rollator walkers allow people with physical limitations to increase their
mobility and give them the confidence and independence to participate in
society for longer. However, rollator walker users often have poor posture,
leading to further health problems and, in the worst case, falls. Integrating
sensors into rollator walker designs can help to address this problem and
results in a platform that allows several other interesting use cases. This
paper briefly overviews existing systems and the current research directions
and challenges in this field. We also present our early HealthWalk rollator
walker prototype for data collection with older people, rheumatism, multiple
sclerosis and Parkinson patients, and individuals with visual impairments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imitation Learning from Observation with Automatic Discount Scheduling <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Liu, Weijun Dong, Yingdong Hu, Chuan Wen, Zhao-Heng Yin, Chongjie Zhang, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans often acquire new skills through observation and imitation. For
robotic agents, learning from the plethora of unlabeled video demonstration
data available on the Internet necessitates imitating the expert without access
to its action, presenting a challenge known as Imitation Learning from
Observations (ILfO). A common approach to tackle ILfO problems is to convert
them into inverse reinforcement learning problems, utilizing a proxy reward
computed from the agent's and the expert's observations. Nonetheless, we
identify that tasks characterized by a progress dependency property pose
significant challenges for such approaches; in these tasks, the agent needs to
initially learn the expert's preceding behaviors before mastering the
subsequent ones. Our investigation reveals that the main cause is that the
reward signals assigned to later steps hinder the learning of initial
behaviors. To address this challenge, we present a novel ILfO framework that
enables the agent to master earlier behaviors before advancing to later ones.
We introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively
alters the discount factor in reinforcement learning during the training phase,
prioritizing earlier rewards initially and gradually engaging later rewards
only when the earlier behaviors have been mastered. Our experiments, conducted
on nine Meta-World tasks, demonstrate that our method significantly outperforms
state-of-the-art methods across all tasks, including those that are unsolvable
by them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RANS: Highly-Parallelised Simulator for Reinforcement Learning based
  Autonomous Navigating Spacecrafts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo El-Hariry, Antoine Richard, Miguel Olivares-Mendez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, realistic simulation environments are essential to validate and
build reliable robotic solutions. This is particularly true when using
Reinforcement Learning (RL) based control policies. To this end, both robotics
and RL developers need tools and workflows to create physically accurate
simulations and synthetic datasets. Gazebo, MuJoCo, Webots, Pybullets or Isaac
Sym are some of the many tools available to simulate robotic systems.
Developing learning-based methods for space navigation is, due to the highly
complex nature of the problem, an intensive data-driven process that requires
highly parallelized simulations. When it comes to the control of spacecrafts,
there is no easy to use simulation library designed for RL. We address this gap
by harnessing the capabilities of NVIDIA Isaac Gym, where both physics
simulation and the policy training reside on GPU. Building on this tool, we
provide an open-source library enabling users to simulate thousands of parallel
spacecrafts, that learn a set of maneuvering tasks, such as position, attitude,
and velocity control. These tasks enable to validate complex space scenarios,
such as trajectory optimization for landing, docking, rendezvous and more.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Kernel and Image Quality Estimators for Optimizing Robotic
  Ultrasound Controller using Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Raina, SH Chandrashekhara, Richard Voyles, Juan Wachs, Subir Kumar Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound is a commonly used medical imaging modality that requires expert
sonographers to manually maneuver the ultrasound probe based on the acquired
image. Autonomous Robotic Ultrasound (A-RUS) is an appealing alternative to
this manual procedure in order to reduce sonographers' workload. The key
challenge to A-RUS is optimizing the ultrasound image quality for the region of
interest across different patients. This requires knowledge of anatomy,
recognition of error sources and precise probe position, orientation and
pressure. Sample efficiency is important while optimizing these parameters
associated with the robotized probe controller. Bayesian Optimization (BO), a
sample-efficient optimization framework, has recently been applied to optimize
the 2D motion of the probe. Nevertheless, further improvements are needed to
improve the sample efficiency for high-dimensional control of the probe. We aim
to overcome this problem by using a neural network to learn a low-dimensional
kernel in BO, termed as Deep Kernel (DK). The neural network of DK is trained
using probe and image data acquired during the procedure. The two image quality
estimators are proposed that use a deep convolution neural network and provide
real-time feedback to the BO. We validated our framework using these two
feedback functions on three urinary bladder phantoms. We obtained over 50%
increase in sample efficiency for 6D control of the robotized probe.
Furthermore, our results indicate that this performance enhancement in BO is
independent of the specific training dataset, demonstrating inter-patient
adaptability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE International Symposium on Medical Robotics (ISMR)
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LESS-Map: Lightweight and Evolving Semantic Map in Parking Lots for
  Long-term Self-Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Liu, Xinyang Tang, Yeqiang Qian, Jiming Chen, Liang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise and long-term stable localization is essential in parking lots for
tasks like autonomous driving or autonomous valet parking, \textit{etc}.
Existing methods rely on a fixed and memory-inefficient map, which lacks robust
data association approaches. And it is not suitable for precise localization or
long-term map maintenance. In this paper, we propose a novel mapping,
localization, and map update system based on ground semantic features,
utilizing low-cost cameras. We present a precise and lightweight
parameterization method to establish improved data association and achieve
accurate localization at centimeter-level. Furthermore, we propose a novel map
update approach by implementing high-quality data association for parameterized
semantic features, allowing continuous map update and refinement during
re-localization, while maintaining centimeter-level accuracy. We validate the
performance of the proposed method in real-world experiments and compare it
against state-of-the-art algorithms. The proposed method achieves an average
accuracy improvement of 5cm during the registration process. The generated maps
consume only a compact size of 450 KB/km and remain adaptable to evolving
environments through continuous update.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Social Motion Latent Space and Human Awareness for Effective
  Robot Navigation in Crowded Environments <span class="chip">IROS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junaid Ahmed Ansari, Satyajit Tourani, Gourav Kumar, Brojeshwar Bhowmick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a novel approach to social robot navigation by learning to
generate robot controls from a social motion latent space. By leveraging this
social motion latent space, the proposed method achieves significant
improvements in social navigation metrics such as success rate, navigation
time, and trajectory length while producing smoother (less jerk and angular
deviations) and more anticipatory trajectories. The superiority of the proposed
method is demonstrated through comparison with baseline models in various
scenarios. Additionally, the concept of humans' awareness towards the robot is
introduced into the social robot navigation framework, showing that
incorporating human awareness leads to shorter and smoother trajectories owing
to humans' ability to positively interact with the robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IROS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoPAL: Corrective Planning of Robot Actions with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Joublin, Antonello Ceravola, Pavel Smirnov, Felix Ocker, Joerg Deigmoeller, Anna Belardinelli, Chao Wang, Stephan Hasler, Daniel Tanneberg, Michael Gienger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the pursuit of fully autonomous robotic systems capable of taking over
tasks traditionally performed by humans, the complexity of open-world
environments poses a considerable challenge. Addressing this imperative, this
study contributes to the field of Large Language Models (LLMs) applied to task
and motion planning for robots. We propose a system architecture that
orchestrates a seamless interplay between multiple cognitive levels,
encompassing reasoning, planning, and motion generation. At its core lies a
novel replanning strategy that handles physically grounded, logical, and
semantic errors in the generated plans. We demonstrate the efficacy of the
proposed feedback architecture, particularly its impact on executability,
correctness, and time complexity via empirical evaluation in the context of a
simulation and two intricate real-world scenarios: blocks world, barman and
pizza preparation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing the Placement of Roadside LiDARs for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Jiang, Hao Xiang, Xinyu Cai, Runsheng Xu, Jiaqi Ma, Yikang Li, Gim Hee Lee, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent cooperative perception is an increasingly popular topic in the
field of autonomous driving, where roadside LiDARs play an essential role.
However, how to optimize the placement of roadside LiDARs is a crucial but
often overlooked problem. This paper proposes an approach to optimize the
placement of roadside LiDARs by selecting optimized positions within the scene
for better perception performance. To efficiently obtain the best combination
of locations, a greedy algorithm based on perceptual gain is proposed, which
selects the location that can maximize the perceptual gain sequentially. We
define perceptual gain as the increased perceptual capability when a new LiDAR
is placed. To obtain the perception capability, we propose a perception
predictor that learns to evaluate LiDAR placement using only a single point
cloud frame. A dataset named Roadside-Opt is created using the CARLA simulator
to facilitate research on the roadside LiDAR placement problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAGE-ICP: Semantic Information-Assisted ICP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Cui, Jiming Chen, Liang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust and accurate pose estimation in unknown environments is an essential
part of robotic applications. We focus on LiDAR-based point-to-point ICP
combined with effective semantic information. This paper proposes a novel
semantic information-assisted ICP method named SAGE-ICP, which leverages
semantics in odometry. The semantic information for the whole scan is timely
and efficiently extracted by a 3D convolution network, and these point-wise
labels are deeply involved in every part of the registration, including
semantic voxel downsampling, data association, adaptive local map, and dynamic
vehicle removal. Unlike previous semantic-aided approaches, the proposed method
can improve localization accuracy in large-scale scenes even if the semantic
information has certain errors. Experimental evaluations on KITTI and KITTI-360
show that our method outperforms the baseline methods, and improves accuracy
while maintaining real-time performance, i.e., runs faster than the sensor
frame rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6+1 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangguo Yu, Hamidreza Kasaei, Ming Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In advanced human-robot interaction tasks, visual target navigation is
crucial for autonomous robots navigating unknown environments. While numerous
approaches have been developed in the past, most are designed for single-robot
operations, which often suffer from reduced efficiency and robustness due to
environmental complexities. Furthermore, learning policies for multi-robot
collaboration are resource-intensive. To address these challenges, we propose
Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs)
as a global planner for multi-robot cooperative visual target navigation.
Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs'
scene comprehension. It then assigns exploration frontiers to each robot for
efficient target search. Experimental results on Habitat-Matterport 3D (HM3D)
demonstrate that Co-NavGPT surpasses existing models in success rates and
efficiency without any learning process, demonstrating the vast potential of
LLMs in multi-robot collaboration domains. The supplementary video, prompts,
and code can be accessed via the following link:
\href{https://sites.google.com/view/co-navgpt}{https://sites.google.com/view/co-navgpt}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Matters to You? Towards Visual Representation Alignment for Robot
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Tian, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, Andrea Bajcsy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When operating in service of people, robots need to optimize rewards aligned
with end-user preferences. Since robots will rely on raw perceptual inputs like
RGB images, their rewards will inevitably use visual representations. Recently
there has been excitement in using representations from pre-trained visual
models, but key to making these work in robotics is fine-tuning, which is
typically done via proxy tasks like dynamics prediction or enforcing temporal
cycle-consistency. However, all these proxy tasks bypass the human's input on
what matters to them, exacerbating spurious correlations and ultimately leading
to robot behaviors that are misaligned with user preferences. In this work, we
propose that robots should leverage human feedback to align their visual
representations with the end-user and disentangle what matters for the task. We
propose Representation-Aligned Preference-based Learning (RAPL), a method for
solving the visual representation alignment problem and visual reward learning
problem through the lens of preference-based learning and optimal transport.
Across experiments in X-MAGICAL and in robotic manipulation, we find that
RAPL's reward consistently generates preferred robot behaviors with high sample
efficiency, and shows strong zero-shot generalization when the visual
representation is learned from a different embodiment than the robot's.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unraveling the Single Tangent Space Fallacy: An Analysis and
  Clarification for Applying Riemannian Geometry in Robot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noémie Jaquier, Leonel Rozo, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of robotics, numerous downstream robotics tasks leverage machine
learning methods for processing, modeling, or synthesizing data. Often, this
data comprises variables that inherently carry geometric constraints, such as
the unit-norm condition of quaternions representing rigid-body orientations or
the positive definiteness of stiffness and manipulability ellipsoids. Handling
such geometric constraints effectively requires the incorporation of tools from
differential geometry into the formulation of machine learning methods. In this
context, Riemannian manifolds emerge as a powerful mathematical framework to
handle such geometric constraints. Nevertheless, their recent adoption in robot
learning has been largely characterized by a mathematically-flawed
simplification, hereinafter referred to as the ``single tangent space fallacy".
This approach involves merely projecting the data of interest onto a single
tangent (Euclidean) space, over which an off-the-shelf learning algorithm is
applied. This paper provides a theoretical elucidation of various
misconceptions surrounding this approach and offers experimental evidence of
its shortcomings. Finally, it presents valuable insights to promote best
practices when employing Riemannian geometry within robot learning
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoboCLIP: One Demonstration is Enough to Learn Robot Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumedh A Sontakke, Jesse Zhang, Sébastien M. R. Arnold, Karl Pertsch, Erdem Bıyık, Dorsa Sadigh, Chelsea Finn, Laurent Itti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward specification is a notoriously difficult problem in reinforcement
learning, requiring extensive expert supervision to design robust reward
functions. Imitation learning (IL) methods attempt to circumvent these problems
by utilizing expert demonstrations but typically require a large number of
in-domain expert demonstrations. Inspired by advances in the field of
Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation
learning method that uses a single demonstration (overcoming the large data
requirement) in the form of a video demonstration or a textual description of
the task to generate rewards without manual reward function design.
Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like
videos of humans solving the task for reward generation, circumventing the need
to have the same demonstration and deployment domains. RoboCLIP utilizes
pretrained VLMs without any finetuning for reward generation. Reinforcement
learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher
zero-shot performance than competing imitation learning methods on downstream
robot manipulation tasks, doing so using only one video/text demonstration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay Sridhar, Dhruv Shah, Catherine Glossop, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic learning for navigation in unfamiliar environments needs to provide
policies for both task-oriented navigation (i.e., reaching a goal that the
robot has located), and task-agnostic exploration (i.e., searching for a goal
in a novel setting). Typically, these roles are handled by separate models, for
example by using subgoal proposals, planning, or separate navigation
strategies. In this paper, we describe how we can train a single unified
diffusion policy to handle both goal-directed navigation and goal-agnostic
exploration, with the latter providing the ability to search novel
environments, and the former providing the ability to reach a user-specified
goal once it has been located. We show that this unified policy results in
better overall performance when navigating to visually indicated goals in novel
environments, as compared to approaches that use subgoal proposals from
generative models, or prior methods based on latent variable models. We
instantiate our method by using a large-scale Transformer-based policy trained
on data from multiple ground robots, with a diffusion model decoder to flexibly
handle both goal-conditioned and goal-agnostic navigation. Our experiments,
conducted on a real-world mobile robot platform, show effective navigation in
unseen environments in comparison with five alternative methods, and
demonstrate significant improvements in performance and lower collision rates,
despite utilizing smaller models than state-of-the-art approaches. For more
videos, code, and pre-trained model checkpoints, see
https://general-navigation-models.github.io/nomad/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page https://general-navigation-models.github.io/nomad/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASV Station Keeping under Wind Disturbances using Neural Network
  Simulation Error Minimization Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jalil Chavez-Galaviz, Jianwen Li, Ajinkya Chaudhary, Nina Mahmoudian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Station keeping is an essential maneuver for Autonomous Surface Vehicles
(ASVs), mainly when used in confined spaces, to carry out surveys that require
the ASV to keep its position or in collaboration with other vehicles where the
relative position has an impact over the mission. However, this maneuver can
become challenging for classic feedback controllers due to the need for an
accurate model of the ASV dynamics and the environmental disturbances. This
work proposes a Model Predictive Controller using Neural Network Simulation
Error Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV
under wind disturbances. The performance of the proposed scheme under wind
disturbances is tested and compared against other controllers in simulation,
using the Robotics Operating System (ROS) and the multipurpose simulation
environment Gazebo. A set of six tests were conducted by combining two wind
speeds (3 m/s and 6 m/s) and three wind directions (0$^\circ$, 90$^\circ$, and
180$^\circ$). The simulation results clearly show the advantage of the
NNSEM-MPC over the following methods: backstepping controller, sliding mode
controller, simplified dynamics MPC (SD-MPC), neural ordinary differential
equation MPC (NODE-MPC), and knowledge-based NODE MPC (KNODE-MPC). The proposed
NNSEM-MPC approach performs better than the rest in 4 out of the 6 test
conditions, and it is the second best in the 2 remaining test cases, reducing
the mean position and heading error by at least 31\% and 46\% respectively
across all the test cases. In terms of execution speed, the proposed NNSEM-MPC
is at least 36\% faster than the rest of the MPC controllers. The field
experiments on two different ASV platforms showed that ASVs can effectively
keep the station utilizing the proposed method, with a position error as low as
$1.68$ m and a heading error as low as $6.14^{\circ}$ within time windows of at
least $150$s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LangNav: Language as a Perceptual Representation for Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva, Phillip Isola, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the use of language as a perceptual representation for
vision-and-language navigation. Our approach uses off-the-shelf vision systems
(for image captioning and object detection) to convert an agent's egocentric
panoramic view at each time step into natural language descriptions. We then
finetune a pretrained language model to select an action, based on the current
view and the trajectory history, that would best fulfill the navigation
instructions. In contrast to the standard setup which adapts a pretrained
language model to work directly with continuous visual features from pretrained
vision models, our approach instead uses (discrete) language as the perceptual
representation. We explore two use cases of our language-based navigation
(LangNav) approach on the R2R vision-and-language navigation benchmark:
generating synthetic trajectories from a prompted large language model (GPT-4)
with which to finetune a smaller language model; and sim-to-real transfer where
we transfer a policy learned on a simulated environment (ALFRED) to a
real-world environment (R2R). Our approach is found to improve upon strong
baselines that rely on visual features in settings where only a few gold
trajectories (10-100) are available, demonstrating the potential of using
language as a perceptual representation for navigation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VaPr: Variable-Precision Tensors to Accelerate Robot Motion Planning <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Shun Hsiao, Siva Kumar Sastry Hari, Balakumar Sundaralingam, Jason Yik, Thierry Tambe, Charbel Sakr, Stephen W. Keckler, Vijay Janapa Reddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-dimensional motion generation requires numerical precision for smooth,
collision-free solutions. Typically, double-precision or single-precision
floating-point (FP) formats are utilized. Using these for big tensors imposes a
strain on the memory bandwidth provided by the devices and alters the memory
footprint, hence limiting their applicability to low-power edge devices needed
for mobile robots. The uniform application of reduced precision can be
advantageous but severely degrades solutions. Using decreased precision data
types for important tensors, we propose to accelerate motion generation by
removing memory bottlenecks. We propose variable-precision (VaPr) search
optimization to determine the appropriate precision for large tensors from a
vast search space of approximately 4 million unique combinations for FP data
types across the tensors. To obtain the efficiency gains, we exploit existing
platform support for an out-of-the-box GPU speedup and evaluate prospective
precision converter units for GPU types that are not currently supported. Our
experimental results on 800 planning problems for the Franka Panda robot on the
MotionBenchmaker dataset across 8 environments show that a 4-bit FP format is
sufficient for the largest set of tensors in the motion generation stack. With
the software-only solution, VaPr achieves 6.3% and 6.3% speedups on average for
a significant portion of motion generation over the SOTA solution (CuRobo) on
Jetson Orin and RTX2080 Ti GPU, respectively, and 9.9%, 17.7% speedups with the
FP converter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 8 tables, to be published in 2023 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Saturation-Aware Angular Velocity Estimation: Extending the Robustness
  of <span class="highlight-title">SLAM</span> to Aggressive Motions <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon-Pierre Deschênes, Dominic Baril, Matěj Boxan, Johann Laconte, Philippe Giguère, François Pomerleau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel angular velocity estimation method to increase the
robustness of Simultaneous Localization And Mapping (SLAM) algorithms against
gyroscope saturations induced by aggressive motions. Field robotics expose
robots to various hazards, including steep terrains, landslides, and
staircases, where substantial accelerations and angular velocities can occur if
the robot loses stability and tumbles. These extreme motions can saturate
sensor measurements, especially gyroscopes, which are the first sensors to
become inoperative. While the structural integrity of the robot is at risk, the
resilience of the SLAM framework is oftentimes given little consideration.
Consequently, even if the robot is physically capable of continuing the
mission, its operation will be compromised due to a corrupted representation of
the world. Regarding this problem, we propose a way to estimate the angular
velocity using accelerometers during extreme rotations caused by tumbling. We
show that our method reduces the median localization error by 71.5 % in
translation and 65.5 % in rotation and reduces the number of SLAM failures by
73.3 % on the collected data. We also propose the Tumbling-Induced Gyroscope
Saturation (TIGS) dataset, which consists of outdoor experiments recording the
motion of a lidar subject to angular velocities four times higher than other
available datasets. The dataset is available online at
https://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, submitted to the 2024 IEEE International
  Conference on Robotics and Automation (ICRA2024), Yokohama, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiPPeR: Diffusion-based 2D Path Planner applied on Legged Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianwei Liu, Maria Stamatopoulou, Dimitrios Kanoulas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present DiPPeR, a novel and fast 2D path planning framework
for quadrupedal locomotion, leveraging diffusion-driven techniques. Our
contributions include a scalable dataset of map images and corresponding
end-to-end trajectories, an image-conditioned diffusion planner for mobile
robots, and a training/inference pipeline employing CNNs. We validate our
approach in several mazes, as well as in real-world deployment scenarios on
Boston Dynamic's Spot and Unitree's Go1 robots. DiPPeR performs on average 70
times faster for trajectory generation against both search based and data
driven path planning algorithms with an average of 80% consistency in producing
feasible paths of various length in maps of variable size, and obstacle
structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Learning with Dual Model Predictive Path-Integral Control for
  Interaction-Aware Autonomous Highway On-ramp Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Knaup, Jovin D'sa, Behdad Chalaki, Tyler Naes, Hossein Nourkhiz Mahjoub, Ehsan Moradi-Pari, Panagiotis Tsiotras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Merging into dense highway traffic for an autonomous vehicle is a complex
decision-making task, wherein the vehicle must identify a potential gap and
coordinate with surrounding human drivers, each of whom may exhibit diverse
driving behaviors. Many existing methods consider other drivers to be dynamic
obstacles and, as a result, are incapable of capturing the full intent of the
human drivers via this passive planning. In this paper, we propose a novel dual
control framework based on Model Predictive Path-Integral control to generate
interactive trajectories. This framework incorporates a Bayesian inference
approach to actively learn the agents' parameters, i.e., other drivers' model
parameters. The proposed framework employs a sampling-based approach that is
suitable for real-time implementation through the utilization of GPUs. We
illustrate the effectiveness of our proposed methodology through comprehensive
numerical simulations conducted in both high and low-fidelity simulation
scenarios focusing on autonomous on-ramp merging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Body-mounted MR-conditional Robot for Minimally Invasive Liver
  Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefeng Huang, Anthony L. Gunderman, Samuel E. Wilcox, Saikat Sengupta, Aiming Lu, David Woodrum, Jay Shah, Yue Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MR-guided microwave ablation (MWA) has proven effective in treating
hepatocellular carcinoma (HCC) with small-sized tumors, but the
state-of-the-art technique suffers from sub-optimal workflow due to speed and
accuracy of needle placement. This paper presents a compact body-mounted
MR-conditional robot that can operate in closed-bore MR scanners for accurate
needle guidance. The robotic platform consists of two stacked Cartesian XY
stages, each with two degrees of freedom, that facilitate needle guidance. The
robot is actuated using 3D-printed pneumatic turbines with MR-conditional bevel
gear transmission systems. Pneumatic valves and control mechatronics are
located inside the MRI control room and are connected to the robot with
pneumatic transmission lines and optical fibers. Free space experiments
indicated robot-assisted needle insertion error of 2.6$\pm$1.3 mm at an
insertion depth of 80 mm. The MR-guided phantom studies were conducted to
verify the MR-conditionality and targeting performance of the robot. Future
work will focus on the system optimization and validations in animal trials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory
  Prediction Models for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhe Chen, Mozhgan Pourkeshavarz, Amir Rasouli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarking is a common method for evaluating trajectory prediction models
for autonomous driving. Existing benchmarks rely on datasets, which are biased
towards more common scenarios, such as cruising, and distance-based metrics
that are computed by averaging over all scenarios. Following such a regiment
provides a little insight into the properties of the models both in terms of
how well they can handle different scenarios and how admissible and diverse
their outputs are. There exist a number of complementary metrics designed to
measure the admissibility and diversity of trajectories, however, they suffer
from biases, such as length of trajectories.
  In this paper, we propose a new benChmarking paRadIgm for evaluaTing
trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a
method for extracting driving scenarios at varying levels of specificity
according to the structure of the roads, models' performance, and data
properties for fine-grained ranking of prediction models; 2) A set of new
bias-free metrics for measuring diversity, by incorporating the characteristics
of a given scenario, and admissibility, by considering the structure of roads
and kinematic compliancy, motivated by real-world driving constraints. 3) Using
the proposed benchmark, we conduct extensive experimentation on a
representative set of the prediction models using the large scale Argoverse
dataset. We show that the proposed benchmark can produce a more accurate
ranking of the models and serve as a means of characterizing their behavior. We
further present ablation studies to highlight contributions of different
elements that are used to compute the proposed metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of Fuzzy Control Algorithm in Two-Wheeled Differential
  Drive Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing and developing Artificial Intelligence controllers on separately
dedicated chips have many advantages. This report reviews the development of a
real-time fuzzy logic controller for optimizing locomotion control of a
two-wheeled differential drive platform using an Arduino Uno board. Based on
the Raspberry Pi board, fuzzy sets are used to optimize color recognition,
enabling the color sensor to correctly recognize color at long distances,
across a wide range of light intensity, and with high fault tolerance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accountability in Offline Reinforcement Learning: Explaining Decisions
  with a Corpus of Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Alihan Hüyük, Daniel Jarrett, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning transparent, interpretable controllers with offline data in
decision-making systems is an essential area of research due to its potential
to reduce the risk of applications in real-world systems. However, in
responsibility-sensitive settings such as healthcare, decision accountability
is of paramount importance, yet has not been adequately addressed by the
literature. This paper introduces the Accountable Offline Controller (AOC) that
employs the offline dataset as the Decision Corpus and performs accountable
control based on a tailored selection of examples, referred to as the Corpus
Subset. ABC operates effectively in low-data scenarios, can be extended to the
strictly offline imitation setting, and displays qualities of both conservation
and adaptability. We assess ABC's performance in both simulated and real-world
healthcare scenarios, emphasizing its capability to manage offline control
tasks with high levels of performance while maintaining accountability.
  Keywords: Interpretable Reinforcement Learning, Explainable Reinforcement
Learning, Reinforcement Learning Transparency, Offline Reinforcement Learning,
Batched Control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Terrain-adaptive Central Pattern Generators with Reinforcement Learning
  for Hexapod Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyue Yang, Yue Gao, Shaoyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by biological motion generation, central pattern generators (CPGs)
is frequently employed in legged robot locomotion control to produce natural
gait pattern with low-dimensional control signals. However, the limited
adaptability and stability over complex terrains hinder its application. To
address this issue, this paper proposes a terrain-adaptive locomotion control
method that incorporates deep reinforcement learning (DRL) framework into CPG,
where the CPG model is responsible for the generation of synchronized signals,
providing basic locomotion gait, while DRL is integrated to enhance the
adaptability of robot towards uneven terrains by adjusting the parameters of
CPG mapping functions. The experiments conducted on the hexapod robot in Isaac
Gym simulation environment demonstrated the superiority of the proposed method
in terrain-adaptability, convergence rate and reward design complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TidyBot: Personalized Robot Assistance with Large Language Models <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05658v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05658v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a robot to personalize physical assistance effectively, it must learn
user preferences that can be generally reapplied to future scenarios. In this
work, we investigate personalization of household cleanup with robots that can
tidy up rooms by picking up objects and putting them away. A key challenge is
determining the proper place to put each object, as people's preferences can
vary greatly depending on personal taste or cultural background. For instance,
one person may prefer storing shirts in the drawer, while another may prefer
them on the shelf. We aim to build systems that can learn such preferences from
just a handful of examples via prior interactions with a particular person. We
show that robots can combine language-based planning and perception with the
few-shot summarization capabilities of large language models (LLMs) to infer
generalized user preferences that are broadly applicable to future
interactions. This approach enables fast adaptation and achieves 91.2% accuracy
on unseen objects in our benchmark dataset. We also demonstrate our approach on
a real-world mobile manipulator called TidyBot, which successfully puts away
85.0% of objects in real-world test scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Autonomous Robots (AuRo) - Special Issue: Large Language
  Models in Robotics, 2023 and IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS), 2023. Project page:
  https://tidybot.cs.princeton.edu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat with the Environment: Interactive Multimodal Perception Using Large
  Language Models <span class="chip">IROS2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programming robot behavior in a complex world faces challenges on multiple
levels, from dextrous low-level skills to high-level planning and reasoning.
Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning
ability in few-shot robotic planning. However, it remains challenging to ground
LLMs in multimodal sensory input and continuous action output, while enabling a
robot to interact with its environment and acquire novel information as its
policies unfold. We develop a robot interaction scenario with a partially
observable state, which necessitates a robot to decide on a range of epistemic
actions in order to sample sensory information among multiple modalities,
before being able to execute the task correctly. Matcha (Multimodal environment
chatting) agent, an interactive perception framework, is therefore proposed
with an LLM as its backbone, whose ability is exploited to instruct epistemic
actions and to reason over the resulting multimodal sensations (vision, sound,
haptics, proprioception), as well as to plan an entire task execution based on
the interactively acquired information. Our study demonstrates that LLMs can
provide high-level planning and reasoning skills and control interactive robot
behavior in a multimodal environment, while multimodal modules with the context
of the environmental state help ground the LLMs and extend their processing
ability. The project website can be found at https://matcha-agent.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IROS2023, Detroit. See the project website at
  https://matcha-agent.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Singularity Distance Computations for 3-RPR Manipulators Using Intrinsic
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.14721v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.14721v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kapilavai, Georg Nawratil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an efficient algorithm for computing the closest singular
configuration to each non-singular pose of a 3-RPR planar manipulator
performing a 1-parametric motion. By considering a 3-RPR manipulator as a
planar framework, one can use methods from rigidity theory to compute the
singularity distance with respect to an intrinsic metric. Such a metric has the
advantage over any performance index used for indicating the closeness to
singularities, that the obtained value is a distance, which equals the radius
of a guaranteed singularity-free sphere in the joint space of the manipulator.
The proposed method can take different design options into account as the
platform/base can be seen as a triangular plate or as a pin-jointed triangular
bar structure. Moreover, we also allow the additional possibility of pinning
down the base/platform triangle to the fixed/moving system thus it cannot be
deformed. For the resulting nine interpretations, we compute the corresponding
intrinsic metrics based on the total elastic strain energy density of the
framework using the physical concept of Green-Lagrange strain. The global
optimization problem of finding the closest singular configuration with respect
to these metrics is solved by using tools from numerical algebraic geometry.
The proposed algorithm is demonstrated based on an example, which is also used
to compare the obtained intrinsic singularity distances with the corresponding
extrinsic ones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Transformers under Occlusion: How Physics and Background
  Attributes Impact Large Models for Robotic Manipulation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02044v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02044v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutong Jin, Ruiyu Wang, Muhammad Zahid, Florian T. Pokorny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As transformer architectures and dataset sizes continue to scale, the need to
understand the specific dataset factors affecting model performance becomes
increasingly urgent. This paper investigates how object physics attributes
(color, friction coefficient, shape) and background characteristics (static,
dynamic, background complexity) influence the performance of Video Transformers
in trajectory prediction tasks under occlusion. Beyond mere occlusion
challenges, this study aims to investigate three questions: How do object
physics attributes and background characteristics influence the model
performance? What kinds of attributes are most influential to the model
generalization? Is there a data saturation point for large transformer model
performance within a single task? To facilitate this research, we present
OccluManip, a real-world video-based robot pushing dataset comprising 460,000
consistent recordings of objects with different physics and varying
backgrounds. 1.4 TB and in total 1278 hours of high-quality videos of flexible
temporal length along with target object trajectories are collected,
accommodating tasks with different temporal requirements. Additionally, we
propose Video Occlusion Transformer (VOT), a generic video-transformer-based
network achieving an average 96% accuracy across all 18 sub-datasets provided
in OccluManip. OccluManip and VOT will be released at:
https://github.com/ShutongJIN/OccluManip.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IEEE ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rapid Exploration for Open-World Navigation with Latent Goal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.05859v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.05859v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Shah, Benjamin Eysenbach, Gregory Kahn, Nicholas Rhinehart, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a robotic learning system for autonomous exploration and
navigation in diverse, open-world environments. At the core of our method is a
learned latent variable model of distances and actions, along with a
non-parametric topological memory of images. We use an information bottleneck
to regularize the learned policy, giving us (i) a compact visual representation
of goals, (ii) improved generalization capabilities, and (iii) a mechanism for
sampling feasible goals for exploration. Trained on a large offline dataset of
prior experience, the model acquires a representation of visual goals that is
robust to task-irrelevant distractors. We demonstrate our method on a mobile
ground robot in open-world exploration scenarios. Given an image of a goal that
is up to 80 meters away, our method leverages its representation to explore and
discover the goal in under 20 minutes, even amidst previously-unseen obstacles
and weather conditions. Please check out the project website for videos of our
experiments and information about the real-world dataset used at
https://sites.google.com/view/recon-robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at 5th Annual Conference on Robot Learning (CoRL 2021),
  London, UK as an Oral Talk. Project page and dataset release at
  https://sites.google.com/view/recon-robot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robotic Applications of Pre-Trained Vision-Language Models to Various
  Recognition Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Kawaharazuka, Yoshiki Obinata, Naoaki Kanazawa, Kei Okada, Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, a number of models that learn the relations between vision
and language from large datasets have been released. These models perform a
variety of tasks, such as answering questions about images, retrieving
sentences that best correspond to images, and finding regions in images that
correspond to phrases. Although there are some examples, the connection between
these pre-trained vision-language models and robotics is still weak. If they
are directly connected to robot motions, they lose their versatility due to the
embodiment of the robot and the difficulty of data collection, and become
inapplicable to a wide range of bodies and situations. Therefore, in this
study, we categorize and summarize the methods to utilize the pre-trained
vision-language models flexibly and easily in a way that the robot can
understand, without directly connecting them to robot motions. We discuss how
to use these models for robot motion selection and motion planning without
re-training the models. We consider five types of methods to extract
information understandable for robots, and show the results of state
recognition, object recognition, affordance recognition, relation recognition,
and anomaly detection based on the combination of these five methods. We expect
that this study will add flexibility and ease-of-use, as well as new
applications, to the recognition behavior of existing robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Humanoids2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric
  Information Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09531v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09531v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Huang, Junqiao Zhao, Zhongyang Zhu, Chen Ye, Tiantian Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local geometric information, i.e. normal and distribution of points, is
crucial for LiDAR-based simultaneous localization and mapping (SLAM) because it
provides constraints for data association, which further determines the
direction of optimization and ultimately affects the accuracy of localization.
However, estimating normal and distribution of points are time-consuming tasks
even with the assistance of kdtree or volumetric maps. To achieve fast normal
estimation, we look into the structure of LiDAR scan and propose a ring-based
fast approximate least squares (Ring FALS) method. With the Ring structural
information, estimating the normal requires only the range information of the
points when a new scan arrives. To efficiently estimate the distribution of
points, we extend the ikd-tree to manage the map in voxels and update the
distribution of points in each voxel incrementally while maintaining its
consistency with the normal estimation. We further fix the distribution after
its convergence to balance the time consumption and the correctness of
representation. Based on the extracted and maintained local geometric
information, we devise a robust and accurate hierarchical data association
scheme where point-to-surfel association is prioritized over point-to-plane.
Extensive experiments on diverse public datasets demonstrate the advantages of
our system compared to other state-of-the-art methods. Our open source
implementation is available at https://github.com/tiev-tongji/LOG-LIO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatio-Temporal Transformer-Based Reinforcement Learning for Robot Crowd
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong He, Hao Fu, Qiang Wang, Shuai Zhou, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The social robot navigation is an open and challenging problem. In existing
work, separate modules are used to capture spatial and temporal features,
respectively. However, such methods lead to extra difficulties in improving the
utilization of spatio-temporal features and reducing the conservative nature of
navigation policy. In light of this, we present a spatio-temporal
transformer-based policy optimization algorithm to enhance the utilization of
spatio-temporal features, thereby facilitating the capture of human-robot
interactions. Specifically, this paper introduces a gated embedding mechanism
that effectively aligns the spatial and temporal representations by integrating
both modalities at the feature level. Then Transformer is leveraged to encode
the spatio-temporal semantic information, with hope of finding the optimal
navigation policy. Finally, a combination of spatio-temporal Transformer and
self-adjusting policy entropy significantly reduces the conservatism of
navigation policies. Experimental results demonstrate the effectiveness of the
proposed framework, where our method shows superior performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The duplication rate is too high and the manuscript needs to be
  withdrawn</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Terrain-Aware Quadrupedal Locomotion via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Shi, Qingxu Zhu, Lei Han, Wanchao Chi, Tingguang Li, Max Q. -H. Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In nature, legged animals have developed the ability to adapt to challenging
terrains through perception, allowing them to plan safe body and foot
trajectories in advance, which leads to safe and energy-efficient locomotion.
Inspired by this observation, we present a novel approach to train a Deep
Neural Network (DNN) policy that integrates proprioceptive and exteroceptive
states with a parameterized trajectory generator for quadruped robots to
traverse rough terrains. Our key idea is to use a DNN policy that can modify
the parameters of the trajectory generator, such as foot height and frequency,
to adapt to different terrains. To encourage the robot to step on safe regions
and save energy consumption, we propose foot terrain reward and lifting foot
height reward, respectively. By incorporating these rewards, our method can
learn a safer and more efficient terrain-aware locomotion policy that can move
a quadruped robot flexibly in any direction. To evaluate the effectiveness of
our approach, we conduct simulation experiments on challenging terrains,
including stairs, stepping stones, and poles. The simulation results
demonstrate that our approach can successfully direct the robot to traverse
such tough terrains in any direction. Furthermore, we validate our method on a
real legged robot, which learns to traverse stepping stones with gaps over
25.5cm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trajectory Tracking Control of Dual-PAM Soft Actuator with Hysteresis
  Compensator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12088v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12088v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Shen, Tetsuro Miyazaki, Shingo Ohno, Maina Sogabe, Kenji Kawashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft robotics is an emergent and swiftly evolving field. Pneumatic actuators
are suitable for driving soft robots because of their superior performance.
However, their control is not easy due to their hysteresis characteristics. In
response to these challenges, we propose an adaptive control method to
compensate hysteresis of a soft actuator. Employing a novel dual pneumatic
artificial muscle (PAM) bending actuator, the innovative control strategy
abates hysteresis effects by dynamically modulating gains within a traditional
PID controller corresponding with the predicted motion of the reference
trajectory. Through comparative experimental evaluation, we found that the new
control method outperforms its conventional counterparts regarding tracking
accuracy and response speed. Our work reveals a new direction for advancing
control in soft actuators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE Robotics and Automation Letters (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coordination of Drones at Scale: Decentralized Energy-aware Swarm
  Intelligence for Spatio-temporal Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhao Qin, Evangelos Pournaras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smart City applications, such as traffic monitoring and disaster response,
often use swarms of intelligent and cooperative drones to efficiently collect
sensor data over different areas of interest and time spans. However, when the
required sensing becomes spatio-temporally large and varying, a collective
arrangement of sensing tasks to a large number of battery-restricted and
distributed drones is challenging. To address this problem, this paper
introduces a scalable and energy-aware model for planning and coordination of
spatio-temporal sensing. The coordination model is built upon a decentralized
multi-agent collective learning algorithm (EPOS) to ensure scalability,
resilience, and flexibility that existing approaches lack of. Experimental
results illustrate the outstanding performance of the proposed method compared
to state-of-the-art methods. Analytical results contribute a deeper
understanding of how coordinated mobility of drones influences sensing
performance. This novel coordination solution is applied to traffic monitoring
using real-world data to demonstrate a $46.45\%$ more accurate and $2.88\%$
more efficient detection of vehicles as the number of drones become a scarce
resource.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, 6 tables. Accepted in Transportation Research
  Part C: Emerging Technologies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Metrics Matter: A Better Standard for Trajectory Forecasting <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06292v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06292v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erica Weng, Hana Hoshino, Deva Ramanan, Kris Kitani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal trajectory forecasting methods commonly evaluate using
single-agent metrics (marginal metrics), such as minimum Average Displacement
Error (ADE) and Final Displacement Error (FDE), which fail to capture joint
performance of multiple interacting agents. Only focusing on marginal metrics
can lead to unnatural predictions, such as colliding trajectories or diverging
trajectories for people who are clearly walking together as a group.
Consequently, methods optimized for marginal metrics lead to overly-optimistic
estimations of performance, which is detrimental to progress in trajectory
forecasting research. In response to the limitations of marginal metrics, we
present the first comprehensive evaluation of state-of-the-art (SOTA)
trajectory forecasting methods with respect to multi-agent metrics (joint
metrics): JADE, JFDE, and collision rate. We demonstrate the importance of
joint metrics as opposed to marginal metrics with quantitative evidence and
qualitative examples drawn from the ETH / UCY and Stanford Drone datasets. We
introduce a new loss function incorporating joint metrics that, when applied to
a SOTA trajectory forecasting method, achieves a 7\% improvement in JADE / JFDE
on the ETH / UCY datasets with respect to the previous SOTA. Our results also
indicate that optimizing for joint metrics naturally leads to an improvement in
interaction modeling, as evidenced by a 16\% decrease in mean collision rate on
the ETH / UCY datasets with respect to the previous SOTA. Code is available at
\texttt{\hyperlink{https://github.com/ericaweng/joint-metrics-matter}{github.com/ericaweng/joint-metrics-matter}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Role of Morphological Variation in Evolutionary Robotics: Maximizing
  Performance and Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonata Tyska Carvalho, Stefano Nolfi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exposing an Evolutionary Algorithm that is used to evolve robot controllers
to variable conditions is necessary to obtain solutions which are robust and
can cross the reality gap. However, we do not yet have methods for analyzing
and understanding the impact of the varying morphological conditions which
impact the evolutionary process, and therefore for choosing suitable variation
ranges. By morphological conditions, we refer to the starting state of the
robot, and to variations in its sensor readings during operation due to noise.
In this article, we introduce a method that permits us to measure the impact of
these morphological variations and we analyze the relation between the
amplitude of variations, the modality with which they are introduced, and the
performance and robustness of evolving agents. Our results demonstrate that (i)
the evolutionary algorithm can tolerate morphological variations which have a
very high impact, (ii) variations affecting the actions of the agent are
tolerated much better than variations affecting the initial state of the agent
or of the environment, and (iii) improving the accuracy of the fitness measure
through multiple evaluations is not always useful. Moreover, our results show
that morphological variations permit generating solutions which perform better
both in varying and non-varying conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to MIT Evolutionary Computation Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inverse Dynamics Trajectory Optimization for Contact-Implicit Model
  Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01813v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01813v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vince Kurtz, Alejandro Castro, Aykut Özgün Önol, Hai Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots must make and break contact with the environment to perform useful
tasks, but planning and control through contact remains a formidable challenge.
In this work, we achieve real-time contact-implicit model predictive control
with a surprisingly simple method: inverse dynamics trajectory optimization.
While trajectory optimization with inverse dynamics is not new, we introduce a
series of incremental innovations that collectively enable fast model
predictive control on a variety of challenging manipulation and locomotion
tasks. We implement these innovations in an open-source solver and present
simulation examples to support the effectiveness of the proposed approach.
Additionally, we demonstrate contact-implicit model predictive control on
hardware at over 100 Hz for a 20-degree-of-freedom bi-manual manipulation task.
Video and code are available at https://idto.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KITE: Keypoint-Conditioned Policies for Semantic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16605v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16605v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priya Sundaresan, Suneel Belkhale, Dorsa Sadigh, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While natural language offers a convenient shared interface for humans and
robots, enabling robots to interpret and follow language commands remains a
longstanding challenge in manipulation. A crucial step to realizing a
performant instruction-following robot is achieving semantic manipulation,
where a robot interprets language at different specificities, from high-level
instructions like "Pick up the stuffed animal" to more detailed inputs like
"Grab the left ear of the elephant." To tackle this, we propose Keypoints +
Instructions to Execution (KITE), a two-step framework for semantic
manipulation which attends to both scene semantics (distinguishing between
different objects in a visual scene) and object semantics (precisely localizing
different parts within an object instance). KITE first grounds an input
instruction in a visual scene through 2D image keypoints, providing a highly
accurate object-centric bias for downstream action inference. Provided an RGB-D
scene observation, KITE then executes a learned keypoint-conditioned skill to
carry out the instruction. The combined precision of keypoints and
parameterized skills enables fine-grained manipulation with generalization to
scene and object variations. Empirically, we demonstrate KITE in 3 real-world
environments: long-horizon 6-DoF tabletop manipulation, semantic grasping, and
a high-precision coffee-making task. In these settings, KITE achieves a 75%,
70%, and 71% overall success rate for instruction-following, respectively. KITE
outperforms frameworks that opt for pre-trained visual language models over
keypoint-based grounding, or omit skills in favor of end-to-end visuomotor
control, all while being trained from fewer or comparable amounts of
demonstrations. Supplementary material, datasets, code, and videos can be found
on our website: http://tinyurl.com/kite-site.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">149</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Zhou, Weize Li, Lihan Jiang, Guoliang Wang, Guyue Zhou, Shanghang Zhang, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object anomaly detection is an important problem in the field of machine
vision and has seen remarkable progress recently. However, two significant
challenges hinder its research and application. First, existing datasets lack
comprehensive visual information from various pose angles. They usually have an
unrealistic assumption that the anomaly-free training dataset is pose-aligned,
and the testing samples have the same pose as the training data. However, in
practice, anomaly may exist in any regions on a object, the training and query
samples may have different poses, calling for the study on pose-agnostic
anomaly detection. Second, the absence of a consensus on experimental protocols
for pose-agnostic anomaly detection leads to unfair comparisons of different
methods, hindering the research on pose-agnostic anomaly detection. To address
these issues, we develop Multi-pose Anomaly Detection (MAD) dataset and
Pose-agnostic Anomaly Detection (PAD) benchmark, which takes the first step to
address the pose-agnostic anomaly detection problem. Specifically, we build MAD
using 20 complex-shaped LEGO toys including 4K views with various poses, and
high-quality and diverse 3D anomalies in both simulated and real environments.
Additionally, we propose a novel method OmniposeAD, trained using MAD,
specifically designed for pose-agnostic anomaly detection. Through
comprehensive evaluations, we demonstrate the relevance of our dataset and
method. Furthermore, we provide an open-source benchmark library, including
dataset and baseline methods that cover 8 anomaly detection paradigms, to
facilitate future research and application in this domain. Code, data, and
models are publicly available at https://github.com/EricLee0224/PAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023. Codes are available at
  https://github.com/EricLee0224/PAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MatFormer: Nested Transformer for Elastic Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Devvrit, Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models are deployed in a wide range of settings, from
multi-accelerator clusters to standalone mobile phones. The diverse inference
constraints in these scenarios necessitate practitioners to train foundation
models such as PaLM 2, Llama, & ViTs as a series of models of varying sizes.
Due to significant training costs, only a select few model sizes are trained
and supported, limiting more fine-grained control over relevant tradeoffs,
including latency, cost, and accuracy. This work introduces MatFormer, a nested
Transformer architecture designed to offer elasticity in a variety of
deployment constraints. Each Feed Forward Network (FFN) block of a MatFormer
model is jointly optimized with a few nested smaller FFN blocks. This training
procedure allows for the Mix'n'Match of model granularities across layers --
i.e., a trained universal MatFormer model enables extraction of hundreds of
accurate smaller models, which were never explicitly optimized. We empirically
demonstrate MatFormer's effectiveness across different model classes (decoders
& encoders), modalities (language & vision), and scales (up to 2.6B
parameters). We find that a 2.6B decoder-only MatFormer language model (MatLM)
allows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting
comparable validation loss and one-shot downstream evaluations to their
independently trained counterparts. Furthermore, we observe that smaller
encoders extracted from a universal MatFormer-based ViT (MatViT) encoder
preserve the metric-space structure for adaptive large-scale retrieval.
Finally, we showcase that speculative decoding with the accurate and consistent
submodels extracted from MatFormer can further reduce inference latency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 12 figures, first three authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ferret: Refer and Ground Anything Anywhere at Any Granularity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of
understanding spatial referring of any shape or granularity within an image and
accurately grounding open-vocabulary descriptions. To unify referring and
grounding in the LLM paradigm, Ferret employs a novel and powerful hybrid
region representation that integrates discrete coordinates and continuous
features jointly to represent a region in the image. To extract the continuous
features of versatile regions, we propose a spatial-aware visual sampler, adept
at handling varying sparsity across different shapes. Consequently, Ferret can
accept diverse region inputs, such as points, bounding boxes, and free-form
shapes. To bolster the desired capability of Ferret, we curate GRIT, a
comprehensive refer-and-ground instruction tuning dataset including 1.1M
samples that contain rich hierarchical spatial knowledge, with 95K hard
negative data to promote model robustness. The resulting model not only
achieves superior performance in classical referring and grounding tasks, but
also greatly outperforms existing MLLMs in region-based and
localization-demanded multimodal chatting. Our evaluations also reveal a
significantly improved capability of describing image details and a remarkable
alleviation in object hallucination. Code and data will be available at
https://github.com/apple/ml-ferret
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures. Code/Project Website:
  https://github.com/apple/ml-ferret</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate the capability of generating images from
pre-trained diffusion models at much higher resolutions than the training image
sizes. In addition, the generated images should have arbitrary image aspect
ratios. When generating images directly at a higher resolution, 1024 x 1024,
with the pre-trained Stable Diffusion using training images of resolution 512 x
512, we observe persistent problems of object repetition and unreasonable
object structures. Existing works for higher-resolution generation, such as
attention-based and joint-diffusion approaches, cannot well address these
issues. As a new perspective, we examine the structural components of the U-Net
in diffusion models and identify the crucial cause as the limited perception
field of convolutional kernels. Based on this key observation, we propose a
simple yet effective re-dilation that can dynamically adjust the convolutional
perception field during inference. We further propose the dispersed convolution
and noise-damped classifier-free guidance, which can enable
ultra-high-resolution image generation (e.g., 4096 x 4096). Notably, our
approach does not require any training or optimization. Extensive experiments
demonstrate that our approach can address the repetition issue well and achieve
state-of-the-art performance on higher-resolution image synthesis, especially
in texture details. Our work also suggests that a pre-trained diffusion model
trained on low-resolution images can be directly used for high-resolution
visual generation without further tuning, which may provide insights for future
research on ultra-high-resolution image and video synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yingqinghe.github.io/scalecrafter/ Github:
  https://github.com/YingqingHe/ScaleCrafter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched
  Captions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengfeng Lai, Haotian Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, Meng Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web-crawled datasets are pivotal to the success of pre-training
vision-language models, exemplified by CLIP. However, web-crawled AltTexts can
be noisy and potentially irrelevant to images, thereby undermining the crucial
image-text alignment. Existing methods for rewriting captions using large
language models (LLMs) have shown promise on small, curated datasets like CC3M
and CC12M. Nevertheless, their efficacy on massive web-captured captions is
constrained by the inherent noise and randomness in such data. In this study,
we address this limitation by focusing on two key aspects: data quality and
data variety. Unlike recent LLM rewriting techniques, we emphasize exploiting
visual concepts and their integration into the captions to improve data
quality. For data variety, we propose a novel mixed training scheme that
optimally leverages AltTexts alongside newly generated Visual-enriched Captions
(VeC). We use CLIP as one example and adapt the method for CLIP training on
large-scale web-crawled datasets, named VeCLIP. We conduct a comprehensive
evaluation of VeCLIP across small, medium, and large scales of raw data. Our
results show significant advantages in image-text alignment and overall model
performance, underscoring the effectiveness of VeCLIP in improving CLIP
training. For example, VeCLIP achieves a remarkable over 20% improvement in
COCO and Flickr30k retrieval tasks under the 12M setting. For data efficiency,
we also achieve a notable over 3% improvement while using only 14% of the data
employed in the vanilla CLIP and 11% in ALIGN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CV/ML</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Peng, Xinyuan Chen, Yaohui Wang, Chaochao Lu, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have successfully extended large-scale text-to-image models to
the video domain, producing promising results but at a high computational cost
and requiring a large amount of video data. In this work, we introduce
ConditionVideo, a training-free approach to text-to-video generation based on
the provided condition, video, and input text, by leveraging the power of
off-the-shelf text-to-image generation methods (e.g., Stable Diffusion).
ConditionVideo generates realistic dynamic videos from random noise or given
scene videos. Our method explicitly disentangles the motion representation into
condition-guided and scenery motion components. To this end, the ConditionVideo
model is designed with a UNet branch and a control branch. To improve temporal
coherence, we introduce sparse bi-directional spatial-temporal attention
(sBiST-Attn). The 3D control network extends the conventional 2D controlnet
model, aiming to strengthen conditional generation accuracy by additionally
leveraging the bi-directional frames in the temporal domain. Our method
exhibits superior performance in terms of frame consistency, clip score, and
conditional accuracy, outperforming other compared methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Orbital Polarimetric Tomography of a Flare Near the Sagittarius A*
  Supermassive Black Hole 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviad Levis, Andrew A. Chael, Katherine L. Bouman, Maciek Wielgus, Pratul P. Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interaction between the supermassive black hole at the center of the
Milky Way, Sagittarius A$^*$, and its accretion disk, occasionally produces
high energy flares seen in X-ray, infrared and radio. One mechanism for
observed flares is the formation of compact bright regions that appear within
the accretion disk and close to the event horizon. Understanding these flares
can provide a window into black hole accretion processes. Although
sophisticated simulations predict the formation of these flares, their
structure has yet to be recovered by observations. Here we show the first
three-dimensional (3D) reconstruction of an emission flare in orbit recovered
from ALMA light curves observed on April 11, 2017. Our recovery results show
compact bright regions at a distance of roughly 6 times the event horizon.
Moreover, our recovery suggests a clockwise rotation in a low-inclination
orbital plane, a result consistent with prior studies by EHT and GRAVITY
collaborations. To recover this emission structure we solve a highly ill-posed
tomography problem by integrating a neural 3D representation (an emergent
artificial intelligence approach for 3D reconstruction) with a gravitational
model for black holes. Although the recovered 3D structure is subject, and
sometimes sensitive, to the model assumptions, under physically motivated
choices we find that our results are stable and our approach is successful on
simulated data. We anticipate that in the future, this approach could be used
to analyze a richer collection of time-series data that could shed light on the
mechanisms governing black hole and plasma dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas
  from Hematoxylin and Eosin Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kshitij Ingale, Sun Hae Hong, Josh S. K. Bell, Abbas Rizvi, Amy Welch, Lingdao Sha, Irvin Ho, Kunal Nagpal, Aicha BenTaieb, Rohan P Joshi, Martin C Stumpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MET protein overexpression is a targetable event in non-small cell lung
cancer (NSCLC) and is the subject of active drug development. Challenges in
identifying patients for these therapies include lack of access to validated
testing, such as standardized immunohistochemistry (IHC) assessment, and
consumption of valuable tissue for a single gene/protein assay. Development of
pre-screening algorithms using routinely available digitized hematoxylin and
eosin (H&E)-stained slides to predict MET overexpression could promote testing
for those who will benefit most. While assessment of MET expression using IHC
is currently not routinely performed in NSCLC, next-generation sequencing is
common and in some cases includes RNA expression panel testing. In this work,
we leveraged a large database of matched H&E slides and RNA expression data to
train a weakly supervised model to predict MET RNA overexpression directly from
H&E images. This model was evaluated on an independent holdout test set of 300
over-expressed and 289 normal patients, demonstrating an ROC-AUC of 0.70 (95th
percentile interval: 0.66 - 0.74) with stable performance characteristics
across different patient clinical variables and robust to synthetic noise on
the test set. These results suggest that H&E-based predictive models could be
useful to prioritize patients for confirmatory testing of MET protein or MET
gene expression status.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis E. Livieris, Emmanuel Pintelas, Niki Kiriakidou, Panagiotis Pintelas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the proliferation of image-based applications in various domains, the
need for accurate and interpretable image similarity measures has become
increasingly critical. Existing image similarity models often lack
transparency, making it challenging to understand the reasons why two images
are considered similar. In this paper, we propose the concept of explainable
image similarity, where the goal is the development of an approach, which is
capable of providing similarity scores along with visual factual and
counterfactual explanations. Along this line, we present a new framework, which
integrates Siamese Networks and Grad-CAM for providing explainable image
similarity and discuss the potential benefits and challenges of adopting this
approach. In addition, we provide a comprehensive discussion about factual and
counterfactual explanations provided by the proposed framework for assisting
decision making. The proposed approach has the potential to enhance the
interpretability, trustworthiness and user acceptance of image-based systems in
real-world image similarity applications. The implementation code can be found
in https://github.com/ioannislivieris/Grad_CAM_Siamese.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript has been submitted for publication in "Journal of
  Imaging"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HaarNet: Large-scale Linear-Morphological Hybrid Network for RGB-D
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rick Groenendijk, Leo Dorst, Theo Gevers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Signals from different modalities each have their own combination algebra
which affects their sampling processing. RGB is mostly linear; depth is a
geometric signal following the operations of mathematical morphology. If a
network obtaining RGB-D input has both kinds of operators available in its
layers, it should be able to give effective output with fewer parameters. In
this paper, morphological elements in conjunction with more familiar linear
modules are used to construct a mixed linear-morphological network called
HaarNet. This is the first large-scale linear-morphological hybrid, evaluated
on a set of sizeable real-world datasets. In the network, morphological Haar
sampling is applied to both feature channels in several layers, which splits
extreme values and high-frequency information such that both can be processed
to improve both modalities. Moreover, morphologically parameterised ReLU is
used, and morphologically-sound up-sampling is applied to obtain a
full-resolution output. Experiments show that HaarNet is competitive with a
state-of-the-art CNN, implying that morphological networks are a promising
research direction for geometry-based learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Vision Transformers Based on Heterogeneous Attention
  Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deli Yu, Teng Xi, Jianwei Li, Baopu Li, Gang Zhang, Haocheng Feng, Junyu Han, Jingtuo Liu, Errui Ding, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Vision Transformers (ViTs) have attracted a lot of attention in the
field of computer vision. Generally, the powerful representative capacity of
ViTs mainly benefits from the self-attention mechanism, which has a high
computation complexity. To accelerate ViTs, we propose an integrated
compression pipeline based on observed heterogeneous attention patterns across
layers. On one hand, different images share more similar attention patterns in
early layers than later layers, indicating that the dynamic query-by-key
self-attention matrix may be replaced with a static self-attention matrix in
early layers. Then, we propose a dynamic-guided static self-attention (DGSSA)
method where the matrix inherits self-attention information from the replaced
dynamic self-attention to effectively improve the feature representation
ability of ViTs. On the other hand, the attention maps have more low-rank
patterns, which reflect token redundancy, in later layers than early layers. In
a view of linear dimension reduction, we further propose a method of global
aggregation pyramid (GLAD) to reduce the number of tokens in later layers of
ViTs, such as Deit. Experimentally, the integrated compression pipeline of
DGSSA and GLAD can accelerate up to 121% run-time throughput compared with
DeiT, which surpasses all SOTA approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Video Inpainting Guided by Audio-Visual Self-Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyuyeon Kim, Junsik Jung, Woo Jae Kim, Sung-Eui Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can easily imagine a scene from auditory information based on their
prior knowledge of audio-visual events. In this paper, we mimic this innate
human ability in deep learning models to improve the quality of video
inpainting. To implement the prior knowledge, we first train the audio-visual
network, which learns the correspondence between auditory and visual
information. Then, the audio-visual network is employed as a guider that
conveys the prior knowledge of audio-visual correspondence to the video
inpainting network. This prior knowledge is transferred through our proposed
two novel losses: audio-visual attention loss and audio-visual pseudo-class
consistency loss. These two losses further improve the performance of the video
inpainting by encouraging the inpainting result to have a high correspondence
to its synchronized audio. Experimental results demonstrate that our proposed
method can restore a wider domain of video scenes and is particularly effective
when the sounding object in the scene is partially blinded.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Enhanced Detector For Building Detection From Remote Sensing
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Huang, Mingming Zhang, Qingjie Liu, Wei Wang, Zhe Dong, Yunhong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of building detection from remote sensing images has made
significant progress, but faces challenges in achieving high-accuracy detection
due to the diversity in building appearances and the complexity of vast scenes.
To address these challenges, we propose a novel approach called
Context-Enhanced Detector (CEDet). Our approach utilizes a three-stage cascade
structure to enhance the extraction of contextual information and improve
building detection accuracy. Specifically, we introduce two modules: the
Semantic Guided Contextual Mining (SGCM) module, which aggregates multi-scale
contexts and incorporates an attention mechanism to capture long-range
interactions, and the Instance Context Mining Module (ICMM), which captures
instance-level relationship context by constructing a spatial relationship
graph and aggregating instance features. Additionally, we introduce a semantic
segmentation loss based on pseudo-masks to guide contextual information
extraction. Our method achieves state-of-the-art performance on three building
detection benchmarks, including CNBuilding-9P, CNBuilding-23P, and SpaceNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-Map Augmentation for Hypercomplex Breast Cancer Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleonora Lopez, Filippo Betello, Federico Carmignani, Eleonora Grassucci, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer is the most widespread neoplasm among women and early detection
of this disease is critical. Deep learning techniques have become of great
interest to improve diagnostic performance. Nonetheless, discriminating between
malignant and benign masses from whole mammograms remains challenging due to
them being almost identical to an untrained eye and the region of interest
(ROI) occupying a minuscule portion of the entire image. In this paper, we
propose a framework, parameterized hypercomplex attention maps (PHAM), to
overcome these problems. Specifically, we deploy an augmentation step based on
computing attention maps. Then, the attention maps are used to condition the
classification step by constructing a multi-dimensional input comprised of the
original breast cancer image and the corresponding attention map. In this step,
a parameterized hypercomplex neural network (PHNN) is employed to perform
breast cancer classification. The framework offers two main advantages. First,
attention maps provide critical information regarding the ROI and allow the
neural model to concentrate on it. Second, the hypercomplex architecture has
the ability to model local relations between input dimensions thanks to
hypercomplex algebra rules, thus properly exploiting the information provided
by the attention map. We demonstrate the efficacy of the proposed framework on
both mammography images as well as histopathological ones, surpassing
attention-based state-of-the-art networks and the real-valued counterpart of
our method. The code of our work is available at
https://github.com/elelo22/AttentionBCS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Pattern Recognition Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prompt Backdoors in Visual Prompt Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large pre-trained computer vision models is infeasible for
resource-limited users. Visual prompt learning (VPL) has thus emerged to
provide an efficient and flexible alternative to model fine-tuning through
Visual Prompt as a Service (VPPTaaS). Specifically, the VPPTaaS provider
optimizes a visual prompt given downstream data, and downstream users can use
this prompt together with the large pre-trained model for prediction. However,
this new learning paradigm may also pose security risks when the VPPTaaS
provider instead provides a malicious visual prompt. In this paper, we take the
first step to explore such risks through the lens of backdoor attacks.
Specifically, we propose BadVisualPrompt, a simple yet effective backdoor
attack against VPL. For example, poisoning $5\%$ CIFAR10 training data leads to
above $99\%$ attack success rates with only negligible model accuracy drop by
$1.5\%$. In particular, we identify and then address a new technical challenge
related to interactions between the backdoor trigger and visual prompt, which
does not exist in conventional, model-level backdoors. Moreover, we provide
in-depth analyses of seven backdoor defenses from model, prompt, and input
levels. Overall, all these defenses are either ineffective or impractical to
mitigate our BadVisualPrompt, implying the critical vulnerability of VPL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Quaternion Rotational and Translational Equivariance in 3D Rigid
  Motion Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme Vieira, Eleonora Grassucci, Marcos Eduardo Valle, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objects' rigid motions in 3D space are described by rotations and
translations of a highly-correlated set of points, each with associated $x,y,z$
coordinates that real-valued networks consider as separate entities, losing
information. Previous works exploit quaternion algebra and their ability to
model rotations in 3D space. However, these algebras do not properly encode
translations, leading to sub-optimal performance in 3D learning tasks. To
overcome these limitations, we employ a dual quaternion representation of rigid
motions in the 3D space that jointly describes rotations and translations of
point sets, processing each of the points as a single entity. Our approach is
translation and rotation equivariant, so it does not suffer from shifts in the
data and better learns object trajectories, as we validate in the experimental
evaluations. Models endowed with this formulation outperform previous
approaches in a human pose forecasting application, attesting to the
effectiveness of the proposed dual quaternion formulation for rigid motions in
3D space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE MLSP 2023 (Honorable Mention Top 10% Outstanding
  Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autononous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhang, Li Wang, Jian Chen, Cheng Fang, Lei Yang, Ziying Song, Guangqi Yang, Yichen Wang, Xiaofei Zhang, Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radar has stronger adaptability in adverse scenarios for autonomous driving
environmental perception compared to widely adopted cameras and LiDARs.
Compared with commonly used 3D radars, latest 4D radars have precise vertical
resolution and higher point cloud density, making it a highly promising sensor
for autonomous driving in complex environmental perception. However, due to the
much higher noise than LiDAR, manufacturers choose different filtering
strategies, resulting in an inverse ratio between noise level and point cloud
density. There is still a lack of comparative analysis on which method is
beneficial for deep learning-based perception algorithms in autonomous driving.
One of the main reasons is that current datasets only adopt one type of 4D
radar, making it difficult to compare different 4D radars in the same scene.
Therefore, in this paper, we introduce a novel large-scale multi-modal dataset
featuring, for the first time, two types of 4D radars captured simultaneously.
This dataset enables further research into effective 4D radar perception
algorithms.Our dataset consists of 151 consecutive series, most of which last
20 seconds and contain 10,007 meticulously synchronized and annotated frames.
Moreover, our dataset captures a variety of challenging driving scenarios,
including many road conditions, weather conditions, nighttime and daytime with
different lighting intensities and periods. Our dataset annotates consecutive
frames, which can be applied to 3D object detection and tracking, and also
supports the study of multi-modal tasks. We experimentally validate our
dataset, providing valuable results for studying different types of 4D radars.
This dataset is released on https://github.com/adept-thu/Dual-Radar.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PeP: a Point enhanced Painting method for unified point cloud tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichao Dong, Hang Ji, Xufeng Huang, Weikun Zhang, Xin Zhan, Junbo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point encoder is of vital importance for point cloud recognition. As the very
beginning step of whole model pipeline, adding features from diverse sources
and providing stronger feature encoding mechanism would provide better input
for downstream modules. In our work, we proposed a novel PeP module to tackle
above issue. PeP contains two main parts, a refined point painting method and a
LM-based point encoder. Experiments results on the nuScenes and KITTI datasets
validate the superior performance of our PeP. The advantages leads to strong
performance on both semantic segmentation and object detection, in both lidar
and multi-modal settings. Notably, our PeP module is model agnostic and
plug-and-play. Our code will be publicly available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Discrepancy Aware Framework for Robust Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Cai, Dingkang Liang, Dongliang Luo, Xinwei He, Xin Yang, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defect detection is a critical research area in artificial intelligence.
Recently, synthetic data-based self-supervised learning has shown great
potential on this task. Although many sophisticated synthesizing strategies
exist, little research has been done to investigate the robustness of models
when faced with different strategies. In this paper, we focus on this issue and
find that existing methods are highly sensitive to them. To alleviate this
issue, we present a Discrepancy Aware Framework (DAF), which demonstrates
robust performance consistently with simple and cheap strategies across
different anomaly detection benchmarks. We hypothesize that the high
sensitivity to synthetic data of existing self-supervised methods arises from
their heavy reliance on the visual appearance of synthetic data during
decoding. In contrast, our method leverages an appearance-agnostic cue to guide
the decoder in identifying defects, thereby alleviating its reliance on
synthetic appearance. To this end, inspired by existing knowledge distillation
methods, we employ a teacher-student network, which is trained based on
synthesized outliers, to compute the discrepancy map as the cue. Extensive
experiments on two challenging datasets prove the robustness of our method.
Under the simple synthesis strategies, it outperforms existing methods by a
large margin. Furthermore, it also achieves the state-of-the-art localization
performance. Code is available at: https://github.com/caiyuxuan1120/DAF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Industrial Informatics. Code is
  available at: https://github.com/caiyuxuan1120/DAF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Centrality of the Fingerprint Core Location 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurenz Ruzicka, Bernhard Strobl, Bernhard Kohn, Clemens Heitzinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fingerprints have long been recognized as a unique and reliable means of
personal identification. Central to the analysis and enhancement of
fingerprints is the concept of the fingerprint core. Although the location of
the core is used in many applications, to the best of our knowledge, this study
is the first to investigate the empirical distribution of the core over a
large, combined dataset of rolled, as well as plain fingerprint recordings. We
identify and investigate the extent of incomplete rolling during the rolled
fingerprint acquisition and investigate the centrality of the core. After
correcting for the incomplete rolling, we find that the core deviates from the
fingerprint center by 5.7% $\pm$ 5.2% to 7.6% $\pm$ 6.9%, depending on the
finger. Additionally, we find that the assumption of normal distribution of the
core position of plain fingerprint recordings cannot be rejected, but for
rolled ones it can. Therefore, we use a multi-step process to find the
distribution of the rolled fingerprint recordings. The process consists of an
Anderson-Darling normality test, the Bayesian Information Criterion to reduce
the number of possible candidate distributions and finally a Generalized Monte
Carlo goodness-of-fit procedure to find the best fitting distribution. We find
the non-central Fischer distribution best describes the cores' horizontal
positions. Finally, we investigate the correlation between mean core position
offset and the NFIQ 2 score and find that the NFIQ 2 prefers rolled fingerprint
recordings where the core sits slightly below the fingerprint center.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Prior Knowledge Graphs for Detection and Instance
  Segmentation <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osman Ülger, Yu Wang, Ysbrand Galama, Sezer Karaoglu, Theo Gevers, Martin R. Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans have a remarkable ability to perceive and reason about the world
around them by understanding the relationships between objects. In this paper,
we investigate the effectiveness of using such relationships for object
detection and instance segmentation. To this end, we propose a Relational
Prior-based Feature Enhancement Model (RP-FEM), a graph transformer that
enhances object proposal features using relational priors. The proposed
architecture operates on top of scene graphs obtained from initial proposals
and aims to concurrently learn relational context modeling for object detection
and instance segmentation. Experimental evaluations on COCO show that the
utilization of scene graphs, augmented with relational priors, offer benefits
for object detection and instance segmentation. RP-FEM demonstrates its
capacity to suppress improbable class predictions within the image while also
preventing the model from generating duplicate predictions, leading to
improvements over the baseline model on which it is built.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICCV2023 SG2RL Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Label Types on Training SWIN Models with Overhead Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Ford, Kenneth Hutchison, Nicholas Felts, Benjamin Cheng, Jesse Lew, Kyle Jackson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the impact of data set design on model training and performance
can help alleviate the costs associated with generating remote sensing and
overhead labeled data. This work examined the impact of training shifted window
transformers using bounding boxes and segmentation labels, where the latter are
more expensive to produce. We examined classification tasks by comparing models
trained with both target and backgrounds against models trained with only
target pixels, extracted by segmentation labels. For object detection models,
we compared performance using either label type when training. We found that
the models trained on only target pixels do not show performance improvement
for classification tasks, appearing to conflate background pixels in the
evaluation set with target pixels. For object detection, we found that models
trained with either label type showed equivalent performance across testing. We
found that bounding boxes appeared to be sufficient for tasks that did not
require more complex labels, such as object segmentation. Continuing work to
determine consistency of this result across data types and model architectures
could potentially result in substantial savings in generating remote sensing
data sets for deep learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does resistance to Style-Transfer equal Shape Bias? Evaluating Shape
  Bias by Distorted Shape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Wen, Tianqin Li, Tai Sing Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models are known to exhibit a strong texture bias, while human
tends to rely heavily on global shape for object recognition. The current
benchmark for evaluating a model's shape bias is a set of style-transferred
images with the assumption that resistance to the attack of style transfer is
related to the development of shape sensitivity in the model. In this work, we
show that networks trained with style-transfer images indeed learn to ignore
style, but its shape bias arises primarily from local shapes. We provide a
Distorted Shape Testbench (DiST) as an alternative measurement of global shape
sensitivity. Our test includes 2400 original images from ImageNet-1K, each of
which is accompanied by two images with the global shapes of the original image
distorted while preserving its texture via the texture synthesis program. We
found that (1) models that performed well on the previous shape bias evaluation
do not fare well in the proposed DiST; (2) the widely adopted ViT models do not
show significant advantages over Convolutional Neural Networks (CNNs) on this
benchmark despite that ViTs rank higher on the previous shape bias tests. (3)
training with DiST images bridges the significant gap between human and
existing SOTA models' performance while preserving the models' accuracy on
standard image classification tasks; training with DiST images and
style-transferred images are complementary, and can be combined to train
network together to enhance both the global and local shape sensitivity of the
network. Our code will be host at: https://github.com/leelabcnbc/DiST
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProtoHPE: Prototype-guided High-frequency Patch Enhancement for
  Visible-Infrared Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guiwei Zhang, Yongfei Zhang, Zichang Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible-infrared person re-identification is challenging due to the large
modality gap. To bridge the gap, most studies heavily rely on the correlation
of visible-infrared holistic person images, which may perform poorly under
severe distribution shifts. In contrast, we find that some cross-modal
correlated high-frequency components contain discriminative visual patterns and
are less affected by variations such as wavelength, pose, and background
clutter than holistic images. Therefore, we are motivated to bridge the
modality gap based on such high-frequency components, and propose
\textbf{Proto}type-guided \textbf{H}igh-frequency \textbf{P}atch
\textbf{E}nhancement (ProtoHPE) with two core designs. \textbf{First}, to
enhance the representation ability of cross-modal correlated high-frequency
components, we split patches with such components by Wavelet Transform and
exponential moving average Vision Transformer (ViT), then empower ViT to take
the split patches as auxiliary input. \textbf{Second}, to obtain semantically
compact and discriminative high-frequency representations of the same identity,
we propose Multimodal Prototypical Contrast. To be specific, it hierarchically
captures the comprehensive semantics of different modal instances, facilitating
the aggregation of high-frequency representations belonging to the same
identity. With it, ViT can capture key high-frequency components during
inference without relying on ProtoHPE, thus bringing no extra complexity.
Extensive experiments validate the effectiveness of ProtoHPE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attribute Localization and Revision Network for Zero-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzhe Xu, Suling Duan, Chenwei Tang, Zhenan He, Jiancheng Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot learning enables the model to recognize unseen categories with the
aid of auxiliary semantic information such as attributes. Current works
proposed to detect attributes from local image regions and align extracted
features with class-level semantics. In this paper, we find that the choice
between local and global features is not a zero-sum game, global features can
also contribute to the understanding of attributes. In addition, aligning
attribute features with class-level semantics ignores potential intra-class
attribute variation. To mitigate these disadvantages, we present Attribute
Localization and Revision Network in this paper. First, we design Attribute
Localization Module (ALM) to capture both local and global features from image
regions, a novel module called Scale Control Unit is incorporated to fuse
global and local representations. Second, we propose Attribute Revision Module
(ARM), which generates image-level semantics by revising the ground-truth value
of each attribute, compensating for performance degradation caused by ignoring
intra-class variation. Finally, the output of ALM will be aligned with revised
semantics produced by ARM to achieve the training process. Comprehensive
experimental results on three widely used benchmarks demonstrate the
effectiveness of our model in the zero-shot prediction task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Centered Evaluation of XAI Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karam Dawoud, Wojciech Samek, Sebastian Lapuschkin, Sebastian Bosse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-evolving field of Artificial Intelligence, a critical challenge
has been to decipher the decision-making processes within the so-called "black
boxes" in deep learning. Over recent years, a plethora of methods have emerged,
dedicated to explaining decisions across diverse tasks. Particularly in tasks
like image classification, these methods typically identify and emphasize the
pivotal pixels that most influence a classifier's prediction. Interestingly,
this approach mirrors human behavior: when asked to explain our rationale for
classifying an image, we often point to the most salient features or aspects.
Capitalizing on this parallel, our research embarked on a user-centric study.
We sought to objectively measure the interpretability of three leading
explanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3)
Layer-wise Relevance Propagation. Intriguingly, our results highlight that
while the regions spotlighted by these methods can vary widely, they all offer
humans a nearly equivalent depth of understanding. This enables users to
discern and categorize images efficiently, reinforcing the value of these
methods in enhancing AI transparency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S4C: Self-Supervised Semantic Scene Completion with Neural Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Hayler, Felix Wimbauer, Dominik Muhle, Christian Rupprecht, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D semantic scene understanding is a fundamental challenge in computer
vision. It enables mobile agents to autonomously plan and navigate arbitrary
environments. SSC formalizes this challenge as jointly estimating dense
geometry and semantic information from sparse observations of a scene. Current
methods for SSC are generally trained on 3D ground truth based on aggregated
LiDAR scans. This process relies on special sensors and annotation by hand
which are costly and do not scale well. To overcome this issue, our work
presents the first self-supervised approach to SSC called S4C that does not
rely on 3D ground truth data. Our proposed method can reconstruct a scene from
a single image and only relies on videos and pseudo segmentation ground truth
generated from off-the-shelf image segmentation network during training. Unlike
existing methods, which use discrete voxel grids, we represent scenes as
implicit semantic fields. This formulation allows querying any point within the
camera frustum for occupancy and semantic class. Our architecture is trained
through rendering-based self-supervised losses. Nonetheless, our method
achieves performance close to fully supervised state-of-the-art methods.
Additionally, our method demonstrates strong generalization capabilities and
can synthesize accurate segmentation maps for far away viewpoints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CM-PIE: Cross-modal perception for interactive-enhanced audio-visual
  video parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaru Chen, Ruohao Guo, Xubo Liu, Peipei Wu, Guangyao Li, Zhenbo Li, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual video parsing is the task of categorizing a video at the segment
level with weak labels, and predicting them as audible or visible events.
Recent methods for this task leverage the attention mechanism to capture the
semantic correlations among the whole video across the audio-visual modalities.
However, these approaches have overlooked the importance of individual segments
within a video and the relationship among them, and tend to rely on a single
modality when learning features. In this paper, we propose a novel
interactive-enhanced cross-modal perception method~(CM-PIE), which can learn
fine-grained features by applying a segment-based attention module.
Furthermore, a cross-modal aggregation block is introduced to jointly optimize
the semantic representation of audio and visual signals by enhancing
inter-modal interactions. The experimental results show that our model offers
improved parsing performance on the Look, Listen, and Parse dataset compared to
other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 15 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes
  via Deviation Relationship Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtao Li, Xinyu Wang, Hengwei Zhao, Liangpei Zhang, Yanfei Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing anomaly detector can find the objects deviating from the
background as potential targets. Given the diversity in earth anomaly types, a
unified anomaly detector across modalities and scenes should be cost-effective
and flexible to new earth observation sources and anomaly types. However, the
current anomaly detectors are limited to a single modality and single scene,
since they aim to learn the varying background distribution. Motivated by the
universal anomaly deviation pattern, in that anomalies exhibit deviations from
their local context, we exploit this characteristic to build a unified anomaly
detector. Firstly, we reformulate the anomaly detection task as an undirected
bilayer graph based on the deviation relationship, where the anomaly score is
modeled as the conditional probability, given the pattern of the background and
normal objects. The learning objective is then expressed as a conditional
probability ranking problem. Furthermore, we design an instantiation of the
reformulation in the data, architecture, and optimization aspects. Simulated
spectral and spatial anomalies drive the instantiated architecture. The model
is optimized directly for the conditional probability ranking. The proposed
model was validated in five modalities including the hyperspectral, visible
light, synthetic aperture radar (SAR), infrared and low light to show its
unified detection ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heuristic Vision Pre-Training with Self-Supervised and Supervised
  Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To mimic human vision with the way of recognizing the diverse and open world,
foundation vision models are much critical. While recent techniques of
self-supervised learning show the promising potentiality of this mission, we
argue that signals from labelled data are also important for common-sense
recognition, and properly chosen pre-text tasks can facilitate the efficiency
of vision representation learning. To this end, we propose a novel pre-training
framework by adopting both self-supervised and supervised visual pre-text tasks
in a multi-task manner. Specifically, given an image, we take a heuristic way
by considering its intrinsic style properties, inside objects with their
locations and correlations, and how it looks like in 3D space for basic visual
understanding. However, large-scale object bounding boxes and correlations are
usually hard to achieve. Alternatively, we develop a hybrid method by
leveraging both multi-label classification and self-supervised learning. On the
one hand, under the multi-label supervision, the pre-trained model can explore
the detailed information of an image, e.g., image types, objects, and part of
semantic relations. On the other hand, self-supervised learning tasks, with
respect to Masked Image Modeling (MIM) and contrastive learning, can help the
model learn pixel details and patch correlations. Results show that our
pre-trained models can deliver results on par with or better than
state-of-the-art (SOTA) results on multiple visual tasks. For example, with a
vanilla Swin-B backbone, we achieve 85.3\% top-1 accuracy on ImageNet-1K
classification, 47.9 box AP on COCO object detection for Mask R-CNN, and 50.6
mIoU on ADE-20K semantic segmentation when using Upernet. The performance shows
the ability of our vision foundation model to serve general purpose vision
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Hierarchical Feature Sharing for Efficient Dataset
  Condensation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haizhong Zheng, Jiachen Sun, Shutong Wu, Bhavya Kailkhura, Zhuoqing Mao, Chaowei Xiao, Atul Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a real-world dataset, data condensation (DC) aims to synthesize a
significantly smaller dataset that captures the knowledge of this dataset for
model training with high performance. Recent works propose to enhance DC with
data parameterization, which condenses data into parameterized data containers
rather than pixel space. The intuition behind data parameterization is to
encode shared features of images to avoid additional storage costs. In this
paper, we recognize that images share common features in a hierarchical way due
to the inherent hierarchical structure of the classification system, which is
overlooked by current data parameterization methods. To better align DC with
this hierarchical nature and encourage more efficient information sharing
inside data containers, we propose a novel data parameterization architecture,
Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier
structure, representing the dataset-level, class-level, and instance-level
features. Another helpful property of the hierarchical architecture is that HMN
naturally ensures good independence among images despite achieving information
sharing. This enables instance-level pruning for HMN to reduce redundant
information, thereby further minimizing redundancy and enhancing performance.
We evaluate HMN on four public datasets (SVHN, CIFAR10, CIFAR100, and
Tiny-ImageNet) and compare HMN with eight DC baselines. The evaluation results
show that our proposed method outperforms all baselines, even when trained with
a batch-based loss consuming less GPU memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PtychoDV: Vision Transformer-Based Deep Unrolling Network for
  Ptychographic Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Gan, Qiuchen Zhai, Michael Thompson McCann, Cristina Garcia Cardona, Ulugbek S. Kamilov, Brendt Wohlberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ptychography is an imaging technique that captures multiple overlapping
snapshots of a sample, illuminated coherently by a moving localized probe. The
image recovery from ptychographic data is generally achieved via an iterative
algorithm that solves a nonlinear phase-field problem derived from measured
diffraction patterns. However, these approaches have high computational cost.
In this paper, we introduce PtychoDV, a novel deep model-based network designed
for efficient, high-quality ptychographic image reconstruction. PtychoDV
comprises a vision transformer that generates an initial image from the set of
raw measurements, taking into consideration their mutual correlations. This is
followed by a deep unrolling network that refines the initial image using
learnable convolutional priors and the ptychography measurement model.
Experimental results on simulated data demonstrate that PtychoDV is capable of
outperforming existing deep learning methods for this problem, and
significantly reduces computational cost compared to iterative methodologies,
while maintaining competitive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Black-box Attack to Deep Neural Networks with Conditional
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renyang Liu, Wei Zhou, Tianwei Zhang, Kangjie Chen, Jun Zhao, Kwok-Yan Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing black-box attacks have demonstrated promising potential in creating
adversarial examples (AE) to deceive deep learning models. Most of these
attacks need to handle a vast optimization space and require a large number of
queries, hence exhibiting limited practical impacts in real-world scenarios. In
this paper, we propose a novel black-box attack strategy, Conditional Diffusion
Model Attack (CDMA), to improve the query efficiency of generating AEs under
query-limited situations. The key insight of CDMA is to formulate the task of
AE synthesis as a distribution transformation problem, i.e., benign examples
and their corresponding AEs can be regarded as coming from two distinctive
distributions and can transform from each other with a particular converter.
Unlike the conventional \textit{query-and-optimization} approach, we generate
eligible AEs with direct conditional transform using the aforementioned data
converter, which can significantly reduce the number of queries needed. CDMA
adopts the conditional Denoising Diffusion Probabilistic Model as the
converter, which can learn the transformation from clean samples to AEs, and
ensure the smooth development of perturbed noise resistant to various defense
strategies. We demonstrate the effectiveness and efficiency of CDMA by
comparing it with nine state-of-the-art black-box attacks across three
benchmark datasets. On average, CDMA can reduce the query count to a handful of
times; in most cases, the query count is only ONE. We also show that CDMA can
obtain $>99\%$ attack success rate for untarget attacks over all datasets and
targeted attack over CIFAR-10 with the noise budget of $\epsilon=16$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Sun, Peihao Chen, Jugang Fan, Thomas H. Li, Jian Chen, Mingkui Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning to navigate to an image-specified goal is an important but
challenging task for autonomous systems. The agent is required to reason the
goal location from where a picture is shot. Existing methods try to solve this
problem by learning a navigation policy, which captures semantic features of
the goal image and observation image independently and lastly fuses them for
predicting a sequence of navigation actions. However, these methods suffer from
two major limitations. 1) They may miss detailed information in the goal image,
and thus fail to reason the goal location. 2) More critically, it is hard to
focus on the goal-relevant regions in the observation image, because they
attempt to understand observation without goal conditioning. In this paper, we
aim to overcome these limitations by designing a Fine-grained Goal Prompting
(FGPrompt) method for image-goal navigation. In particular, we leverage
fine-grained and high-resolution feature maps in the goal image as prompts to
perform conditioned embedding, which preserves detailed information in the goal
image and guides the observation encoder to pay attention to goal-relevant
regions. Compared with existing methods on the image-goal navigation benchmark,
our method brings significant performance improvement on 3 benchmark datasets
(i.e., Gibson, MP3D, and HM3D). Especially on Gibson, we surpass the
state-of-the-art success rate by 8% with only 1/50 model size. Project page:
https://xinyusun.github.io/fgprompt-pages
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Wang Bian, Wenjing Bian, Victor Adrian Prisacariu, Philip Torr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural surface reconstruction is sensitive to the camera pose noise, even if
state-of-the-art pose estimators like COLMAP or ARKit are used. More
importantly, existing Pose-NeRF joint optimisation methods have struggled to
improve pose accuracy in challenging real-world scenarios. To overcome the
challenges, we introduce the pose residual field (\textbf{PoRF}), a novel
implicit representation that uses an MLP for regressing pose updates. This is
more robust than the conventional pose parameter optimisation due to parameter
sharing that leverages global information over the entire sequence.
Furthermore, we propose an epipolar geometry loss to enhance the supervision
that leverages the correspondences exported from COLMAP results without the
extra computational overhead. Our method yields promising results. On the DTU
dataset, we reduce the rotation error by 78\% for COLMAP poses, leading to the
decreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the
MobileBrick dataset that contains casually captured unbounded 360-degree
videos, our method refines ARKit poses and improves the reconstruction F1 score
from 69.18 to 75.67, outperforming that with the dataset provided ground-truth
pose (75.14). These achievements demonstrate the efficacy of our approach in
refining camera poses and improving the accuracy of neural surface
reconstruction in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distance-based Weighted Transformer Network for Image Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pourya Shamsolmoali, Masoumeh Zareapoor, Huiyu Zhou, Xuelong Li, Yue Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of image generation has been effectively modeled as a problem
of structure priors or transformation. However, existing models have
unsatisfactory performance in understanding the global input image structures
because of particular inherent features (for example, local inductive prior).
Recent studies have shown that self-attention is an efficient modeling
technique for image completion problems. In this paper, we propose a new
architecture that relies on Distance-based Weighted Transformer (DWT) to better
understand the relationships between an image's components. In our model, we
leverage the strengths of both Convolutional Neural Networks (CNNs) and DWT
blocks to enhance the image completion process. Specifically, CNNs are used to
augment the local texture information of coarse priors and DWT blocks are used
to recover certain coarse textures and coherent visual structures. Unlike
current approaches that generally use CNNs to create feature maps, we use the
DWT to encode global dependencies and compute distance-based weighted feature
maps, which substantially minimizes the problem of visual ambiguities.
Meanwhile, to better produce repeated textures, we introduce Residual Fast
Fourier Convolution (Res-FFC) blocks to combine the encoder's skip features
with the coarse features provided by our generator. Furthermore, a simple yet
effective technique is proposed to normalize the non-zero values of
convolutions, and fine-tune the network layers for regularization of the
gradient norms to provide an efficient training stabiliser. Extensive
quantitative and qualitative experiments on three challenging datasets
demonstrate the superiority of our proposed model compared to existing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for
  Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rezaul Karim, Soheil Mohamad Alizadeh Shabestary, Amir Rasouli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting temporally consistent road users' trajectories in a multi-agent
setting is a challenging task due to unknown characteristics of agents and
their varying intentions. Besides using semantic map information and modeling
interactions, it is important to build an effective mechanism capable of
reasoning about behaviors at different levels of granularity. To this end, we
propose Dynamic goal quErieS with temporal Transductive alIgNmEnt (DESTINE)
method. Unlike past arts, our approach 1) dynamically predicts agents' goals
irrespective of particular road structures, such as lanes, allowing the method
to produce a more accurate estimation of destinations; 2) achieves map
compliant predictions by generating future trajectories in a coarse-to-fine
fashion, where the coarser predictions at a lower frame rate serve as
intermediate goals; and 3) uses an attention module designed to temporally
align predicted trajectories via masked attention. Using the common Argoverse
benchmark dataset, we show that our method achieves state-of-the-art
performance on various metrics, and further investigate the contributions of
proposed modules via comprehensive ablation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 tables 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing
  Else 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazarapet Tunanyan, Dejia Xu, Shant Navasardyan, Zhangyang Wang, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-image diffusion models have enabled the
photorealistic generation of images from text prompts. Despite the great
progress, existing models still struggle to generate compositional
multi-concept images naturally, limiting their ability to visualize human
imagination. While several recent works have attempted to address this issue,
they either introduce additional training or adopt guidance at inference time.
In this work, we consider a more ambitious goal: natural multi-concept
generation using a pre-trained diffusion model, and with almost no extra cost.
To achieve this goal, we identify the limitations in the text embeddings used
for the pre-trained text-to-image diffusion models. Specifically, we observe
concept dominance and non-localized contribution that severely degrade
multi-concept generation performance. We further design a minimal low-cost
solution that overcomes the above issues by tweaking (not re-training) the text
embeddings for more realistic multi-concept text-to-image generation. Our
Correction by Similarities method tweaks the embedding of concepts by
collecting semantic features from most similar tokens to localize the
contribution. To avoid mixing features of concepts, we also apply Cross-Token
Non-Maximum Suppression, which excludes the overlap of contributions from
different concepts. Experiments show that our approach outperforms previous
methods in text-to-image, image manipulation, and personalization tasks,
despite not introducing additional training or inference costs to the diffusion
steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Voronoi-based Convolutional Neural Network Framework for Pushing
  Person Detection in Crowd Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Alia, Mohammed Maree, Mohcine Chraibi, Armin Seyfried
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing the microscopic dynamics of pushing behavior within crowds can
offer valuable insights into crowd patterns and interactions. By identifying
instances of pushing in crowd videos, a deeper understanding of when, where,
and why such behavior occurs can be achieved. This knowledge is crucial to
creating more effective crowd management strategies, optimizing crowd flow, and
enhancing overall crowd experiences. However, manually identifying pushing
behavior at the microscopic level is challenging, and the existing automatic
approaches cannot detect such microscopic behavior. Thus, this article
introduces a novel automatic framework for identifying pushing in videos of
crowds on a microscopic level. The framework comprises two main components: i)
Feature extraction and ii) Video labeling. In the feature extraction component,
a new Voronoi-based method is developed for determining the local regions
associated with each person in the input video. Subsequently, these regions are
fed into EfficientNetV1B0 Convolutional Neural Network to extract the deep
features of each person over time. In the second component, a combination of a
fully connected layer with a Sigmoid activation function is employed to analyze
these deep features and annotate the individuals involved in pushing within the
video. The framework is trained and evaluated on a new dataset created using
six real-world experiments, including their corresponding ground truths. The
experimental findings indicate that the suggested framework outperforms seven
baseline methods that are employed for comparative analysis purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP for Lightweight Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Jin, Wankou Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large-scale pretrained model CLIP, trained on 400 million image-text
pairs, offers a promising paradigm for tackling vision tasks, albeit at the
image level. Later works, such as DenseCLIP and LSeg, extend this paradigm to
dense prediction, including semantic segmentation, and have achieved excellent
results. However, the above methods either rely on CLIP-pretrained visual
backbones or use none-pretrained but heavy backbones such as Swin, while
falling ineffective when applied to lightweight backbones. The reason for this
is that the lightweitht networks, feature extraction ability of which are
relatively limited, meet difficulty embedding the image feature aligned with
text embeddings perfectly. In this work, we present a new feature fusion module
which tackles this problem and enables language-guided paradigm to be applied
to lightweight networks. Specifically, the module is a parallel design of CNN
and transformer with a two-way bridge in between, where CNN extracts spatial
information and visual context of the feature map from the image encoder, and
the transformer propagates text embeddings from the text encoder forward. The
core of the module is the bidirectional fusion of visual and text feature
across the bridge which prompts their proximity and alignment in embedding
space. The module is model-agnostic, which can not only make language-guided
lightweight semantic segmentation practical, but also fully exploit the
pretrained knowledge of language priors and achieve better performance than
previous SOTA work, such as DenseCLIP, whatever the vision backbone is.
Extensive experiments have been conducted to demonstrate the superiority of our
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Unsupervised Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Kim, Byung-Kwan Lee, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised semantic segmentation aims to achieve high-quality semantic
grouping without human-labeled annotations. With the advent of self-supervised
pre-training, various frameworks utilize the pre-trained features to train
prediction heads for unsupervised dense prediction. However, a significant
challenge in this unsupervised setup is determining the appropriate level of
clustering required for segmenting concepts. To address it, we propose a novel
framework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages
insights from causal inference. Specifically, we bridge intervention-oriented
approach (i.e., frontdoor adjustment) to define suitable two-step tasks for
unsupervised prediction. The first step involves constructing a concept
clusterbook as a mediator, which represents possible concept prototypes at
different levels of granularity in a discretized form. Then, the mediator
establishes an explicit link to the subsequent concept-wise self-supervised
learning for pixel-level grouping. Through extensive experiments and analyses
on various datasets, we corroborate the effectiveness of CAUSE and achieve
state-of-the-art performance in unsupervised semantic segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code available:
  https://github.com/ByungKwanLee/Causal-Unsupervised-Segmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point Cloud Denoising and Outlier Detection with Local Geometric
  Structure by Dynamic Graph CNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Nakayama, Hiroto Fukuta, Hiroshi Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The digitalization of society is rapidly developing toward the realization of
the digital twin and metaverse. In particular, point clouds are attracting
attention as a media format for 3D space. Point cloud data is contaminated with
noise and outliers due to measurement errors. Therefore, denoising and outlier
detection are necessary for point cloud processing. Among them, PointCleanNet
is an effective method for point cloud denoising and outlier detection.
However, it does not consider the local geometric structure of the patch. We
solve this problem by applying two types of graph convolutional layer designed
based on the Dynamic Graph CNN. Experimental results show that the proposed
methods outperform the conventional method in AUPR, which indicates outlier
detection accuracy, and Chamfer Distance, which indicates denoising accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2023 IEEE 12th Global Conference on Consumer Electronics (GCCE 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Generalization Guided by Gradient Signal to Noise Ratio of
  Parameters <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Michalkiewicz, Masoud Faraki, Xiang Yu, Manmohan Chandraker, Mahsa Baktashmotlagh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overfitting to the source domain is a common issue in gradient-based training
of deep neural networks. To compensate for the over-parameterized models,
numerous regularization techniques have been introduced such as those based on
dropout. While these methods achieve significant improvements on classical
benchmarks such as ImageNet, their performance diminishes with the introduction
of domain shift in the test set i.e. when the unseen data comes from a
significantly different distribution. In this paper, we move away from the
classical approach of Bernoulli sampled dropout mask construction and propose
to base the selection on gradient-signal-to-noise ratio (GSNR) of network's
parameters. Specifically, at each training step, parameters with high GSNR will
be discarded. Furthermore, we alleviate the burden of manually searching for
the optimal dropout ratio by leveraging a meta-learning approach. We evaluate
our method on standard domain generalization benchmarks and achieve competitive
results on classification and face anti-spoofing problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper was accepted to ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance
  Images Using a Hybrid GAN-CNN Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masood Hamed Saghayan, Mohammad Hossein Zolfagharnasab, Ali Khadem, Farzam Matinfar, Hassan Rashidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bipolar Disorder (BD) is a psychiatric condition diagnosed by repetitive
cycles of hypomania and depression. Since diagnosing BD relies on subjective
behavioral assessments over a long period, a solid diagnosis based on objective
criteria is not straightforward. The current study responded to the described
obstacle by proposing a hybrid GAN-CNN model to diagnose BD from 3-D structural
MRI Images (sMRI). The novelty of this study stems from diagnosing BD from sMRI
samples rather than conventional datasets such as functional MRI (fMRI),
electroencephalography (EEG), and behavioral symptoms while removing the data
insufficiency usually encountered when dealing with sMRI samples. The impact of
various augmentation ratios is also tested using 5-fold cross-validation. Based
on the results, this study obtains an accuracy rate of 75.8%, a sensitivity of
60.3%, and a specificity of 82.5%, which are 3-5% higher than prior work while
utilizing less than 6% sample counts. Next, it is demonstrated that a 2- D
layer-based GAN generator can effectively reproduce complex 3D brain samples, a
more straightforward technique than manual image processing. Lastly, the
optimum augmentation threshold for the current study using 172 sMRI samples is
50%, showing the applicability of the described method for larger sMRI
datasets. In conclusion, it is established that data augmentation using GAN
improves the accuracy of the CNN classifier using sMRI samples, thus developing
more reliable decision support systems to assist practitioners in identifying
BD patients more reliably and in a shorter period
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che Liu, Sibo Cheng, Miaojing Shi, Anand Shah, Wenjia Bai, Rossella Arcucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of medical Vision-Language Pre-training (VLP), significant
efforts have been devoted to deriving text and image features from both
clinical reports and associated medical images. However, most existing methods
may have overlooked the opportunity in leveraging the inherent hierarchical
structure of clinical reports, which are generally split into `findings' for
descriptive content and `impressions' for conclusive observation. Instead of
utilizing this rich, structured format, current medical VLP approaches often
simplify the report into either a unified entity or fragmented tokens. In this
work, we propose a novel clinical prior guided VLP framework named IMITATE to
learn the structure information from medical reports with hierarchical
vision-language alignment. The framework derives multi-level visual features
from the chest X-ray (CXR) images and separately aligns these features with the
descriptive and the conclusive text encoded in the hierarchical medical report.
Furthermore, a new clinical-informed contrastive loss is introduced for
cross-modal learning, which accounts for clinical prior knowledge in
formulating sample correlations in contrastive learning. The proposed model,
IMITATE, outperforms baseline VLP methods across six different datasets,
spanning five medical imaging downstream tasks. Comprehensive experimental
results highlight the advantages of integrating the hierarchical structure of
medical reports for vision-language alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Attention for Interpretable Motion Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Radouane, Andon Tchechmedjiev, Sylvie Ranwez, Julien Lagarde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While much effort has been invested in generating human motion from text,
relatively few studies have been dedicated to the reverse direction, that is,
generating text from motion. Much of the research focuses on maximizing
generation quality without any regard for the interpretability of the
architectures, particularly regarding the influence of particular body parts in
the generation and the temporal synchronization of words with specific
movements and actions. This study explores the combination of movement encoders
with spatio-temporal attention models and proposes strategies to guide the
attention during training to highlight perceptually pertinent areas of the
skeleton in time. We show that adding guided attention with adaptive gate leads
to interpretable captioning while improving performance compared to higher
parameter-count non-interpretable SOTA systems. On the KIT MLD dataset, we
obtain a BLEU@4 of 24.4% (SOTA+6%), a ROUGE-L of 58.30% (SOTA +14.1%), a CIDEr
of 112.10 (SOTA +32.6) and a Bertscore of 41.20% (SOTA +18.20%). On HumanML3D,
we obtain a BLEU@4 of 25.00 (SOTA +2.7%), a ROUGE-L score of 55.4% (SOTA
+6.1%), a CIDEr of 61.6 (SOTA -10.9%), a Bertscore of 40.3% (SOTA +2.5%). Our
code implementation and reproduction details will be soon available at
https://github.com/rd20karim/M2T-Interpretable/tree/main.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A webcam-based machine learning approach for three-dimensional range of
  motion evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoye Michael Wang, Derek T. Smith, Qin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background. Joint range of motion (ROM) is an important quantitative measure
for physical therapy. Commonly relying on a goniometer, accurate and reliable
ROM measurement requires extensive training and practice. This, in turn,
imposes a significant barrier for those who have limited in-person access to
healthcare.
  Objective. The current study presents and evaluates an alternative machine
learning-based ROM evaluation method that could be remotely accessed via a
webcam.
  Methods. To evaluate its reliability, the ROM measurements for a diverse set
of joints (neck, spine, and upper and lower extremities) derived using this
method were compared to those obtained from a marker-based optical motion
capture system.
  Results. Data collected from 25 healthy adults demonstrated that the webcam
solution exhibited high test-retest reliability, with substantial to almost
perfect intraclass correlation coefficients for most joints. Compared with the
marker-based system, the webcam-based system demonstrated substantial to almost
perfect inter-rater reliability for some joints, and lower inter-rater
reliability for other joints (e.g., shoulder flexion and elbow flexion), which
could be attributed to the reduced sensitivity to joint locations at the apex
of the movement.
  Conclusions. The proposed webcam-based method exhibited high test-retest and
inter-rater reliability, making it a versatile alternative for existing ROM
evaluation methods in clinical practice and the tele-implementation of physical
therapy and rehabilitation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Aramaic: Towards a Synthetic Data Paradigm Enabling Machine
  Learning in Epigraphy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei C. Aioanei, Regine Hunziker-Rodewald, Konstantin Klein, Dominik L. Michels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Epigraphy increasingly turns to modern artificial intelligence (AI)
technologies such as machine learning (ML) for extracting insights from ancient
inscriptions. However, scarce labeled data for training ML algorithms severely
limits current techniques, especially for ancient scripts like Old Aramaic. Our
research pioneers an innovative methodology for generating synthetic training
data tailored to Old Aramaic letters. Our pipeline synthesizes photo-realistic
Aramaic letter datasets, incorporating textural features, lighting, damage, and
augmentations to mimic real-world inscription diversity. Despite minimal real
examples, we engineer a dataset of 250,000 training and 25,000 validation
images covering the 22 letter classes in the Aramaic alphabet. This
comprehensive corpus provides a robust volume of data for training a residual
neural network (ResNet) to classify highly degraded Aramaic letters. The ResNet
model demonstrates high accuracy in classifying real images from the 8th
century BCE Hadad statue inscription. Additional experiments validate
performance on varying materials and styles, proving effective generalization.
Our results validate the model's capabilities in handling diverse real-world
scenarios, proving the viability of our synthetic data approach and avoiding
the dependence on scarce training data that has constrained epigraphic
analysis. Our innovative framework elevates interpretation accuracy on damaged
inscriptions, thus enhancing knowledge extraction from these historical
resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 19 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Efficient Vision Transformers from CNNs for Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Zheng, Yunhao Luo, Pengyuan Zhou, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle a new problem: how to transfer knowledge from the
pre-trained cumbersome yet well-performed CNN-based model to learn a compact
Vision Transformer (ViT)-based model while maintaining its learning capacity?
Due to the completely different characteristics of ViT and CNN and the
long-existing capacity gap between teacher and student models in Knowledge
Distillation (KD), directly transferring the cross-model knowledge is
non-trivial. To this end, we subtly leverage the visual and
linguistic-compatible feature character of ViT (i.e., student), and its
capacity gap with the CNN (i.e., teacher) and propose a novel CNN-to-ViT KD
framework, dubbed C2VKD. Importantly, as the teacher's features are
heterogeneous to those of the student, we first propose a novel
visual-linguistic feature distillation (VLFD) module that explores efficient KD
among the aligned visual and linguistic-compatible representations. Moreover,
due to the large capacity gap between the teacher and student and the
inevitable prediction errors of the teacher, we then propose a pixel-wise
decoupled distillation (PDD) module to supervise the student under the
combination of labels and teacher's predictions from the decoupled target and
non-target classes. Experiments on three semantic segmentation benchmark
datasets consistently show that the increment of mIoU of our method is over
200% of the SoTA KD methods
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Hidden Connections: Iterative Tracking and Reasoning for
  Video-grounded Dialog 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhang, Meng Liu, Yaowei Wang, Da Cao, Weili Guan, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to conventional visual question answering, video-grounded dialog
necessitates a profound understanding of both dialog history and video content
for accurate response generation. Despite commendable strides made by existing
methodologies, they often grapple with the challenges of incrementally
understanding intricate dialog histories and assimilating video information. In
response to this gap, we present an iterative tracking and reasoning strategy
that amalgamates a textual encoder, a visual encoder, and a generator. At its
core, our textual encoder is fortified with a path tracking and aggregation
mechanism, adept at gleaning nuances from dialog history that are pivotal to
deciphering the posed questions. Concurrently, our visual encoder harnesses an
iterative reasoning network, meticulously crafted to distill and emphasize
critical visual markers from videos, enhancing the depth of visual
comprehension. Culminating this enriched information, we employ the pre-trained
GPT-2 model as our response generator, stitching together coherent and
contextually apt answers. Our empirical assessments, conducted on two renowned
datasets, testify to the prowess and adaptability of our proposed design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADASR: An Adversarial Auto-Augmentation Framework for Hyperspectral and
  Multispectral Data Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghui Qin, Lihuang Fang, Ruitao Lu, Liang Lin, Yukai Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based hyperspectral image (HSI) super-resolution, which aims to
generate high spatial resolution HSI (HR-HSI) by fusing hyperspectral image
(HSI) and multispectral image (MSI) with deep neural networks (DNNs), has
attracted lots of attention. However, neural networks require large amounts of
training data, hindering their application in real-world scenarios. In this
letter, we propose a novel adversarial automatic data augmentation framework
ADASR that automatically optimizes and augments HSI-MSI sample pairs to enrich
data diversity for HSI-MSI fusion. Our framework is sample-aware and optimizes
an augmentor network and two downsampling networks jointly by adversarial
learning so that we can learn more robust downsampling networks for training
the upsampling network. Extensive experiments on two public classical
hyperspectral datasets demonstrate the effectiveness of our ADASR compared to
the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE Geoscience and Remote Sensing
  Letters. Code is released at https://github.com/fangfang11-plog/ADASR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Pre-trained CNNs and GRU-Based Attention for
  Image Caption Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rashid Khan, Bingding Huang, Haseeb Hassan, Asim Zaman, Zhongfu Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning is a challenging task involving generating a textual
description for an image using computer vision and natural language processing
techniques. This paper proposes a deep neural framework for image caption
generation using a GRU-based attention mechanism. Our approach employs multiple
pre-trained convolutional neural networks as the encoder to extract features
from the image and a GRU-based language model as the decoder to generate
descriptive sentences. To improve performance, we integrate the Bahdanau
attention model with the GRU decoder to enable learning to focus on specific
image parts. We evaluate our approach using the MSCOCO and Flickr30k datasets
and show that it achieves competitive scores compared to state-of-the-art
methods. Our proposed framework can bridge the gap between computer vision and
natural language and can be extended to specific domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15pages, 10 figures, 5 tables. 2023 the 5th International Conference
  on Robotics and Computer Vision (ICRCV 2023). arXiv admin note: substantial
  text overlap with arXiv:2203.01594</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing Missing MRI Sequences from Available Modalities using
  Generative Adversarial Networks in BraTS Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Ethem Hamamci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic
resonance imaging (MRI) plays a significant role in the diagnosis, treatment
planning, and follow-up of glioblastoma patients due to its non-invasive and
radiation-free nature. The International Brain Tumor Segmentation (BraTS)
challenge has contributed to generating numerous AI algorithms to accurately
and efficiently segment glioblastoma sub-compartments using four structural
(T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not
always be available. To address this issue, Generative Adversarial Networks
(GANs) can be used to synthesize the missing MRI sequences. In this paper, we
implement and utilize an open-source GAN approach that takes any three MRI
sequences as input to generate the missing fourth structural sequence. Our
proposed approach is contributed to the community-driven generally nuanced deep
learning framework (GaNDLF) and demonstrates promising results in synthesizing
high-quality and realistic MRI sequences, enabling clinicians to improve their
diagnostic capabilities and support the application of AI methods to brain
tumor MRI quantification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via
  Improved Box-dice and Contrastive Latent-anchors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Wang, Qiang Hu, Hongkuan Shi, Li He, Man He, Wenxuan Dai, Ting Li, Yitong Zhang, Dun Li, Mei Liu, Qiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Box-supervised polyp segmentation attracts increasing attention for its
cost-effective potential. Existing solutions often rely on learning-free
methods or pretrained models to laboriously generate pseudo masks, triggering
Dice constraint subsequently. In this paper, we found that a model guided by
the simplest box-filled masks can accurately predict polyp locations/sizes, but
suffers from shape collapsing. In response, we propose two innovative learning
fashions, Improved Box-dice (IBox) and Contrastive Latent-Anchors (CLA), and
combine them to train a robust box-supervised model IBoxCLA. The core idea
behind IBoxCLA is to decouple the learning of location/size and shape, allowing
for focused constraints on each of them. Specifically, IBox transforms the
segmentation map into a proxy map using shape decoupling and confusion-region
swapping sequentially. Within the proxy map, shapes are disentangled, while
locations/sizes are encoded as box-like responses. By constraining the proxy
map instead of the raw prediction, the box-filled mask can well supervise
IBoxCLA without misleading its shape learning. Furthermore, CLA contributes to
shape learning by generating two types of latent anchors, which are learned and
updated using momentum and segmented polyps to steadily represent polyp and
background features. The latent anchors facilitate IBoxCLA to capture
discriminative features within and outside boxes in a contrastive manner,
yielding clearer boundaries. We benchmark IBoxCLA on five public polyp
datasets. The experimental results demonstrate the competitive performance of
IBoxCLA compared to recent fully-supervised polyp segmentation methods, and its
superiority over other box-supervised state-of-the-arts with a relative
increase of overall mDice and mIoU by at least 6.5% and 7.5%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing the Placement of Roadside LiDARs for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Jiang, Hao Xiang, Xinyu Cai, Runsheng Xu, Jiaqi Ma, Yikang Li, Gim Hee Lee, Si Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent cooperative perception is an increasingly popular topic in the
field of autonomous driving, where roadside LiDARs play an essential role.
However, how to optimize the placement of roadside LiDARs is a crucial but
often overlooked problem. This paper proposes an approach to optimize the
placement of roadside LiDARs by selecting optimized positions within the scene
for better perception performance. To efficiently obtain the best combination
of locations, a greedy algorithm based on perceptual gain is proposed, which
selects the location that can maximize the perceptual gain sequentially. We
define perceptual gain as the increased perceptual capability when a new LiDAR
is placed. To obtain the perception capability, we propose a perception
predictor that learns to evaluate LiDAR placement using only a single point
cloud frame. A dataset named Roadside-Opt is created using the CARLA simulator
to facilitate research on the roadside LiDAR placement problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Asif Khan, Hamid Menouar, Ridha Hamila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual crowd counting estimates the density of the crowd using deep learning
models such as convolution neural networks (CNNs). The performance of the model
heavily relies on the quality of the training data that constitutes crowd
images. In harsh weather such as fog, dust, and low light conditions, the
inference performance may severely degrade on the noisy and blur images. In
this paper, we propose the use of Pix2Pix generative adversarial network (GAN)
to first denoise the crowd images prior to passing them to the counting model.
A Pix2Pix network is trained using synthetic noisy images generated from
original crowd images and then the pretrained generator is then used in the
inference engine to estimate the crowd density in unseen, noisy crowd images.
The performance is tested on JHU-Crowd dataset to validate the significance of
the proposed method particularly when high reliability and accuracy are
required.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted for presentation in IEEE 38th
  International Conference on Image and Vision Computing New Zealand (IVCNZ
  2023). The final manuscript can be accessed at ieeexplore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAGE-ICP: Semantic Information-Assisted ICP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Cui, Jiming Chen, Liang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust and accurate pose estimation in unknown environments is an essential
part of robotic applications. We focus on LiDAR-based point-to-point ICP
combined with effective semantic information. This paper proposes a novel
semantic information-assisted ICP method named SAGE-ICP, which leverages
semantics in odometry. The semantic information for the whole scan is timely
and efficiently extracted by a 3D convolution network, and these point-wise
labels are deeply involved in every part of the registration, including
semantic voxel downsampling, data association, adaptive local map, and dynamic
vehicle removal. Unlike previous semantic-aided approaches, the proposed method
can improve localization accuracy in large-scale scenes even if the semantic
information has certain errors. Experimental evaluations on KITTI and KITTI-360
show that our method outperforms the baseline methods, and improves accuracy
while maintaining real-time performance, i.e., runs faster than the sensor
frame rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6+1 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaMesh: Personalized Facial Expressions and Head Poses for
  Speech-Driven 3D Facial Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyang Chen, Weihong Bao, Shun Lei, Boshi Tang, Zhiyong Wu, Shiyin Kang, Haozhi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven 3D facial animation aims at generating facial movements that
are synchronized with the driving speech, which has been widely explored
recently. Existing works mostly neglect the person-specific talking style in
generation, including facial expression and head pose styles. Several works
intend to capture the personalities by fine-tuning modules. However, limited
training data leads to the lack of vividness. In this work, we propose AdaMesh,
a novel adaptive speech-driven facial animation approach, which learns the
personalized talking style from a reference video of about 10 seconds and
generates vivid facial expressions and head poses. Specifically, we propose
mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter,
which efficiently captures the facial expression style. For the personalized
pose style, we propose a pose adapter by building a discrete pose prior and
retrieving the appropriate style embedding with a semantic-aware pose style
matrix without fine-tuning. Extensive experimental results show that our
approach outperforms state-of-the-art methods, preserves the talking style in
the reference video, and generates vivid facial animation. The supplementary
video and code will be available at https://adamesh.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://adamesh.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for blind spectral unmixing of LULC classes with MODIS
  multispectral time series and ancillary data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Rodríguez-Ortega, Rohaifa Khaldi, Domingo Alcaraz-Segura, Siham Tabik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC)
types. Spectral unmixing is a technique to extract information from mixed
pixels into their constituent LULC types and corresponding abundance fractions.
Traditionally, solving this task has relied on either classical methods that
require prior knowledge of endmembers or machine learning methods that avoid
explicit endmembers calculation, also known as blind spectral unmixing (BSU).
Most BSU studies based on Deep Learning (DL) focus on one time-step
hyperspectral data, yet its acquisition remains quite costly compared with
multispectral data. To our knowledge, here we provide the first study on BSU of
LULC classes using multispectral time series data with DL models. We further
boost the performance of a Long-Short Term Memory (LSTM)-based model by
incorporating geographic plus topographic (geo-topographic) and climatic
ancillary information. Our experiments show that combining spectral-temporal
input data together with geo-topographic and climatic information substantially
improves the abundance estimation of LULC classes in mixed pixels. To carry out
this study, we built a new labeled dataset of the region of Andalusia (Spain)
with monthly multispectral time series of pixels for the year 2013 from MODIS
at 460m resolution, for two hierarchical levels of LULC classes, named
Andalusia MultiSpectral MultiTemporal Unmixing (Andalusia-MSMTU). This dataset
provides, at the pixel level, a multispectral time series plus ancillary
information annotated with the abundance of each LULC class inside each pixel.
The dataset and code are available to the public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-paint: A Unified Framework for Multimodal Image Inpainting with
  Pretrained Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Yang, Xiaodong Chen, Jing Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, text-to-image denoising diffusion probabilistic models (DDPMs) have
demonstrated impressive image generation capabilities and have also been
successfully applied to image inpainting. However, in practice, users often
require more control over the inpainting process beyond textual guidance,
especially when they want to composite objects with customized appearance,
color, shape, and layout. Unfortunately, existing diffusion-based inpainting
methods are limited to single-modal guidance and require task-specific
training, hindering their cross-modal scalability. To address these
limitations, we propose Uni-paint, a unified framework for multimodal
inpainting that offers various modes of guidance, including unconditional,
text-driven, stroke-driven, exemplar-driven inpainting, as well as a
combination of these modes. Furthermore, our Uni-paint is based on pretrained
Stable Diffusion and does not require task-specific training on specific
datasets, enabling few-shot generalizability to customized images. We have
conducted extensive qualitative and quantitative evaluations that show our
approach achieves comparable results to existing single-modal methods while
offering multimodal inpainting capabilities not available in other methods.
Code will be available at https://github.com/ysy31415/unipaint.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACMMM'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Learning-Enabled Automatic Vessel Draft Reading for
  Intelligent Maritime Surveillance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxiang Qu, Ryan Wen Liu, Chenjie Zhao, Yu Guo, Sendren Sheng-Dong Xu, Fenghua Zhu, Yisheng Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate and efficient vessel draft reading (VDR) is an important
component of intelligent maritime surveillance, which could be exploited to
assist in judging whether the vessel is normally loaded or overloaded. The
computer vision technique with an excellent price-to-performance ratio has
become a popular medium to estimate vessel draft depth. However, the
traditional estimation methods easily suffer from several limitations, such as
sensitivity to low-quality images, high computational cost, etc. In this work,
we propose a multi-task learning-enabled computational method (termed MTL-VDR)
for generating highly reliable VDR. In particular, our MTL-VDR mainly consists
of four components, i.e., draft mark detection, draft scale recognition,
vessel/water segmentation, and final draft depth estimation. We first construct
a benchmark dataset related to draft mark detection and employ a powerful and
efficient convolutional neural network to accurately perform the detection
task. The multi-task learning method is then proposed for simultaneous draft
scale recognition and vessel/water segmentation. To obtain more robust VDR
under complex conditions (e.g., damaged and stained scales, etc.), the accurate
draft scales are generated by an automatic correction method, which is
presented based on the spatial distribution rules of draft scales. Finally, an
adaptive computational method is exploited to yield an accurate and robust
draft depth. Extensive experiments have been implemented on the realistic
dataset to compare our MTL-VDR with state-of-the-art methods. The results have
demonstrated its superior performance in terms of accuracy, robustness, and
efficiency. The computational speed exceeds 40 FPS, which satisfies the
requirements of real-time maritime surveillance to guarantee vessel traffic
safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,11 figures, submitted to IEEE T-ITS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-task Explainable Skin Lesion Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahapara Khurshid, Mayank Vatsa, Richa Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is one of the deadliest diseases and has a high mortality rate if
left untreated. The diagnosis generally starts with visual screening and is
followed by a biopsy or histopathological examination. Early detection can aid
in lowering mortality rates. Visual screening can be limited by the experience
of the doctor. Due to the long tail distribution of dermatological datasets and
significant intra-variability between classes, automatic classification
utilizing computer-aided methods becomes challenging. In this work, we propose
a multitask few-shot-based approach for skin lesions that generalizes well with
few labelled data to address the small sample space challenge. The proposed
approach comprises a fusion of a segmentation network that acts as an attention
module and classification network. The output of the segmentation network helps
to focus on the most discriminatory features while making a decision by the
classification network. To further enhance the classification performance, we
have combined segmentation and classification loss in a weighted manner. We
have also included the visualization results that explain the decisions made by
the algorithm. Three dermatological datasets are used to evaluate the proposed
method thoroughly. We also conducted cross-database experiments to ensure that
the proposed approach is generalizable across similar datasets. Experimental
results demonstrate the efficacy of the proposed work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via
  Physics Simulation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rong Wang, Wei Mao, Hongdong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the task of 3D pose estimation for a hand interacting
with an object from a single image observation. When modeling hand-object
interaction, previous works mainly exploit proximity cues, while overlooking
the dynamical nature that the hand must stably grasp the object to counteract
gravity and thus preventing the object from slipping or falling. These works
fail to leverage dynamical constraints in the estimation and consequently often
produce unstable results. Meanwhile, refining unstable configurations with
physics-based reasoning remains challenging, both by the complexity of contact
dynamics and by the lack of effective and efficient physics inference in the
data-driven learning framework. To address both issues, we present DeepSimHO: a
novel deep-learning pipeline that combines forward physics simulation and
backward gradient approximation with a neural network. Specifically, for an
initial hand-object pose estimated by a base network, we forward it to a
physics simulator to evaluate its stability. However, due to non-smooth contact
geometry and penetration, existing differentiable simulators can not provide
reliable state gradient. To remedy this, we further introduce a deep network to
learn the stability evaluation process from the simulator, while smoothly
approximating its gradient and thus enabling effective back-propagation.
Extensive experiments show that our method noticeably improves the stability of
the estimation and achieves superior efficiency over test-time optimization.
The code is available at https://github.com/rongakowang/DeepSimHO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikePoint: An Efficient Point-based Spiking Neural Network for Event
  Cameras Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwei Ren, Yue Zhou, Yulong Huang, Haotian Fu, Xiaopeng Lin, Jie Song, Bojun Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are bio-inspired sensors that respond to local changes in light
intensity and feature low latency, high energy efficiency, and high dynamic
range. Meanwhile, Spiking Neural Networks (SNNs) have gained significant
attention due to their remarkable efficiency and fault tolerance. By
synergistically harnessing the energy efficiency inherent in event cameras and
the spike-based processing capabilities of SNNs, their integration could enable
ultra-low-power application scenarios, such as action recognition tasks.
However, existing approaches often entail converting asynchronous events into
conventional frames, leading to additional data mapping efforts and a loss of
sparsity, contradicting the design concept of SNNs and event cameras. To
address this challenge, we propose SpikePoint, a novel end-to-end point-based
SNN architecture. SpikePoint excels at processing sparse event cloud data,
effectively extracting both global and local features through a singular-stage
structure. Leveraging the surrogate training method, SpikePoint achieves high
accuracy with few parameters and maintains low power consumption, specifically
employing the identity mapping feature extractor on diverse datasets.
SpikePoint achieves state-of-the-art (SOTA) performance on four event-based
action recognition datasets using only 16 timesteps, surpassing other SNN
methods. Moreover, it also achieves SOTA performance across all methods on
three datasets, utilizing approximately 0.3\% of the parameters and 0.5\% of
power consumption employed by artificial neural networks (ANNs). These results
emphasize the significance of Point Cloud and pave the way for many
ultra-low-power event-based data processing applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiview Transformer: Rethinking Spatial Information in Hyperspectral
  Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhang, Yongshan Zhang, Yicong Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the land cover category for each pixel in a hyperspectral image
(HSI) relies on spectral and spatial information. An HSI cuboid with a specific
patch size is utilized to extract spatial-spectral feature representation for
the central pixel. In this article, we investigate that scene-specific but not
essential correlations may be recorded in an HSI cuboid. This additional
information improves the model performance on existing HSI datasets and makes
it hard to properly evaluate the ability of a model. We refer to this problem
as the spatial overfitting issue and utilize strict experimental settings to
avoid it. We further propose a multiview transformer for HSI classification,
which consists of multiview principal component analysis (MPCA), spectral
encoder-decoder (SED), and spatial-pooling tokenization transformer (SPTT).
MPCA performs dimension reduction on an HSI via constructing spectral multiview
observations and applying PCA on each view data to extract low-dimensional view
representation. The combination of view representations, named multiview
representation, is the dimension reduction output of the MPCA. To aggregate the
multiview information, a fully-convolutional SED with a U-shape in spectral
dimension is introduced to extract a multiview feature map. SPTT transforms the
multiview features into tokens using the spatial-pooling tokenization strategy
and learns robust and discriminative spatial-spectral features for land cover
identification. Classification is conducted with a linear classifier.
Experiments on three HSI datasets with rigid settings demonstrate the
superiority of the proposed multiview transformer over the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuroInspect: Interpretable Neuron-based Debugging Framework through
  Class-conditional Visualizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite deep learning (DL) has achieved remarkable progress in various
domains, the DL models are still prone to making mistakes. This issue
necessitates effective debugging tools for DL practitioners to interpret the
decision-making process within the networks. However, existing debugging
methods often demand extra data or adjustments to the decision process,
limiting their applicability. To tackle this problem, we present NeuroInspect,
an interpretable neuron-based debugging framework with three key stages:
counterfactual explanations, feature visualizations, and false correlation
mitigation. Our debugging framework first pinpoints neurons responsible for
mistakes in the network and then visualizes features embedded in the neurons to
be human-interpretable. To provide these explanations, we introduce
CLIP-Illusion, a novel feature visualization method that generates images
representing features conditioned on classes to examine the connection between
neurons and the decision layer. We alleviate convoluted explanations of the
conventional visualization approach by employing class information, thereby
isolating mixed properties. This process offers more human-interpretable
explanations for model errors without altering the trained network or requiring
additional data. Furthermore, our framework mitigates false correlations
learned from a dataset under a stochastic perspective, modifying decisions for
the neurons considered as the main causes. We validate the effectiveness of our
framework by addressing false correlations and improving inferences for classes
with the worst performance in real-world settings. Moreover, we demonstrate
that NeuroInspect helps debug the mistakes of DL models through evaluation for
human understanding. The code is openly available at
https://github.com/yeongjoonJu/NeuroInspect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Summitted to IEEE Transactions on Neural Networks and Learning
  Systems (TNNLS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongtong Zhang, Yuanxiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel view synthesis of satellite images holds a wide range of practical
applications. While recent advances in the Neural Radiance Field have
predominantly targeted pin-hole cameras, and models for satellite cameras often
demand sufficient input views. This paper presents rpcPRF, a Multiplane Images
(MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC).
Unlike coordinate-based neural radiance fields in need of sufficient views of
one scene, our model is applicable to single or few inputs and performs well on
images from unseen scenes. To enable generalization across scenes, we propose
to use reprojection supervision to induce the predicted MPI to learn the
correct geometry between the 3D coordinates and the images. Moreover, we remove
the stringent requirement of dense depth supervision from deep
multiview-stereo-based methods by introducing rendering techniques of radiance
fields. rpcPRF combines the superiority of implicit representations and the
advantages of the RPC model, to capture the continuous altitude space while
learning the 3D structure. Given an RGB image and its corresponding RPC, the
end-to-end model learns to synthesize the novel view with a new RPC and
reconstruct the altitude of the scene. When multiple views are provided as
inputs, rpcPRF exerts extra supervision provided by the extra views. On the TLC
dataset from ZY-3, and the SatMVS3D dataset with urban scenes from WV-3, rpcPRF
outperforms state-of-the-art nerf-based methods by a significant margin in
terms of image fidelity, reconstruction accuracy, and efficiency, for both
single-view and multiview task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving mitosis detection on histopathology images using large
  vision-language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiwen Ding, James Hall, Neil Tenenholtz, Kristen Severson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In certain types of cancerous tissue, mitotic count has been shown to be
associated with tumor proliferation, poor prognosis, and therapeutic
resistance. Due to the high inter-rater variability of mitotic counting by
pathologists, convolutional neural networks (CNNs) have been employed to reduce
the subjectivity of mitosis detection in hematoxylin and eosin (H&E)-stained
whole slide images. However, most existing models have performance that lags
behind expert panel review and only incorporate visual information. In this
work, we demonstrate that pre-trained large-scale vision-language models that
leverage both visual features and natural language improve mitosis detection
accuracy. We formulate the mitosis detection task as an image captioning task
and a visual question answering (VQA) task by including metadata such as tumor
and scanner types as context. The effectiveness of our pipeline is demonstrated
via comparison with various baseline models using 9,501 mitotic figures and
11,051 hard negatives (non-mitotic figures that are difficult to characterize)
from the publicly available Mitosis Domain Generalization Challenge (MIDOG22)
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE ISBI 2024. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anchor-based Multi-view Subspace Clustering with Hierarchical Feature
  Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Ou, Siwei Wang, Pei Zhang, Sihang Zhou, En Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view clustering has attracted growing attention owing to its
capabilities of aggregating information from various sources and its promising
horizons in public affairs. Up till now, many advanced approaches have been
proposed in recent literature. However, there are several ongoing difficulties
to be tackled. One common dilemma occurs while attempting to align the features
of different views. We dig out as well as deploy the dependency amongst views
through hierarchical feature descent, which leads to a common latent space(
STAGE 1). This latent space, for the first time of its kind, is regarded as a
'resemblance space', as it reveals certain correlations and dependencies of
different views. To be exact, the one-hot encoding of a category can also be
referred to as a resemblance space in its terminal phase. Moreover, due to the
intrinsic fact that most of the existing multi-view clustering algorithms stem
from k-means clustering and spectral clustering, this results in cubic time
complexity w.r.t. the number of the objects. However, we propose Anchor-based
Multi-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) to
further reduce the computing complexity to linear time cost through a unified
sampling strategy in resemblance space( STAGE 2), followed by subspace
clustering to learn the representation collectively( STAGE 3). Extensive
experimental results on public benchmark datasets demonstrate that our proposed
model consistently outperforms the state-of-the-art techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Unsupervised Domain Adaptation by Retaining Confident Entropy via
  Edge Concatenation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hye-Seong Hong, Abhishek Kumar, Dong-Gyu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generalization capability of unsupervised domain adaptation can mitigate
the need for extensive pixel-level annotations to train semantic segmentation
networks by training models on synthetic data as a source with
computer-generated annotations. Entropy-based adversarial networks are proposed
to improve source domain prediction; however, they disregard significant
external information, such as edges, which have the potential to identify and
distinguish various objects within an image accurately. To address this issue,
we introduce a novel approach to domain adaptation, leveraging the synergy of
internal and external information within entropy-based adversarial networks. In
this approach, we enrich the discriminator network with edge-predicted
probability values within this innovative framework to enhance the clarity of
class boundaries. Furthermore, we devised a probability-sharing network that
integrates diverse information for more effective segmentation. Incorporating
object edges addresses a pivotal aspect of unsupervised domain adaptation that
has frequently been neglected in the past -- the precise delineation of object
boundaries. Conventional unsupervised domain adaptation methods usually center
around aligning feature distributions and may not explicitly model object
boundaries. Our approach effectively bridges this gap by offering clear
guidance on object boundaries, thereby elevating the quality of domain
adaptation. Our approach undergoes rigorous evaluation on the established
unsupervised domain adaptation benchmarks, specifically in adapting SYNTHIA
$\rightarrow$ Cityscapes and SYNTHIA $\rightarrow$ Mapillary. Experimental
results show that the proposed model attains better performance than
state-of-the-art methods. The superior performance across different
unsupervised domain adaptation scenarios highlights the versatility and
robustness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Task Routing for Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, Changick Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models generate highly realistic images through learning a
multi-step denoising process, naturally embodying the principles of multi-task
learning (MTL). Despite the inherent connection between diffusion models and
MTL, there remains an unexplored area in designing neural architectures that
explicitly incorporate MTL into the framework of diffusion models. In this
paper, we present Denoising Task Routing (DTR), a simple add-on strategy for
existing diffusion model architectures to establish distinct information
pathways for individual tasks within a single architecture by selectively
activating subsets of channels in the model. What makes DTR particularly
compelling is its seamless integration of prior knowledge of denoising tasks
into the framework: (1) Task Affinity: DTR activates similar channels for tasks
at adjacent timesteps and shifts activated channels as sliding windows through
timesteps, capitalizing on the inherent strong affinity between tasks at
adjacent timesteps. (2) Task Weights: During the early stages (higher
timesteps) of the denoising process, DTR assigns a greater number of
task-specific channels, leveraging the insight that diffusion models prioritize
reconstructing global structure and perceptually rich contents in earlier
stages, and focus on simple noise removal in later stages. Our experiments
demonstrate that DTR consistently enhances the performance of diffusion models
across various evaluation protocols, all without introducing additional
parameters. Furthermore, DTR contributes to accelerating convergence during
training. Finally, we show the complementarity between our architectural
approach and existing MTL optimization techniques, providing a more complete
view of MTL within the context of diffusion training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Echocardiography video synthesis from end diastolic semantic map via
  diffusion model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phi Nguyen Van, Duc Tran Minh, Hieu Pham Huy, Long Tran Quoc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Probabilistic Models (DDPMs) have demonstrated
significant achievements in various image and video generation tasks, including
the domain of medical imaging. However, generating echocardiography videos
based on semantic anatomical information remains an unexplored area of
research. This is mostly due to the constraints imposed by the currently
available datasets, which lack sufficient scale and comprehensive frame-wise
annotations for every cardiac cycle. This paper aims to tackle the
aforementioned challenges by expanding upon existing video diffusion models for
the purpose of cardiac video synthesis. More specifically, our focus lies in
generating video using semantic maps of the initial frame during the cardiac
cycle, commonly referred to as end diastole. To further improve the synthesis
process, we integrate spatial adaptive normalization into multiscale feature
maps. This enables the inclusion of semantic guidance during synthesis,
resulting in enhanced realism and coherence of the resultant video sequences.
Experiments are conducted on the CAMUS dataset, which is a highly used dataset
in the field of echocardiography. Our model exhibits better performance
compared to the standard diffusion technique in terms of multiple metrics,
including FID, FVD, and SSMI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Matters to You? Towards Visual Representation Alignment for Robot
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Tian, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, Andrea Bajcsy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When operating in service of people, robots need to optimize rewards aligned
with end-user preferences. Since robots will rely on raw perceptual inputs like
RGB images, their rewards will inevitably use visual representations. Recently
there has been excitement in using representations from pre-trained visual
models, but key to making these work in robotics is fine-tuning, which is
typically done via proxy tasks like dynamics prediction or enforcing temporal
cycle-consistency. However, all these proxy tasks bypass the human's input on
what matters to them, exacerbating spurious correlations and ultimately leading
to robot behaviors that are misaligned with user preferences. In this work, we
propose that robots should leverage human feedback to align their visual
representations with the end-user and disentangle what matters for the task. We
propose Representation-Aligned Preference-based Learning (RAPL), a method for
solving the visual representation alignment problem and visual reward learning
problem through the lens of preference-based learning and optimal transport.
Across experiments in X-MAGICAL and in robotic manipulation, we find that
RAPL's reward consistently generates preferred robot behaviors with high sample
efficiency, and shows strong zero-shot generalization when the visual
representation is learned from a different embodiment than the robot's.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D2 Pruning: Message Passing for Balancing Diversity and Difficulty in
  Data Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adyasha Maharana, Prateek Yadav, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analytical theories suggest that higher-quality data can lead to lower test
errors in models trained on a fixed data budget. Moreover, a model can be
trained on a lower compute budget without compromising performance if a dataset
can be stripped of its redundancies. Coreset selection (or data pruning) seeks
to select a subset of the training data so as to maximize the performance of
models trained on this subset, also referred to as coreset. There are two
dominant approaches: (1) geometry-based data selection for maximizing data
diversity in the coreset, and (2) functions that assign difficulty scores to
samples based on training dynamics. Optimizing for data diversity leads to a
coreset that is biased towards easier samples, whereas, selection by difficulty
ranking omits easy samples that are necessary for the training of deep learning
models. This demonstrates that data diversity and importance scores are two
complementary factors that need to be jointly considered during coreset
selection. We represent a dataset as an undirected graph and propose a novel
pruning algorithm, D2 Pruning, that uses forward and reverse message passing
over this dataset graph for coreset selection. D2 Pruning updates the
difficulty scores of each example by incorporating the difficulty of its
neighboring examples in the dataset graph. Then, these updated difficulty
scores direct a graph-based sampling method to select a coreset that
encapsulates both diverse and difficult regions of the dataset space. We
evaluate supervised and self-supervised versions of our method on various
vision and language datasets. Results show that D2 Pruning improves coreset
selection over previous state-of-the-art methods for up to 70% pruning rates.
Additionally, we find that using D2 Pruning for filtering large multimodal
datasets leads to increased diversity in the dataset and improved
generalization of pretrained models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages (Our code is available at
  https://github.com/adymaharana/d2pruning)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Appearance Particle Neural Radiance Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ancheng Lin, Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D
scenes. Dynamic NeRFs extend this model by capturing time-varying elements,
typically using deformation fields. The existing dynamic NeRFs employ a similar
Eulerian representation for both light radiance and deformation fields. This
leads to a close coupling of appearance and motion and lacks a physical
interpretation. In this work, we propose Dynamic Appearance Particle Neural
Radiance Field (DAP-NeRF), which introduces particle-based representation to
model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists
of superposition of a static field and a dynamic field. The dynamic field is
quantised as a collection of {\em appearance particles}, which carries the
visual information of a small dynamic element in the scene and is equipped with
a motion model. All components, including the static field, the visual features
and motion models of the particles, are learned from monocular videos without
any prior geometric knowledge of the scene. We develop an efficient
computational framework for the particle-based model. We also construct a new
dataset to evaluate motion modelling. Experimental results show that DAP-NeRF
is an effective technique to capture not only the appearance but also the
physically meaningful motions in a 3D dynamic scene.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay Sridhar, Dhruv Shah, Catherine Glossop, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic learning for navigation in unfamiliar environments needs to provide
policies for both task-oriented navigation (i.e., reaching a goal that the
robot has located), and task-agnostic exploration (i.e., searching for a goal
in a novel setting). Typically, these roles are handled by separate models, for
example by using subgoal proposals, planning, or separate navigation
strategies. In this paper, we describe how we can train a single unified
diffusion policy to handle both goal-directed navigation and goal-agnostic
exploration, with the latter providing the ability to search novel
environments, and the former providing the ability to reach a user-specified
goal once it has been located. We show that this unified policy results in
better overall performance when navigating to visually indicated goals in novel
environments, as compared to approaches that use subgoal proposals from
generative models, or prior methods based on latent variable models. We
instantiate our method by using a large-scale Transformer-based policy trained
on data from multiple ground robots, with a diffusion model decoder to flexibly
handle both goal-conditioned and goal-agnostic navigation. Our experiments,
conducted on a real-world mobile robot platform, show effective navigation in
unseen environments in comparison with five alternative methods, and
demonstrate significant improvements in performance and lower collision rates,
despite utilizing smaller models than state-of-the-art approaches. For more
videos, code, and pre-trained model checkpoints, see
https://general-navigation-models.github.io/nomad/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page https://general-navigation-models.github.io/nomad/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Integrators for Diffusion Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushagra Pandey, Maja Rudolph, Stephan Mandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models suffer from slow sample generation at inference time.
Therefore, developing a principled framework for fast deterministic/stochastic
sampling for a broader class of diffusion models is a promising direction. We
propose two complementary frameworks for accelerating sample generation in
pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate
integrators generalize DDIM, mapping the reverse diffusion dynamics to a more
amenable space for sampling. In contrast, splitting-based integrators, commonly
used in molecular dynamics, reduce the numerical simulation error by cleverly
alternating between numerical updates involving the data and auxiliary
variables. After extensively studying these methods empirically and
theoretically, we present a hybrid method that leads to the best-reported
performance for diffusion models in augmented spaces. Applied to Phase Space
Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and
stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network
function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing
baselines, respectively. Our code and model checkpoints will be made publicly
available at \url{https://github.com/mandt-lab/PSLD}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LangNav: Language as a Perceptual Representation for Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva, Phillip Isola, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the use of language as a perceptual representation for
vision-and-language navigation. Our approach uses off-the-shelf vision systems
(for image captioning and object detection) to convert an agent's egocentric
panoramic view at each time step into natural language descriptions. We then
finetune a pretrained language model to select an action, based on the current
view and the trajectory history, that would best fulfill the navigation
instructions. In contrast to the standard setup which adapts a pretrained
language model to work directly with continuous visual features from pretrained
vision models, our approach instead uses (discrete) language as the perceptual
representation. We explore two use cases of our language-based navigation
(LangNav) approach on the R2R vision-and-language navigation benchmark:
generating synthetic trajectories from a prompted large language model (GPT-4)
with which to finetune a smaller language model; and sim-to-real transfer where
we transfer a policy learned on a simulated environment (ALFRED) to a
real-world environment (R2R). Our approach is found to improve upon strong
baselines that rely on visual features in settings where only a few gold
trajectories (10-100) are available, demonstrating the potential of using
language as a perceptual representation for navigation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Structured Noise Removal with Variational Lossy Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Salmon, Alexander Krull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most unsupervised denoising methods are based on the assumption that imaging
noise is either pixel-independent, i.e., spatially uncorrelated, or
signal-independent, i.e., purely additive. However, in practice many imaging
setups, especially in microscopy, suffer from a combination of signal-dependent
noise (e.g. Poisson shot noise) and axis-aligned correlated noise (e.g. stripe
shaped scanning or readout artifacts). In this paper, we present the first
unsupervised deep learning-based denoiser that can remove this type of noise
without access to any clean images or a noise model. Unlike self-supervised
techniques, our method does not rely on removing pixels by masking or
subsampling so can utilize all available information. We implement a
Variational Autoencoder (VAE) with a specially designed autoregressive decoder
capable of modelling the noise component of an image but incapable of
independently modelling the underlying clean signal component. As a
consequence, our VAE's encoder learns to encode only underlying clean signal
content and to discard imaging noise. We also propose an additional decoder for
mapping the encoder's latent variables back into image space, thereby sampling
denoised images. Experimental results demonstrate that our approach surpasses
existing methods for self- and unsupervised image denoising while being robust
with respect to the size of the autoregressive receptive field. Code for this
project can be found at https://github.com/krulllab/DVLAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Survey of Feature Types and Their Contributions for Camera Tampering
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Mantini, Shishir K. Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera tamper detection is the ability to detect unauthorized and
unintentional alterations in surveillance cameras by analyzing the video.
Camera tampering can occur due to natural events or it can be caused
intentionally to disrupt surveillance. We cast tampering detection as a change
detection problem, and perform a review of the existing literature with
emphasis on feature types. We formulate tampering detection as a time series
analysis problem, and design experiments to study the robustness and capability
of various feature types. We compute ten features on real-world surveillance
video and apply time series analysis to ascertain their predictability, and
their capability to detect tampering. Finally, we quantify the performance of
various time series models using each feature type to detect tampering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrIBo: Self-Supervised Learning via Cross-Image Object-Level
  Bootstrapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Lebailly, Thomas Stegmüller, Behzad Bozorgtabar, Jean-Philippe Thiran, Tinne Tuytelaars
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging nearest neighbor retrieval for self-supervised representation
learning has proven beneficial with object-centric images. However, this
approach faces limitations when applied to scene-centric datasets, where
multiple objects within an image are only implicitly captured in the global
representation. Such global bootstrapping can lead to undesirable entanglement
of object representations. Furthermore, even object-centric datasets stand to
benefit from a finer-grained bootstrapping approach. In response to these
challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method
tailored to enhance dense visual representation learning. By employing
object-level nearest neighbor bootstrapping throughout the training, CrIBo
emerges as a notably strong and adequate candidate for in-context learning,
leveraging nearest neighbor retrieval at test time. CrIBo shows
state-of-the-art performance on the latter task while being highly competitive
in more standard downstream segmentation tasks. Our code and pretrained models
will be publicly available upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explorable Mesh Deformation Subspaces from Unstructured Generative
  Models <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Maesumi, Paul Guerrero, Vladimir G. Kim, Matthew Fisher, Siddhartha Chaudhuri, Noam Aigerman, Daniel Ritchie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring variations of 3D shapes is a time-consuming process in traditional
3D modeling tools. Deep generative models of 3D shapes often feature continuous
latent spaces that can, in principle, be used to explore potential variations
starting from a set of input shapes. In practice, doing so can be problematic:
latent spaces are high dimensional and hard to visualize, contain shapes that
are not relevant to the input shapes, and linear paths through them often lead
to sub-optimal shape transitions. Furthermore, one would ideally be able to
explore variations in the original high-quality meshes used to train the
generative model, not its lower-quality output geometry. In this paper, we
present a method to explore variations among a given set of landmark shapes by
constructing a mapping from an easily-navigable 2D exploration space to a
subspace of a pre-trained generative model. We first describe how to find a
mapping that spans the set of input landmark shapes and exhibits smooth
variations between them. We then show how to turn the variations in this
subspace into deformation fields, to transfer those variations to high-quality
meshes for the landmark shapes. Our results show that our method can produce
visually-pleasing and easily-navigable 2D exploration spaces for several
different shape categories, especially as compared to prior work on learning
deformation spaces for 3D shapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2023, 15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory
  Prediction Models for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhe Chen, Mozhgan Pourkeshavarz, Amir Rasouli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarking is a common method for evaluating trajectory prediction models
for autonomous driving. Existing benchmarks rely on datasets, which are biased
towards more common scenarios, such as cruising, and distance-based metrics
that are computed by averaging over all scenarios. Following such a regiment
provides a little insight into the properties of the models both in terms of
how well they can handle different scenarios and how admissible and diverse
their outputs are. There exist a number of complementary metrics designed to
measure the admissibility and diversity of trajectories, however, they suffer
from biases, such as length of trajectories.
  In this paper, we propose a new benChmarking paRadIgm for evaluaTing
trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a
method for extracting driving scenarios at varying levels of specificity
according to the structure of the roads, models' performance, and data
properties for fine-grained ranking of prediction models; 2) A set of new
bias-free metrics for measuring diversity, by incorporating the characteristics
of a given scenario, and admissibility, by considering the structure of roads
and kinematic compliancy, motivated by real-world driving constraints. 3) Using
the proposed benchmark, we conduct extensive experimentation on a
representative set of the prediction models using the large scale Argoverse
dataset. We show that the proposed benchmark can produce a more accurate
ranking of the models and serve as a means of characterizing their behavior. We
further present ablation studies to highlight contributions of different
elements that are used to compute the proposed metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An automated approach for improving the inference latency and energy
  efficiency of pretrained CNNs by removing irrelevant pixels with focused
  convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caleb Tung, Nicholas Eliopoulos, Purvish Jajal, Gowri Ramshankar, Chen-Yun Yang, Nicholas Synovic, Xuecen Zhang, Vipin Chaudhary, George K. Thiruvathukal, Yung-Hsiang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision often uses highly accurate Convolutional Neural Networks
(CNNs), but these deep learning models are associated with ever-increasing
energy and computation requirements. Producing more energy-efficient CNNs often
requires model training which can be cost-prohibitive. We propose a novel,
automated method to make a pretrained CNN more energy-efficient without
re-training. Given a pretrained CNN, we insert a threshold layer that filters
activations from the preceding layers to identify regions of the image that are
irrelevant, i.e. can be ignored by the following layers while maintaining
accuracy. Our modified focused convolution operation saves inference latency
(by up to 25%) and energy costs (by up to 22%) on various popular pretrained
CNNs, with little to no loss in accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D TransUNet: Advancing Medical Image Segmentation through Vision
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieneng Chen, Jieru Mei, Xianhang Li, Yongyi Lu, Qihang Yu, Qingyue Wei, Xiangde Luo, Yutong Xie, Ehsan Adeli, Yan Wang, Matthew Lungren, Lei Xing, Le Lu, Alan Yuille, Yuyin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation plays a crucial role in advancing healthcare
systems for disease diagnosis and treatment planning. The u-shaped
architecture, popularly known as U-Net, has proven highly successful for
various medical image segmentation tasks. However, U-Net's convolution-based
operations inherently limit its ability to model long-range dependencies
effectively. To address these limitations, researchers have turned to
Transformers, renowned for their global self-attention mechanisms, as
alternative architectures. One popular network is our previous TransUNet, which
leverages Transformers' self-attention to complement U-Net's localized
information with the global context. In this paper, we extend the 2D TransUNet
architecture to a 3D network by building upon the state-of-the-art nnU-Net
architecture, and fully exploring Transformers' potential in both the encoder
and decoder design. We introduce two key components: 1) A Transformer encoder
that tokenizes image patches from a convolution neural network (CNN) feature
map, enabling the extraction of global contexts, and 2) A Transformer decoder
that adaptively refines candidate regions by utilizing cross-attention between
candidate proposals and U-Net features. Our investigations reveal that
different medical tasks benefit from distinct architectural designs. The
Transformer encoder excels in multi-organ segmentation, where the relationship
among organs is crucial. On the other hand, the Transformer decoder proves more
beneficial for dealing with small and challenging segmented targets such as
tumor segmentation. Extensive experiments showcase the significant potential of
integrating a Transformer-based encoder and decoder into the u-shaped medical
image segmentation architecture. TransUNet outperforms competitors in various
medical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/Beckschen/3D-TransUNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DrivingDiffusion: Layout-Guided multi-view driving scene video
  generation with latent diffusion model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofan Li, Yifu Zhang, Xiaoqing Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing popularity of autonomous driving based on the powerful
and unified bird's-eye-view (BEV) representation, a demand for high-quality and
large-scale multi-view video data with accurate annotation is urgently
required. However, such large-scale multi-view data is hard to obtain due to
expensive collection and annotation costs. To alleviate the problem, we propose
a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate
realistic multi-view videos controlled by 3D layout. There are three challenges
when synthesizing multi-view videos given a 3D layout: How to keep 1)
cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the
quality of the generated instances? Our DrivingDiffusion solves the problem by
cascading the multi-view single-frame image generation step, the single-view
video generation step shared by multiple cameras, and post-processing that can
handle long video generation. In the multi-view model, the consistency of
multi-view images is ensured by information exchange between adjacent cameras.
In the temporal model, we mainly query the information that needs attention in
subsequent frame generation from the multi-view images of the first frame. We
also introduce the local prompt to effectively improve the quality of generated
instances. In post-processing, we further enhance the cross-view consistency of
subsequent frames and extend the video length by employing temporal sliding
window algorithm. Without any extra cost, our model can generate large-scale
realistic multi-camera driving videos in complex urban scenes, fueling the
downstream driving tasks. The code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie An, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Lijuan Wang, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates a challenging task named open-domain interleaved
image-text generation, which generates interleaved texts and images following
an input query. We propose a new interleaved generation framework based on
prompting large-language models (LLMs) and pre-trained text-to-image (T2I)
models, namely OpenLEAF. In OpenLEAF, the LLM generates textual descriptions,
coordinates T2I models, creates visual prompts for generating images, and
incorporates global contexts into the T2I models. This global context improves
the entity and style consistencies of images in the interleaved generation. For
model assessment, we first propose to use large multi-modal models (LMMs) to
evaluate the entity and style consistencies of open-domain interleaved
image-text sequences. According to the LMM evaluation on our constructed
evaluation set, the proposed interleaved generation framework can generate
high-quality image-text content for various domains and applications, such as
how-to question answering, storytelling, graphical story rewriting, and
webpage/poster generation tasks. Moreover, we validate the effectiveness of the
proposed LMM evaluation technique with human assessment. We hope our proposed
framework, benchmark, and LMM evaluation could help establish the intriguing
interleaved image-text generation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TidyBot: Personalized Robot Assistance with Large Language Models <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05658v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05658v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a robot to personalize physical assistance effectively, it must learn
user preferences that can be generally reapplied to future scenarios. In this
work, we investigate personalization of household cleanup with robots that can
tidy up rooms by picking up objects and putting them away. A key challenge is
determining the proper place to put each object, as people's preferences can
vary greatly depending on personal taste or cultural background. For instance,
one person may prefer storing shirts in the drawer, while another may prefer
them on the shelf. We aim to build systems that can learn such preferences from
just a handful of examples via prior interactions with a particular person. We
show that robots can combine language-based planning and perception with the
few-shot summarization capabilities of large language models (LLMs) to infer
generalized user preferences that are broadly applicable to future
interactions. This approach enables fast adaptation and achieves 91.2% accuracy
on unseen objects in our benchmark dataset. We also demonstrate our approach on
a real-world mobile manipulator called TidyBot, which successfully puts away
85.0% of objects in real-world test scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Autonomous Robots (AuRo) - Special Issue: Large Language
  Models in Robotics, 2023 and IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS), 2023. Project page:
  https://tidybot.cs.princeton.edu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FateZero: Fusing Attentions for Zero-shot Text-based Video Editing <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09535v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09535v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diffusion-based generative models have achieved remarkable success in
text-based image generation. However, since it contains enormous randomness in
generation progress, it is still challenging to apply such models for
real-world visual content editing, especially in videos. In this paper, we
propose FateZero, a zero-shot text-based editing method on real-world videos
without per-prompt training or use-specific mask. To edit videos consistently,
we propose several techniques based on the pre-trained models. Firstly, in
contrast to the straightforward DDIM inversion technique, our approach captures
intermediate attention maps during inversion, which effectively retain both
structural and motion information. These maps are directly fused in the editing
process rather than generated during denoising. To further minimize semantic
leakage of the source video, we then fuse self-attentions with a blending mask
obtained by cross-attention features from the source prompt. Furthermore, we
have implemented a reform of the self-attention mechanism in denoising UNet by
introducing spatial-temporal attention to ensure frame consistency. Yet
succinct, our method is the first one to show the ability of zero-shot
text-driven video style and local attribute editing from the trained
text-to-image model. We also have a better zero-shot shape-aware editing
ability based on the text-to-video model. Extensive experiments demonstrate our
superior temporal consistency and editing capability than previous works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2023 as an Oral Presentation. Project page:
  https://fate-zero-edit.github.io ; GitHub repository:
  https://github.com/ChenyangQiQi/FateZero</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Transformer-based 3D Object Detection with Dynamic Token
  Halting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mao Ye, Gregory P. Meyer, Yuning Chai, Qiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Balancing efficiency and accuracy is a long-standing problem for deploying
deep learning models. The trade-off is even more important for real-time
safety-critical systems like autonomous vehicles. In this paper, we propose an
effective approach for accelerating transformer-based 3D object detectors by
dynamically halting tokens at different layers depending on their contribution
to the detection task. Although halting a token is a non-differentiable
operation, our method allows for differentiable end-to-end learning by
leveraging an equivalent differentiable forward-pass. Furthermore, our
framework allows halted tokens to be reused to inform the model's predictions
through a straightforward token recycling mechanism. Our method significantly
improves the Pareto frontier of efficiency versus accuracy when compared with
the existing approaches. By halting tokens and increasing model capacity, we
are able to improve the baseline model's performance without increasing the
model's latency on the Waymo Open Dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisoGender: A dataset for benchmarking gender bias in image-text pronoun
  resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12424v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12424v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siobhan Mackenzie Hall, Fernanda Gonçalves Abrantes, Hanwen Zhu, Grace Sodunke, Aleksandar Shtedritski, Hannah Rose Kirk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce VisoGender, a novel dataset for benchmarking gender bias in
vision-language models. We focus on occupation-related biases within a
hegemonic system of binary gender, inspired by Winograd and Winogender schemas,
where each image is associated with a caption containing a pronoun relationship
of subjects and objects in the scene. VisoGender is balanced by gender
representation in professional roles, supporting bias evaluation in two ways:
i) resolution bias, where we evaluate the difference between pronoun resolution
accuracies for image subjects with gender presentations perceived as masculine
versus feminine by human annotators and ii) retrieval bias, where we compare
ratios of professionals perceived to have masculine and feminine gender
presentations retrieved for a gender-neutral search query. We benchmark several
state-of-the-art vision-language models and find that they demonstrate bias in
resolving binary gender in complex scenes. While the direction and magnitude of
gender bias depends on the task and the model being evaluated, captioning
models are generally less biased than Vision-Language Encoders. Dataset and
code are available at https://github.com/oxai/visogender
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data and code available at https://github.com/oxai/visogender</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Editing Large Language Models: Problems, Methods, and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13172v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13172v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the ability to train capable LLMs, the methodology for maintaining
their relevancy and rectifying errors remains elusive. To this end, the past
few years have witnessed a surge in techniques for editing LLMs, the objective
of which is to efficiently alter the behavior of LLMs within a specific domain
without negatively impacting performance across other inputs. This paper
embarks on a deep exploration of the problems, methods, and opportunities
related to model editing for LLMs. In particular, we provide an exhaustive
overview of the task definition and challenges associated with model editing,
along with an in-depth empirical analysis of the most progressive methods
currently at our disposal. We also build a new benchmark dataset to facilitate
a more robust evaluation and pinpoint enduring issues intrinsic to existing
techniques. Our objective is to provide valuable insights into the
effectiveness and feasibility of each editing technique, thereby assisting the
community in making informed decisions on the selection of the most appropriate
method for a specific task or context. Code and datasets are available at
https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023. Updated with new experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Edge Video Analytics: A Survey on Applications, Systems and Enabling
  Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15751v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15751v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Xu, Saiedeh Razavi, Rong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video, as a key driver in the global explosion of digital information, can
create tremendous benefits for human society. Governments and enterprises are
deploying innumerable cameras for a variety of applications, e.g., law
enforcement, emergency management, traffic control, and security surveillance,
all facilitated by video analytics (VA). This trend is spurred by the rapid
advancement of deep learning (DL), which enables more precise models for object
classification, detection, and tracking. Meanwhile, with the proliferation of
Internet-connected devices, massive amounts of data are generated daily,
overwhelming the cloud. Edge computing, an emerging paradigm that moves
workloads and services from the network core to the network edge, has been
widely recognized as a promising solution. The resulting new intersection, edge
video analytics (EVA), begins to attract widespread attention. Nevertheless,
only a few loosely-related surveys exist on this topic. The basic concepts of
EVA (e.g., definition, architectures) were not fully elucidated due to the
rapid development of this domain. To fill these gaps, we provide a
comprehensive survey of the recent efforts on EVA. In this paper, we first
review the fundamentals of edge computing, followed by an overview of VA. EVA
systems and their enabling techniques are discussed next. In addition, we
introduce prevalent frameworks and datasets to aid future researchers in the
development of EVA systems. Finally, we discuss existing challenges and foresee
future research directions. We believe this survey will help readers comprehend
the relationship between VA and edge computing, and spark new ideas on EVA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Communications Surveys and Tutorials, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Study of Multimodal Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging (e.g., via interpolation or task arithmetic) fuses multiple
models trained on different tasks to generate a multi-task solution. The
technique has been proven successful in previous studies, where the models are
trained on similar tasks and with the same initialization. In this paper, we
expand on this concept to a multimodal setup by merging transformers trained on
different modalities. Furthermore, we conduct our study for a novel goal where
we can merge vision, language, and cross-modal transformers of a
modality-specific architecture to create a parameter-efficient
modality-agnostic architecture. Through comprehensive experiments, we
systematically investigate the key factors impacting model performance after
merging, including initialization, merging mechanisms, and model architectures.
We also propose two metrics that assess the distance between weights to be
merged and can serve as an indicator of the merging outcomes. Our analysis
leads to an effective training recipe for matching the performance of the
modality-agnostic baseline (i.e., pre-trained from scratch) via model merging.
Our method also outperforms naive merging significantly on various tasks, with
improvements of 3% on VQA, 7% on COCO retrieval, 25% on NLVR2, 14% on Flickr30k
and 3% on ADE20k. Our code is available at https://github.com/ylsung/vl-merging
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models are typically trained in two stages: first
pre-training on image-text pairs, and then fine-tuning using supervised
vision-language instruction data. Recent studies have shown that large language
models can achieve satisfactory results even with a limited amount of
high-quality instruction-following data. In this paper, we introduce
InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200
examples, amounting to approximately 6\% of the instruction-following data used
in the alignment dataset for MiniGPT-4. To achieve this, we first propose
several metrics to access the quality of multimodal instruction data. Based on
these metrics, we present an effective and trainable data selector to
automatically identify and filter low-quality vision-language data. By
employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on
various evaluations. Overall, our findings demonstrate that less but
high-quality instruction tuning data is efficient in enabling multimodal large
language models to generate better output. Our code is available at
https://github.com/waltonfuture/InstructionGPT-4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Described Object Detection: Liberating Object Detection with Flexible
  Expressions <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12813v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12813v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Xie, Zhao Zhang, Yixuan Wu, Feng Zhu, Rui Zhao, Shuang Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting objects based on language information is a popular task that
includes Open-Vocabulary object Detection (OVD) and Referring Expression
Comprehension (REC). In this paper, we advance them to a more practical setting
called Described Object Detection (DOD) by expanding category names to flexible
language expressions for OVD and overcoming the limitation of REC only
grounding the pre-existing object. We establish the research foundation for DOD
by constructing a Description Detection Dataset ($D^3$). This dataset features
flexible language expressions, whether short category names or long
descriptions, and annotating all described objects on all images without
omission. By evaluating previous SOTA methods on $D^3$, we find some
troublemakers that fail current REC, OVD, and bi-functional methods. REC
methods struggle with confidence scores, rejecting negative instances, and
multi-target scenarios, while OVD methods face constraints with long and
complex descriptions. Recent bi-functional methods also do not work well on DOD
due to their separated training procedures and inference strategies for REC and
OVD tasks. Building upon the aforementioned findings, we propose a baseline
that largely improves REC methods by reconstructing the training data and
introducing a binary classification sub-task, outperforming existing methods.
Data and code are available at https://github.com/shikras/d-cube and related
works are tracked in
https://github.com/Charles-Xie/awesome-described-object-detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MP-FedCL: Multiprototype Federated Contrastive Learning for Edge
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Qiao, Md. Shirajum Munir, Apurba Adhikary, Huy Q. Le, Avi Deb Raha, Chaoning Zhang, Choong Seon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning-assisted edge intelligence enables privacy protection in
modern intelligent services. However, not independent and identically
distributed (non-IID) distribution among edge clients can impair the local
model performance. The existing single prototype-based strategy represents a
class by using the mean of the feature space. However, feature spaces are
usually not clustered, and a single prototype may not represent a class well.
Motivated by this, this paper proposes a multi-prototype federated contrastive
learning approach (MP-FedCL) which demonstrates the effectiveness of using a
multi-prototype strategy over a single-prototype under non-IID settings,
including both label and feature skewness. Specifically, a multi-prototype
computation strategy based on \textit{k-means} is first proposed to capture
different embedding representations for each class space, using multiple
prototypes ($k$ centroids) to represent a class in the embedding space. In each
global round, the computed multiple prototypes and their respective model
parameters are sent to the edge server for aggregation into a global prototype
pool, which is then sent back to all clients to guide their local training.
Finally, local training for each client minimizes their own supervised learning
tasks and learns from shared prototypes in the global prototype pool through
supervised contrastive learning, which encourages them to learn knowledge
related to their own class from others and reduces the absorption of unrelated
knowledge in each global iteration. Experimental results on MNIST, Digit-5,
Office-10, and DomainNet show that our method outperforms multiple baselines,
with an average test accuracy improvement of about 4.6\% and 10.4\% under
feature and label non-IID distributions, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Internet of Things</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECPC-IDS:A benchmark endometrail cancer PET/CT image dataset for
  evaluation of semantic segmentation and detection of hypermetabolic regions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08313v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08313v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dechao Tang, Tianming Du, Deguo Ma, Zhiyu Ma, Hongzan Sun, Marcin Grzegorzek, Huiyan Jiang, Chen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Endometrial cancer is one of the most common tumors in the female
reproductive system and is the third most common gynecological malignancy that
causes death after ovarian and cervical cancer. Early diagnosis can
significantly improve the 5-year survival rate of patients. With the
development of artificial intelligence, computer-assisted diagnosis plays an
increasingly important role in improving the accuracy and objectivity of
diagnosis, as well as reducing the workload of doctors. However, the absence of
publicly available endometrial cancer image datasets restricts the application
of computer-assisted diagnostic techniques.In this paper, a publicly available
Endometrial Cancer PET/CT Image Dataset for Evaluation of Semantic Segmentation
and Detection of Hypermetabolic Regions (ECPC-IDS) are published. Specifically,
the segmentation section includes PET and CT images, with a total of 7159
images in multiple formats. In order to prove the effectiveness of segmentation
methods on ECPC-IDS, five classical deep learning semantic segmentation methods
are selected to test the image segmentation task. The object detection section
also includes PET and CT images, with a total of 3579 images and XML files with
annotation information. Six deep learning methods are selected for experiments
on the detection task.This study conduct extensive experiments using deep
learning-based semantic segmentation and object detection methods to
demonstrate the differences between various methods on ECPC-IDS. As far as we
know, this is the first publicly available dataset of endometrial cancer with a
large number of multiple images, including a large amount of information
required for image and target detection. ECPC-IDS can aid researchers in
exploring new algorithms to enhance computer-assisted technology, benefiting
both clinical doctors and patients greatly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ altiro3D: Scene representation from single image and novel view
  synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        E. Canessa, L. Tenze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce altiro3D, a free extended library developed to represent reality
starting from a given original RGB image or flat video. It allows to generate a
light-field (or Native) image or video and get a realistic 3D experience. To
synthesize N-number of virtual images and add them sequentially into a Quilt
collage, we apply MiDaS models for the monocular depth estimation, simple
OpenCV and Telea inpainting techniques to map all pixels, and implement a
'Fast' algorithm to handle 3D projection camera and scene transformations along
N-viewpoints. We use the degree of depth to move proportionally the pixels,
assuming the original image to be at the center of all the viewpoints. altiro3D
can also be used with DIBR algorithm to compute intermediate snapshots from a
equivalent 'Real (slower)' camera with N-geometric viewpoints, which requires
to calibrate a priori several intrinsic and extrinsic camera parameters. We
adopt a pixel- and device-based Lookup Table to optimize computing time. The
multiple viewpoints and video generated from a single image or frame can be
displayed in a free-view LCD display.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In press (2023) Springer International Journal of Information
  Technology (IJIT) 10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EXACT: How to Train Your Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.09615v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.09615v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Karpukhin, Stanislav Dereka, Sergey Kolesnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification tasks are usually evaluated in terms of accuracy. However,
accuracy is discontinuous and cannot be directly optimized using gradient
ascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate
losses, which can lead to suboptimal results. In this paper, we propose a new
optimization framework by introducing stochasticity to a model's output and
optimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive
experiments on linear models and deep image classification show that the
proposed optimization method is a powerful alternative to widely used
classification losses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile
  Platform Real-Time RGB-D Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14065v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14065v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Du, Weixi Wang, Renzhong Guo, Shengjun Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of robotic intelligence, achieving efficient and precise RGB-D
semantic segmentation is a key cornerstone. State-of-the-art multimodal
semantic segmentation methods, primarily rooted in symmetrical skeleton
networks, find it challenging to harmonize computational efficiency and
precision. In this work, we propose AsymFormer, a novel network for real-time
RGB-D semantic segmentation, which targets the minimization of superfluous
parameters by optimizing the distribution of computational resources and
introduces an asymmetrical backbone to allow for the effective fusion of
multimodal features. Furthermore, we explore techniques to bolster network
accuracy by redefining feature selection and extracting multi-modal
self-similarity features without a substantial increase in the parameter count,
thereby ensuring real-time execution on robotic platforms. Additionally, a
Local Attention-Guided Feature Selection (LAFS) module is used to selectively
fuse features from different modalities by leveraging their dependencies.
Subsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding
(CMA) module is introduced to further extract cross-modal representations. This
method is evaluated on NYUv2 and SUNRGBD datasets, with AsymFormer
demonstrating competitive results with 52.0% mIoU on NYUv2 and 49.1% mIoU on
SUNRGBD. Notably, AsymFormer achieves an inference speed of 65 FPS and after
implementing mixed precision quantization, it attains an impressive inference
speed of 79 FPS on RTX3090. This significantly outperforms existing multi-modal
methods, thereby demonstrating that AsymFormer can strike a balance between
high accuracy and efficiency for RGB-D semantic segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiM: Distilling Dataset into Generative Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04707v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04707v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Wang, Jianyang Gu, Daquan Zhou, Zheng Zhu, Wei Jiang, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation reduces the network training cost by synthesizing small
and informative datasets from large-scale ones. Despite the success of the
recent dataset distillation algorithms, three drawbacks still limit their wider
application: i). the synthetic images perform poorly on large architectures;
ii). they need to be re-optimized when the distillation ratio changes; iii).
the limited diversity restricts the performance when the distillation ratio is
large. In this paper, we propose a novel distillation scheme to
\textbf{D}istill information of large train sets \textbf{i}nto generative
\textbf{M}odels, named DiM. Specifically, DiM learns to use a generative model
to store the information of the target dataset. During the distillation phase,
we minimize the differences in logits predicted by a models pool between real
and generated images. At the deployment stage, the generative model synthesizes
various training samples from random noises on the fly. Due to the simple yet
effective designs, the trained DiM can be directly applied to different
distillation ratios and large architectures without extra cost. We validate the
proposed DiM across 4 datasets and achieve state-of-the-art results on all of
them. To the best of our knowledge, we are the first to achieve higher accuracy
on complex architectures than simple ones, such as 75.1\% with ResNet-18 and
72.6\% with ConvNet-3 on ten images per class of CIFAR-10. Besides, DiM
outperforms previous methods with 10\% $\sim$ 22\% when images per class are 1
and 10 on the SVHN dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Distilling datasets into generative models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analysis of Rainfall Variability and Water Extent of Selected Hydropower
  Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical
  Countries, Sri Lanka and Vietnam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Punsisi Rajakaruna, Surajit Ghosh, Bunyod Holmatov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a comprehensive remote sensing analysis of rainfall
patterns and selected hydropower reservoir water extent in two tropical monsoon
countries, Vietnam and Sri Lanka. The aim is to understand the relationship
between remotely sensed rainfall data and the dynamic changes (monthly) in
reservoir water extent. The analysis utilizes high-resolution optical imagery
and Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water
bodies during different weather conditions, especially during the monsoon
season. The average annual rainfall for both countries is determined, and
spatiotemporal variations in monthly average rainfall are examined at regional
and reservoir basin levels using the Climate Hazards Group InfraRed
Precipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents
are derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected
(GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are
pre-processed and corrected using terrain correction and refined Lee filter. An
automated thresholding algorithm, OTSU, distinguishes water and land, taking
advantage of both VV and VH polarization data. The connected pixel count
threshold is applied to enhance result accuracy. The results indicate a clear
relationship between rainfall patterns and reservoir water extent, with
increased precipitation during the monsoon season leading to higher water
extents in the later months. This study contributes to understanding how
rainfall variability impacts reservoir water resources in tropical monsoon
regions. The preliminary findings can inform water resource management
strategies and support these countries' decision-making processes related to
hydropower generation, flood management, and irrigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point2Vec for Self-Supervised Representation Learning on Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Abou Zeid, Jonas Schult, Alexander Hermans, Bastian Leibe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the self-supervised learning framework data2vec has shown inspiring
performance for various modalities using a masked student-teacher approach.
However, it remains open whether such a framework generalizes to the unique
challenges of 3D point clouds. To answer this question, we extend data2vec to
the point cloud domain and report encouraging results on several downstream
tasks. In an in-depth analysis, we discover that the leakage of positional
information reveals the overall object shape to the student even under heavy
masking and thus hampers data2vec to learn strong representations for point
clouds. We address this 3D-specific shortcoming by proposing point2vec, which
unleashes the full potential of data2vec-like pre-training on point clouds. Our
experiments show that point2vec outperforms other self-supervised methods on
shape classification and few-shot learning on ModelNet40 and ScanObjectNN,
while achieving competitive results on part segmentation on ShapeNetParts.
These results suggest that the learned representations are strong and
transferable, highlighting point2vec as a promising direction for
self-supervised learning of point cloud representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at GCPR 2023. Project page at
  https://vision.rwth-aachen.de/point2vec</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DifFSS: Diffusion Model for Few-Shot Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00773v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00773v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weimin Tan, Siyuan Chen, Bo Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated excellent performance in image generation.
Although various few-shot semantic segmentation (FSS) models with different
network structures have been proposed, performance improvement has reached a
bottleneck. This paper presents the first work to leverage the diffusion model
for FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve
the performance of the state-of-the-art FSS models by a large margin without
modifying their network structure. Specifically, we utilize the powerful
generation ability of diffusion models to generate diverse auxiliary support
images by using the semantic mask, scribble or soft HED boundary of the support
image as control conditions. This generation process simulates the variety
within the class of the query image, such as color, texture variation,
lighting, $etc$. As a result, FSS models can refer to more diverse support
images, yielding more robust representations, thereby achieving a consistent
improvement in segmentation performance. Extensive experiments on three
publicly available datasets based on existing advanced FSS models demonstrate
the effectiveness of the diffusion model for FSS task. Furthermore, we explore
in detail the impact of different input settings of the diffusion model on
segmentation performance. Hopefully, this completely new paradigm will bring
inspiration to the study of FSS task integrated with AI-generated content. Code
is available at https://github.com/TrinitialChan/DifFSS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code is available at https://github.com/TrinitialChan/DifFSS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEO: Generative Latent Image Animator for Human Video Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaohui Wang, Xin Ma, Xinyuan Chen, Antitza Dantcheva, Bo Dai, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal coherency is a major challenge in synthesizing high quality
videos, particularly in synthesizing human videos that contain rich global and
local deformations. To resolve this challenge, previous approaches have
resorted to different features in the generation process aimed at representing
appearance and motion. However, in the absence of strict mechanisms to
guarantee such disentanglement, a separation of motion from appearance has
remained challenging, resulting in spatial distortions and temporal jittering
that break the spatio-temporal coherency. Motivated by this, we here propose
LEO, a novel framework for human video synthesis, placing emphasis on
spatio-temporal coherency. Our key idea is to represent motion as a sequence of
flow maps in the generation process, which inherently isolate motion from
appearance. We implement this idea via a flow-based image animator and a Latent
Motion Diffusion Model (LMDM). The former bridges a space of motion codes with
the space of flow maps, and synthesizes video frames in a warp-and-inpaint
manner. LMDM learns to capture motion prior in the training data by
synthesizing sequences of motion codes. Extensive quantitative and qualitative
analysis suggests that LEO significantly improves coherent synthesis of human
videos over previous methods on the datasets TaichiHD, FaceForensics and
CelebV-HQ. In addition, the effective disentanglement of appearance and motion
in LEO allows for two additional tasks, namely infinite-length human video
synthesis, as well as content-preserving video editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://wyhsirius.github.io/LEO-project/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DisCo: Disentangled Control for Realistic Human Dance Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI has made significant strides in computer vision, particularly
in text-driven image/video synthesis (T2I/T2V). Despite the notable
advancements, it remains challenging in human-centric content synthesis such as
realistic dance generation. Current methodologies, primarily tailored for human
motion transfer, encounter difficulties when confronted with real-world dance
scenarios (e.g., social media dance) which require to generalize across a wide
spectrum of poses and intricate human details. In this paper, we depart from
the traditional paradigm of human motion transfer and emphasize two additional
critical attributes for the synthesis of human dance content in social media
contexts: (i) Generalizability: the model should be able to generalize beyond
generic human viewpoints as well as unseen human subjects, backgrounds, and
poses; (ii) Compositionality: it should allow for composition of seen/unseen
subjects, backgrounds, and poses from different sources seamlessly. To address
these challenges, we introduce DisCo, which includes a novel model architecture
with disentangled control to improve the compositionality of dance synthesis,
and an effective human attribute pre-training for better generalizability to
unseen humans. Extensive qualitative and quantitative results demonstrate that
DisCo can generate high-quality human dance images and videos with diverse
appearances and flexible motions. Code, demo, video and visualization are
available at: https://disco-dance.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://disco-dance.github.io/ ; Add temporal module ;
  Synchronize FVD computation with MCVD ; More baselines and visualizations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MomentDiff: Generative Video Moment Retrieval from Random to Real 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pandeng Li, Chen-Wei Xie, Hongtao Xie, Liming Zhao, Lei Zhang, Yun Zheng, Deli Zhao, Yongdong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video moment retrieval pursues an efficient and generalized solution to
identify the specific temporal segments within an untrimmed video that
correspond to a given language description. To achieve this goal, we provide a
generative diffusion-based framework called MomentDiff, which simulates a
typical human retrieval process from random browsing to gradual localization.
Specifically, we first diffuse the real span to random noise, and learn to
denoise the random noise to the original span with the guidance of similarity
between text and video. This allows the model to learn a mapping from arbitrary
random locations to real moments, enabling the ability to locate segments from
random initialization. Once trained, MomentDiff could sample random temporal
segments as initial guesses and iteratively refine them to generate an accurate
temporal boundary. Different from discriminative works (e.g., based on
learnable proposals or queries), MomentDiff with random initialized spans could
resist the temporal location biases from datasets. To evaluate the influence of
the temporal location biases, we propose two anti-bias datasets with location
distribution shifts, named Charades-STA-Len and Charades-STA-Mom. The
experimental results demonstrate that our efficient framework consistently
outperforms state-of-the-art methods on three public benchmarks, and exhibits
better generalization and robustness on the proposed anti-bias datasets. The
code, model, and anti-bias evaluation datasets are available at
https://github.com/IMCCretrieval/MomentDiff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video alignment using unsupervised learning of local and global features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niloufar Fakhfour, Mohammad ShahverdiKondori, Hoda Mohammadzade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle the problem of video alignment, the process of
matching the frames of a pair of videos containing similar actions. The main
challenge in video alignment is that accurate correspondence should be
established despite the differences in the execution processes and appearances
between the two videos. We introduce an unsupervised method for alignment that
uses global and local features of the frames. In particular, we introduce
effective features for each video frame using three machine vision tools:
person detection, pose estimation, and VGG network. Then, the features are
processed and combined to construct a multidimensional time series that
represents the video. The resulting time series are used to align videos of the
same actions using a novel version of dynamic time warping named Diagonalized
Dynamic Time Warping(DDTW). The main advantage of our approach is that no
training is required, which makes it applicable for any new type of action
without any need to collect training samples for it. For evaluation, we
considered video synchronization and phase classification tasks on the Penn
action dataset. Also, for an effective evaluation of the video synchronization
task, we present a new metric called Enclosed Area Error(EAE). The results show
that our method outperforms previous state-of-the-art methods, such as TCC, and
other self-supervised and weakly supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Transformers under Occlusion: How Physics and Background
  Attributes Impact Large Models for Robotic Manipulation <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02044v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02044v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shutong Jin, Ruiyu Wang, Muhammad Zahid, Florian T. Pokorny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As transformer architectures and dataset sizes continue to scale, the need to
understand the specific dataset factors affecting model performance becomes
increasingly urgent. This paper investigates how object physics attributes
(color, friction coefficient, shape) and background characteristics (static,
dynamic, background complexity) influence the performance of Video Transformers
in trajectory prediction tasks under occlusion. Beyond mere occlusion
challenges, this study aims to investigate three questions: How do object
physics attributes and background characteristics influence the model
performance? What kinds of attributes are most influential to the model
generalization? Is there a data saturation point for large transformer model
performance within a single task? To facilitate this research, we present
OccluManip, a real-world video-based robot pushing dataset comprising 460,000
consistent recordings of objects with different physics and varying
backgrounds. 1.4 TB and in total 1278 hours of high-quality videos of flexible
temporal length along with target object trajectories are collected,
accommodating tasks with different temporal requirements. Additionally, we
propose Video Occlusion Transformer (VOT), a generic video-transformer-based
network achieving an average 96% accuracy across all 18 sub-datasets provided
in OccluManip. OccluManip and VOT will be released at:
https://github.com/ShutongJIN/OccluManip.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IEEE ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRAM-HD: 3D-Consistent Image Generation at High Resolution with
  Generative Radiance Manifolds <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfeng Xiang, Jiaolong Yang, Yu Deng, Xin Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that 3D-aware GANs trained on unstructured single
image collections can generate multiview images of novel instances. The key
underpinnings to achieve this are a 3D radiance field generator and a volume
rendering process. However, existing methods either cannot generate
high-resolution images (e.g., up to 256X256) due to the high computation cost
of neural volume rendering, or rely on 2D CNNs for image-space upsampling which
jeopardizes the 3D consistency across different views. This paper proposes a
novel 3D-aware GAN that can generate high resolution images (up to 1024X1024)
while keeping strict 3D consistency as in volume rendering. Our motivation is
to achieve super-resolution directly in the 3D space to preserve 3D
consistency. We avoid the otherwise prohibitively-expensive computation cost by
applying 2D convolutions on a set of 2D radiance manifolds defined in the
recent generative radiance manifold (GRAM) approach, and apply dedicated loss
functions for effective GAN training at high resolution. Experiments on FFHQ
and AFHQv2 datasets show that our method can produce high-quality 3D-consistent
results that significantly outperform existing methods. It makes a significant
step towards closing the gap between traditional 2D image generation and
3D-consistent free-view generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV2023 camera ready version (more results and method comparisons).
  Project page: https://jeffreyxiang.github.io/GRAM-HD/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Face Recognition with Latent Space Data Augmentation and
  Facial Posture Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Hashemifar, Abdolreza Marefat, Javad Hassannataj Joloudari, Hamid Hassanpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The small amount of training data for many state-of-the-art deep
learning-based Face Recognition (FR) systems causes a marked deterioration in
their performance. Although a considerable amount of research has addressed
this issue by inventing new data augmentation techniques, using either input
space transformations or Generative Adversarial Networks (GAN) for feature
space augmentations, these techniques have yet to satisfy expectations. In this
paper, we propose an approach named the Face Representation Augmentation (FRA)
for augmenting face datasets. To the best of our knowledge, FRA is the first
method that shifts its focus towards manipulating the face embeddings generated
by any face representation learning algorithm to create new embeddings
representing the same identity and facial emotion but with an altered posture.
Extensive experiments conducted in this study convince of the efficacy of our
methodology and its power to provide noiseless, completely new facial
representations to improve the training procedure of any FR algorithm.
Therefore, FRA can help the recent state-of-the-art FR methods by providing
more data for training FR systems. The proposed method, using experiments
conducted on the Karolinska Directed Emotional Faces (KDEF) dataset, improves
the identity classification accuracies by 9.52 %, 10.04 %, and 16.60 %, in
comparison with the base models of MagFace, ArcFace, and CosFace, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Gap between Human Motion and Action Semantics via Kinematic
  Phrases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpeng Liu, Yong-Lu Li, Ailing Zeng, Zizheng Zhou, Yang You, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of motion understanding is to establish a reliable mapping between
motion and action semantics, while it is a challenging many-to-many problem. An
abstract action semantic (i.e., walk forwards) could be conveyed by
perceptually diverse motions (walk with arms up or swinging), while a motion
could carry different semantics w.r.t. its context and intention. This makes an
elegant mapping between them difficult. Previous attempts adopted
direct-mapping paradigms with limited reliability. Also, current automatic
metrics fail to provide reliable assessments of the consistency between motions
and action semantics. We identify the source of these problems as the
significant gap between the two modalities. To alleviate this gap, we propose
Kinematic Phrases (KP) that take the objective kinematic facts of human motion
with proper abstraction, interpretability, and generality characteristics.
Based on KP as a mediator, we can unify a motion knowledge base and build a
motion understanding system. Meanwhile, KP can be automatically converted from
motions and to text descriptions with no subjective bias, inspiring Kinematic
Prompt Generation (KPG) as a novel automatic motion generation benchmark. In
extensive experiments, our approach shows superiority over other methods. Our
code and data would be made publicly available at https://foruck.github.io/KP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Yong-Lu Li and Cewu Lu are the corresponding authors. Project page is
  available at https://foruck.github.io/KP/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Diffusion Models: Image to Zero and Zero to Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13720v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13720v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Huang, Liang Zheng, Zheng Qin, Xinwang Liu, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent diffusion probabilistic models (DPMs) have shown remarkable abilities
of generated content, however, they often suffer from complex forward
processes, resulting in inefficient solutions for the reversed process and
prolonged sampling times. In this paper, we aim to address the aforementioned
challenges by focusing on the diffusion process itself that we propose to
decouple the intricate diffusion process into two comparatively simpler process
to improve the generative efficacy and speed. In particular, we present a novel
diffusion paradigm named DDM (Decoupled Diffusion Models) based on the Ito
diffusion process, in which the image distribution is approximated by an
explicit transition probability while the noise path is controlled by the
standard Wiener process. We find that decoupling the diffusion process reduces
the learning difficulty and the explicit transition probability improves the
generative speed significantly. We prove a new training objective for DPM,
which enables the model to learn to predict the noise and image components
separately. Moreover, given the novel forward diffusion equation, we derive the
reverse denoising formula of DDM that naturally supports fewer steps of
generation without ordinary differential equation (ODE) based accelerators. Our
experiments demonstrate that DDM outperforms previous DPMs by a large margin in
fewer function evaluations setting and gets comparable performances in long
function evaluations setting. We also show that our framework can be applied to
image-conditioned generation and high-resolution image synthesis, and that it
can generate high-quality images with only 10 function evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-Teller: Enhancing Cross-Modal Generation with Fusion and
  Decoupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04991v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04991v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haogeng Liu, Qihang Fan, Tingkai Liu, Linjie Yang, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Video-Teller, a video-language foundation model that
leverages multi-modal fusion and fine-grained modality alignment to
significantly enhance the video-to-text generation task. Video-Teller boosts
the training efficiency by utilizing frozen pretrained vision and language
modules. It capitalizes on the robust linguistic capabilities of large language
models, enabling the generation of both concise and elaborate video
descriptions. To effectively integrate visual and auditory information,
Video-Teller builds upon the image-based BLIP-2 model and introduces a cascaded
Q-Former which fuses information across frames and ASR texts. To better guide
video summarization, we introduce a fine-grained modality alignment objective,
where the cascaded Q-Former's output embedding is trained to align with the
caption/summary embedding created by a pretrained text auto-encoder.
Experimental results demonstrate the efficacy of our proposed video-language
foundation model in accurately comprehending videos and generating coherent and
precise language descriptions. It is worth noting that the fine-grained
alignment enhances the model's capabilities (4% improvement of CIDEr score on
MSR-VTT) with only 13% extra parameters in training and zero additional cost in
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object
  Detection <span class="chip">ICCV23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhu Ma, Yongtao Wang, Yinmin Zhang, Zhiyi Xia, Yuan Meng, Zhihui Wang, Haojie Li, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we build a modular-designed codebase, formulate strong training
recipes, design an error diagnosis toolbox, and discuss current methods for
image-based 3D object detection. In particular, different from other highly
mature tasks, e.g., 2D object detection, the community of image-based 3D object
detection is still evolving, where methods often adopt different training
recipes and tricks resulting in unfair evaluations and comparisons. What is
worse, these tricks may overwhelm their proposed designs in performance, even
leading to wrong conclusions. To address this issue, we build a module-designed
codebase and formulate unified training standards for the community.
Furthermore, we also design an error diagnosis toolbox to measure the detailed
characterization of detection models. Using these tools, we analyze current
methods in-depth under varying settings and provide discussions for some open
questions, e.g., discrepancies in conclusions on KITTI-3D and nuScenes
datasets, which have led to different dominant methods for these datasets. We
hope that this work will facilitate future research in image-based 3D object
detection. Our codes will be released at
\url{https://github.com/OpenGVLab/3dodi}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV23, code will be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring uncertainty in human visual segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07807v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07807v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Vacher, Claire Launay, Pascal Mamassian, Ruben Coen-Cagli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting visual stimuli into distinct groups of features and visual objects
is central to visual function. Classical psychophysical methods have helped
uncover many rules of human perceptual segmentation, and recent progress in
machine learning has produced successful algorithms. Yet, the computational
logic of human segmentation remains unclear, partially because we lack
well-controlled paradigms to measure perceptual segmentation maps and compare
models quantitatively. Here we propose a new, integrated approach: given an
image, we measure multiple pixel-based same--different judgments and perform
model--based reconstruction of the underlying segmentation map. The
reconstruction is robust to several experimental manipulations and captures the
variability of individual participants. We demonstrate the validity of the
approach on human segmentation of natural images and composite textures. We
show that image uncertainty affects measured human variability, and it
influences how participants weigh different visual features. Because any
putative segmentation algorithm can be inserted to perform the reconstruction,
our paradigm affords quantitative tests of theories of perception as well as
new benchmarks for segmentation algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 9 figures, 5 appendix, 5 figures in appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ APRIL-GAN: A Zero-/Few-Shot Anomaly Classification and Segmentation
  Method for CVPR 2023 VAND Workshop Challenge Tracks 1&2: 1st Place on
  Zero-shot AD and 4th Place on Few-shot AD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17382v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17382v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhai Chen, Yue Han, Jiangning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical report, we briefly introduce our solution for the
Zero/Few-shot Track of the Visual Anomaly and Novelty Detection (VAND) 2023
Challenge. For industrial visual inspection, building a single model that can
be rapidly adapted to numerous categories without or with only a few normal
reference images is a promising research direction. This is primarily because
of the vast variety of the product types. For the zero-shot track, we propose a
solution based on the CLIP model by adding extra linear layers. These layers
are used to map the image features to the joint embedding space, so that they
can compare with the text features to generate the anomaly maps. Besides, when
the reference images are available, we utilize multiple memory banks to store
their features and compare them with the features of the test images during the
testing phase. In this challenge, our method achieved first place in the
zero-shot track, especially excelling in segmentation with an impressive F1
score improvement of 0.0489 over the second-ranked participant. Furthermore, in
the few-shot track, we secured the fourth position overall, with our
classification F1 score of 0.8687 ranking first among all participating teams.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using
  Transformer-based Neural Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16254v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16254v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhang Liu, Yan Zeng, Yifan Qin, Hao Li, Jiakai Zhang, Lan Xu, Jingyi Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryo-electron microscopy (cryo-EM) allows for the high-resolution
reconstruction of 3D structures of proteins and other biomolecules. Successful
reconstruction of both shape and movement greatly helps understand the
fundamental processes of life. However, it is still challenging to reconstruct
the continuous motions of 3D structures from hundreds of thousands of noisy and
randomly oriented 2D cryo-EM images. Recent advancements use Fourier domain
coordinate-based neural networks to continuously model 3D conformations, yet
they often struggle to capture local flexible regions accurately. We propose
CryoFormer, a new approach for continuous heterogeneous cryo-EM reconstruction.
Our approach leverages an implicit feature volume directly in the real domain
as the 3D representation. We further introduce a novel query-based deformation
transformer decoder to improve the reconstruction quality. Our approach is
capable of refining pre-computed pose estimations and locating flexible
regions. In experiments, our method outperforms current approaches on three
public datasets (1 synthetic and 2 experimental) and a new synthetic dataset of
PEDV spike protein. The code and new synthetic dataset will be released for
better reproducibility of our results. Project page:
https://cryoformer.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Superpixel Segmentation from Biologically Inspired Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13438v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13438v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyu Zhao, Bo Peng, Yuan Sun, Daipeng Yang, Zhenguang Zhang, Xi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, advancements in deep learning-based superpixel segmentation methods
have brought about improvements in both the efficiency and the performance of
segmentation. However, a significant challenge remains in generating
superpixels that strictly adhere to object boundaries while conveying rich
visual significance, especially when cross-surface color correlations may
interfere with objects. Drawing inspiration from neural structure and visual
mechanisms, we propose a biological network architecture comprising an Enhanced
Screening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel
segmentation. The ESM enhances semantic information by simulating the
interactive projection mechanisms of the visual cortex. Additionally, the BAL
emulates the spatial frequency characteristics of visual cortical cells to
facilitate the generation of superpixels with strong boundary adherence. We
demonstrate the effectiveness of our approach through evaluations on both the
BSDS500 dataset and the NYUv2 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VDT: General-purpose Video Diffusion Transformers via Mask Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, Mingyu Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces Video Diffusion Transformer (VDT), which pioneers the
use of transformers in diffusion-based video generation. It features
transformer blocks with modularized temporal and spatial attention modules to
leverage the rich spatial-temporal representation inherited in transformers. We
also propose a unified spatial-temporal mask modeling mechanism, seamlessly
integrated with the model, to cater to diverse video generation scenarios. VDT
offers several appealing benefits. 1) It excels at capturing temporal
dependencies to produce temporally consistent video frames and even simulate
the physics and dynamics of 3D objects over time. 2) It facilitates flexible
conditioning information, \eg, simple concatenation in the token space,
effectively unifying different token lengths and modalities. 3) Pairing with
our proposed spatial-temporal mask modeling mechanism, it becomes a
general-purpose video diffuser for harnessing a range of tasks, including
unconditional generation, video prediction, interpolation, animation, and
completion, etc. Extensive experiments on these tasks spanning various
scenarios, including autonomous driving, natural weather, human action, and
physics-based simulation, demonstrate the effectiveness of VDT. Additionally,
we present comprehensive studies on how \model handles conditioning information
with the mask modeling mechanism, which we believe will benefit future research
and advance the field. Project page: https:VDT-2023.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cell Tracking-by-detection using Elliptical Bounding Boxes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas N. Kirsten, Cláudio R. Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cell detection and tracking are paramount for bio-analysis. Recent approaches
rely on the tracking-by-model evolution paradigm, which usually consists of
training end-to-end deep learning models to detect and track the cells on the
frames with promising results. However, such methods require extensive amounts
of annotated data, which is time-consuming to obtain and often requires
specialized annotators. This work proposes a new approach based on the
classical tracking-by-detection paradigm that alleviates the requirement of
annotated data. More precisely, it approximates the cell shapes as oriented
ellipses and then uses generic-purpose oriented object detectors to identify
the cells in each frame. We then rely on a global data association algorithm
that explores temporal cell similarity using probability distance metrics,
considering that the ellipses relate to two-dimensional Gaussian distributions.
Our results show that our method can achieve detection and tracking results
competitively with state-of-the-art techniques that require considerably more
extensive data annotation. Our code is available at:
https://github.com/LucasKirsten/Deep-Cell-Tracking-EBB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper under review on IEEE/ACM Transactions on Computational Biology
  and Bioinformatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Critical Look at Classic Test-Time Adaptation Methods in Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05341v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05341v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang'an Yi, Haotian Chen, Yifan Zhang, Yonghui Xu, Lizhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation (TTA) aims to adapt a model, initially trained on
training data, to potential distribution shifts in the test data. Most existing
TTA studies, however, focus on classification tasks, leaving a notable gap in
the exploration of TTA for semantic segmentation. This pronounced emphasis on
classification might lead numerous newcomers and engineers to mistakenly assume
that classic TTA methods designed for classification can be directly applied to
segmentation. Nonetheless, this assumption remains unverified, posing an open
question. To address this, we conduct a systematic, empirical study to disclose
the unique challenges of segmentation TTA, and to determine whether classic TTA
strategies can effectively address this task. Our comprehensive results have
led to three key observations. First, the classic batch norm updating strategy,
commonly used in classification TTA, only brings slight performance
improvement, and in some cases it might even adversely affect the results. Even
with the application of advanced distribution estimation techniques like batch
renormalization, the problem remains unresolved. Second, the teacher-student
scheme does enhance training stability for segmentation TTA in the presence of
noisy pseudo-labels. However, it cannot directly result in performance
improvement compared to the original model without TTA. Third, segmentation TTA
suffers a severe long-tailed imbalance problem, which is substantially more
complex than that in TTA for classification. This long-tailed challenge
significantly affects segmentation TTA performance, even when the accuracy of
pseudo-labels is high. In light of these observations, we conclude that TTA for
segmentation presents significant challenges, and simply using classic TTA
methods cannot address this problem well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic
  Clothing Driven by Sparse RGB-D Input <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donglai Xiang, Fabian Prada, Zhe Cao, Kaiwen Guo, Chenglei Wu, Jessica Hodgins, Timur Bagautdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clothing is an important part of human appearance but challenging to model in
photorealistic avatars. In this work we present avatars with dynamically moving
loose clothing that can be faithfully driven by sparse RGB-D inputs as well as
body and face motion. We propose a Neural Iterative Closest Point (N-ICP)
algorithm that can efficiently track the coarse garment shape given sparse
depth input. Given the coarse tracking results, the input RGB-D images are then
remapped to texel-aligned features, which are fed into the drivable avatar
models to faithfully reconstruct appearance details. We evaluate our method
against recent image-driven synthesis baselines, and conduct a comprehensive
analysis of the N-ICP algorithm. We demonstrate that our method can generalize
to a novel testing environment, while preserving the ability to produce
high-fidelity and faithful clothing dynamics and appearance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2023 Conference Paper. Project website:
  https://xiangdonglai.github.io/www-sa23-drivable-clothing/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) extend large language models (LLMs) with
multi-sensory skills, such as visual understanding, to achieve stronger generic
intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to
deepen the understanding of LMMs. The analysis focuses on the intriguing tasks
that GPT-4V can perform, containing test samples to probe the quality and
genericity of GPT-4V's capabilities, its supported inputs and working modes,
and the effective ways to prompt the model. In our approach to exploring
GPT-4V, we curate and organize a collection of carefully designed qualitative
samples spanning a variety of domains and tasks. Observations from these
samples demonstrate that GPT-4V's unprecedented ability in processing
arbitrarily interleaved multimodal inputs and the genericity of its
capabilities together make GPT-4V a powerful multimodal generalist system.
Furthermore, GPT-4V's unique capability of understanding visual markers drawn
on input images can give rise to new human-computer interaction methods such as
visual referring prompting. We conclude the report with in-depth discussions on
the emerging application scenarios and the future research directions for
GPT-4V-based systems. We hope that this preliminary exploration will inspire
future research on the next-generation multimodal task formulation, new ways to
exploit and enhance LMMs to solve real-world problems, and gaining better
understanding of multimodal foundation models. Finally, we acknowledge that the
model under our study is solely the product of OpenAI's innovative work, and
they should be fully credited for its development. Please see the GPT-4V
contributions paper for the authorship and credit attribution:
https://cdn.openai.com/contributions/gpt-4v.pdf
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructDET: Diversifying Referring Object Detection with Generalized
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05136v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05136v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song, Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, Yibing Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose InstructDET, a data-centric method for referring object detection
(ROD) that localizes target objects based on user instructions. While deriving
from referring expressions (REC), the instructions we leverage are greatly
diversified to encompass common user intentions related to object detection.
For one image, we produce tremendous instructions that refer to every single
object and different combinations of multiple objects. Each instruction and its
corresponding object bounding boxes (bbxs) constitute one training data pair.
In order to encompass common detection expressions, we involve emerging
vision-language model (VLM) and large language model (LLM) to generate
instructions guided by text prompts and object bbxs, as the generalizations of
foundation models are effective to produce human-like expressions (e.g.,
describing object property, category, and relationship). We name our
constructed dataset as InDET. It contains images, bbxs and generalized
instructions that are from foundation models. Our InDET is developed from
existing REC datasets and object detection datasets, with the expanding
potential that any image with object bbxs can be incorporated through using our
InstructDET method. By using our InDET dataset, we show that a conventional ROD
model surpasses existing methods on standard REC datasets and our InDET test
set. Our data-centric method InstructDET, with automatic data expansion by
leveraging foundation models, directs a promising field that ROD can be greatly
diversified to execute common object detection instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Adjust the subject</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAIR-Diffusion: A Comprehensive Multimodal Object-Level Image Editor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Xingqian Xu, Nicu Sebe, Trevor Darrell, Zhangyang Wang, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative image editing has recently witnessed extremely fast-paced growth.
Some works use high-level conditioning such as text, while others use low-level
conditioning. Nevertheless, most of them lack fine-grained control over the
properties of the different objects present in the image, i.e.\,object-level
image editing. In this work, we tackle the task by perceiving the images as an
amalgamation of various objects and aim to control the properties of each
object in a fine-grained manner. Out of these properties, we identify structure
and appearance as the most intuitive to understand and useful for editing
purposes. We propose \textbf{PAIR} Diffusion, a generic framework that can
enable a diffusion model to control the structure and appearance properties of
each object in the image. We show that having control over the properties of
each object in an image leads to comprehensive editing capabilities. Our
framework allows for various object-level editing operations on real images
such as reference image-based appearance editing, free-form shape editing,
adding objects, and variations. Thanks to our design, we do not require any
inversion step. Additionally, we propose multimodal classifier-free guidance
which enables editing images using both reference images and text when using
our approach with foundational diffusion models. We validate the above claims
by extensively evaluating our framework on both unconditional and foundational
diffusion models. Please refer to
https://vidit98.github.io/publication/conference-paper/pair_diff.html for code
and model release.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages and 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IPD:An Incremental Prototype based DBSCAN for large-scale data with
  cluster representatives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayasree Saha, Jayanta Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DBSCAN is a fundamental density-based clustering technique that identifies
any arbitrary shape of the clusters. However, it becomes infeasible while
handling big data. On the other hand, centroid-based clustering is important
for detecting patterns in a dataset since unprocessed data points can be
labeled to their nearest centroid. However, it can not detect non-spherical
clusters. For a large data, it is not feasible to store and compute labels of
every samples. These can be done as and when the information is required. The
purpose can be accomplished when clustering act as a tool to identify cluster
representatives and query is served by assigning cluster labels of nearest
representative. In this paper, we propose an Incremental Prototype-based DBSCAN
(IPD) algorithm which is designed to identify arbitrary-shaped clusters for
large-scale data. Additionally, it chooses a set of representatives for each
cluster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YOLO-Drone:Airborne real-time detection of dense small objects from
  high-altitude perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhu, Jiahui Xiong, Feng Xiong, Hanzheng Hu, Zhengnan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicles (UAVs), specifically drones equipped with remote
sensing object detection technology, have rapidly gained a broad spectrum of
applications and emerged as one of the primary research focuses in the field of
computer vision. Although UAV remote sensing systems have the ability to detect
various objects, small-scale objects can be challenging to detect reliably due
to factors such as object size, image degradation, and real-time limitations.
To tackle these issues, a real-time object detection algorithm (YOLO-Drone) is
proposed and applied to two new UAV platforms as well as a specific light
source (silicon-based golden LED). YOLO-Drone presents several novelties: 1)
including a new backbone Darknet59; 2) a new complex feature aggregation module
MSPP-FPN that incorporated one spatial pyramid pooling and three atrous spatial
pyramid pooling modules; 3) and the use of Generalized Intersection over Union
(GIoU) as the loss function. To evaluate performance, two benchmark datasets,
UAVDT and VisDrone, along with one homemade dataset acquired at night under
silicon-based golden LEDs, are utilized. The experimental results show that, in
both UAVDT and VisDrone, the proposed YOLO-Drone outperforms state-of-the-art
(SOTA) object detection methods by improving the mAP of 10.13% and 8.59%,
respectively. With regards to UAVDT, the YOLO-Drone exhibits both high
real-time inference speed of 53 FPS and a maximum mAP of 34.04%. Notably,
YOLO-Drone achieves high performance under the silicon-based golden LEDs, with
a mAP of up to 87.71%, surpassing the performance of YOLO series under ordinary
light sources. To conclude, the proposed YOLO-Drone is a highly effective
solution for object detection in UAV applications, particularly for night
detection tasks where silicon-based golden light LED technology exhibits
significant superiority.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Some contributing authors are not signed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MuseChat: A Conversational Music Recommendation System for Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhikang Dong, Bin Chen, Xiulong Liu, Pawel Polak, Peng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MuseChat, an innovative dialog-based music recommendation
system. This unique platform not only offers interactive user engagement but
also suggests music tailored for input videos, so that users can refine and
personalize their music selections. In contrast, previous systems predominantly
emphasized content compatibility, often overlooking the nuances of users'
individual preferences. For example, all the datasets only provide basic
music-video pairings or such pairings with textual music descriptions. To
address this gap, our research offers three contributions. First, we devise a
conversation-synthesis method that simulates a two-turn interaction between a
user and a recommendation system, which leverages pre-trained music tags and
artist information. In this interaction, users submit a video to the system,
which then suggests a suitable music piece with a rationale. Afterwards, users
communicate their musical preferences, and the system presents a refined music
recommendation with reasoning. Second, we introduce a multi-modal
recommendation engine that matches music either by aligning it with visual cues
from the video or by harmonizing visual information, feedback from previously
recommended music, and the user's textual input. Third, we bridge music
representations and textual data with a Large Language Model(Vicuna-7B). This
alignment equips MuseChat to deliver music recommendations and their underlying
reasoning in a manner resembling human communication. Our evaluations show that
MuseChat surpasses existing state-of-the-art models in music retrieval tasks
and pioneers the integration of the recommendation process within a natural
language framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot Inversion Process for Image Attribute Editing with Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanbo Feng, Zenan Ling, Ci Gong, Feng Zhou, Jie Li, Robert C. Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models have shown outstanding performance in image
editing. Existing works tend to use either image-guided methods, which provide
a visual reference but lack control over semantic coherence, or text-guided
methods, which ensure faithfulness to text guidance but lack visual quality. To
address the problem, we propose the Zero-shot Inversion Process (ZIP), a
framework that injects a fusion of generated visual reference and text guidance
into the semantic latent space of a \textit{frozen} pre-trained diffusion
model. Only using a tiny neural network, the proposed ZIP produces diverse
content and attributes under the intuitive control of the text prompt.
Moreover, ZIP shows remarkable robustness for both in-domain and out-of-domain
attribute manipulation on real images. We perform detailed experiments on
various benchmark datasets. Compared to state-of-the-art methods, ZIP produces
images of equivalent quality while providing a realistic editing effect.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Visual Scene Understanding: Incremental Scene Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naitik Khandelwal, Xiao Liu, Mengmi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene graph generation (SGG) involves analyzing images to extract meaningful
information about objects and their relationships. Given the dynamic nature of
the visual world, it becomes crucial for AI systems to detect new objects and
establish their new relationships with existing objects. To address the lack of
continual learning methodologies in SGG, we introduce the comprehensive
Continual ScenE Graph Generation (CSEGG) dataset along with 3 learning
scenarios and 8 evaluation metrics. Our research investigates the continual
learning performances of existing SGG methods on the retention of previous
object entities and relationships as they learn new ones. Moreover, we also
explore how continual object detection enhances generalization in classifying
known relationships on unknown objects. We conduct extensive experiments
benchmarking and analyzing the classical two-stage SGG methods and the most
recent transformer-based SGG methods in continual learning settings, and gain
valuable insights into the CSEGG problem. We invite the research community to
explore this emerging field of study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LRANet: Towards Accurate and Efficient Scene Text Detection with
  Low-Rank Approximation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15142v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15142v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Su, Zhineng Chen, Zhiwen Shao, Yuning Du, Zhilong Ji, Jinfeng Bai, Yong Zhou, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, regression-based methods, which predict parameterized text shapes
for text localization, have gained popularity in scene text detection. However,
the existing parameterized text shape methods still have limitations in
modeling arbitrary-shaped texts due to ignoring the utilization of
text-specific shape information. Moreover, the time consumption of the entire
pipeline has been largely overlooked, leading to a suboptimal overall inference
speed. To address these issues, we first propose a novel parameterized text
shape method based on low-rank approximation. Unlike other shape representation
methods that employ data-irrelevant parameterization, our approach utilizes
singular value decomposition and reconstructs the text shape using a few
eigenvectors learned from labeled text contours. By exploring the shape
correlation among different text contours, our method achieves consistency,
compactness, simplicity, and robustness in shape representation. Next, we
propose a dual assignment scheme for speed acceleration. It adopts a sparse
assignment branch to accelerate the inference speed, and meanwhile, provides
ample supervised signals for training through a dense assignment branch.
Building upon these designs, we implement an accurate and efficient
arbitrary-shaped text detector named LRANet. Extensive experiments are
conducted on several challenging benchmarks, demonstrating the superior
accuracy and efficiency of LRANet compared to state-of-the-art methods. Code
will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PNet -- A Deep Learning Based Photometry and Astrometry Bayesian
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.14349v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.14349v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Sun, Peng Jia, Yongyang Sun, Zhimin Yang, Qiang Liu, Hongyan Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time domain astronomy has emerged as a vibrant research field in recent
years, focusing on celestial objects that exhibit variable magnitudes or
positions. Given the urgency of conducting follow-up observations for such
objects, the development of an algorithm capable of detecting them and
determining their magnitudes and positions has become imperative. Leveraging
the advancements in deep neural networks, we present the PNet, an end-to-end
framework designed not only to detect celestial objects and extract their
magnitudes and positions but also to estimate photometry uncertainty. The PNet
comprises two essential steps. Firstly, it detects stars and retrieves their
positions, magnitudes, and calibrated magnitudes. Subsequently, in the second
phase, the PNet estimates the uncertainty associated with the photometry
results, serving as a valuable reference for the light curve classification
algorithm. Our algorithm has been tested using both simulated and real
observation data, demonstrating the PNet's ability to deliver consistent and
reliable outcomes. Integration of the PNet into data processing pipelines for
time-domain astronomy holds significant potential for enhancing response speed
and improving the detection capabilities for celestial objects with variable
positions and magnitudes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the AJ and welcome to any comments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IPMix: Label-Preserving Data Augmentation Method for Training Robust
  Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenglin Huang, Xianan Bao, Na Zhang, Qingqi Zhang, Xiaomei Tu, Biao Wu, Xi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation has been proven effective for training high-accuracy
convolutional neural network classifiers by preventing overfitting. However,
building deep neural networks in real-world scenarios requires not only high
accuracy on clean data but also robustness when data distributions shift. While
prior methods have proposed that there is a trade-off between accuracy and
robustness, we propose IPMix, a simple data augmentation approach to improve
robustness without hurting clean accuracy. IPMix integrates three levels of
data augmentation (image-level, patch-level, and pixel-level) into a coherent
and label-preserving technique to increase the diversity of training data with
limited computational overhead. To further improve the robustness, IPMix
introduces structural complexity at different levels to generate more diverse
images and adopts the random mixing method for multi-scale information fusion.
Experiments demonstrate that IPMix outperforms state-of-the-art corruption
robustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also
significantly improves the other safety measures, including robustness to
adversarial perturbations, calibration, prediction consistency, and anomaly
detection, achieving state-of-the-art or comparable results on several
benchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPEC2: SPECtral SParsE CNN Accelerator on FPGAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1910.11103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1910.11103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Niu, Hanqing Zeng, Ajitesh Srivastava, Kartik Lakhotia, Rajgopal Kannan, Yanzhi Wang, Viktor Prasanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To accelerate inference of Convolutional Neural Networks (CNNs), various
techniques have been proposed to reduce computation redundancy. Converting
convolutional layers into frequency domain significantly reduces the
computation complexity of the sliding window operations in space domain. On the
other hand, weight pruning techniques address the redundancy in model
parameters by converting dense convolutional kernels into sparse ones. To
obtain high-throughput FPGA implementation, we propose SPEC2 -- the first work
to prune and accelerate spectral CNNs. First, we propose a systematic pruning
algorithm based on Alternative Direction Method of Multipliers (ADMM). The
offline pruning iteratively sets the majority of spectral weights to zero,
without using any handcrafted heuristics. Then, we design an optimized pipeline
architecture on FPGA that has efficient random access into the sparse kernels
and exploits various dimensions of parallelism in convolutional layers.
Overall, SPEC2 achieves high inference throughput with extremely low
computation complexity and negligible accuracy degradation. We demonstrate
SPEC2 by pruning and implementing LeNet and VGG16 on the Xilinx Virtex
platform. After pruning 75% of the spectral weights, SPEC2 achieves 0% accuracy
loss for LeNet, and <1% accuracy loss for VGG16. The resulting accelerators
achieve up to 24x higher throughput, compared with the state-of-the-art FPGA
implementations for VGG16.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a 10-page conference paper in 26TH IEEE International
  Conference On High Performance Computing, Data, and Analytics (HiPC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Joint Latent Space EBM Prior Model for Multi-layer Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06323v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06323v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiali Cui, Ying Nian Wu, Tian Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the fundamental problem of learning multi-layer generator
models. The multi-layer generator model builds multiple layers of latent
variables as a prior model on top of the generator, which benefits learning
complex data distribution and hierarchical representations. However, such a
prior model usually focuses on modeling inter-layer relations between latent
variables by assuming non-informative (conditional) Gaussian distributions,
which can be limited in model expressivity. To tackle this issue and learn more
expressive prior models, we propose an energy-based model (EBM) on the joint
latent space over all layers of latent variables with the multi-layer generator
as its backbone. Such joint latent space EBM prior model captures the
intra-layer contextual relations at each layer through layer-wise energy terms,
and latent variables across different layers are jointly corrected. We develop
a joint training scheme via maximum likelihood estimation (MLE), which involves
Markov Chain Monte Carlo (MCMC) sampling for both prior and posterior
distributions of the latent variables from different layers. To ensure
efficient inference and learning, we further propose a variational training
scheme where an inference model is used to amortize the costly posterior MCMC
sampling. Our experiments demonstrate that the learned model can be expressive
in generating high-quality images and capturing hierarchical features for
better outlier detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Large Vision-Language Model with Out-of-Distribution
  Generalizability <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03135v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03135v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanlin Li, Yunhao Fang, Minghua Liu, Zhan Ling, Zhuowen Tu, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models have achieved outstanding performance, but their
size and computational requirements make their deployment on
resource-constrained devices and time-sensitive tasks impractical. Model
distillation, the process of creating smaller, faster models that maintain the
performance of larger models, is a promising direction towards the solution.
This paper investigates the distillation of visual representations in large
teacher vision-language models into lightweight student models using a small-
or mid-scale dataset. Notably, this study focuses on open-vocabulary
out-of-distribution (OOD) generalization, a challenging problem that has been
overlooked in previous model distillation literature. We propose two principles
from vision and language modality perspectives to enhance student's OOD
generalization: (1) by better imitating teacher's visual representation space,
and carefully promoting better coherence in vision-language alignment with the
teacher; (2) by enriching the teacher's language representations with
informative and finegrained semantic attributes to effectively distinguish
between different labels. We propose several metrics and conduct extensive
experiments to investigate their techniques. The results demonstrate
significant improvements in zero-shot and few-shot student performance on
open-vocabulary out-of-distribution classification, highlighting the
effectiveness of our proposed approaches. Poster:
https://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf
Code: https://github.com/xuanlinli17/large_vlm_distillation_ood
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at International Conference on Computer Vision (ICCV) 2023.
  Poster at
  https://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Saliency-based Video Summarization for Face Anti-spoofing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usman Muhammad, Mourad Oussalah, Jorma Laaksonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing availability of databases for face presentation attack
detection, researchers are increasingly focusing on video-based face
anti-spoofing methods that involve hundreds to thousands of images for training
the models. However, there is currently no clear consensus on the optimal
number of frames in a video to improve face spoofing detection. Inspired by the
visual saliency theory, we present a video summarization method for face
anti-spoofing detection that aims to enhance the performance and efficiency of
deep learning models by leveraging visual saliency. In particular, saliency
information is extracted from the differences between the Laplacian and Wiener
filter outputs of the source images, enabling identification of the most
visually salient regions within each frame. Subsequently, the source images are
decomposed into base and detail images, enhancing the representation of the
most important information. Weighting maps are then computed based on the
saliency information, indicating the importance of each pixel in the image. By
linearly combining the base and detail images using the weighting maps, the
method fuses the source images to create a single representative image that
summarizes the entire video. The key contribution of the proposed method lies
in demonstrating how visual saliency can be used as a data-centric approach to
improve the performance and efficiency for face presentation attack detection.
By focusing on the most salient images or regions within the images, a more
representative and diverse training set can be created, potentially leading to
more effective models. To validate the method's effectiveness, a simple CNN-RNN
deep learning architecture was used, and the experimental results showcased
state-of-the-art performance on five challenging face anti-spoofing datasets
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized
  Device Coordinates Space <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14616v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14616v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Yao, Chuming Li, Keqiang Sun, Yingjie Cai, Hao Li, Wanli Ouyang, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D Semantic Scene Completion (SSC) has garnered significant
attention in recent years due to its potential to predict complex semantics and
geometry shapes from a single image, requiring no 3D inputs. In this paper, we
identify several critical issues in current state-of-the-art methods, including
the Feature Ambiguity of projected 2D features in the ray to the 3D space, the
Pose Ambiguity of the 3D convolution, and the Computation Imbalance in the 3D
convolution across different depth levels. To address these problems, we devise
a novel Normalized Device Coordinates scene completion network (NDC-Scene) that
directly extends the 2D feature map to a Normalized Device Coordinates (NDC)
space, rather than to the world space directly, through progressive restoration
of the dimension of depth with deconvolution operations. Experiment results
demonstrate that transferring the majority of computation from the target 3D
space to the proposed normalized device coordinates space benefits monocular
SSC tasks. Additionally, we design a Depth-Adaptive Dual Decoder to
simultaneously upsample and fuse the 2D and 3D feature maps, further improving
overall performance. Our extensive experiments confirm that the proposed method
consistently outperforms state-of-the-art methods on both outdoor SemanticKITTI
and indoor NYUv2 datasets. Our code are available at
https://github.com/Jiawei-Yao0812/NDCScene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2023. Project page:
  https://jiawei-yao0812.github.io/NDC-Scene/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image as First-Order Norm+Linear Autoregression: Unveiling Mathematical
  Invariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Lu Yuan, Zicheng Liu, Youzuo Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel mathematical property applicable to diverse
images, referred to as FINOLA (First-Order Norm+Linear Autoregressive). FINOLA
represents each image in the latent space as a first-order autoregressive
process, in which each regression step simply applies a shared linear model on
the normalized value of its immediate neighbor. This intriguing property
reveals a mathematical invariance that transcends individual images. Expanding
from image grids to continuous coordinates, we unveil the presence of two
underlying partial differential equations. We validate the FINOLA property from
two distinct angles: image reconstruction and self-supervised learning.
Firstly, we demonstrate the ability of FINOLA to auto-regress up to a 256x256
feature map (the same resolution to the image) from a single vector placed at
the center, successfully reconstructing the original image by only using three
3x3 convolution layers as decoder. Secondly, we leverage FINOLA for
self-supervised learning by employing a simple masked prediction approach.
Encoding a single unmasked quadrant block, we autoregressively predict the
surrounding masked region. Remarkably, this pre-trained representation proves
highly effective in image classification and object detection tasks, even when
integrated into lightweight networks, all without the need for extensive
fine-tuning. The code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Complete Recipe for Diffusion Generative Models <span class="chip">ICCV'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushagra Pandey, Stephan Mandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score-based Generative Models (SGMs) have demonstrated exceptional synthesis
outcomes across various tasks. However, the current design landscape of the
forward diffusion process remains largely untapped and often relies on physical
heuristics or simplifying assumptions. Utilizing insights from the development
of scalable Bayesian posterior samplers, we present a complete recipe for
formulating forward processes in SGMs, ensuring convergence to the desired
target distribution. Our approach reveals that several existing SGMs can be
seen as specific manifestations of our framework. Building upon this method, we
introduce Phase Space Langevin Diffusion (PSLD), which relies on score-based
modeling within an augmented space enriched by auxiliary variables akin to
physical phase space. Empirical results exhibit the superior sample quality and
improved speed-quality trade-off of PSLD compared to various competing
approaches on established image synthesis benchmarks. Remarkably, PSLD achieves
sample quality akin to state-of-the-art SGMs (FID: 2.10 for unconditional
CIFAR-10 generation). Lastly, we demonstrate the applicability of PSLD in
conditional synthesis using pre-trained score networks, offering an appealing
alternative as an SGM backbone for future advancements. Code and model
checkpoints can be accessed at \url{https://github.com/mandt-lab/PSLD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICCV'23 (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMTSA: Multimodal Temporal Segment Attention Network for Efficient Human
  Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Gao, Yuntao Wang, Jianguo Chen, Junliang Xing, Shwetak Patel, Xin Liu, Yuanchun Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal sensors provide complementary information to develop accurate
machine-learning methods for human activity recognition (HAR), but introduce
significantly higher computational load, which reduces efficiency. This paper
proposes an efficient multimodal neural architecture for HAR using an RGB
camera and inertial measurement units (IMUs) called Multimodal Temporal Segment
Attention Network (MMTSA). MMTSA first transforms IMU sensor data into a
temporal and structure-preserving gray-scale image using the Gramian Angular
Field (GAF), representing the inherent properties of human activities. MMTSA
then applies a multimodal sparse sampling method to reduce data redundancy.
Lastly, MMTSA adopts an inter-segment attention module for efficient multimodal
fusion. Using three well-established public datasets, we evaluated MMTSA's
effectiveness and efficiency in HAR. Results show that our method achieves
superior performance improvements 11.13% of cross-subject F1-score on the MMAct
dataset than the previous state-of-the-art (SOTA) methods. The ablation study
and analysis suggest that MMTSA's effectiveness in fusing multimodal data for
accurate HAR. The efficiency evaluation on an edge device showed that MMTSA
achieved significantly better accuracy, lower computational load, and lower
inference latency than SOTA methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SYRAC: Synthesize, Rank, and Count 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01662v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01662v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adriano D'Alessandro, Ali Mahdavi-Amiri, Ghassan Hamarneh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowd counting is a critical task in computer vision, with several important
applications. However, existing counting methods rely on labor-intensive
density map annotations, necessitating the manual localization of each
individual pedestrian. While recent efforts have attempted to alleviate the
annotation burden through weakly or semi-supervised learning, these approaches
fall short of significantly reducing the workload. We propose a novel approach
to eliminate the annotation burden by leveraging latent diffusion models to
generate synthetic data. However, these models struggle to reliably understand
object quantities, leading to noisy annotations when prompted to produce images
with a specific quantity of objects. To address this, we use latent diffusion
models to create two types of synthetic data: one by removing pedestrians from
real images, which generates ranked image pairs with a weak but reliable object
quantity signal, and the other by generating synthetic images with a
predetermined number of objects, offering a strong but noisy counting signal.
Our method utilizes the ranking image pairs for pre-training and then fits a
linear layer to the noisy synthetic images using these crowd quantity features.
We report state-of-the-art results for unsupervised crowd counting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAPConNet: Self-supervised 3D Pose Transfer with Mesh and Point
  Contrastive Learning <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaze Sun, Zhixiang Chen, Tae-Kyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D pose transfer is a challenging generation task that aims to transfer the
pose of a source geometry onto a target geometry with the target identity
preserved. Many prior methods require keypoint annotations to find
correspondence between the source and target. Current pose transfer methods
allow end-to-end correspondence learning but require the desired final output
as ground truth for supervision. Unsupervised methods have been proposed for
graph convolutional models but they require ground truth correspondence between
the source and target inputs. We present a novel self-supervised framework for
3D pose transfer which can be trained in unsupervised, semi-supervised, or
fully supervised settings without any correspondence labels. We introduce two
contrastive learning constraints in the latent space: a mesh-level loss for
disentangling global patterns including pose and identity, and a point-level
loss for discriminating local semantics. We demonstrate quantitatively and
qualitatively that our method achieves state-of-the-art results in supervised
3D pose transfer, with comparable results in unsupervised and semi-supervised
settings. Our method is also generalisable to unseen human and animal data with
complex topologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compressing And Debiasing Vision-Language Pre-Trained Models for Visual
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyi Si, Yuanxin Liu, Zheng Lin, Peng Fu, Weiping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the excellent performance of vision-language pre-trained models
(VLPs) on conventional VQA task, they still suffer from two problems: First,
VLPs tend to rely on language biases in datasets and fail to generalize to
out-of-distribution (OOD) data. Second, they are inefficient in terms of memory
footprint and computation. Although promising progress has been made in both
problems, most existing works tackle them independently. To facilitate the
application of VLP to VQA tasks, it is imperative to jointly study VLP
compression and OOD robustness, which, however, has not yet been explored. This
paper investigates whether a VLP can be compressed and debiased simultaneously
by searching sparse and robust subnetworks. To this end, we systematically
study the design of a training and compression pipeline to search the
subnetworks, as well as the assignment of sparsity to different
modality-specific modules. Our experiments involve 3 VLPs, 2 compression
methods, 4 training methods, 2 datasets and a range of sparsity levels and
random seeds. Our results show that there indeed exist sparse and robust
subnetworks, which are competitive with the debiased full VLP and clearly
outperform the debiasing SoTAs with fewer parameters on OOD datasets VQA-CP v2
and VQA-VS. The codes can be found at
https://github.com/PhoebusSi/Compress-Robust-VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KITE: Keypoint-Conditioned Policies for Semantic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16605v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16605v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priya Sundaresan, Suneel Belkhale, Dorsa Sadigh, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While natural language offers a convenient shared interface for humans and
robots, enabling robots to interpret and follow language commands remains a
longstanding challenge in manipulation. A crucial step to realizing a
performant instruction-following robot is achieving semantic manipulation,
where a robot interprets language at different specificities, from high-level
instructions like "Pick up the stuffed animal" to more detailed inputs like
"Grab the left ear of the elephant." To tackle this, we propose Keypoints +
Instructions to Execution (KITE), a two-step framework for semantic
manipulation which attends to both scene semantics (distinguishing between
different objects in a visual scene) and object semantics (precisely localizing
different parts within an object instance). KITE first grounds an input
instruction in a visual scene through 2D image keypoints, providing a highly
accurate object-centric bias for downstream action inference. Provided an RGB-D
scene observation, KITE then executes a learned keypoint-conditioned skill to
carry out the instruction. The combined precision of keypoints and
parameterized skills enables fine-grained manipulation with generalization to
scene and object variations. Empirically, we demonstrate KITE in 3 real-world
environments: long-horizon 6-DoF tabletop manipulation, semantic grasping, and
a high-precision coffee-making task. In these settings, KITE achieves a 75%,
70%, and 71% overall success rate for instruction-following, respectively. KITE
outperforms frameworks that opt for pre-trained visual language models over
keypoint-based grounding, or omit skills in favor of end-to-end visuomotor
control, all while being trained from fewer or comparable amounts of
demonstrations. Supplementary material, datasets, code, and videos can be found
on our website: http://tinyurl.com/kite-site.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual and Object Geo-localization: A Comprehensive Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.15202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.15202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Wilson, Xiaohan Zhang, Waqas Sultani, Safwan Wshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The concept of geo-localization refers to the process of determining where on
earth some `entity' is located, typically using Global Positioning System (GPS)
coordinates. The entity of interest may be an image, sequence of images, a
video, satellite image, or even objects visible within the image. As massive
datasets of GPS tagged media have rapidly become available due to smartphones
and the internet, and deep learning has risen to enhance the performance
capabilities of machine learning models, the fields of visual and object
geo-localization have emerged due to its significant impact on a wide range of
applications such as augmented reality, robotics, self-driving vehicles, road
maintenance, and 3D reconstruction. This paper provides a comprehensive survey
of geo-localization involving images, which involves either determining from
where an image has been captured (Image geo-localization) or geo-locating
objects within an image (Object geo-localization). We will provide an in-depth
study, including a summary of popular algorithms, a description of proposed
datasets, and an analysis of performance results to illustrate the current
state of each field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RMT: Retentive Networks Meet Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihang Fan, Huaibo Huang, Mingrui Chen, Hongmin Liu, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer first appears in the field of natural language processing and is
later migrated to the computer vision domain, where it demonstrates excellent
performance in vision tasks. However, recently, Retentive Network (RetNet) has
emerged as an architecture with the potential to replace Transformer,
attracting widespread attention in the NLP community. Therefore, we raise the
question of whether transferring RetNet's idea to vision can also bring
outstanding performance to vision tasks. To address this, we combine RetNet and
Transformer to propose RMT. Inspired by RetNet, RMT introduces explicit decay
into the vision backbone, bringing prior knowledge related to spatial distances
to the vision model. This distance-related spatial prior allows for explicit
control of the range of tokens that each token can attend to. Additionally, to
reduce the computational cost of global modeling, we decompose this modeling
process along the two coordinate axes of the image. Abundant experiments have
demonstrated that our RMT exhibits exceptional performance across various
computer vision tasks. For example, RMT achieves 84.1% Top1-acc on ImageNet-1k
using merely 4.5G FLOPs. To the best of our knowledge, among all models, RMT
achieves the highest Top1-acc when models are of similar size and trained with
the same strategy. Moreover, RMT significantly outperforms existing vision
backbones in downstream tasks such as object detection, instance segmentation,
and semantic segmentation. Our work is still in progress.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The work is still in progress</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-10T00:00:00Z">2023-10-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">45</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoboHive: A Unified Framework for Robot Learning <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikash Kumar, Rutav Shah, Gaoyue Zhou, Vincent Moens, Vittorio Caggiano, Jay Vakil, Abhishek Gupta, Aravind Rajeswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present RoboHive, a comprehensive software platform and ecosystem for
research in the field of Robot Learning and Embodied Artificial Intelligence.
Our platform encompasses a diverse range of pre-existing and novel
environments, including dexterous manipulation with the Shadow Hand, whole-arm
manipulation tasks with Franka and Fetch robots, quadruped locomotion, among
others. Included environments are organized within and cover multiple domains
such as hand manipulation, locomotion, multi-task, multi-agent, muscles, etc.
In comparison to prior works, RoboHive offers a streamlined and unified task
interface taking dependency on only a minimal set of well-maintained packages,
features tasks with high physics fidelity and rich visual diversity, and
supports common hardware drivers for real-world deployment. The unified
interface of RoboHive offers a convenient and accessible abstraction for
algorithmic research in imitation, reinforcement, multi-task, and hierarchical
learning. Furthermore, RoboHive includes expert demonstrations and baseline
results for most environments, providing a standard for benchmarking and
comparisons. Details: https://sites.google.com/view/robohive
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023) Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $f$-Policy Gradients: A General Framework for Goal Conditioned RL using
  $f$-Divergences <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhant Agarwal, Ishan Durugkar, Peter Stone, Amy Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-Conditioned Reinforcement Learning (RL) problems often have access to
sparse rewards where the agent receives a reward signal only when it has
achieved the goal, making policy optimization a difficult problem. Several
works augment this sparse reward with a learned dense reward function, but this
can lead to sub-optimal policies if the reward is misaligned. Moreover, recent
works have demonstrated that effective shaping rewards for a particular problem
can depend on the underlying learning algorithm. This paper introduces a novel
way to encourage exploration called $f$-Policy Gradients, or $f$-PG. $f$-PG
minimizes the f-divergence between the agent's state visitation distribution
and the goal, which we show can lead to an optimal policy. We derive gradients
for various f-divergences to optimize this objective. Our learning paradigm
provides dense learning signals for exploration in sparse reward settings. We
further introduce an entropy-regularized policy optimization objective, that we
call $state$-MaxEnt RL (or $s$-MaxEnt RL) as a special case of our objective.
We show that several metric-based shaping rewards like L2 can be used with
$s$-MaxEnt RL, providing a common ground to study such metric-based shaping
rewards with efficient exploration. We find that $f$-PG has better performance
compared to standard policy gradient methods on a challenging gridworld as well
as the Point Maze and FetchReach environments. More information on our website
https://agarwalsiddhant10.github.io/projects/fpg.html.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Graduated Non-Convexity for Pose Graph Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonseok Kang, Jaehyun Kim, Jiseong Chung, Seungwon Choi, Tae-wan Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to Graduated Non-Convexity (GNC) and demonstrate
its efficacy through its application in robust pose graph optimization, a key
component in SLAM backends. Traditional GNC methods often rely on heuristic
methods for GNC schedule, updating control parameter {\mu} for escalating the
non-convexity. In contrast, our approach leverages the properties of convex
functions and convex optimization to identify the boundary points beyond which
convexity is no longer guaranteed, thereby eliminating redundant optimization
steps in existing methodologies and enhancing both speed and robustness. We
show that our method outperforms the state-of-the-art method in terms of speed
and accuracy when used for robust back-end pose graph optimization via GNC. Our
work builds upon and enhances the open-source riSAM framework. Our
implementation can be accessed from: https://github.com/SNU-DLLAB/EGNC-PGO
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EARL: Eye-on-Hand Reinforcement Learner for Dynamic Grasping with Active
  Pose Estimation <span class="chip">IROS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baichuan Huang, Jingjin Yu, Siddarth Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the dynamic grasping of moving objects through
active pose tracking and reinforcement learning for hand-eye coordination
systems. Most existing vision-based robotic grasping methods implicitly assume
target objects are stationary or moving predictably. Performing grasping of
unpredictably moving objects presents a unique set of challenges. For example,
a pre-computed robust grasp can become unreachable or unstable as the target
object moves, and motion planning must also be adaptive. In this work, we
present a new approach, Eye-on-hAnd Reinforcement Learner (EARL), for enabling
coupled Eye-on-Hand (EoH) robotic manipulation systems to perform real-time
active pose tracking and dynamic grasping of novel objects without explicit
motion prediction. EARL readily addresses many thorny issues in automated
hand-eye coordination, including fast-tracking of 6D object pose from vision,
learning control policy for a robotic arm to track a moving object while
keeping the object in the camera's field of view, and performing dynamic
grasping. We demonstrate the effectiveness of our approach in extensive
experiments validated on multiple commercial robotic arms in both simulations
and complex real-world tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented on IROS 2023 Corresponding author Siddarth Jain</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Explanation Methods for Vision-and-Language Navigation <span class="chip">ECAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanqi Chen, Lei Yang, Guanhua Chen, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to navigate robots with natural language instructions in an
unknown environment is a crucial step for achieving embodied artificial
intelligence (AI). With the improving performance of deep neural models
proposed in the field of vision-and-language navigation (VLN), it is equally
interesting to know what information the models utilize for their
decision-making in the navigation tasks. To understand the inner workings of
deep neural models, various explanation methods have been developed for
promoting explainable AI (XAI). But they are mostly applied to deep neural
models for image or text classification tasks and little work has been done in
explaining deep neural models for VLN tasks. In this paper, we address these
problems by building quantitative benchmarks to evaluate explanation methods
for VLN models in terms of faithfulness. We propose a new erasure-based
evaluation pipeline to measure the step-wise textual explanation in the
sequential decision-making setting. We evaluate several explanation methods for
two representative VLN models on two popular VLN datasets and reveal valuable
findings through our experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forgetful Large Language Models: Lessons Learned from Using LLMs in
  Robot Programming <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juo-Tung Chen, Chien-Ming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models offer new ways of empowering people to program robot
applications-namely, code generation via prompting. However, the code generated
by LLMs is susceptible to errors. This work reports a preliminary exploration
that empirically characterizes common errors produced by LLMs in robot
programming. We categorize these errors into two phases: interpretation and
execution. In this work, we focus on errors in execution and observe that they
are caused by LLMs being "forgetful" of key information provided in user
prompts. Based on this observation, we propose prompt engineering tactics
designed to reduce errors in execution. We then demonstrate the effectiveness
of these tactics with three language models: ChatGPT, Bard, and LLaMA-2.
Finally, we discuss lessons learned from using LLMs in robot programming and
call for the benchmarking of LLM-powered end-user development of robot
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages ,8 figures, accepted by the AAAI 2023 Fall Symposium Series</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SYNLOCO: Synthesizing Central Pattern Generator and Reinforcement
  Learning for Quadruped Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhang, Zhiyuan Xiao, Qingrui Zhang, Wei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Central Pattern Generator (CPG) is adept at generating rhythmic gait
patterns characterized by consistent timing and adequate foot clearance. Yet,
its open-loop configuration often compromises the system's control performance
in response to environmental variations. On the other hand, Reinforcement
Learning (RL), celebrated for its model-free properties, has gained significant
traction in robotics due to its inherent adaptability and robustness. However,
initiating traditional RL approaches from the ground up presents computational
challenges and a heightened risk of converging to suboptimal local minima. In
this paper, we propose an innovative quadruped locomotion framework, SYNLOCO,
by synthesizing CPG and RL that can ingeniously integrate the strengths of both
methods, enabling the development of a locomotion controller that is both
stable and natural. Furthermore, we introduce a set of performance-driven
reward metrics that augment the learning of locomotion control. To optimize the
learning trajectory of SYNLOCO, a two-phased training strategy is presented.
Our empirical evaluation, conducted on a Unitree GO1 robot under varied
conditions--including distinct velocities, terrains, and payload
capacities--showcases SYNLOCO's ability to produce consistent and clear-footed
gaits across diverse scenarios. The developed controller exhibits resilience
against substantial parameter variations, underscoring its potential for robust
real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric
  Heterogenous Distillation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caizhen He, Hai Wang, Long Chen, Tong Luo, Yingfeng Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection is the central issue of intelligent traffic systems, and
recent advancements in single-vehicle lidar-based 3D detection indicate that it
can provide accurate position information for intelligent agents to make
decisions and plan. Compared with single-vehicle perception, multi-view
vehicle-road cooperation perception has fundamental advantages, such as the
elimination of blind spots and a broader range of perception, and has become a
research hotspot. However, the current perception of cooperation focuses on
improving the complexity of fusion while ignoring the fundamental problems
caused by the absence of single-view outlines. We propose a multi-view
vehicle-road cooperation perception system, vehicle-to-everything cooperative
perception (V2X-AHD), in order to enhance the identification capability,
particularly for predicting the vehicle's shape. At first, we propose an
asymmetric heterogeneous distillation network fed with different training data
to improve the accuracy of contour recognition, with multi-view teacher
features transferring to single-view student features. While the point cloud
data are sparse, we propose Spara Pillar, a spare convolutional-based plug-in
feature extraction backbone, to reduce the number of parameters and improve and
enhance feature extraction capabilities. Moreover, we leverage the multi-head
self-attention (MSA) to fuse the single-view feature, and the lightweight
design makes the fusion feature a smooth expression. The results of applying
our algorithm to the massive open dataset V2Xset demonstrate that our method
achieves the state-of-the-art result. The V2X-AHD can effectively improve the
accuracy of 3D object detection and reduce the number of network parameters,
according to this study, which serves as a benchmark for cooperative
perception. The code for this article is available at
https://github.com/feeling0414-lab/V2X-AHD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Black-Box Physics-Informed Estimator based on Gaussian Process
  Regression for Robot Inverse Dynamics Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Giacomuzzo, Alberto Dalla Libera, Diego Romeres, Ruggero Carli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a black-box model based on Gaussian process
regression for the identification of the inverse dynamics of robotic
manipulators. The proposed model relies on a novel multidimensional kernel,
called \textit{Lagrangian Inspired Polynomial} (\kernelInitials{}) kernel. The
\kernelInitials{} kernel is based on two main ideas. First, instead of directly
modeling the inverse dynamics components, we model as GPs the kinetic and
potential energy of the system. The GP prior on the inverse dynamics components
is derived from those on the energies by applying the properties of GPs under
linear operators. Second, as regards the energy prior definition, we prove a
polynomial structure of the kinetic and potential energy, and we derive a
polynomial kernel that encodes this property. As a consequence, the proposed
model allows also to estimate the kinetic and potential energy without
requiring any label on these quantities. Results on simulation and on two real
robotic manipulators, namely a 7 DOF Franka Emika Panda and a 6 DOF MELFA
RV4FL, show that the proposed model outperforms state-of-the-art black-box
estimators based both on Gaussian Processes and Neural Networks in terms of
accuracy, generality and data efficiency. The experiments on the MELFA robot
also demonstrate that our approach achieves performance comparable to
fine-tuned model-based estimators, despite requiring less prior information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven mode shape selection and model-based vibration suppression
  of 3-RRR parallel manipulator with flexible actuation links 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingxu Guo, Jian Xu, Shu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mode shape function is difficult to determine in modeling manipulators
with flexible links using the assumed mode method. In this paper, for a planar
3-RRR parallel manipulator with flexible actuation links, we provide a
data-driven method to identify the mode shape of the flexible links and propose
a model-based controller for the vibration suppression. By deriving the inverse
kinematics of the studied mechanism in analytical form, the dynamic model is
established by using the assumed mode method. To select the mode shape
function, the software of multi-body system dynamics is used to simulate the
dynamic behavior of the mechanism, and then the data-driven method which
combines the DMD and SINDy algorithms is employed to identify the reasonable
mode shape functions for the flexible links. To suppress the vibration of the
flexible links, a state observer for the end-effector is constructed by a
neural network, and the model-based control law is designed on this basis. In
comparison with the model-free controller, the proposed controller with
developed dynamic model has promising performance in terms of tracking accuracy
and vibration suppression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feel the Tension: Manipulation of Deformable Linear Objects in
  Environments with Fixtures using Force Information <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Finn Süberkrüb, Rita Laezza, Yiannis Karayiannidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans are able to manipulate Deformable Linear Objects (DLOs) such as cables
and wires, with little or no visual information, relying mostly on force
sensing. In this work, we propose a reduced DLO model which enables such blind
manipulation by keeping the object under tension. Further, an online model
estimation procedure is also proposed. A set of elementary sliding and clipping
manipulation primitives are defined based on our model. The combination of
these primitives allows for more complex motions such as winding of a DLO. The
model estimation and manipulation primitives are tested individually but also
together in a real-world cable harness production task, using a dual-arm YuMi,
thus demonstrating that force-based perception can be sufficient even for such
a complex scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2022 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plane Constraints Aided Multi-Vehicle Cooperative Positioning Using
  Factor Graph Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhuang, Hongbo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of vehicle-to-vehicle (V2V) communication facil-itates the
study of cooperative positioning (CP) techniques for vehicular applications.
The CP methods can improve the posi-tioning availability and accuracy by
inter-vehicle ranging and data exchange between vehicles. However, the
inter-vehicle rang-ing can be easily interrupted due to many factors such as
obsta-cles in-between two cars. Without inter-vehicle ranging, the other
cooperative data such as vehicle positions will be wasted, leading to
performance degradation of range-based CP methods. To fully utilize the
cooperative data and mitigate the impact of inter-vehicle ranging loss, a novel
cooperative positioning method aided by plane constraints is proposed in this
paper. The positioning results received from cooperative vehicles are used to
construct the road plane for each vehicle. The plane parameters are then
introduced into CP scheme to impose constraints on positioning solutions. The
state-of-art factor graph optimization (FGO) algo-rithm is employed to
integrate the plane constraints with raw data of Global Navigation Satellite
Systems (GNSS) as well as inter-vehicle ranging measurements. The proposed CP
method has the ability to resist the interruptions of inter-vehicle ranging
since the plane constraints are computed by just using position-related data. A
vehicle can still benefit from the position data of cooperative vehicles even
if the inter-vehicle ranging is unavaila-ble. The experimental results indicate
the superiority of the pro-posed CP method in positioning performance over the
existing methods, especially when the inter-ranging interruptions occur.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures, IEEE trans on ITS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DS-<span class="highlight-title">SLAM</span>: A 3D Object Detection based Semantic <span class="highlight-title">SLAM</span> towards Dynamic
  Indoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghanta Sai Krishna, Kundrapu Supriya, Sabur Baidya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existence of variable factors within the environment can cause a decline
in camera localization accuracy, as it violates the fundamental assumption of a
static environment in Simultaneous Localization and Mapping (SLAM) algorithms.
Recent semantic SLAM systems towards dynamic environments either rely solely on
2D semantic information, or solely on geometric information, or combine their
results in a loosely integrated manner. In this research paper, we introduce
3DS-SLAM, 3D Semantic SLAM, tailored for dynamic scenes with visual 3D object
detection. The 3DS-SLAM is a tightly-coupled algorithm resolving both semantic
and geometric constraints sequentially. We designed a 3D part-aware hybrid
transformer for point cloud-based object detection to identify dynamic objects.
Subsequently, we propose a dynamic feature filter based on HDBSCAN clustering
to extract objects with significant absolute depth differences. When compared
against ORB-SLAM2, 3DS-SLAM exhibits an average improvement of 98.01% across
the dynamic sequences of the TUM RGB-D dataset. Furthermore, it surpasses the
performance of the other four leading SLAM systems designed for dynamic
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Redundant and Loosely Coupled LiDAR-Wi-Fi Integration for Robust Global
  Localization in Autonomous Mobile Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Stathoulopoulos, Emanuele Pagliari, Luca Davoli, George Nikolakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a framework addressing the challenge of global
localization in autonomous mobile robotics by integrating LiDAR-based
descriptors and Wi-Fi fingerprinting in a pre-mapped environment. This is
motivated by the increasing demand for reliable localization in complex
scenarios, such as urban areas or underground mines, requiring robust systems
able to overcome limitations faced by traditional Global Navigation Satellite
System (GNSS)-based localization methods. By leveraging the complementary
strengths of LiDAR and Wi-Fi sensors used to generate predictions and evaluate
the confidence of each prediction as an indicator of potential degradation, we
propose a redundancy-based approach that enhances the system's overall
robustness and accuracy. The proposed framework allows independent operation of
the LiDAR and Wi-Fi sensors, ensuring system redundancy. By combining the
predictions while considering their confidence levels, we achieve enhanced and
consistent performance in localization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures. Accepted for publication in the 21st
  International Conference on Advanced Robotics (ICAR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dobby: A Conversational Service Robot Driven by GPT-4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carson Stark, Bohkyung Chun, Casey Charleston, Varsha Ravi, Luis Pabon, Surya Sunkari, Tarun Mohan, Peter Stone, Justin Hart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a robotics platform which embeds a conversational AI
agent in an embodied system for natural language understanding and intelligent
decision-making for service tasks; integrating task planning and human-like
conversation. The agent is derived from a large language model, which has
learned from a vast corpus of general knowledge. In addition to generating
dialogue, this agent can interface with the physical world by invoking commands
on the robot; seamlessly merging communication and behavior. This system is
demonstrated in a free-form tour-guide scenario, in an HRI study combining
robots with and without conversational AI capabilities. Performance is measured
along five dimensions: overall effectiveness, exploration abilities,
scrutinization abilities, receptiveness to personification, and adaptability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ l-dyno: framework to learn consistent visual features using robot's
  motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartikeya Singh, Charuvaran Adhivarahan, Karthik Dantu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historically, feature-based approaches have been used extensively for
camera-based robot perception tasks such as localization, mapping, tracking,
and others. Several of these approaches also combine other sensors (inertial
sensing, for example) to perform combined state estimation. Our work rethinks
this approach; we present a representation learning mechanism that identifies
visual features that best correspond to robot motion as estimated by an
external signal. Specifically, we utilize the robot's transformations through
an external signal (inertial sensing, for example) and give attention to image
space that is most consistent with the external signal. We use a pairwise
consistency metric as a representation to keep the visual features consistent
through a sequence with the robot's relative pose transformations. This
approach enables us to incorporate information from the robot's perspective
instead of solely relying on the image attributes. We evaluate our approach on
real-world datasets such as KITTI & EuRoC and compare the refined features with
existing feature descriptors. We also evaluate our method using our real robot
experiment. We notice an average of 49% reduction in the image search space
without compromising the trajectory estimation accuracy. Our method reduces the
execution time of visual odometry by 4.3% and also reduces reprojection errors.
We demonstrate the need to select only the most important features and show the
competitiveness using various feature detection baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Words into Action: Learning Diverse Humanoid Robot Behaviors using
  Language Guided Iterative Motion Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K. Niranjan Kumar, Irfan Essa, Sehoon Ha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid robots are well suited for human habitats due to their morphological
similarity, but developing controllers for them is a challenging task that
involves multiple sub-problems, such as control, planning and perception. In
this paper, we introduce a method to simplify controller design by enabling
users to train and fine-tune robot control policies using natural language
commands. We first learn a neural network policy that generates behaviors given
a natural language command, such as "walk forward", by combining Large Language
Models (LLMs), motion retargeting, and motion imitation. Based on the
synthesized motion, we iteratively fine-tune by updating the text prompt and
querying LLMs to find the best checkpoint associated with the closest motion in
history. We validate our approach using a simulated Digit humanoid robot and
demonstrate learning of diverse motions, such as walking, hopping, and kicking,
without the burden of complex reward engineering. In addition, we show that our
iterative refinement enables us to learn 3x times faster than a naive
formulation that learns from scratch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D2M2N: Decentralized Differentiable Memory-Enabled Mapping and
  Navigation for Multiple Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Ishat-E-Rabban, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a number of learning-based models have been proposed for
multi-robot navigation. However, these models lack memory and only rely on the
current observations of the robot to plan their actions. They are unable to
leverage past observations to plan better paths, especially in complex
environments. In this work, we propose a fully differentiable and decentralized
memory-enabled architecture for multi-robot navigation and mapping called
D2M2N. D2M2N maintains a compact representation of the environment to remember
past observations and uses Value Iteration Network for complex navigation. We
conduct extensive experiments to show that D2M2N significantly outperforms the
state-of-the-art model in complex mapping and navigation task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pre-Trained Masked Image Model for Mobile Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Dutt Sharma, Anukriti Singh, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D top-down maps are commonly used for the navigation and exploration of
mobile robots through unknown areas. Typically, the robot builds the navigation
maps incrementally from local observations using onboard sensors. Recent works
have shown that predicting the structural patterns in the environment through
learning-based approaches can greatly enhance task efficiency. While many such
works build task-specific networks using limited datasets, we show that the
existing foundational vision networks can accomplish the same without any
fine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street
images, to present novel applications for field-of-view expansion, single-agent
topological exploration, and multi-agent exploration for indoor mapping, across
different input modalities. Our work motivates the use of foundational vision
models for generalized structure prediction-driven applications, especially in
the dearth of training data. For more qualitative results see
https://raaslab.org/projects/MIM4Robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NEWTON: Are Large Language Models Capable of Physical Reasoning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ru Wang, Jiafei Duan, Dieter Fox, Siddhartha Srinivasa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), through their contextualized representations,
have been empirically proven to encapsulate syntactic, semantic, word sense,
and common-sense knowledge. However, there has been limited exploration of
their physical reasoning abilities, specifically concerning the crucial
attributes for comprehending everyday objects. To address this gap, we
introduce NEWTON, a repository and benchmark for evaluating the physics
reasoning skills of LLMs. Further, to enable domain-specific adaptation of this
benchmark, we present a pipeline to enable researchers to generate a variant of
this benchmark that has been customized to the objects and attributes relevant
for their application. The NEWTON repository comprises a collection of 2800
object-attribute pairs, providing the foundation for generating infinite-scale
assessment templates. The NEWTON benchmark consists of 160K QA questions,
curated using the NEWTON repository to investigate the physical reasoning
capabilities of several mainstream language models across foundational,
explicit, and implicit reasoning tasks. Through extensive empirical analysis,
our results highlight the capabilities of LLMs for physical reasoning. We find
that LLMs like GPT-4 demonstrate strong reasoning capabilities in
scenario-based tasks but exhibit less consistency in object-attribute reasoning
compared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates
its potential for evaluating and enhancing language models, paving the way for
their integration into physically grounded settings, such as robotic
manipulation. Project site: https://newtonreasoning.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings; 8 pages, 3 figures, 7 tables; Project page:
  https://newtonreasoning.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Neural Radiance Fields for Uncertainty-Aware Visual
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Chen, Weirong Chen, Rui Wang, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a promising fashion for visual localization, scene coordinate regression
(SCR) has seen tremendous progress in the past decade. Most recent methods
usually adopt neural networks to learn the mapping from image pixels to 3D
scene coordinates, which requires a vast amount of annotated training data. We
propose to leverage Neural Radiance Fields (NeRF) to generate training samples
for SCR. Despite NeRF's efficiency in rendering, many of the rendered data are
polluted by artifacts or only contain minimal information gain, which can
hinder the regression accuracy or bring unnecessary computational costs with
redundant data. These challenges are addressed in three folds in this paper:
(1) A NeRF is designed to separately predict uncertainties for the rendered
color and depth images, which reveal data reliability at the pixel level. (2)
SCR is formulated as deep evidential learning with epistemic uncertainty, which
is used to evaluate information gain and scene coordinate quality. (3) Based on
the three arts of uncertainties, a novel view selection policy is formed that
significantly improves data efficiency. Experiments on public datasets
demonstrate that our method could select the samples that bring the most
information gain and promote the performance with the highest efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Path Planning in Large Unknown Environments with Switchable
  System Models for Automated Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Schumann, Michael Buchholz, Klaus Dietmayer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large environments are challenging for path planning algorithms as the size
of the configuration space increases. Furthermore, if the environment is mainly
unexplored, large amounts of the path are planned through unknown areas. Hence,
a complete replanning of the entire path occurs whenever the path collides with
newly discovered obstacles. We propose a novel method that stops the path
planning algorithm after a certain distance. It is used to navigate the
algorithm in large environments and is not prone to problems of existing
navigation approaches. Furthermore, we developed a method to detect significant
environment changes to allow a more efficient replanning. At last, we extend
the path planner to be used in the U-Shift concept vehicle. It can switch to
another system model and rotate around the center of its rear axis. The results
show that the proposed methods generate nearly identical paths compared to the
standard Hybrid A* while drastically reducing the execution time. Furthermore,
we show that the extended path planning algorithm enables the efficient use of
the maneuvering capabilities of the concept vehicle to plan concise paths in
narrow environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Robot Cooperative Navigation in Crowds: A Game-Theoretic
  Learning-Based Model Predictive Control Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viet-Anh Le, Vaishnav Tadiparthi, Behdad Chalaki, Hossein Nourkhiz Mahjoub, Jovin D'sa, Ehsan Moradi-Pari, Andreas A. Malikopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a control framework for the coordination of
multiple robots as they navigate through crowded environments. Our framework
comprises of a local model predictive control (MPC) for each robot and a social
long short-term memory model that forecasts pedestrians' trajectories. We
formulate the local MPC formulation for each individual robot that includes
both individual and shared objectives, in which the latter encourages the
emergence of coordination among robots. Next, we consider the multi-robot
navigation and human-robot interaction, respectively, as a potential game and a
two-player game, then employ an iterative best response approach to solve the
resulting optimization problems in a centralized and distributed fashion.
Finally, we demonstrate the effectiveness of coordination among robots in
simulated crowd navigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eclares: Energy-Aware Clarity-Driven Ergodic Search <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaleb Ben Naveed, Devansh Agrawal, Christopher Vermillion, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning informative trajectories while considering the spatial distribution
of the information over the environment, as well as constraints such as the
robot's limited battery capacity, makes the long-time horizon persistent
coverage problem complex. Ergodic search methods consider the spatial
distribution of environmental information while optimizing robot trajectories;
however, current methods lack the ability to construct the target information
spatial distribution for environments that vary stochastically across space and
time. Moreover, current coverage methods dealing with battery capacity
constraints either assume simple robot and battery models, or are
computationally expensive. To address these problems, we propose a framework
called Eclares, in which our contribution is two-fold. 1) First, we propose a
method to construct the target information spatial distribution for ergodic
trajectory optimization using clarity, an information measure bounded between
[0,1]. The clarity dynamics allows us to capture information decay due to lack
of measurements and to quantify the maximum attainable information in
stochastic spatiotemporal environments. 2) Second, instead of directly tracking
the ergodic trajectory, we introduce the energy-aware (eware) filter, which
iteratively validates the ergodic trajectory to ensure that the robot has
enough energy to return to the charging station when needed. The proposed eware
filter is applicable to nonlinear robot models and is computationally
lightweight. We demonstrate the working of the framework through a simulation
case study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Conference of Robotics and Automation
  (ICRA) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAILing CAVs: Speed-Adaptive Infrastructure-Linked Connected and
  Automated Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Nice, Matthew Bunting, George Gunter, William Barbour, Jonathan Sprinkle, Dan Work
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work demonstrates a new capability in roadway control: Speed-adaptive,
infrastructure-linked connected and automated vehicles. We develop and deploy a
lightly modified vehicle that is able to dynamically adjust the vehicle speed
in response to posted variable speed limit messages generated by the
infrastructure using LTE connectivity. This work describes the open source
hardware and software platform that enables integration between
infrastructure-based variable posted speed limits, and existing vehicle
platforms for automated control. The vehicle is deployed in heavy morning
traffic on I-24 in Nashville, TN. The control vehicle follows the posted
variable speed limits, resulting in as much as a 25% reduction in speed
variability compared to a human-piloted vehicle in the same traffic stream.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning in a Safety-Embedded MDP with Trajectory
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang, Wenxuan Zhou, Zuxin Liu, Ding Zhao, David Held
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe Reinforcement Learning (RL) plays an important role in applying RL
algorithms to safety-critical real-world applications, addressing the trade-off
between maximizing rewards and adhering to safety constraints. This work
introduces a novel approach that combines RL with trajectory optimization to
manage this trade-off effectively. Our approach embeds safety constraints
within the action space of a modified Markov Decision Process (MDP). The RL
agent produces a sequence of actions that are transformed into safe
trajectories by a trajectory optimizer, thereby effectively ensuring safety and
increasing training stability. This novel approach excels in its performance on
challenging Safety Gym tasks, achieving significantly higher rewards and
near-zero safety violations during inference. The method's real-world
applicability is demonstrated through a safe and effective deployment in a real
robot task of box-pushing around obstacles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Robot Patrol Algorithm with Distributed Coordination and
  Consciousness of the Base Station's Situation Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuho Kobayashi, Seiya Ueno, Takehiro Higuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-robot patrolling is the potential application for robotic systems to
survey wide areas efficiently without human burdens and mistakes. However, such
systems have few examples of real-world applications due to their lack of human
predictability. This paper proposes an algorithm: Local Reactive (LR) for
multi-robot patrolling to satisfy both needs: (i)patrol efficiently and
(ii)provide humans with better situation awareness to enhance system
predictability. Each robot operating according to the proposed algorithm
selects its patrol target from the local areas around the robot's current
location by two requirements: (i)patrol location with greater need, (ii)report
its achievements to the base station. The algorithm is distributed and
coordinates the robots without centralized control by sharing their patrol
achievements and degree of need to report to the base station. The proposed
algorithm performed better than existing algorithms in both patrolling and the
base station's situation awareness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust MADER: Decentralized Multiagent Trajectory Planner Robust to
  Communication Delay in <span class="highlight-title">Dynamic Environment</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06222v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06222v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kota Kondo, Reinaldo Figueroa, Juan Rached, Jesus Tordesillas, Parker C. Lusk, Jonathan P. How
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communication delays can be catastrophic for multiagent systems. However,
most existing state-of-the-art multiagent trajectory planners assume perfect
communication and therefore lack a strategy to rectify this issue in real-world
environments. To address this challenge, we propose Robust MADER (RMADER), a
decentralized, asynchronous multiagent trajectory planner robust to
communication delay. RMADER ensures safety by introducing (1) a Delay Check
step, (2) a two-step trajectory publication scheme, and (3) a novel
trajectory-storing-and-checking approach. Our primary contributions include:
proving recursive feasibility for collision-free trajectory generation in
asynchronous decentralized trajectory-sharing, simulation benchmark studies,
and hardware experiments with different network topologies and dynamic
obstacles. We show that RMADER outperforms existing approaches by achieving a
100% success rate of collision-free trajectory generation, whereas the next
best asynchronous decentralized method only achieves 83% success.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pagers, 10 figures,. arXiv admin note: substantial text overlap
  with arXiv:2209.13667</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Reinforcement Learning for Jumping Monopods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07038v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07038v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Bussola, Michele Focchi, Andrea Del Prete, Daniele Fontanelli, Luigi Palopoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider the complex control problem of making a monopod
reach a target with a jump. The monopod can jump in any direction and the
terrain underneath its foot can be uneven. This is a template of a much larger
class of problems, which are extremely challenging and computationally
expensive to solve using standard optimisation-based techniques. Reinforcement
Learning (RL) could be an interesting alternative, but the application of an
end-to-end approach in which the controller must learn everything from scratch,
is impractical. The solution advocated in this paper is to guide the learning
process within an RL framework by injecting physical knowledge. This expedient
brings to widespread benefits, such as a drastic reduction of the learning
time, and the ability to learn and compensate for possible errors in the
low-level controller executing the motion. We demonstrate the advantage of our
approach with respect to both optimization-based and end-to-end RL approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Hypernetworks are Surprisingly Strong in Meta-RL <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14970v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14970v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Beck, Risto Vuorio, Zheng Xiong, Shimon Whiteson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (RL) is notoriously impractical to deploy due to
sample inefficiency. Meta-RL directly addresses this sample inefficiency by
learning to perform few-shot learning when a distribution of related tasks is
available for meta-training. While many specialized meta-RL methods have been
proposed, recent work suggests that end-to-end learning in conjunction with an
off-the-shelf sequential model, such as a recurrent network, is a surprisingly
strong baseline. However, such claims have been controversial due to limited
supporting evidence, particularly in the face of prior work establishing
precisely the opposite. In this paper, we conduct an empirical investigation.
While we likewise find that a recurrent network can achieve strong performance,
we demonstrate that the use of hypernetworks is crucial to maximizing their
potential. Surprisingly, when combined with hypernetworks, the recurrent
baselines that are far simpler than existing specialized methods actually
achieve the strongest performance of all methods evaluated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with
  Bird's Eye View based Appearance and Motion Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07336v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07336v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Zhou, Jiapeng Xie, Yan Pan, Jiajie Wu, Chuanzhao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying moving objects is an essential capability for autonomous systems,
as it provides critical information for pose estimation, navigation, collision
avoidance, and static map construction. In this paper, we present MotionBEV, a
fast and accurate framework for LiDAR moving object segmentation, which
segments moving objects with appearance and motion features in the bird's eye
view (BEV) domain. Our approach converts 3D LiDAR scans into a 2D polar BEV
representation to improve computational efficiency. Specifically, we learn
appearance features with a simplified PointNet and compute motion features
through the height differences of consecutive frames of point clouds projected
onto vertical columns in the polar BEV coordinate system. We employ a
dual-branch network bridged by the Appearance-Motion Co-attention Module (AMCM)
to adaptively fuse the spatio-temporal information from appearance and motion
features. Our approach achieves state-of-the-art performance on the
SemanticKITTI-MOS benchmark. Furthermore, to demonstrate the practical
effectiveness of our method, we provide a LiDAR-MOS dataset recorded by a
solid-state LiDAR, which features non-repetitive scanning patterns and a small
field of view.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating <span class="highlight-title">Visual Odometry</span> Methods for Autonomous Driving in Rain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Xiang Tan, Marcel Bartholomeus Prasetyo, Mohammad Alif Daffa, Deshpande Sunny Nitin, Malika Meghjani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing demand for autonomous vehicles has created a need for robust
navigation systems that can also operate effectively in adverse weather
conditions. Visual odometry is a technique used in these navigation systems,
enabling the estimation of vehicle position and motion using input from onboard
cameras. However, visual odometry accuracy can be significantly impacted in
challenging weather conditions, such as heavy rain, snow, or fog. In this
paper, we evaluate a range of visual odometry methods, including our DROID-SLAM
based heuristic approach. Specifically, these algorithms are tested on both
clear and rainy weather urban driving data to evaluate their robustness. We
compiled a dataset comprising of a range of rainy weather conditions from
different cities. This includes, the Oxford Robotcar dataset from Oxford, the
4Seasons dataset from Munich and an internal dataset collected in Singapore. We
evaluated different visual odometry algorithms for both monocular and stereo
camera setups using the Absolute Trajectory Error (ATE). From the range of
approaches evaluated, our findings suggest that the Depth and Flow for Visual
Odometry (DF-VO) algorithm with monocular setup performed the best for short
range distances (< 500m) and our proposed DROID-SLAM based heuristic approach
for the stereo setup performed relatively well for long-term localization. Both
VO algorithms suggested a need for a more robust sensor fusion based approach
for localization in rain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, Accepted at IEEE International Conference on
  Automation Science and Engineering (CASE) 2023. Fixed grammar and phrasing to
  improve clarity of the statements made. Emphasized on the need for a more
  robust sensor fusion based approach for localization in rain for autonomous
  driving</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Reinforcement Learning: towards Embodied Generalist Agents
  with Foundation Prior Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weirui Ye, Yunsheng Zhang, Mengchen Wang, Shengjie Wang, Xianfan Gu, Pieter Abbeel, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, people have shown that large-scale pre-training from internet-scale
data is the key to building generalist models, as witnessed in NLP. To build
embodied generalist agents, we and many other researchers hypothesize that such
foundation prior is also an indispensable component. However, it is unclear
what is the proper concrete form to represent those embodied foundation priors
and how they should be used in the downstream task. In this paper, we propose
an intuitive and effective set of embodied priors that consist of foundation
policy, value, and success reward. The proposed priors are based on the
goal-conditioned MDP. To verify their effectiveness, we instantiate an
actor-critic method assisted by the priors, called Foundation Actor-Critic
(FAC). We name our framework as Foundation Reinforcement Learning (FRL), since
it completely relies on embodied foundation priors to explore, learn and
reinforce. The benefits of FRL are threefold. (1) Sample efficient. With
foundation priors, FAC learns significantly faster than traditional RL. Our
evaluation on the Meta-World has proved that FAC can achieve 100% success rates
for 7/8 tasks under less than 200k frames, which outperforms the baseline
method with careful manual-designed rewards under 1M frames. (2) Robust to
noisy priors. Our method tolerates the unavoidable noise in embodied foundation
models. We show that FAC works well even under heavy noise or quantization
errors. (3) Minimal human intervention: FAC completely learns from the
foundation priors, without the need of human-specified dense reward, or
providing teleoperated demos. Thus, FAC can be easily scaled up. We believe our
FRL framework could enable the future robot to autonomously explore and learn
without human intervention in the physical world. In summary, our proposed FRL
is a novel and powerful learning paradigm, towards achieving embodied
generalist agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-Grounded Control for Coordinated Robot Motion and Speech <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05456v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05456v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ravi Tejwani, Chengyuan Ma, Paco Gomez-Paz, Paolo Bonato, H. Harry Asada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have enabled human-robot collaboration through physical
assistance and verbal guidance. However, limitations persist in coordinating
robots' physical motions and speech in response to real-time changes in human
behavior during collaborative contact tasks. We first derive principles from
analyzing physical therapists' movements and speech during patient exercises.
These principles are translated into control objectives to: 1) guide users
through trajectories, 2) control motion and speech pace to align completion
times with varying user cooperation, and 3) dynamically paraphrase speech along
the trajectory. We then propose a Language Controller that synchronizes motion
and speech, modulating both based on user cooperation. Experiments with 12
users show the Language Controller successfully aligns motion and speech
compared to baselines. This provides a framework for fluent human-robot
collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review in ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal Decision Theory: Safe Autonomous Decisions from Imperfect
  Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jordan Lekeufack, Anastasios N. Angelopoulos, Andrea Bajcsy, Michael I. Jordan, Jitendra Malik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Conformal Decision Theory, a framework for producing safe
autonomous decisions despite imperfect machine learning predictions. Examples
of such decisions are ubiquitous, from robot planning algorithms that rely on
pedestrian predictions, to calibrating autonomous manufacturing to exhibit high
throughput and low error, to the choice of trusting a nominal policy versus
switching to a safe backup policy at run-time. The decisions produced by our
algorithms are safe in the sense that they come with provable statistical
guarantees of having low risk without any assumptions on the world model
whatsoever; the observations need not be I.I.D. and can even be adversarial.
The theory extends results from conformal prediction to calibrate decisions
directly, without requiring the construction of prediction sets. Experiments
demonstrate the utility of our approach in robot motion planning around humans,
automated stock trading, and robot manufacturing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via
  Differentiable Physics Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Xiang, Feitong Chen, Qinsi Wang, Yang Gang, Xiang Zhang, Xinghao Zhu, Xingyu Liu, Lin Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability to transfer mastered skills to accomplish a range of similar
yet novel tasks is crucial for intelligent robots. In this work, we introduce
$\textit{Diff-Transfer}$, a novel framework leveraging differentiable physics
simulation to efficiently transfer robotic skills. Specifically,
$\textit{Diff-Transfer}$ discovers a feasible path within the task space that
brings the source task to the target task. At each pair of adjacent points
along this task path, which is two sub-tasks, $\textit{Diff-Transfer}$ adapts
known actions from one sub-task to tackle the other sub-task successfully. The
adaptation is guided by the gradient information from differentiable physics
simulations. We propose a novel path-planning method to generate sub-tasks,
leveraging $Q$-learning with a task-level state and reward. We implement our
framework in simulation experiments and execute four challenging transfer tasks
on robotic manipulation, demonstrating the efficacy of $\textit{Diff-Transfer}$
through comprehensive experiments. Supplementary and Videos are on the website
https://sites.google.com/view/difftransfer
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Efficient MPPI Trajectory Generation with Unscented Guidance:
  U-MPPI Control Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ihab S. Mohamed, Junhong Xu, Gaurav S Sukhatme, Lantao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classical Model Predictive Path Integral (MPPI) control framework lacks
reliable safety guarantees since it relies on a risk-neutral trajectory
evaluation technique, which can present challenges for safety-critical
applications such as autonomous driving. Additionally, if the majority of MPPI
sampled trajectories concentrate in high-cost regions, it may generate an
infeasible control sequence. To address this challenge, we propose the U-MPPI
control strategy, a novel methodology that can effectively manage system
uncertainties while integrating a more efficient trajectory sampling strategy.
The core concept is to leverage the Unscented Transform (UT) to propagate not
only the mean but also the covariance of the system dynamics, going beyond the
traditional MPPI method. As a result, it introduces a novel and more efficient
trajectory sampling strategy, significantly enhancing state-space exploration
and ultimately reducing the risk of being trapped in local minima. Furthermore,
by leveraging the uncertainty information provided by UT, we incorporate a
risk-sensitive cost function that explicitly accounts for risk or uncertainty
throughout the trajectory evaluation process, resulting in a more resilient
control system capable of handling uncertain conditions. By conducting
extensive simulations of 2D aggressive autonomous navigation in both known and
unknown cluttered environments, we verify the efficiency and robustness of our
proposed U-MPPI control strategy compared to the baseline MPPI. We further
validate the practicality of U-MPPI through real-world demonstrations in
unknown cluttered environments, showcasing its superior ability to incorporate
both the UT and local costmap into the optimization problem without introducing
additional complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has 15 pages, 10 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMMap: Memory-Efficient Continuous Occupancy Map Using Gaussian Mixture
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Zhi Xuan Li, Sertac Karaman, Vivienne Sze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy consumption of memory accesses dominates the compute energy in
energy-constrained robots which require a compact 3D map of the environment to
achieve autonomy. Recent mapping frameworks only focused on reducing the map
size while incurring significant memory usage during map construction due to
multi-pass processing of each depth image. In this work, we present a
memory-efficient continuous occupancy map, named GMMap, that accurately models
the 3D environment using a Gaussian Mixture Model (GMM). Memory-efficient GMMap
construction is enabled by the single-pass compression of depth images into
local GMMs which are directly fused together into a globally-consistent map. By
extending Gaussian Mixture Regression to model unexplored regions, occupancy
probability is directly computed from Gaussians. Using a low-power ARM Cortex
A57 CPU, GMMap can be constructed in real-time at up to 60 images per second.
Compared with prior works, GMMap maintains high accuracy while reducing the map
size by at least 56%, memory overhead by at least 88%, DRAM access by at least
78%, and energy consumption by at least 69%. Thus, GMMap enables real-time 3D
mapping on energy-constrained robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Biconnectivity Restoration in Multi-Robot Systems for Robust
  Communication Maintenance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.00685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.00685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Ishat-E-Rabban, Guangyao Shi, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maintaining a robust communication network plays an important role in the
success of a multi-robot team jointly performing an optimization task. A key
characteristic of a robust multi-robot system is the ability to repair the
communication topology itself in the case of robot failure. In this paper, we
focus on the Fast Biconnectivity Restoration (FBR) problem, which aims to
repair a connected network to make it biconnected as fast as possible, where a
biconnected network is a communication topology that cannot be disconnected by
removing one node. We develop a Quadratically Constrained Program (QCP)
formulation of the FBR problem, which provides a way to optimally solve the
problem. We also propose an approximation algorithm for the FBR problem based
on graph theory. By conducting empirical studies, we demonstrate that our
proposed approximation algorithm performs close to the optimal while
significantly outperforming the existing solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parametric roll oscillations of a hydrodynamic Chaplygin sleigh 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartik Loya, Phanindra Tallapragada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomimetic underwater robots use lateral periodic oscillatory motion to
propel forward, which is seen in most fishes known as body caudal fin (BCF)
propulsion. The lateral oscillatory motion makes slender-bodied fish-like
robots roll unstable. Unlike the case of human-engineered aquatic robots, many
species of fish can stabilize their roll motion to perturbations arising from
the periodic motions of propulsors. To first understand the origin of the roll
instability, the objective of this paper is to analyze the parameters affecting
the roll-angle stability of an autonomous fish-like underwater swimmer.
Eschewing complex models of fluid-structure interaction, we instead consider
the roll motion of a nonholonomic system inspired by the Chaplygin sleigh,
whose center of mass is above the ground. In past work, the dynamics of a
fish-like periodic swimmer have been shown to be similar to that of a Chaplygin
sleigh. The Chaplygin sleigh is propelled by periodic torque in the yaw
direction. The roll dynamics of the Chaplygin sleigh are linearized and around
a nominal limit cycle solution of the planar hydrodynamic Chaplygin sleigh in
the reduced velocity space. It is shown that the roll dynamics are then
described as a nonhomogeneous Mathieu equation where the periodic yaw motion
provides the parametric excitation. We study the added mass effects on the
sleigh's linear dynamics and use the Floquet theory to investigate the roll
stability due to parametric excitation. We show that fast motions of the model
for swimming are frequently associated with roll instability. The paper thus
sheds light on the fundamental mechanics that present trade-offs between speed,
efficiency, and stability of motion of fish-like robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 9 figures, submitted to Nonlinear Dynamics journal by
  Springer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Benders Decomposition with Continual Learning for Hybrid
  Model Predictive Control in <span class="highlight-title">Dynamic Environment</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid model predictive control (MPC) with both continuous and discrete
variables is widely applicable to robotic control tasks, especially those
involving contact with the environment. Due to the combinatorial complexity,
the solving speed of hybrid MPC can be insufficient for real-time applications.
In this paper, we proposed a hybrid MPC solver based on Generalized Benders
Decomposition (GBD) with continual learning. The algorithm accumulates cutting
planes from the invariant dual space of the subproblems. After a short
cold-start phase, the accumulated cuts provide warm-starts for the new problem
instances to increase the solving speed. Despite the randomly changing
environment that the control is unprepared for, the solving speed maintains. We
verified our solver on controlling a cart-pole system with randomly moving soft
contact walls and show that the solving speed is 2-3 times faster than the
off-the-shelf solver Gurobi.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A correction of the author name in the metadata</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AirIMU: Learning Uncertainty Propagation for Inertial Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Qiu, Chen Wang, Xunfei Zhou, Youjie Xia, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty estimation for inertial odometry is the foundation to
achieve optimal fusion in multi-sensor systems, such as visual or LiDAR
inertial odometry. Prior studies often simplify the assumptions regarding the
uncertainty of inertial measurements, presuming fixed covariance parameters and
empirical IMU sensor models. However, the inherent physical limitations and
non-linear characteristics of sensors are difficult to capture. Moreover,
uncertainty may fluctuate based on sensor rates and motion modalities, leading
to variations across different IMUs. To address these challenges, we formulate
a learning-based method that not only encapsulate the non-linearities inherent
to IMUs but also ensure the accurate propagation of covariance in a data-driven
manner. We extend the PyPose library to enable differentiable batched IMU
integration with covariance propagation on manifolds, leading to significant
runtime speedup. To demonstrate our method's adaptability, we evaluate it on
several benchmarks as well as a large-scale helicopter dataset spanning over
262 kilometers. The drift rate of the inertial odometry on these datasets is
reduced by a factor of between 2.2 and 4 times. Our method lays the groundwork
for advanced developments in inertial odometry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Game-theoretic Objective Space Planning <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongrui Zheng, Zhijun Zhuang, Johannes Betz, Rahul Mangharam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating competitive strategies and performing continuous motion planning
simultaneously in an adversarial setting is a challenging problem. In addition,
understanding the intent of other agents is crucial to deploying autonomous
systems in adversarial multi-agent environments. Existing approaches either
discretize agent action by grouping similar control inputs, sacrificing
performance in motion planning, or plan in uninterpretable latent spaces,
producing hard-to-understand agent behaviors. Furthermore, the most popular
policy optimization frameworks do not recognize the long-term effect of actions
and become myopic. This paper proposes an agent action discretization method
via abstraction that provides clear intentions of agent actions, an efficient
offline pipeline of agent population synthesis, and a planning strategy using
counterfactual regret minimization with function approximation. Finally, we
experimentally validate our findings on scaled autonomous vehicles in a
head-to-head racing setting. We demonstrate that using the proposed framework
significantly improves learning, improves the win rate against different
opponents, and the improvements can be transferred to unseen opponents in an
unseen environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to 2024 International Conference on Autonomous Agents and
  Multi-Agent Systems (AAMAS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining the simulation-to-reality gap of a wheel loader digging in
  deformable terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koji Aoshima, Martin Servin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate how well a physics-based simulator can replicate a real wheel
loader performing bucket filling in a pile of soil. The comparison is made
using field test time series of the vehicle motion and actuation forces, loaded
mass, and total work. The vehicle was modeled as a rigid multibody system with
frictional contacts, driveline, and linear actuators. For the soil, we tested
discrete element models of different resolutions, with and without multiscale
acceleration. The spatio-temporal resolution ranged between 50-400 mm and 2-500
ms, and the computational speed was between 1/10,000 to 5 times faster than
real-time. The simulation-to-reality gap was found to be around 10% and
exhibited a weak dependence on the level of fidelity, e.g., compatible with
real-time simulation. Furthermore, the sensitivity of an optimized force
feedback controller under transfer between different simulation domains was
investigated. The domain bias was observed to cause a performance reduction of
5% despite the domain gap being about 15%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptual Factors for Environmental Modeling in Robotic Active
  Perception <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Morilla-Cabello, Jonas Westheider, Marija Popovic, Eduardo Montijano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately assessing the potential value of new sensor observations is a
critical aspect of planning for active perception. This task is particularly
challenging when reasoning about high-level scene understanding using
measurements from vision-based neural networks. Due to appearance-based
reasoning, the measurements are susceptible to several environmental effects
such as the presence of occluders, variations in lighting conditions, and
redundancy of information due to similarity in appearance between nearby
viewpoints. To address this, we propose a new active perception framework
incorporating an arbitrary number of perceptual effects in planning and fusion.
Our method models the correlation with the environment by a set of general
functions termed perceptual factors to construct a perceptual map, which
quantifies the aggregated influence of the environment on candidate viewpoints.
This information is seamlessly incorporated into the planning and fusion
processes by adjusting the uncertainty associated with measurements to weigh
their contributions. We evaluate our perceptual maps in a simulated environment
that reproduces environmental conditions common in robotics applications. Our
results show that, by accounting for environmental effects within our
perceptual maps, we improve in the state estimation by correctly selecting the
viewpoints and considering the measurement noise correctly when affected by
environmental factors. We furthermore deploy our approach on a ground robot to
showcase its applicability for real-world active perception missions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, under review for IEEE ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">156</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoAD II: The Sequel -- Who, When, and What in Movie Audio Description <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio Description (AD) is the task of generating descriptions of visual
content, at suitable time intervals, for the benefit of visually impaired
audiences. For movies, this presents notable challenges -- AD must occur only
during existing pauses in dialogue, should refer to characters by name, and
ought to aid understanding of the storyline as a whole. To this end, we develop
a new model for automatically generating movie AD, given CLIP visual features
of the frames, the cast list, and the temporal locations of the speech;
addressing all three of the 'who', 'when', and 'what' questions: (i) who -- we
introduce a character bank consisting of the character's name, the actor that
played the part, and a CLIP feature of their face, for the principal cast of
each movie, and demonstrate how this can be used to improve naming in the
generated AD; (ii) when -- we investigate several models for determining
whether an AD should be generated for a time interval or not, based on the
visual content of the interval and its neighbours; and (iii) what -- we
implement a new vision-language model for this task, that can ingest the
proposals from the character bank, whilst conditioning on the visual features
using cross-attention, and demonstrate how this improves over previous
architectures for AD text generation in an apples-to-apples comparison.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV2023. Project page:
  https://www.robots.ox.ac.uk/vgg/research/autoad/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Does Stable Diffusion Know about the 3D Scene? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanqi Zhan, Chuanxia Zheng, Weidi Xie, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in generative models like Stable Diffusion enable the
generation of highly photo-realistic images. Our objective in this paper is to
probe the diffusion network to determine to what extent it 'understands'
different properties of the 3D scene depicted in an image. To this end, we make
the following contributions: (i) We introduce a protocol to evaluate whether a
network models a number of physical 'properties' of the 3D scene by probing for
explicit features that represent these properties. The probes are applied on
datasets of real images with annotations for the property. (ii) We apply this
protocol to properties covering scene geometry, scene material, support
relations, lighting, and view dependent measures. (iii) We find that Stable
Diffusion is good at a number of properties including scene geometry, support
relations, shadows and depth, but less performant for occlusion. (iv) We also
apply the probes to other models trained at large-scale, including DINO and
CLIP, and find their performance inferior to that of Stable Diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NECO: NEural Collapse Based Out-of-distribution detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mouïn Ben Ammar, Nacim Belkhir, Sebastian Popescu, Antoine Manzanera, Gianni Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting out-of-distribution (OOD) data is a critical challenge in machine
learning due to model overconfidence, often without awareness of their
epistemological limits. We hypothesize that ``neural collapse'', a phenomenon
affecting in-distribution data for models trained beyond loss convergence, also
influences OOD data. To benefit from this interplay, we introduce NECO, a novel
post-hoc method for OOD detection, which leverages the geometric properties of
``neural collapse'' and of principal component spaces to identify OOD data. Our
extensive experiments demonstrate that NECO achieves state-of-the-art results
on both small and large-scale OOD detection tasks while exhibiting strong
generalization capabilities across different network architectures.
Furthermore, we provide a theoretical explanation for the effectiveness of our
method in OOD detection. We plan to release the code after the anonymity
period.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Bounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxin Liu, Michael Fischer, Paul D. Yoo, Tobias Ritschel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bounding volumes are an established concept in computer graphics and vision
tasks but have seen little change since their early inception. In this work, we
study the use of neural networks as bounding volumes. Our key observation is
that bounding, which so far has primarily been considered a problem of
computational geometry, can be redefined as a problem of learning to classify
space into free and empty. This learning-based approach is particularly
advantageous in high-dimensional spaces, such as animated scenes with complex
queries, where neural networks are known to excel. However, unlocking neural
bounding requires a twist: allowing -- but also limiting -- false positives,
while ensuring that the number of false negatives is strictly zero. We enable
such tight and conservative results using a dynamically-weighted asymmetric
loss function. Our results show that our neural bounding produces up to an
order of magnitude fewer false positives than traditional methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni3D: Exploring Unified 3D Representation at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up representations for images or text has been extensively
investigated in the past few years and has led to revolutions in learning
vision and language. However, scalable representation for 3D objects and scenes
is relatively unexplored. In this work, we present Uni3D, a 3D foundation model
to explore the unified 3D representation at scale. Uni3D uses a 2D initialized
ViT end-to-end pretrained to align the 3D point cloud features with the
image-text aligned features. Via the simple architecture and pretext task,
Uni3D can leverage abundant 2D pretrained models as initialization and
image-text aligned models as the target, unlocking the great potential of 2D
models and scaling-up strategies to the 3D world. We efficiently scale up Uni3D
to one billion parameters, and set new records on a broad range of 3D tasks,
such as zero-shot classification, few-shot classification, open-world
understanding and part segmentation. We show that the strong Uni3D
representation also enables applications such as 3D painting and retrieval in
the wild. We believe that Uni3D provides a new direction for exploring both
scaling up and efficiency of the representation in 3D domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and Demo: https://github.com/baaivision/Uni3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TopoMLP: An Simple yet Strong Pipeline for Driving Topology Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongming Wu, Jiahao Chang, Fan Jia, Yingfei Liu, Tiancai Wang, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topology reasoning aims to comprehensively understand road scenes and present
drivable routes in autonomous driving. It requires detecting road centerlines
(lane) and traffic elements, further reasoning their topology relationship,
i.e., lane-lane topology, and lane-traffic topology. In this work, we first
present that the topology score relies heavily on detection performance on lane
and traffic elements. Therefore, we introduce a powerful 3D lane detector and
an improved 2D traffic element detector to extend the upper limit of topology
performance. Further, we propose TopoMLP, a simple yet high-performance
pipeline for driving topology reasoning. Based on the impressive detection
performance, we develop two simple MLP-based heads for topology generation.
TopoMLP achieves state-of-the-art performance on OpenLane-V2 benchmark, i.e.,
41.2% OLS with ResNet-50 backbone. It is also the 1st solution for 1st OpenLane
Topology in Autonomous Driving Challenge. We hope such simple and strong
pipeline can provide some new insights to the community. Code is at
https://github.com/wudongming97/TopoMLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 1st solution for 1st OpenLane Topology in Autonomous Driving
  Challenge. Code is at https://github.com/wudongming97/TopoMLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiFi-123: Towards High-fidelity One Image to 3D Content Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangbo Yu, Li Yuan, Yan-Pei Cao, Xiangjun Gao, Xiaoyu Li, Long Quan, Ying Shan, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-image diffusion models have enabled 3D generation
from a single image. However, current image-to-3D methods often produce
suboptimal results for novel views, with blurred textures and deviations from
the reference image, limiting their practical applications. In this paper, we
introduce HiFi-123, a method designed for high-fidelity and multi-view
consistent 3D generation. Our contributions are twofold: First, we propose a
reference-guided novel view enhancement technique that substantially reduces
the quality gap between synthesized and reference views. Second, capitalizing
on the novel view enhancement, we present a novel reference-guided state
distillation loss. When incorporated into the optimization-based image-to-3D
pipeline, our method significantly improves 3D generation quality, achieving
state-of-the-art performance. Comprehensive evaluations demonstrate the
effectiveness of our approach over existing methods, both qualitatively and
quantitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-domain improves out-of-distribution and data-limited scenarios for
  medical image analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ece Ozkan, Xavier Boix
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current machine learning methods for medical image analysis primarily focus
on developing models tailored for their specific tasks, utilizing data within
their target domain. These specialized models tend to be data-hungry and often
exhibit limitations in generalizing to out-of-distribution samples. Recently,
foundation models have been proposed, which combine data from various domains
and demonstrate excellent generalization capabilities. Building upon this, this
work introduces the incorporation of diverse medical image domains, including
different imaging modalities like X-ray, MRI, CT, and ultrasound images, as
well as various viewpoints such as axial, coronal, and sagittal views. We refer
to this approach as multi-domain model and compare its performance to that of
specialized models. Our findings underscore the superior generalization
capabilities of multi-domain models, particularly in scenarios characterized by
limited data availability and out-of-distribution, frequently encountered in
healthcare applications. The integration of diverse data allows multi-domain
models to utilize shared information across domains, enhancing the overall
outcomes significantly. To illustrate, for organ recognition, multi-domain
model can enhance accuracy by up to 10% compared to conventional specialized
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Generalization by Rejecting Extreme Augmentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masih Aminbeidokhti, Fidel A. Guerrero Peña, Heitor Rapela Medeiros, Thomas Dubail, Eric Granger, Marco Pedersoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is one of the most effective techniques for regularizing
deep learning models and improving their recognition performance in a variety
of tasks and domains. However, this holds for standard in-domain settings, in
which the training and test data follow the same distribution. For the
out-of-domain case, where the test data follow a different and unknown
distribution, the best recipe for data augmentation is unclear. In this paper,
we show that for out-of-domain and domain generalization settings, data
augmentation can provide a conspicuous and robust improvement in performance.
To do that, we propose a simple training procedure: (i) use uniform sampling on
standard data augmentation transformations; (ii) increase the strength
transformations to account for the higher data variance expected when working
out-of-domain, and (iii) devise a new reward function to reject extreme
transformations that can harm the training. With this procedure, our data
augmentation scheme achieves a level of accuracy that is comparable to or
better than state-of-the-art methods on benchmark domain generalization
datasets. Code: \url{https://github.com/Masseeh/DCAug}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Diffusion Counterfactual Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Farid, Simon Schrodi, Max Argus, Thomas Brox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations have emerged as a promising method for
elucidating the behavior of opaque black-box models. Recently, several works
leveraged pixel-space diffusion models for counterfactual generation. To handle
noisy, adversarial gradients during counterfactual generation -- causing
unrealistic artifacts or mere adversarial perturbations -- they required either
auxiliary adversarially robust models or computationally intensive guidance
schemes. However, such requirements limit their applicability, e.g., in
scenarios with restricted access to the model's training data. To address these
limitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE).
LDCE harnesses the capabilities of recent class- or text-conditional foundation
latent diffusion models to expedite counterfactual generation and focus on the
important, semantic parts of the data. Furthermore, we propose a novel
consensus guidance mechanism to filter out noisy, adversarial gradients that
are misaligned with the diffusion model's implicit classifier. We demonstrate
the versatility of LDCE across a wide spectrum of models trained on diverse
datasets with different learning paradigms. Finally, we showcase how LDCE can
provide insights into model errors, enhancing our understanding of black-box
model behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SC2GAN: Rethinking Entanglement by Self-correcting Correlated GAN Space <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikun Chen, Han Zhao, Parham Aarabi, Ruowei Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) can synthesize realistic images, with
the learned latent space shown to encode rich semantic information with various
interpretable directions. However, due to the unstructured nature of the
learned latent space, it inherits the bias from the training data where
specific groups of visual attributes that are not causally related tend to
appear together, a phenomenon also known as spurious correlations, e.g., age
and eyeglasses or women and lipsticks. Consequently, the learned distribution
often lacks the proper modelling of the missing examples. The interpolation
following editing directions for one attribute could result in entangled
changes with other attributes. To address this problem, previous works
typically adjust the learned directions to minimize the changes in other
attributes, yet they still fail on strongly correlated features. In this work,
we study the entanglement issue in both the training data and the learned
latent space for the StyleGAN2-FFHQ model. We propose a novel framework
SC$^2$GAN that achieves disentanglement by re-projecting low-density latent
code samples in the original latent space and correcting the editing directions
based on both the high-density and low-density regions. By leveraging the
original meaningful directions and semantic region-specific layers, our
framework interpolates the original latent codes to generate images with
attribute combination that appears infrequently, then inverts these samples
back to the original latent space. We apply our framework to pre-existing
methods that learn meaningful latent directions and showcase its strong
capability to disentangle the attributes with small amounts of low-density
region samples added.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Out Of Distribution Generalization in Computer Vision
  workshop at ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Explanation Methods for Vision-and-Language Navigation <span class="chip">ECAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanqi Chen, Lei Yang, Guanhua Chen, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to navigate robots with natural language instructions in an
unknown environment is a crucial step for achieving embodied artificial
intelligence (AI). With the improving performance of deep neural models
proposed in the field of vision-and-language navigation (VLN), it is equally
interesting to know what information the models utilize for their
decision-making in the navigation tasks. To understand the inner workings of
deep neural models, various explanation methods have been developed for
promoting explainable AI (XAI). But they are mostly applied to deep neural
models for image or text classification tasks and little work has been done in
explaining deep neural models for VLN tasks. In this paper, we address these
problems by building quantitative benchmarks to evaluate explanation methods
for VLN models in terms of faithfulness. We propose a new erasure-based
evaluation pipeline to measure the step-wise textual explanation in the
sequential decision-making setting. We evaluate several explanation methods for
two representative VLN models on two popular VLN datasets and reveal valuable
findings through our experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How (not) to ensemble LVLMs for VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Alazraki, Lluis Castrejon, Mostafa Dehghani, Fantine Huot, Jasper Uijlings, Thomas Mensink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies ensembling in the era of Large Vision-Language Models
(LVLMs). Ensembling is a classical method to combine different models to get
increased performance. In the recent work on Encyclopedic-VQA the authors
examine a wide variety of models to solve their task: from vanilla LVLMs, to
models including the caption as extra context, to models augmented with
Lens-based retrieval of Wikipedia pages. Intuitively these models are highly
complementary, which should make them ideal for ensembling. Indeed, an oracle
experiment shows potential gains from 48.8% accuracy (the best single model)
all the way up to 67% (best possible ensemble). So it is a trivial exercise to
create an ensemble with substantial real gains. Or is it?
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blind Dates: Examining the Expression of Temporality in Historical
  Photographs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Barancová, Melvin Wevers, Nanne van Noord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the capacity of computer vision models to discern
temporal information in visual content, focusing specifically on historical
photographs. We investigate the dating of images using OpenCLIP, an open-source
implementation of CLIP, a multi-modal language and vision model. Our experiment
consists of three steps: zero-shot classification, fine-tuning, and analysis of
visual content. We use the \textit{De Boer Scene Detection} dataset, containing
39,866 gray-scale historical press photographs from 1950 to 1999. The results
show that zero-shot classification is relatively ineffective for image dating,
with a bias towards predicting dates in the past. Fine-tuning OpenCLIP with a
logistic classifier improves performance and eliminates the bias. Additionally,
our analysis reveals that images featuring buses, cars, cats, dogs, and people
are more accurately dated, suggesting the presence of temporal markers. The
study highlights the potential of machine learning models like OpenCLIP in
dating images and emphasizes the importance of fine-tuning for accurate
temporal analysis. Future research should explore the application of these
findings to color photographs and diverse datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulong Shi, Mingwei Sun, Yongshuai Wang, Rui Wang, Hui Sun, Zengqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Because of the advancement of deep learning technology, vision transformer
has demonstrated competitive performance in various computer vision tasks.
Unfortunately, vision transformer still faces some challenges such as high
computational complexity and absence of desirable inductive bias. To alleviate
these problems, this study proposes a novel Bi-Fovea Self-Attention (BFSA)
inspired by the physiological structure and characteristics of bi-fovea vision
in eagle eyes. This BFSA can simulate the shallow fovea and deep fovea
functions of eagle vision, enabling the network to extract feature
representations of targets from coarse to fine, facilitating the interaction of
multi-scale feature representations. Additionally, this study designs a Bionic
Eagle Vision (BEV) block based on BFSA and CNN. It combines CNN and Vision
Transformer, to enhance the network's local and global representation ability
for targets. Furthermore, this study develops a unified and efficient general
pyramid backbone network family, named Eagle Vision Transformers (EViTs) by
stacking the BEV blocks. Experimental results on various computer vision tasks
including image classification, object detection, instance segmentation and
other transfer learning tasks show that the proposed EViTs perform
significantly better than the baselines under similar model sizes, which
exhibits faster speed on graphics processing unit compared to other models.
Code will be released at https://github.com/nkusyl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Cardiac MRI Reconstruction with ADMM <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Yiasemis, Nikita Moriakov, Jan-Jakob Sonke, Jonas Teuwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac magnetic resonance imaging is a valuable non-invasive tool for
identifying cardiovascular diseases. For instance, Cine MRI is the benchmark
modality for assessing the cardiac function and anatomy. On the other hand,
multi-contrast (T1 and T2) mapping has the potential to assess pathologies and
abnormalities in the myocardium and interstitium. However, voluntary
breath-holding and often arrhythmia, in combination with MRI's slow imaging
speed, can lead to motion artifacts, hindering real-time acquisition image
quality. Although performing accelerated acquisitions can facilitate dynamic
imaging, it induces aliasing, causing low reconstructed image quality in Cine
MRI and inaccurate T1 and T2 mapping estimation. In this work, inspired by
related work in accelerated MRI reconstruction, we present a deep learning
(DL)-based method for accelerated cine and multi-contrast reconstruction in the
context of dynamic cardiac imaging. We formulate the reconstruction problem as
a least squares regularized optimization task, and employ vSHARP, a
state-of-the-art DL-based inverse problem solver, which incorporates
half-quadratic variable splitting and the alternating direction method of
multipliers with neural networks. We treat the problem in two setups; a 2D
reconstruction and a 2D dynamic reconstruction task, and employ 2D and 3D deep
learning networks, respectively. Our method optimizes in both the image and
k-space domains, allowing for high reconstruction fidelity. Although the target
data is undersampled with a Cartesian equispaced scheme, we train our model
using both Cartesian and simulated non-Cartesian undersampling schemes to
enhance generalization of the model to unseen data. Furthermore, our model
adopts a deep neural network to learn and refine the sensitivity maps of
multi-coil k-space data. Lastly, our method is jointly trained on both,
undersampled cine and multi-contrast data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, 2 tables. CMRxRecon Challenge, MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What If the TV Was Off? Examining Counterfactual Reasoning Abilities of
  Multi-modal Language Models <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Xin Wen, Yongshuo Zong, Bingchen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual reasoning ability is one of the core abilities of human
intelligence. This reasoning process involves the processing of alternatives to
observed states or past events, and this process can improve our ability for
planning and decision-making. In this work, we focus on benchmarking the
counterfactual reasoning ability of multi-modal large language models. We take
the question and answer pairs from the VQAv2 dataset and add one counterfactual
presupposition to the questions, with the answer being modified accordingly.
After generating counterfactual questions and answers using ChatGPT, we
manually examine all generated questions and answers to ensure correctness.
Over 2k counterfactual question and answer pairs are collected this way. We
evaluate recent vision language models on our newly collected test dataset and
found that all models exhibit a large performance drop compared to the results
tested on questions without the counterfactual presupposition. This result
indicates that there still exists space for developing vision language models.
Apart from the vision language models, our proposed dataset can also serves as
a benchmark for evaluating the ability of code generation LLMs, results
demonstrate a large gap between GPT-4 and current open-source models. Our code
and dataset are available at \url{https://github.com/Letian2003/C-VQA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short paper accepted at ICCV 2023 VLAR workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric
  Heterogenous Distillation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caizhen He, Hai Wang, Long Chen, Tong Luo, Yingfeng Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection is the central issue of intelligent traffic systems, and
recent advancements in single-vehicle lidar-based 3D detection indicate that it
can provide accurate position information for intelligent agents to make
decisions and plan. Compared with single-vehicle perception, multi-view
vehicle-road cooperation perception has fundamental advantages, such as the
elimination of blind spots and a broader range of perception, and has become a
research hotspot. However, the current perception of cooperation focuses on
improving the complexity of fusion while ignoring the fundamental problems
caused by the absence of single-view outlines. We propose a multi-view
vehicle-road cooperation perception system, vehicle-to-everything cooperative
perception (V2X-AHD), in order to enhance the identification capability,
particularly for predicting the vehicle's shape. At first, we propose an
asymmetric heterogeneous distillation network fed with different training data
to improve the accuracy of contour recognition, with multi-view teacher
features transferring to single-view student features. While the point cloud
data are sparse, we propose Spara Pillar, a spare convolutional-based plug-in
feature extraction backbone, to reduce the number of parameters and improve and
enhance feature extraction capabilities. Moreover, we leverage the multi-head
self-attention (MSA) to fuse the single-view feature, and the lightweight
design makes the fusion feature a smooth expression. The results of applying
our algorithm to the massive open dataset V2Xset demonstrate that our method
achieves the state-of-the-art result. The V2X-AHD can effectively improve the
accuracy of 3D object detection and reduce the number of network parameters,
according to this study, which serves as a benchmark for cooperative
perception. The code for this article is available at
https://github.com/feeling0414-lab/V2X-AHD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pi-DUAL: Using Privileged Information to Distinguish Clean from Noisy
  Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wang, Guillermo Ortiz-Jimenez, Rodolphe Jenatton, Mark Collier, Efi Kokiopoulou, Pascal Frossard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label noise is a pervasive problem in deep learning that often compromises
the generalization performance of trained models. Recently, leveraging
privileged information (PI) -- information available only during training but
not at test time -- has emerged as an effective approach to mitigate this
issue. Yet, existing PI-based methods have failed to consistently outperform
their no-PI counterparts in terms of preventing overfitting to label noise. To
address this deficiency, we introduce Pi-DUAL, an architecture designed to
harness PI to distinguish clean from wrong labels. Pi-DUAL decomposes the
output logits into a prediction term, based on conventional input features, and
a noise-fitting term influenced solely by PI. A gating mechanism steered by PI
adaptively shifts focus between these terms, allowing the model to implicitly
separate the learning paths of clean and wrong labels. Empirically, Pi-DUAL
achieves significant performance improvements on key PI benchmarks (e.g., +6.8%
on ImageNet-PI), establishing a new state-of-the-art test set accuracy.
Additionally, Pi-DUAL is a potent method for identifying noisy samples
post-training, outperforming other strong methods at this task. Overall,
Pi-DUAL is a simple, scalable and practical approach for mitigating the effects
of label noise in a variety of real-world scenarios with PI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning
  Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Liao, Shaofeng Zhang, Renqiu Xia, Bo Zhang, Min Cao, Yu Qiao, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an emerging line of research on multimodal instruction tuning, and a
line of benchmarks have been proposed for evaluating these models recently.
Instead of evaluating the models directly, in this paper we try to evaluate the
Vision-Language Instruction-Tuning (VLIT) datasets themselves and further seek
the way of building a dataset for developing an all-powerful VLIT model, which
we believe could also be of utility for establishing a grounded protocol for
benchmarking VLIT models. For effective analysis of VLIT datasets that remains
an open question, we propose a tune-cross-evaluation paradigm: tuning on one
dataset and evaluating on the others in turn. For each single tune-evaluation
experiment set, we define the Meta Quality (MQ) as the mean score measured by a
series of caption metrics including BLEU, METEOR, and ROUGE-L to quantify the
quality of a certain dataset or a sample. On this basis, to evaluate the
comprehensiveness of a dataset, we develop the Dataset Quality (DQ) covering
all tune-evaluation sets. To lay the foundation for building a comprehensive
dataset and developing an all-powerful model for practical applications, we
further define the Sample Quality (SQ) to quantify the all-sided quality of
each sample. Extensive experiments validate the rationality of the proposed
evaluation paradigm. Based on the holistic evaluation, we build a new dataset,
REVO-LION (REfining VisiOn-Language InstructiOn tuNing), by collecting samples
with higher SQ from each dataset. With only half of the full data, the model
trained on REVO-LION can achieve performance comparable to simply adding all
VLIT datasets up. In addition to developing an all-powerful model, REVO-LION
also includes an evaluation set, which is expected to serve as a convenient
evaluation benchmark for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Mask2Former: Panoptic Segmentation of Crops, Weeds and
  Leaves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madeleine Darbyshire, Elizabeth Sklar, Simon Parsons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in machine vision that enable detailed inferences to be made
from images have the potential to transform many sectors including agriculture.
Precision agriculture, where data analysis enables interventions to be
precisely targeted, has many possible applications. Precision spraying, for
example, can limit the application of herbicide only to weeds, or limit the
application of fertiliser only to undernourished crops, instead of spraying the
entire field. The approach promises to maximise yields, whilst minimising
resource use and harms to the surrounding environment. To this end, we propose
a hierarchical panoptic segmentation method to simultaneously identify
indicators of plant growth and locate weeds within an image. We adapt
Mask2Former, a state-of-the-art architecture for panoptic segmentation, to
predict crop, weed and leaf masks. We achieve a PQ{\dag} of 75.99.
Additionally, we explore approaches to make the architecture more compact and
therefore more suitable for time and compute constrained applications. With our
more compact architecture, inference is up to 60% faster and the reduction in
PQ{\dag} is less than 1%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 2 tables, for code, see
  https://github.com/madeleinedarbyshire/HierarchicalMask2Former</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Efficient Visual Search by Eye Movement and Low-Latency Spiking
  Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhui Zhou, Dongqi Han, Yuguo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human vision incorporates non-uniform resolution retina, efficient eye
movement strategy, and spiking neural network (SNN) to balance the requirements
in visual field size, visual resolution, energy cost, and inference latency.
These properties have inspired interest in developing human-like computer
vision. However, existing models haven't fully incorporated the three features
of human vision, and their learned eye movement strategies haven't been
compared with human's strategy, making the models' behavior difficult to
interpret. Here, we carry out experiments to examine human visual search
behaviors and establish the first SNN-based visual search model. The model
combines an artificial retina with spiking feature extraction, memory, and
saccade decision modules, and it employs population coding for fast and
efficient saccade decisions. The model can learn either a human-like or a
near-optimal fixation strategy, outperform humans in search speed and accuracy,
and achieve high energy efficiency through short saccade decision latency and
sparse activation. It also suggests that the human search strategy is
suboptimal in terms of search speed. Our work connects modeling of vision in
neuroscience and machine learning and sheds light on developing more
energy-efficient computer vision algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SketchBodyNet: A Sketch-Driven Multi-faceted Decoder Network for 3D
  Human Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Wang, Kongzhang Tang, Hefeng Wu, Baoquan Zhao, Hao Cai, Teng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing 3D human shapes from 2D images has received increasing
attention recently due to its fundamental support for many high-level 3D
applications. Compared with natural images, freehand sketches are much more
flexible to depict various shapes, providing a high potential and valuable way
for 3D human reconstruction. However, such a task is highly challenging. The
sparse abstract characteristics of sketches add severe difficulties, such as
arbitrariness, inaccuracy, and lacking image details, to the already badly
ill-posed problem of 2D-to-3D reconstruction. Although current methods have
achieved great success in reconstructing 3D human bodies from a single-view
image, they do not work well on freehand sketches. In this paper, we propose a
novel sketch-driven multi-faceted decoder network termed SketchBodyNet to
address this task. Specifically, the network consists of a backbone and three
separate attention decoder branches, where a multi-head self-attention module
is exploited in each decoder to obtain enhanced features, followed by a
multi-layer perceptron. The multi-faceted decoders aim to predict the camera,
shape, and pose parameters, respectively, which are then associated with the
SMPL model to reconstruct the corresponding 3D human mesh. In learning,
existing 3D meshes are projected via the camera parameters into 2D synthetic
sketches with joints, which are combined with the freehand sketches to optimize
the model. To verify our method, we collect a large-scale dataset of about 26k
freehand sketches and their corresponding 3D meshes containing various poses of
human bodies from 14 different angles. Extensive experimental results
demonstrate our SketchBodyNet achieves superior performance in reconstructing
3D human meshes from freehand sketches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, to appear in Pacific Graphics 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Retrieval of Images with Irregular Patterns using
  Morphological Image Analysis: Applications to Industrial and Healthcare
  datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Zhang, Georgina Cosma, Sarah Bugby, Jason Watkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image retrieval is the process of searching and retrieving images from a
database based on their visual content and features. Recently, much attention
has been directed towards the retrieval of irregular patterns within industrial
or medical images by extracting features from the images, such as deep
features, colour-based features, shape-based features and local features. This
has applications across a spectrum of industries, including fault inspection,
disease diagnosis, and maintenance prediction. This paper proposes an image
retrieval framework to search for images containing similar irregular patterns
by extracting a set of morphological features (DefChars) from images; the
datasets employed in this paper contain wind turbine blade images with defects,
chest computerised tomography scans with COVID-19 infection, heatsink images
with defects, and lake ice images. The proposed framework was evaluated with
different feature extraction methods (DefChars, resized raw image, local binary
pattern, and scale-invariant feature transforms) and distance metrics to
determine the most efficient parameters in terms of retrieval performance
across datasets. The retrieval results show that the proposed framework using
the DefChars and the Manhattan distance metric achieves a mean average
precision of 80% and a low standard deviation of 0.09 across classes of
irregular patterns, outperforming alternative feature-metric combinations
across all datasets. Furthermore, the low standard deviation between each class
highlights DefChars' capability for a reliable image retrieval task, even in
the presence of class imbalances or small-sized datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 5 figures, 19 tables (17 tables in appendix), submitted to
  Special Issue: Advances and Challenges in Multimodal Machine Learning 2nd
  Edition, Journal of Imaging, MDPI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Representation Learning for Brain Tumour Segmentation <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Antanas Kascenas, Hannah Watson, Sotirios A. Tsaftaris, Alison Q. O'Neil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For brain tumour segmentation, deep learning models can achieve human
expert-level performance given a large amount of data and pixel-level
annotations. However, the expensive exercise of obtaining pixel-level
annotations for large amounts of data is not always feasible, and performance
is often heavily reduced in a low-annotated data regime. To tackle this
challenge, we adapt a mixed supervision framework, vMFNet, to learn robust
compositional representations using unsupervised learning and weak supervision
alongside non-exhaustive pixel-level pathology labels. In particular, we use
the BraTS dataset to simulate a collection of 2-point expert pathology
annotations indicating the top and bottom slice of the tumour (or tumour
sub-regions: peritumoural edema, GD-enhancing tumour, and the necrotic /
non-enhancing tumour) in each MRI volume, from which weak image-level labels
that indicate the presence or absence of the tumour (or the tumour sub-regions)
in the image are constructed. Then, vMFNet models the encoded image features
with von-Mises-Fisher (vMF) distributions, via learnable and compositional vMF
kernels which capture information about structures in the images. We show that
good tumour segmentation performance can be achieved with a large amount of
weakly labelled data but only a small amount of fully-annotated data.
Interestingly, emergent learning of anatomical structures occurs in the
compositional representation even given only supervision relating to pathology
(tumour).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DART workshop, MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data efficient deep learning for medical image analysis: A survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suruchi Kumari, Pravendra Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of deep learning has significantly advanced the field of
medical image analysis. However, despite these achievements, the further
enhancement of deep learning models for medical image analysis faces a
significant challenge due to the scarcity of large, well-annotated datasets. To
address this issue, recent years have witnessed a growing emphasis on the
development of data-efficient deep learning methods. This paper conducts a
thorough review of data-efficient deep learning methods for medical image
analysis. To this end, we categorize these methods based on the level of
supervision they rely on, encompassing categories such as no supervision,
inexact supervision, incomplete supervision, inaccurate supervision, and only
limited supervision. We further divide these categories into finer
subcategories. For example, we categorize inexact supervision into multiple
instance learning and learning with weak annotations. Similarly, we categorize
incomplete supervision into semi-supervised learning, active learning, and
domain-adaptive learning and so on. Furthermore, we systematically summarize
commonly used datasets for data efficient deep learning in medical image
analysis and investigate future research directions to conclude this survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield
  but Also a Catalyst for Model Inversion Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Struppek, Dominik Hintersdorf, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Label smoothing -- using softened labels instead of hard ones -- is a widely
adopted regularization method for deep learning, showing diverse benefits such
as enhanced generalization and calibration. Its implications for preserving
model privacy, however, have remained unexplored. To fill this gap, we
investigate the impact of label smoothing on model inversion attacks (MIAs),
which aim to generate class-representative samples by exploiting the knowledge
encoded in a classifier, thereby inferring sensitive information about its
training data. Through extensive analyses, we uncover that traditional label
smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even
more, we reveal that smoothing with negative factors counters this trend,
impeding the extraction of class-related information and leading to privacy
preservation, beating state-of-the-art defenses. This establishes a practical
and powerful novel way for enhancing model resilience against MIAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 tables, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceptual MAE for Image Manipulation Localization: A High-level Vision
  Learner Focusing on Low-level Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Ma, Jizhe Zhou, Xiong Xu, Zhuohang Jiang, Chi-Man Pun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, multimedia forensics faces unprecedented challenges due to the
rapid advancement of multimedia generation technology thereby making Image
Manipulation Localization (IML) crucial in the pursuit of truth. The key to IML
lies in revealing the artifacts or inconsistencies between the tampered and
authentic areas, which are evident under pixel-level features. Consequently,
existing studies treat IML as a low-level vision task, focusing on allocating
tampered masks by crafting pixel-level features such as image RGB noises, edge
signals, or high-frequency features. However, in practice, tampering commonly
occurs at the object level, and different classes of objects have varying
likelihoods of becoming targets of tampering. Therefore, object semantics are
also vital in identifying the tampered areas in addition to pixel-level
features. This necessitates IML models to carry out a semantic understanding of
the entire image. In this paper, we reformulate the IML task as a high-level
vision task that greatly benefits from low-level features. Based on such an
interpretation, we propose a method to enhance the Masked Autoencoder (MAE) by
incorporating high-resolution inputs and a perceptual loss supervision module,
which is termed Perceptual MAE (PMAE). While MAE has demonstrated an impressive
understanding of object semantics, PMAE can also compensate for low-level
semantics with our proposed enhancements. Evidenced by extensive experiments,
this paradigm effectively unites the low-level and high-level features of the
IML task and outperforms state-of-the-art tampering localization methods on all
five publicly available datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Watt For What: Rethinking Deep Learning's Energy-Performance
  Relationship 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyank N Gowda, Xinyue Hao, Gen Li, Laura Sevilla-Lara, Shashank Narayana Gowda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have revolutionized various fields, from image
recognition to natural language processing, by achieving unprecedented levels
of accuracy. However, their increasing energy consumption has raised concerns
about their environmental impact, disadvantaging smaller entities in research
and exacerbating global energy consumption. In this paper, we explore the
trade-off between model accuracy and electricity consumption, proposing a
metric that penalizes large consumption of electricity. We conduct a
comprehensive study on the electricity consumption of various deep learning
models across different GPUs, presenting a detailed analysis of their
accuracy-efficiency trade-offs. By evaluating accuracy per unit of electricity
consumed, we demonstrate how smaller, more energy-efficient models can
significantly expedite research while mitigating environmental concerns. Our
results highlight the potential for a more sustainable approach to deep
learning, emphasizing the importance of optimizing models for efficiency. This
research also contributes to a more equitable research landscape, where smaller
entities can compete effectively with larger counterparts. This advocates for
the adoption of efficient deep learning practices to reduce electricity
consumption, safeguarding the environment for future generations whilst also
helping ensure a fairer competitive landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Automatic Detection and Facial Recognition in Japanese
  Macaques: Illuminating Social Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Paulet, Axel Molina, Benjamin Beltzung, Takafumi Suzumura, Shinya Yamamoto, Cédric Sueur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Individual identification plays a pivotal role in ecology and ethology,
notably as a tool for complex social structures understanding. However,
traditional identification methods often involve invasive physical tags and can
prove both disruptive for animals and time-intensive for researchers. In recent
years, the integration of deep learning in research offered new methodological
perspectives through automatization of complex tasks. Harnessing object
detection and recognition technologies is increasingly used by researchers to
achieve identification on video footage. This study represents a preliminary
exploration into the development of a non-invasive tool for face detection and
individual identification of Japanese macaques (Macaca fuscata) through deep
learning. The ultimate goal of this research is, using identifications done on
the dataset, to automatically generate a social network representation of the
studied population. The current main results are promising: (i) the creation of
a Japanese macaques' face detector (Faster-RCNN model), reaching a 82.2%
accuracy and (ii) the creation of an individual recognizer for K{\=o}jima
island macaques population (YOLOv8n model), reaching a 83% accuracy. We also
created a K{\=o}jima population social network by traditional methods, based on
co-occurrences on videos. Thus, we provide a benchmark against which the
automatically generated network will be assessed for reliability. These
preliminary results are a testament to the potential of this innovative
approach to provide the scientific community with a tool for tracking
individuals and social network studies in Japanese macaques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlong Li, Wenhao Liu, Changze Lv, Jianhan Xu, Cenyuan Zhang, Muling Wu, Xiaoqing Zheng, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs) have demonstrated the capability to achieve
comparable performance to deep neural networks (DNNs) in both visual and
linguistic domains while offering the advantages of improved energy efficiency
and adherence to biological plausibility. However, the extension of such
single-modality SNNs into the realm of multimodal scenarios remains an
unexplored territory. Drawing inspiration from the concept of contrastive
language-image pre-training (CLIP), we introduce a novel framework, named
SpikeCLIP, to address the gap between two modalities within the context of
spike-based computing through a two-step recipe involving ``Alignment
Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that
SNNs achieve comparable results to their DNN counterparts while significantly
reducing energy consumption across a variety of datasets commonly used for
multimodal model evaluation. Furthermore, SpikeCLIP maintains robust
performance in image classification tasks that involve class labels not
predefined within specific categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topological RANSAC for instance verification and retrieval without
  fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoyuan An, Juhyung Seon, Inkyu An, Yuchi Huo, Sung-Eui Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an innovative approach to enhancing explainable image
retrieval, particularly in situations where a fine-tuning set is unavailable.
The widely-used SPatial verification (SP) method, despite its efficacy, relies
on a spatial model and the hypothesis-testing strategy for instance
recognition, leading to inherent limitations, including the assumption of
planar structures and neglect of topological relations among features. To
address these shortcomings, we introduce a pioneering technique that replaces
the spatial model with a topological one within the RANSAC process. We propose
bio-inspired saccade and fovea functions to verify the topological consistency
among features, effectively circumventing the issues associated with SP's
spatial model. Our experimental results demonstrate that our method
significantly outperforms SP, achieving state-of-the-art performance in
non-fine-tuning retrieval. Furthermore, our approach can enhance performance
when used in conjunction with fine-tuned features. Importantly, our method
retains high explainability and is lightweight, offering a practical and
adaptable solution for a variety of real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focus on Local Regions for Query-based Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbin Xu, Yamei Xia, Shuai Zhao, Bo Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query-based methods have garnered significant attention in object detection
since the advent of DETR, the pioneering end-to-end query-based detector.
However, these methods face challenges like slow convergence and suboptimal
performance. Notably, self-attention in object detection often hampers
convergence due to its global focus. To address these issues, we propose FoLR,
a transformer-like architecture with only decoders. We enhance the
self-attention mechanism by isolating connections between irrelevant objects
that makes it focus on local regions but not global regions. We also design the
adaptive sampling method to extract effective features based on queries' local
regions from feature maps. Additionally, we employ a look-back strategy for
decoders to retain prior information, followed by the Feature Mixer module to
fuse features and queries. Experimental results demonstrate FoLR's
state-of-the-art performance in query-based detectors, excelling in convergence
speed and computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Geometrical Approach to Evaluate the Adversarial Robustness of Deep
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Wang, Bo Dong, Ke Xu, Haiyin Piao, Yufei Ding, Baocai Yin, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) are widely used for computer vision tasks.
However, it has been shown that deep models are vulnerable to adversarial
attacks, i.e., their performances drop when imperceptible perturbations are
made to the original inputs, which may further degrade the following visual
tasks or introduce new problems such as data and privacy security. Hence,
metrics for evaluating the robustness of deep models against adversarial
attacks are desired. However, previous metrics are mainly proposed for
evaluating the adversarial robustness of shallow networks on the small-scale
datasets. Although the Cross Lipschitz Extreme Value for nEtwork Robustness
(CLEVER) metric has been proposed for large-scale datasets (e.g., the ImageNet
dataset), it is computationally expensive and its performance relies on a
tractable number of samples. In this paper, we propose the Adversarial
Converging Time Score (ACTS), an attack-dependent metric that quantifies the
adversarial robustness of a DNN on a specific input. Our key observation is
that local neighborhoods on a DNN's output surface would have different shapes
given different inputs. Hence, given different inputs, it requires different
time for converging to an adversarial sample. Based on this geometry meaning,
ACTS measures the converging time as an adversarial robustness metric. We
validate the effectiveness and generalization of the proposed ACTS metric
against different adversarial attacks on the large-scale ImageNet dataset using
state-of-the-art deep networks. Extensive experiments show that our ACTS metric
is an efficient and effective adversarial metric over the previous CLEVER
metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Transactions on Multimedia Computing, Communications, and
  Applications (ACM TOMM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solution for SMART-101 Challenge of ICCV Multi-modal Algorithmic
  Reasoning Task 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Wu, Yang Yang, Shengdong Xu, Yifeng Wu, Qingguo Chen, Jianfeng Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present our solution to a Multi-modal Algorithmic Reasoning
Task: SMART-101 Challenge. Different from the traditional visual
question-answering datasets, this challenge evaluates the abstraction,
deduction, and generalization abilities of neural networks in solving
visuolinguistic puzzles designed specifically for children in the 6-8 age
group. We employed a divide-and-conquer approach. At the data level, inspired
by the challenge paper, we categorized the whole questions into eight types and
utilized the llama-2-chat model to directly generate the type for each question
in a zero-shot manner. Additionally, we trained a yolov7 model on the icon45
dataset for object detection and combined it with the OCR method to recognize
and locate objects and text within the images. At the model level, we utilized
the BLIP-2 model and added eight adapters to the image encoder VIT-G to
adaptively extract visual features for different question types. We fed the
pre-constructed question templates as input and generated answers using the
flan-t5-xxl decoder. Under the puzzle splits configuration, we achieved an
accuracy score of 26.5 on the validation set and 24.30 on the private test set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skeleton Ground Truth Extraction: Methodology, Annotation Tool and
  Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Yang, Bipin Indurkhya, John See, Bo Gao, Yan Ke, Zeyd Boukhers, Zhenyu Yang, Marcin Grzegorzek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton Ground Truth (GT) is critical to the success of supervised skeleton
extraction methods, especially with the popularity of deep learning techniques.
Furthermore, we see skeleton GTs used not only for training skeleton detectors
with Convolutional Neural Networks (CNN) but also for evaluating
skeleton-related pruning and matching algorithms. However, most existing shape
and image datasets suffer from the lack of skeleton GT and inconsistency of GT
standards. As a result, it is difficult to evaluate and reproduce CNN-based
skeleton detectors and algorithms on a fair basis. In this paper, we present a
heuristic strategy for object skeleton GT extraction in binary shapes and
natural images. Our strategy is built on an extended theory of diagnosticity
hypothesis, which enables encoding human-in-the-loop GT extraction based on
clues from the target's context, simplicity, and completeness. Using this
strategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing
shape and image datasets. The GTs are then structurally evaluated with
representative methods to build viable baselines for fair comparisons.
Experiments demonstrate that GTs generated by our strategy yield promising
quality with respect to standard consistency, and also provide a balance
between simplicity and completeness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the International Journal of Computer
  Vision (IJCV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retromorphic Testing: A New Approach to the Test Oracle Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxi Yu, Qiuyang Mang, Qingshuo Guo, Pinjia He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A test oracle serves as a criterion or mechanism to assess the correspondence
between software output and the anticipated behavior for a given input set. In
automated testing, black-box techniques, known for their non-intrusive nature
in test oracle construction, are widely used, including notable methodologies
like differential testing and metamorphic testing. Inspired by the mathematical
concept of inverse function, we present Retromorphic Testing, a novel black-box
testing methodology. It leverages an auxiliary program in conjunction with the
program under test, which establishes a dual-program structure consisting of a
forward program and a backward program. The input data is first processed by
the forward program and then its program output is reversed to its original
input format using the backward program. In particular, the auxiliary program
can operate as either the forward or backward program, leading to different
testing modes. The process concludes by examining the relationship between the
initial input and the transformed output within the input domain. For example,
to test the implementation of the sine function $\sin(x)$, we can employ its
inverse function, $\arcsin(x)$, and validate the equation $x =
\sin(\arcsin(x)+2k\pi), \forall k \in \mathbb{Z}$. In addition to the
high-level concept of Retromorphic Testing, this paper presents its three
testing modes with illustrative use cases across diverse programs, including
algorithms, traditional software, and AI applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Prediction for Deep Classifier via Label Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianguo Huang, Huajun Xi, Linjun Zhang, Huaxiu Yao, Yue Qiu, Hongxin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction is a statistical framework that generates prediction
sets containing ground-truth labels with a desired coverage guarantee. The
predicted probabilities produced by machine learning models are generally
miscalibrated, leading to large prediction sets in conformal prediction. In
this paper, we empirically and theoretically show that disregarding the
probabilities' value will mitigate the undesirable effect of miscalibrated
probability values. Then, we propose a novel algorithm named $\textit{Sorted
Adaptive prediction sets}$ (SAPS), which discards all the probability values
except for the maximum softmax probability. The key idea behind SAPS is to
minimize the dependence of the non-conformity score on the probability values
while retaining the uncertainty information. In this manner, SAPS can produce
sets of small size and communicate instance-wise uncertainty. Theoretically, we
provide a finite-sample coverage guarantee of SAPS and show that the expected
value of set size from SAPS is always smaller than APS. Extensive experiments
validate that SAPS not only lessens the prediction sets but also broadly
enhances the conditional coverage rate and adaptation of prediction sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnoDODE: Anomaly Detection with Diffusion ODE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianyao Hu, Congming Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is the process of identifying atypical data samples that
significantly deviate from the majority of the dataset. In the realm of
clinical screening and diagnosis, detecting abnormalities in medical images
holds great importance. Typically, clinical practice provides access to a vast
collection of normal images, while abnormal images are relatively scarce. We
hypothesize that abnormal images and their associated features tend to manifest
in low-density regions of the data distribution. Following this assumption, we
turn to diffusion ODEs for unsupervised anomaly detection, given their
tractability and superior performance in density estimation tasks. More
precisely, we propose a new anomaly detection method based on diffusion ODEs by
estimating the density of features extracted from multi-scale medical images.
Our anomaly scoring mechanism depends on computing the negative log-likelihood
of features extracted from medical images at different scales, quantified in
bits per dimension. Furthermore, we propose a reconstruction-based anomaly
localization suitable for our method. Our proposed method not only identifie
anomalies but also provides interpretability at both the image and pixel
levels. Through experiments on the BraTS2021 medical dataset, our proposed
method outperforms existing methods. These results confirm the effectiveness
and robustness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boundary Discretization and Reliable Classification Network for Temporal
  Action Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenying Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal action detection aims to recognize the action category and determine
the starting and ending time of each action instance in untrimmed videos. The
mixed methods have achieved remarkable performance by simply merging
anchor-based and anchor-free approaches. However, there are still two crucial
issues in the mixed framework: (1) Brute-force merging and handcrafted anchors
design affect the performance and practical application of the mixed methods.
(2) A large number of false positives in action category predictions further
impact the detection performance. In this paper, we propose a novel Boundary
Discretization and Reliable Classification Network (BDRC-Net) that addresses
the above issues by introducing boundary discretization and reliable
classification modules. Specifically, the boundary discretization module (BDM)
elegantly merges anchor-based and anchor-free approaches in the form of
boundary discretization, avoiding the handcrafted anchors design required by
traditional mixed methods. Furthermore, the reliable classification module
(RCM) predicts reliable action categories to reduce false positives in action
category predictions. Extensive experiments conducted on different benchmarks
demonstrate that our proposed method achieves favorable performance compared
with the state-of-the-art. For example, BDRC-Net hits an average mAP of 68.6%
on THUMOS'14, outperforming the previous best by 1.5%. The code will be
released at https://github.com/zhenyingfang/BDRC-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Stackable and Skippable LEGO Bricks for Efficient,
  Reconfigurable, and Variable-Resolution Diffusion Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huangjie Zheng, Zhendong Wang, Jianbo Yuan, Guanghan Ning, Pengcheng He, Quanzeng You, Hongxia Yang, Mingyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models excel at generating photo-realistic images but come with
significant computational costs in both training and sampling. While various
techniques address these computational challenges, a less-explored issue is
designing an efficient and adaptable network backbone for iterative refinement.
Current options like U-Net and Vision Transformer often rely on
resource-intensive deep networks and lack the flexibility needed for generating
images at variable resolutions or with a smaller network than used in training.
This study introduces LEGO bricks, which seamlessly integrate Local-feature
Enrichment and Global-content Orchestration. These bricks can be stacked to
create a test-time reconfigurable diffusion backbone, allowing selective
skipping of bricks to reduce sampling costs and generate higher-resolution
images than the training data. LEGO bricks enrich local regions with an MLP and
transform them using a Transformer block while maintaining a consistent
full-resolution image across all bricks. Experimental results demonstrate that
LEGO bricks enhance training efficiency, expedite convergence, and facilitate
variable-resolution image generation while maintaining strong generative
performance. Moreover, LEGO significantly reduces sampling time compared to
other methods, establishing it as a valuable enhancement for diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DS-<span class="highlight-title">SLAM</span>: A 3D Object Detection based Semantic <span class="highlight-title">SLAM</span> towards Dynamic
  Indoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghanta Sai Krishna, Kundrapu Supriya, Sabur Baidya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existence of variable factors within the environment can cause a decline
in camera localization accuracy, as it violates the fundamental assumption of a
static environment in Simultaneous Localization and Mapping (SLAM) algorithms.
Recent semantic SLAM systems towards dynamic environments either rely solely on
2D semantic information, or solely on geometric information, or combine their
results in a loosely integrated manner. In this research paper, we introduce
3DS-SLAM, 3D Semantic SLAM, tailored for dynamic scenes with visual 3D object
detection. The 3DS-SLAM is a tightly-coupled algorithm resolving both semantic
and geometric constraints sequentially. We designed a 3D part-aware hybrid
transformer for point cloud-based object detection to identify dynamic objects.
Subsequently, we propose a dynamic feature filter based on HDBSCAN clustering
to extract objects with significant absolute depth differences. When compared
against ORB-SLAM2, 3DS-SLAM exhibits an average improvement of 98.01% across
the dynamic sequences of the TUM RGB-D dataset. Furthermore, it surpasses the
performance of the other four leading SLAM systems designed for dynamic
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Diffusion-Based Image Variations for Robust Training on
  Poisoned Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Struppek, Martin B. Hentschel, Clifton Poth, Dominik Hintersdorf, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attacks pose a serious security threat for training neural networks
as they surreptitiously introduce hidden functionalities into a model. Such
backdoors remain silent during inference on clean inputs, evading detection due
to inconspicuous behavior. However, once a specific trigger pattern appears in
the input data, the backdoor activates, causing the model to execute its
concealed function. Detecting such poisoned samples within vast datasets is
virtually impossible through manual inspection. To address this challenge, we
propose a novel approach that enables model training on potentially poisoned
datasets by utilizing the power of recent diffusion models. Specifically, we
create synthetic variations of all training samples, leveraging the inherent
resilience of diffusion models to potential trigger patterns in the data. By
combining this generative approach with knowledge distillation, we produce
student models that maintain their general performance on the task while
exhibiting robust resistance to backdoor triggers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Efficient Strategy for Detection of Dark Objects Based on
  Spiking Network with Multi-Box Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Munawar Ali, Baoqun Yin, Hazrat Bilal, Aakash Kumar, Ali Muhammad, Avinash Rohra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several deep learning algorithms have shown amazing performance for existing
object detection tasks, but recognizing darker objects is the largest
challenge. Moreover, those techniques struggled to detect or had a slow
recognition rate, resulting in significant performance losses. As a result, an
improved and accurate detection approach is required to address the above
difficulty. The whole study proposes a combination of spiked and normal
convolution layers as an energy-efficient and reliable object detector model.
The proposed model is split into two sections. The first section is developed
as a feature extractor, which utilizes pre-trained VGG16, and the second
section of the proposal structure is the combination of spiked and normal
Convolutional layers to detect the bounding boxes of images. We drew a
pre-trained model for classifying detected objects. With state of the art
Python libraries, spike layers can be trained efficiently. The proposed spike
convolutional object detector (SCOD) has been evaluated on VOC and Ex-Dark
datasets. SCOD reached 66.01% and 41.25% mAP for detecting 20 different objects
in the VOC-12 and 12 objects in the Ex-Dark dataset. SCOD uses 14 Giga FLOPS
for its forward path calculations. Experimental results indicated superior
performance compared to Tiny YOLO, Spike YOLO, YOLO-LITE, Tinier YOLO and
Center of loc+Xception based on mAP for the VOC dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoinSeg: Contrast Inter- and Intra- Class Representations for
  Incremental Segmentation <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekang Zhang, Guangyu Gao, Jianbo Jiao, Chi Harold Liu, Yunchao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class incremental semantic segmentation aims to strike a balance between the
model's stability and plasticity by maintaining old knowledge while adapting to
new concepts. However, most state-of-the-art methods use the freeze strategy
for stability, which compromises the model's plasticity.In contrast, releasing
parameter training for plasticity could lead to the best performance for all
categories, but this requires discriminative feature representation.Therefore,
we prioritize the model's plasticity and propose the Contrast inter- and
intra-class representations for Incremental Segmentation (CoinSeg), which
pursues discriminative representations for flexible parameter tuning. Inspired
by the Gaussian mixture model that samples from a mixture of Gaussian
distributions, CoinSeg emphasizes intra-class diversity with multiple
contrastive representation centroids. Specifically, we use mask proposals to
identify regions with strong objectness that are likely to be diverse
instances/centroids of a category. These mask proposals are then used for
contrastive representations to reinforce intra-class diversity. Meanwhile, to
avoid bias from intra-class diversity, we also apply category-level
pseudo-labels to enhance category-level consistency and inter-category
diversity. Additionally, CoinSeg ensures the model's stability and alleviates
forgetting through a specific flexible tuning strategy. We validate CoinSeg on
Pascal VOC 2012 and ADE20K datasets with multiple incremental scenarios and
achieve superior results compared to previous state-of-the-art methods,
especially in more challenging and realistic long-term scenarios. Code is
available at https://github.com/zkzhang98/CoinSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fire Detection From Image and Video Using YOLOv5 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arafat Islam, Md. Imtiaz Habib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the detection of fire-like targets in indoor, outdoor and forest fire
images, as well as fire detection under different natural lights, an improved
YOLOv5 fire detection deep learning algorithm is proposed. The YOLOv5 detection
model expands the feature extraction network from three dimensions, which
enhances feature propagation of fire small targets identification, improves
network performance, and reduces model parameters. Furthermore, through the
promotion of the feature pyramid, the top-performing prediction box is
obtained. Fire-YOLOv5 attains excellent results compared to state-of-the-art
object detection networks, notably in the detection of small targets of fire
and smoke with mAP 90.5% and f1 score 88%. Overall, the Fire-YOLOv5 detection
model can effectively deal with the inspection of small fire targets, as well
as fire-like and smoke-like objects with F1 score 0.88. When the input image
size is 416 x 416 resolution, the average detection time is 0.12 s per frame,
which can provide real-time forest fire detection. Moreover, the algorithm
proposed in this paper can also be applied to small target detection under
other complicated situations. The proposed system shows an improved approach in
all fire detection metrics such as precision, recall, and mean average
precision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 sections, unpublished paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JointNet: Extending Text-to-Image Diffusion for Dense Distribution
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Zhang, Shiwei Li, Yuanxun Lu, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan, Yao Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce JointNet, a novel neural network architecture for modeling the
joint distribution of images and an additional dense modality (e.g., depth
maps). JointNet is extended from a pre-trained text-to-image diffusion model,
where a copy of the original network is created for the new dense modality
branch and is densely connected with the RGB branch. The RGB branch is locked
during network fine-tuning, which enables efficient learning of the new
modality distribution while maintaining the strong generalization ability of
the large-scale pre-trained diffusion model. We demonstrate the effectiveness
of JointNet by using RGBD diffusion as an example and through extensive
experiments, showcasing its applicability in a variety of applications,
including joint RGBD generation, dense depth prediction, depth-conditioned
image generation, and coherent tile-based 3D panorama generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Filter Pruning For CNN With Enhanced Linear Representation Redundancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bojue Wang, Chunmei Ma, Bin Liu, Nianbo Liu, Jinqi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured network pruning excels non-structured methods because they can
take advantage of the thriving developed parallel computing techniques. In this
paper, we propose a new structured pruning method. Firstly, to create more
structured redundancy, we present a data-driven loss function term calculated
from the correlation coefficient matrix of different feature maps in the same
layer, named CCM-loss. This loss term can encourage the neural network to learn
stronger linear representation relations between feature maps during the
training from the scratch so that more homogenous parts can be removed later in
pruning. CCM-loss provides us with another universal transcendental
mathematical tool besides L*-norm regularization, which concentrates on
generating zeros, to generate more redundancy but for the different genres.
Furthermore, we design a matching channel selection strategy based on principal
components analysis to exploit the maximum potential ability of CCM-loss. In
our new strategy, we mainly focus on the consistency and integrality of the
information flow in the network. Instead of empirically hard-code the retain
ratio for each layer, our channel selection strategy can dynamically adjust
each layer's retain ratio according to the specific circumstance of a
per-trained model to push the prune ratio to the limit. Notably, on the
Cifar-10 dataset, our method brings 93.64% accuracy for pruned VGG-16 with only
1.40M parameters and 49.60M FLOPs, the pruned ratios for parameters and FLOPs
are 90.6% and 84.2%, respectively. For ResNet-50 trained on the ImageNet
dataset, our approach achieves 42.8% and 47.3% storage and computation
reductions, respectively, with an accuracy of 76.23%. Our code is available at
https://github.com/Bojue-Wang/CCM-LRR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Style Awareness of Font Images <span class="chip">ICDAR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daichi Haraguchi, Seiichi Uchida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When we compare fonts, we often pay attention to styles of local parts, such
as serifs and curvatures. This paper proposes an attention mechanism to find
important local parts. The local parts with larger attention are then
considered important. The proposed mechanism can be trained in a
quasi-self-supervised manner that requires no manual annotation other than
knowing that a set of character images is from the same font, such as
Helvetica. After confirming that the trained attention mechanism can find
style-relevant local parts, we utilize the resulting attention for local
style-aware font generation. Specifically, we design a new reconstruction loss
function to put more weight on the local parts with larger attention for
generating character images with more accurate style realization. This loss
function has the merit of applicability to various font generation models. Our
experimental results show that the proposed loss function improves the quality
of generated character images by several few-shot font generation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICDAR WML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrowdRec: 3D Crowd Reconstruction from Single Color Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Buzhen Huang, Jingyi Ju, Yangang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is a technical report for the GigaCrowd challenge. Reconstructing 3D
crowds from monocular images is a challenging problem due to mutual occlusions,
server depth ambiguity, and complex spatial distribution. Since no large-scale
3D crowd dataset can be used to train a robust model, the current multi-person
mesh recovery methods can hardly achieve satisfactory performance in crowded
scenes. In this paper, we exploit the crowd features and propose a
crowd-constrained optimization to improve the common single-person method on
crowd images. To avoid scale variations, we first detect human bounding-boxes
and 2D poses from the original images with off-the-shelf detectors. Then, we
train a single-person mesh recovery network using existing in-the-wild image
datasets. To promote a more reasonable spatial distribution, we further propose
a crowd constraint to refine the single-person network parameters. With the
optimization, we can obtain accurate body poses and shapes with reasonable
absolute positions from a large-scale crowd image using a single-person
backbone. The code will be publicly available
at~\url{https://github.com/boycehbz/CrowdRec}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Payload Delivery via Unmanned Aerial Vehicles: An Approach Using
  Object Detection Algorithms <span class="chip">CEC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Vadduri, Anagh Benjwal, Abhishek Pai, Elkan Quadros, Aniruddh Kammar, Prajwal Uday
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen tremendous advancements in the area of autonomous
payload delivery via unmanned aerial vehicles, or drones. However, most of
these works involve delivering the payload at a predetermined location using
its GPS coordinates. By relying on GPS coordinates for navigation, the
precision of payload delivery is restricted to the accuracy of the GPS network
and the availability and strength of the GPS connection, which may be severely
restricted by the weather condition at the time and place of operation. In this
work we describe the development of a micro-class UAV and propose a novel
navigation method that improves the accuracy of conventional navigation methods
by incorporating a deep-learning-based computer vision approach to identify and
precisely align the UAV with a target marked at the payload delivery position.
This proposed method achieves a 500% increase in average horizontal precision
over conventional GPS-based approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Second International Conference on Artificial Intelligence,
  Computational Electronics and Communication System (AICECS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Masked Image Inpainting for Robust Detection of Mpox and
  Non-Mpox 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubiao Yue, Zhenzhang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the lack of efficient mpox diagnostic technology, mpox cases continue
to increase. Recently, the great potential of deep learning models in detecting
mpox and non-mpox has been proven. However, existing models learn image
representations via image classification, which results in they may be easily
susceptible to interference from real-world noise, require diverse non-mpox
images, and fail to detect abnormal input. These drawbacks make classification
models inapplicable in real-world settings. To address these challenges, we
propose "Mask, Inpainting, and Measure" (MIM). In MIM's pipeline, a generative
adversarial network only learns mpox image representations by inpainting the
masked mpox images. Then, MIM determines whether the input belongs to mpox by
measuring the similarity between the inpainted image and the original image.
The underlying intuition is that since MIM solely models mpox images, it
struggles to accurately inpaint non-mpox images in real-world settings. Without
utilizing any non-mpox images, MIM cleverly detects mpox and non-mpox and can
handle abnormal inputs. We used the recognized mpox dataset (MSLD) and images
of eighteen non-mpox skin diseases to verify the effectiveness and robustness
of MIM. Experimental results show that the average AUROC of MIM achieves
0.8237. In addition, we demonstrated the drawbacks of classification models and
buttressed the potential of MIM through clinical validation. Finally, we
developed an online smartphone app to provide free testing to the public in
affected areas. This work first employs generative models to improve mpox
detection and provides new insights into binary decision-making tasks in
medical images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Pose-Guided Image Synthesis with Progressive Conditional
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Shen, Hu Ye, Jun Zhang, Cong Wang, Xiao Han, Wei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has showcased the significant potential of diffusion models in
pose-guided person image synthesis. However, owing to the inconsistency in pose
between the source and target images, synthesizing an image with a distinct
pose, relying exclusively on the source image and target pose information,
remains a formidable challenge. This paper presents Progressive Conditional
Diffusion Models (PCDMs) that incrementally bridge the gap between person
images under the target and source poses through three stages. Specifically, in
the first stage, we design a simple prior conditional diffusion model that
predicts the global features of the target image by mining the global alignment
relationship between pose coordinates and image appearance. Then, the second
stage establishes a dense correspondence between the source and target images
using the global features from the previous stage, and an inpainting
conditional diffusion model is proposed to further align and enhance the
contextual features, generating a coarse-grained person image. In the third
stage, we propose a refining conditional diffusion model to utilize the
coarsely generated image from the previous stage as a condition, achieving
texture restoration and enhancing fine-detail consistency. The three-stage
PCDMs work progressively to generate the final high-quality and high-fidelity
synthesized image. Both qualitative and quantitative results demonstrate the
consistency and photorealism of our proposed PCDMs under challenging
scenarios.The code and model will be available at
https://github.com/muzishen/PCDMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Compositional Text-to-image Generation with Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Wen, Guian Fang, Renrui Zhang, Peng Gao, Hao Dong, Dimitris Metaxas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-image models, particularly diffusion models,
have shown significant promise. However, compositional text-to-image models
frequently encounter difficulties in generating high-quality images that
accurately align with input texts describing multiple objects, variable
attributes, and intricate spatial relationships. To address this limitation, we
employ large vision-language models (LVLMs) for multi-dimensional assessment of
the alignment between generated images and their corresponding input texts.
Utilizing this assessment, we fine-tune the diffusion model to enhance its
alignment capabilities. During the inference phase, an initial image is
produced using the fine-tuned diffusion model. The LVLM is then employed to
pinpoint areas of misalignment in the initial image, which are subsequently
corrected using the image editing algorithm until no further misalignments are
detected by the LVLM. The resultant image is consequently more closely aligned
with the input text. Our experimental results validate that the proposed
methodology significantly improves text-image alignment in compositional image
generation, particularly with respect to object number, attribute binding,
spatial relationships, and aesthetic quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three-Dimensional Medical Image Fusion with Deformable Cross-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Liu, Xinxin Fan, Chulong Zhang, Jingjing Dai, Yaoqin Xie, Xiaokun Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal medical image fusion plays an instrumental role in several areas
of medical image processing, particularly in disease recognition and tumor
detection. Traditional fusion methods tend to process each modality
independently before combining the features and reconstructing the fusion
image. However, this approach often neglects the fundamental commonalities and
disparities between multimodal information. Furthermore, the prevailing
methodologies are largely confined to fusing two-dimensional (2D) medical image
slices, leading to a lack of contextual supervision in the fusion images and
subsequently, a decreased information yield for physicians relative to
three-dimensional (3D) images. In this study, we introduce an innovative
unsupervised feature mutual learning fusion network designed to rectify these
limitations. Our approach incorporates a Deformable Cross Feature Blend (DCFB)
module that facilitates the dual modalities in discerning their respective
similarities and differences. We have applied our model to the fusion of 3D MRI
and PET images obtained from 660 patients in the Alzheimer's Disease
Neuroimaging Initiative (ADNI) dataset. Through the application of the DCFB
module, our network generates high-quality MRI-PET fusion images. Experimental
results demonstrate that our method surpasses traditional 2D image fusion
methods in performance metrics such as Peak Signal to Noise Ratio (PSNR) and
Structural Similarity Index Measure (SSIM). Importantly, the capacity of our
method to fuse 3D images enhances the information available to physicians and
researchers, thus marking a significant step forward in the field. The code
will soon be available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards More Efficient Depression Risk Recognition via Gait 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Ren, Muchan Tao, Xuecai Hu, Xiaotong Liu, Qiong Li, Yongzhen Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depression, a highly prevalent mental illness, affects over 280 million
individuals worldwide. Early detection and timely intervention are crucial for
promoting remission, preventing relapse, and alleviating the emotional and
financial burdens associated with depression. However, patients with depression
often go undiagnosed in the primary care setting. Unlike many physiological
illnesses, depression lacks objective indicators for recognizing depression
risk, and existing methods for depression risk recognition are time-consuming
and often encounter a shortage of trained medical professionals. The
correlation between gait and depression risk has been empirically established.
Gait can serve as a promising objective biomarker, offering the advantage of
efficient and convenient data collection. However, current methods for
recognizing depression risk based on gait have only been validated on small,
private datasets, lacking large-scale publicly available datasets for research
purposes. Additionally, these methods are primarily limited to hand-crafted
approaches. Gait is a complex form of motion, and hand-crafted gait features
often only capture a fraction of the intricate associations between gait and
depression risk. Therefore, this study first constructs a large-scale gait
database, encompassing over 1,200 individuals, 40,000 gait sequences, and
covering six perspectives and three types of attire. Two commonly used
psychological scales are provided as depression risk annotations. Subsequently,
a deep learning-based depression risk recognition model is proposed, overcoming
the limitations of hand-crafted approaches. Through experiments conducted on
the constructed large-scale database, the effectiveness of the proposed method
is validated, and numerous instructive insights are presented in the paper,
highlighting the significant potential of gait-based depression risk
recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuseChat: A Conversational Music Recommendation System for Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhikang Dong, Bin Chen, Xiulong Liu, Pawel Polak, Peng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MuseChat, an innovative dialog-based music recommendation
system. This unique platform not only offers interactive user engagement but
also suggests music tailored for input videos, so that users can refine and
personalize their music selections. In contrast, previous systems predominantly
emphasized content compatibility, often overlooking the nuances of users'
individual preferences. For example, all the datasets only provide basic
music-video pairings or such pairings with textual music descriptions. To
address this gap, our research offers three contributions. First, we devise a
conversation-synthesis method that simulates a two-turn interaction between a
user and a recommendation system, which leverages pre-trained music tags and
artist information. In this interaction, users submit a video to the system,
which then suggests a suitable music piece with a rationale. Afterwards, users
communicate their musical preferences, and the system presents a refined music
recommendation with reasoning. Second, we introduce a multi-modal
recommendation engine that matches music either by aligning it with visual cues
from the video or by harmonizing visual information, feedback from previously
recommended music, and the user's textual input. Third, we bridge music
representations and textual data with a Large Language Model(Vicuna-7B). This
alignment equips MuseChat to deliver music recommendations and their underlying
reasoning in a manner resembling human communication. Our evaluations show that
MuseChat surpasses existing state-of-the-art models in music retrieval tasks
and pioneers the integration of the recommendation process within a natural
language framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying
  Expression Conditioned Neural Radiance Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Qin, Yifan Liu, Yuelang Xu, Xiaochen Zhao, Yebin Liu, Haoqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One crucial aspect of 3D head avatar reconstruction lies in the details of
facial expressions. Although recent NeRF-based photo-realistic 3D head avatar
methods achieve high-quality avatar rendering, they still encounter challenges
retaining intricate facial expression details because they overlook the
potential of specific expression variations at different spatial positions when
conditioning the radiance field. Motivated by this observation, we introduce a
novel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained
by a simple MLP-based generation network, encompassing both spatial positional
features and global expression information. Benefiting from rich and diverse
information of the SVE at different positions, the proposed SVE-conditioned
neural radiance field can deal with intricate facial expressions and achieve
realistic rendering and geometry details of high-fidelity 3D head avatars.
Additionally, to further elevate the geometric and rendering quality, we
introduce a new coarse-to-fine training strategy, including a geometry
initialization strategy at the coarse stage and an adaptive importance sampling
strategy at the fine stage. Extensive experiments indicate that our method
outperforms other state-of-the-art (SOTA) methods in rendering and geometry
quality on mobile phone-collected and public datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for
  Unbiased Question-Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiulong Liu, Zhikang Dong, Peng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a growing emphasis on the intersection of
audio, vision, and text modalities, driving forward the advancements in
multimodal research. However, strong bias that exists in any modality can lead
to the model neglecting the others. Consequently, the model's ability to
effectively reason across these diverse modalities is compromised, impeding
further advancement. In this paper, we meticulously review each question type
from the original dataset, selecting those with pronounced answer biases. To
counter these biases, we gather complementary videos and questions, ensuring
that no answers have outstanding skewed distribution. In particular, for binary
questions, we strive to ensure that both answers are almost uniformly spread
within each question category. As a result, we construct a new dataset, named
MUSIC-AVQA v2.0, which is more challenging and we believe could better foster
the progress of AVQA task. Furthermore, we present a novel baseline model that
delves deeper into the audio-visual-text interrelation. On MUSIC-AVQA v2.0,
this model surpasses all the existing benchmarks, improving accuracy by 2% on
MUSIC-AVQA v2.0, setting a new state-of-the-art performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Adaptation of Large Vision Transformer via Adapter
  Re-Composing <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Dong, Dawei Yan, Zhijun Lin, Peng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of high-capacity pre-trained models has revolutionized
problem-solving in computer vision, shifting the focus from training
task-specific models to adapting pre-trained models. Consequently, effectively
adapting large pre-trained models to downstream tasks in an efficient manner
has become a prominent research area. Existing solutions primarily concentrate
on designing lightweight adapters and their interaction with pre-trained
models, with the goal of minimizing the number of parameters requiring updates.
In this study, we propose a novel Adapter Re-Composing (ARC) strategy that
addresses efficient pre-trained model adaptation from a fresh perspective. Our
approach considers the reusability of adaptation parameters and introduces a
parameter-sharing scheme. Specifically, we leverage symmetric
down-/up-projections to construct bottleneck operations, which are shared
across layers. By learning low-dimensional re-scaling coefficients, we can
effectively re-compose layer-adaptive adapters. This parameter-sharing strategy
in adapter design allows us to significantly reduce the number of new
parameters while maintaining satisfactory performance, thereby offering a
promising approach to compress the adaptation cost. We conduct experiments on
24 downstream image classification tasks using various Vision Transformer
variants to evaluate our method. The results demonstrate that our approach
achieves compelling transfer learning performance with a reduced parameter
count. Our code is available at
\href{https://github.com/DavidYanAnDe/ARC}{https://github.com/DavidYanAnDe/ARC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper is accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spiking PointNet: Spiking Neural Networks for Point Clouds <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayong Ren, Zhe Ma, Yuanpei Chen, Weihang Peng, Xiaode Liu, Yuhan Zhang, Yufei Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Spiking Neural Networks (SNNs), enjoying extreme energy efficiency,
have drawn much research attention on 2D visual recognition and shown gradually
increasing application potential. However, it still remains underexplored
whether SNNs can be generalized to 3D recognition. To this end, we present
Spiking PointNet in the paper, the first spiking neural model for efficient
deep learning on point clouds. We discover that the two huge obstacles limiting
the application of SNNs in point clouds are: the intrinsic optimization
obstacle of SNNs that impedes the training of a big spiking model with large
time steps, and the expensive memory and computation cost of PointNet that
makes training a big spiking point model unrealistic. To solve the problems
simultaneously, we present a trained-less but learning-more paradigm for
Spiking PointNet with theoretical justifications and in-depth experimental
analysis. In specific, our Spiking PointNet is trained with only a single time
step but can obtain better performance with multiple time steps inference,
compared to the one trained directly with multiple time steps. We conduct
various experiments on ModelNet10, ModelNet40 to demonstrate the effectiveness
of Spiking PointNet. Notably, our Spiking PointNet even can outperform its ANN
counterpart, which is rare in the SNN field thus providing a potential research
direction for the following work. Moreover, Spiking PointNet shows impressive
speedup and storage saving in the training phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eslam Mohamed Bakr, Mohamed Ayman, Mahmoud Ahmed, Habib Slim, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding is the ability to localize objects in 3D scenes
conditioned by utterances. Most existing methods devote the referring head to
localize the referred object directly, causing failure in complex scenarios. In
addition, it does not illustrate how and why the network reaches the final
decision. In this paper, we address this question Can we design an
interpretable 3D visual grounding framework that has the potential to mimic the
human perception system?. To this end, we formulate the 3D visual grounding
problem as a sequence-to-sequence task by first predicting a chain of anchors
and then the final target. Interpretability not only improves the overall
performance but also helps us identify failure cases. Following the chain of
thoughts approach enables us to decompose the referring task into interpretable
intermediate steps, boosting the performance and making our framework extremely
data-efficient. Moreover, our proposed framework can be easily integrated into
any existing architecture. We validate our approach through comprehensive
experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent
performance gains compared to existing methods without requiring manually
annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is
significantly data-efficient, whereas on the Sr3D dataset, when trained only on
10% of the data, we match the SOTA performance that trained on the entire data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeSt-LeS: Benchmarking Stroke Lesion Segmentation using Deep Supervision <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prantik Deb, Lalith Bharadwaj Baru, Kamalaker Dadi, Bapi Raju S
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain stroke has become a significant burden on global health and thus we
need remedies and prevention strategies to overcome this challenge. For this,
the immediate identification of stroke and risk stratification is the primary
task for clinicians. To aid expert clinicians, automated segmentation models
are crucial. In this work, we consider the publicly available dataset ATLAS
$v2.0$ to benchmark various end-to-end supervised U-Net style models.
Specifically, we have benchmarked models on both 2D and 3D brain images and
evaluated them using standard metrics. We have achieved the highest Dice score
of 0.583 on the 2D transformer-based model and 0.504 on the 3D residual U-Net
respectively. We have conducted the Wilcoxon test for 3D models to correlate
the relationship between predicted and actual stroke volume. For
reproducibility, the code and model weights are made publicly available:
https://github.com/prantik-pdeb/BeSt-LeS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MICCAI BrainLes 2023 (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextPSG: Panoptic Scene Graph Generation from Textual Descriptions <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyang Zhao, Yikang Shen, Zhenfang Chen, Mingyu Ding, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoptic Scene Graph has recently been proposed for comprehensive scene
understanding. However, previous works adopt a fully-supervised learning
manner, requiring large amounts of pixel-wise densely-annotated data, which is
always tedious and expensive to obtain. To address this limitation, we study a
new problem of Panoptic Scene Graph Generation from Purely Textual Descriptions
(Caption-to-PSG). The key idea is to leverage the large collection of free
image-caption data on the Web alone to generate panoptic scene graphs. The
problem is very challenging for three constraints: 1) no location priors; 2) no
explicit links between visual regions and textual entities; and 3) no
pre-defined concept sets. To tackle this problem, we propose a new framework
TextPSG consisting of four modules, i.e., a region grouper, an entity grounder,
a segment merger, and a label generator, with several novel techniques. The
region grouper first groups image pixels into different segments and the entity
grounder then aligns visual segments with language entities based on the
textual description of the segment being referred to. The grounding results can
thus serve as pseudo labels enabling the segment merger to learn the segment
similarity as well as guiding the label generator to learn object semantics and
relation predicates, resulting in a fine-grained structured scene
understanding. Our framework is effective, significantly outperforming the
baselines and achieving strong out-of-distribution robustness. We perform
comprehensive ablation studies to corroborate the effectiveness of our design
choices and provide an in-depth analysis to highlight future directions. Our
code, data, and results are available on our project page:
https://vis-www.cs.umass.edu/TextPSG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Pathology at Health System Scale -- Self-Supervised
  Foundation Models from Three Billion Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Campanella, Ricky Kwan, Eugene Fluder, Jennifer Zeng, Aryeh Stock, Brandon Veremis, Alexandros D. Polydorides, Cyrus Hedvat, Adam Schoenfeld, Chad Vanderbilt, Patricia Kovatch, Carlos Cordon-Cardo, Thomas J. Fuchs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in self-supervised learning have enabled the use of
large unlabeled datasets to train visual foundation models that can generalize
to a variety of downstream tasks. While this training paradigm is well suited
for the medical domain where annotations are scarce, large-scale pre-training
in the medical domain, and in particular pathology, has not been extensively
studied. Previous work in self-supervised learning in pathology has leveraged
smaller datasets for both pre-training and evaluating downstream performance.
The aim of this project is to train the largest academic foundation model and
benchmark the most prominent self-supervised learning algorithms by
pre-training and evaluating downstream performance on large clinical pathology
datasets. We collected the largest pathology dataset to date, consisting of
over 3 billion images from over 423 thousand microscopy slides. We compared
pre-training of visual transformer models using the masked autoencoder (MAE)
and DINO algorithms. We evaluated performance on six clinically relevant tasks
from three anatomic sites and two institutions: breast cancer detection,
inflammatory bowel disease detection, breast cancer estrogen receptor
prediction, lung adenocarcinoma EGFR mutation prediction, and lung cancer
immunotherapy response prediction. Our results demonstrate that pre-training on
pathology data is beneficial for downstream performance compared to
pre-training on natural images. Additionally, the DINO algorithm achieved
better generalization performance across all tasks tested. The presented
results signify a phase change in computational pathology research, paving the
way into a new era of more performant models based on large-scale, parallel
pre-training at the billion-image scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Forgery-based Deepfake Detection using Fine-Grained Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aakash Varma Nadimpalli, Ajita Rattani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial forgery by deepfakes has caused major security risks and raised severe
societal concerns. As a countermeasure, a number of deepfake detection methods
have been proposed. Most of them model deepfake detection as a binary
classification problem using a backbone convolutional neural network (CNN)
architecture pretrained for the task. These CNN-based methods have demonstrated
very high efficacy in deepfake detection with the Area under the Curve (AUC) as
high as $0.99$. However, the performance of these methods degrades
significantly when evaluated across datasets and deepfake manipulation
techniques. This draws our attention towards learning more subtle, local, and
discriminative features for deepfake detection. In this paper, we formulate
deepfake detection as a fine-grained classification problem and propose a new
fine-grained solution to it. Specifically, our method is based on learning
subtle and generalizable features by effectively suppressing background noise
and learning discriminative features at various scales for deepfake detection.
Through extensive experimental validation, we demonstrate the superiority of
our method over the published research in cross-dataset and cross-manipulation
generalization of deepfake detectors for the majority of the experimental
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Synthetic Data for Medical Vision-Language Pre-training:
  Bypassing the Need for Real Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che Liu, Anand Shah, Wenjia Bai, Rossella Arcucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical Vision-Language Pre-training (VLP) learns representations jointly
from medical images and paired radiology reports. It typically requires
large-scale paired image-text datasets to achieve effective pre-training for
both the image encoder and text encoder. The advent of text-guided generative
models raises a compelling question: Can VLP be implemented solely with
synthetic images generated from genuine radiology reports, thereby mitigating
the need for extensively pairing and curating image-text datasets? In this
work, we scrutinize this very question by examining the feasibility and
effectiveness of employing synthetic images for medical VLP. We replace real
medical images with their synthetic equivalents, generated from authentic
medical reports. Utilizing three state-of-the-art VLP algorithms, we
exclusively train on these synthetic samples. Our empirical evaluation across
three subsequent tasks, namely image classification, semantic segmentation and
object detection, reveals that the performance achieved through synthetic data
is on par with or even exceeds that obtained with real images. As a pioneering
contribution to this domain, we introduce a large-scale synthetic medical image
dataset, paired with anonymized real radiology reports. This alleviates the
need of sharing medical images, which are not easy to curate and share in
practice. The code and the dataset will be made publicly available upon paper
acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pre-Trained Masked Image Model for Mobile Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Dutt Sharma, Anukriti Singh, Pratap Tokekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  2D top-down maps are commonly used for the navigation and exploration of
mobile robots through unknown areas. Typically, the robot builds the navigation
maps incrementally from local observations using onboard sensors. Recent works
have shown that predicting the structural patterns in the environment through
learning-based approaches can greatly enhance task efficiency. While many such
works build task-specific networks using limited datasets, we show that the
existing foundational vision networks can accomplish the same without any
fine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street
images, to present novel applications for field-of-view expansion, single-agent
topological exploration, and multi-agent exploration for indoor mapping, across
different input modalities. Our work motivates the use of foundational vision
models for generalized structure prediction-driven applications, especially in
the dearth of training data. For more qualitative results see
https://raaslab.org/projects/MIM4Robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Hsuan Chu, Adam W. Harley, Pavel Tokmakov, Achal Dave, Leonidas Guibas, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object tracking is central to robot perception and scene understanding.
Tracking-by-detection has long been a dominant paradigm for object tracking of
specific object categories. Recently, large-scale pre-trained models have shown
promising advances in detecting and segmenting objects and parts in 2D static
images in the wild. This begs the question: can we re-purpose these large-scale
pre-trained static image models for open-vocabulary video tracking? In this
paper, we re-purpose an open-vocabulary detector, segmenter, and dense optical
flow estimator, into a model that tracks and segments objects of any category
in 2D videos. Our method predicts object and part tracks with associated
language descriptions in monocular videos, rebuilding the pipeline of Tractor
with modern large pre-trained models for static image detection and
segmentation: we detect open-vocabulary object instances and propagate their
boxes from frame to frame using a flow-based motion model, refine the
propagated boxes with the box regression module of the visual detector, and
prompt an open-world segmenter with the refined box to segment the objects. We
decide the termination of an object track based on the objectness score of the
propagated boxes, as well as forward-backward optical flow consistency. We
re-identify objects across occlusions using deep feature matching. We show that
our model achieves strong performance on multiple established video object
segmentation and tracking benchmarks, and can produce reasonable tracks in
manipulation data. In particular, our model outperforms previous
state-of-the-art in UVO and BURST, benchmarks for open-world object tracking
and segmentation, despite never being explicitly trained for tracking. We hope
that our approach can serve as a simple and extensible framework for future
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page available at https://wenhsuanchu.github.io/ovtracktor/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Neural Radiance Fields for Uncertainty-Aware Visual
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Chen, Weirong Chen, Rui Wang, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a promising fashion for visual localization, scene coordinate regression
(SCR) has seen tremendous progress in the past decade. Most recent methods
usually adopt neural networks to learn the mapping from image pixels to 3D
scene coordinates, which requires a vast amount of annotated training data. We
propose to leverage Neural Radiance Fields (NeRF) to generate training samples
for SCR. Despite NeRF's efficiency in rendering, many of the rendered data are
polluted by artifacts or only contain minimal information gain, which can
hinder the regression accuracy or bring unnecessary computational costs with
redundant data. These challenges are addressed in three folds in this paper:
(1) A NeRF is designed to separately predict uncertainties for the rendered
color and depth images, which reveal data reliability at the pixel level. (2)
SCR is formulated as deep evidential learning with epistemic uncertainty, which
is used to evaluate information gain and scene coordinate quality. (3) Based on
the three arts of uncertainties, a novel view selection policy is formed that
significantly improves data efficiency. Experiments on public datasets
demonstrate that our method could select the samples that bring the most
information gain and promote the performance with the highest efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Distillation Can Be Like Vodka: Distilling More Times For Better
  Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuxi Chen, Yu Yang, Zhangyang Wang, Baharan Mirzasoleiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation aims to minimize the time and memory needed for training
deep networks on large datasets, by creating a small set of synthetic images
that has a similar generalization performance to that of the full dataset.
However, current dataset distillation techniques fall short, showing a notable
performance gap when compared to training on the original data. In this work,
we are the first to argue that using just one synthetic subset for distillation
will not yield optimal generalization performance. This is because the training
dynamics of deep networks drastically change during the training. Hence,
multiple synthetic subsets are required to capture the training dynamics at
different phases of training. To address this issue, we propose Progressive
Dataset Distillation (PDD). PDD synthesizes multiple small sets of synthetic
images, each conditioned on the previous sets, and trains the model on the
cumulative union of these subsets without requiring additional training time.
Our extensive experiments show that PDD can effectively improve the performance
of existing dataset distillation methods by up to 4.3%. In addition, our method
for the first time enable generating considerably larger synthetic datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ObjectComposer: Consistent Generation of Multiple Objects Without
  Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alec Helbling, Evan Montoya, Duen Horng Chau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent text-to-image generative models can generate high-fidelity images from
text prompts. However, these models struggle to consistently generate the same
objects in different contexts with the same appearance. Consistent object
generation is important to many downstream tasks like generating comic book
illustrations with consistent characters and setting. Numerous approaches
attempt to solve this problem by extending the vocabulary of diffusion models
through fine-tuning. However, even lightweight fine-tuning approaches can be
prohibitively expensive to run at scale and in real-time. We introduce a method
called ObjectComposer for generating compositions of multiple objects that
resemble user-specified images. Our approach is training-free, leveraging the
abilities of preexisting models. We build upon the recent BLIP-Diffusion model,
which can generate images of single objects specified by reference images.
ObjectComposer enables the consistent generation of compositions containing
multiple specific objects simultaneously, all without modifying the weights of
the underlying models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Interpretability of Part-Prototype Based Classifiers: A Human
  Centric Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omid Davoodi, Shayan Mohammadizadehsamakosh, Majid Komeili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Part-prototype networks have recently become methods of interest as an
interpretable alternative to many of the current black-box image classifiers.
However, the interpretability of these methods from the perspective of human
users has not been sufficiently explored. In this work, we have devised a
framework for evaluating the interpretability of part-prototype-based models
from a human perspective. The proposed framework consists of three actionable
metrics and experiments. To demonstrate the usefulness of our framework, we
performed an extensive set of experiments using Amazon Mechanical Turk. They
not only show the capability of our framework in assessing the interpretability
of various part-prototype-based models, but they also are, to the best of our
knowledge, the most comprehensive work on evaluating such methods in a unified
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Intended for submission to Nature Scientific Reports</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing the robustness of modern no-reference image- and video-quality
  metrics to adversarial attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia Antsiferova, Khaled Abud, Aleksandr Gushchin, Sergey Lavrushkin, Ekaterina Shumitskaya, Maksim Velikanov, Dmitriy Vatolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays neural-network-based image- and video-quality metrics show better
performance compared to traditional methods. However, they also became more
vulnerable to adversarial attacks that increase metrics' scores without
improving visual quality. The existing benchmarks of quality metrics compare
their performance in terms of correlation with subjective quality and
calculation time. However, the adversarial robustness of image-quality metrics
is also an area worth researching. In this paper, we analyse modern metrics'
robustness to different adversarial attacks. We adopted adversarial attacks
from computer vision tasks and compared attacks' efficiency against 15
no-reference image/video-quality metrics. Some metrics showed high resistance
to adversarial attacks which makes their usage in benchmarks safer than
vulnerable metrics. The benchmark accepts new metrics submissions for
researchers who want to make their metrics more robust to attacks or to find
such metrics for their needs. Try our benchmark using pip install
robustness-benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end Evaluation of Practical Video Analytics Systems for Face
  Detection and Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praneet Singh, Edward J. Delp, Amy R. Reibman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practical video analytics systems that are deployed in bandwidth constrained
environments like autonomous vehicles perform computer vision tasks such as
face detection and recognition. In an end-to-end face analytics system, inputs
are first compressed using popular video codecs like HEVC and then passed onto
modules that perform face detection, alignment, and recognition sequentially.
Typically, the modules of these systems are evaluated independently using
task-specific imbalanced datasets that can misconstrue performance estimates.
In this paper, we perform a thorough end-to-end evaluation of a face analytics
system using a driving-specific dataset, which enables meaningful
interpretations. We demonstrate how independent task evaluations, dataset
imbalances, and inconsistent annotations can lead to incorrect system
performance estimates. We propose strategies to create balanced evaluation
subsets of our dataset and to make its annotations consistent across multiple
analytics tasks and scenarios. We then evaluate the end-to-end system
performance sequentially to account for task interdependencies. Our experiments
show that our approach provides consistent, accurate, and interpretable
estimates of the system's performance which is critical for real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Autonomous Vehicles and Machines 2023 Conference, IS&T
  Electronic Imaging (EI) Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Transfer Learning with 4th Gen Intel Xeon Processors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshmi Arunachalam, Fahim Mohammad, Vrushabh H. Sanghavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore how transfer learning, coupled with Intel Xeon,
specifically 4th Gen Intel Xeon scalable processor, defies the conventional
belief that training is primarily GPU-dependent. We present a case study where
we achieved near state-of-the-art accuracy for image classification on a
publicly available Image Classification TensorFlow dataset using Intel Advanced
Matrix Extensions(AMX) and distributed training with Horovod.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-supervised Object-Centric Learning for Videos <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Görkay Aydemir, Weidi Xie, Fatma Güney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised multi-object segmentation has shown impressive results on images
by utilizing powerful semantics learned from self-supervised pretraining. An
additional modality such as depth or motion is often used to facilitate the
segmentation in video sequences. However, the performance improvements observed
in synthetic sequences, which rely on the robustness of an additional cue, do
not translate to more challenging real-world scenarios. In this paper, we
propose the first fully unsupervised method for segmenting multiple objects in
real-world sequences. Our object-centric learning framework spatially binds
objects to slots on each frame and then relates these slots across frames. From
these temporally-aware slots, the training objective is to reconstruct the
middle frame in a high-level semantic feature space. We propose a masking
strategy by dropping a significant portion of tokens in the feature space for
efficiency and regularization. Additionally, we address over-clustering by
merging slots based on similarity. Our method can successfully segment multiple
instances of complex and high-variety classes in YouTube videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distillation Improves Visual Place Recognition for Low-Quality Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anbang Yang, Yao Wang, John-Ross Rizzo, Chen Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The shift to online computing for real-time visual localization often
requires streaming query images/videos to a server for visual place recognition
(VPR), where fast video transmission may result in reduced resolution or
increased quantization. This compromises the quality of global image
descriptors, leading to decreased VPR performance. To improve the low recall
rate for low-quality query images, we present a simple yet effective method
that uses high-quality queries only during training to distill better feature
representations for deep-learning-based VPR, such as NetVLAD. Specifically, we
use mean squared error (MSE) loss between the global descriptors of queries
with different qualities, and inter-channel correlation knowledge distillation
(ICKD) loss over their corresponding intermediate features. We validate our
approach using the both Pittsburgh 250k dataset and our own indoor dataset with
varying quantization levels. By fine-tuning NetVLAD parameters with our
distillation-augmented losses, we achieve notable VPR recall-rate improvements
over low-quality queries, as demonstrated in our extensive experimental
results. We believe this work not only pushes forward the VPR research but also
provides valuable insights for applications needing dependable place
recognition under resource-limited conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating stereotypical biases in text to image generative systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piero Esposito, Parmida Atighehchian, Anastasis Germanidis, Deepti Ghadiyaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art generative text-to-image models are known to exhibit social
biases and over-represent certain groups like people of perceived lighter skin
tones and men in their outcomes. In this work, we propose a method to mitigate
such biases and ensure that the outcomes are fair across different groups of
people. We do this by finetuning text-to-image models on synthetic data that
varies in perceived skin tones and genders constructed from diverse text
prompts. These text prompts are constructed from multiplicative combinations of
ethnicities, genders, professions, age groups, and so on, resulting in diverse
synthetic data. Our diversity finetuned (DFT) model improves the group fairness
metric by 150% for perceived skin tone and 97.7% for perceived gender. Compared
to baselines, DFT models generate more people with perceived darker skin tone
and more women. To foster open research, we will release all text prompts and
code to generate training images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image
  Diffusion Models with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Lian, Boyi Li, Adam Yala, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-image diffusion models have yielded impressive
results in generating realistic and diverse images. However, these models still
struggle with complex prompts, such as those that involve numeracy and spatial
reasoning. This work proposes to enhance prompt understanding capabilities in
diffusion models. Our method leverages a pretrained large language model (LLM)
for grounded generation in a novel two-stage process. In the first stage, the
LLM generates a scene layout that comprises captioned bounding boxes from a
given prompt describing the desired image. In the second stage, a novel
controller guides an off-the-shelf diffusion model for layout-grounded image
generation. Both stages utilize existing pretrained models without additional
model parameter optimization. Our method significantly outperforms the base
diffusion model and several strong baselines in accurately generating images
according to prompts that require various capabilities, doubling the generation
accuracy across four tasks on average. Furthermore, our method enables
instruction-based multi-round scene specification and can handle prompts in
languages not supported by the underlying diffusion model. We anticipate that
our method will unleash users' creativity by accurately following more complex
prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://llm-grounded-diffusion.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Review of Deep Learning-based Approaches for Deepfake Content
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leandro A. Passos, Danilo Jodas, Kelton A. P. da Costa, Luis A. Souza Júnior, Douglas Rodrigues, Javier Del Ser, David Camacho, João Paulo Papa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in deep learning generative models have raised concerns
as they can create highly convincing counterfeit images and videos. This poses
a threat to people's integrity and can lead to social instability. To address
this issue, there is a pressing need to develop new computational models that
can efficiently detect forged content and alert users to potential image and
video manipulations. This paper presents a comprehensive review of recent
studies for deepfake content detection using deep learning-based approaches. We
aim to broaden the state-of-the-art research by systematically reviewing the
different categories of fake content detection. Furthermore, we report the
advantages and drawbacks of the examined works and future directions towards
the issues and shortcomings still unsolved on deepfake detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly
  Supervised Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianle Chen, Zheda Mai, Ruiwen Li, Wei-lun Chao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly supervised semantic segmentation (WSSS) aims to bypass the need for
laborious pixel-level annotation by using only image-level annotation. Most
existing methods rely on Class Activation Maps (CAM) to derive pixel-level
pseudo-labels and use them to train a fully supervised semantic segmentation
model. Although these pseudo-labels are class-aware, indicating the coarse
regions for particular classes, they are not object-aware and fail to delineate
accurate object boundaries. To address this, we introduce a simple yet
effective method harnessing the Segment Anything Model (SAM), a class-agnostic
foundation model capable of producing fine-grained instance masks of objects,
parts, and subparts. We use CAM pseudo-labels as cues to select and combine SAM
masks, resulting in high-quality pseudo-labels that are both class-aware and
object-aware. Our approach is highly versatile and can be easily integrated
into existing WSSS methods without any modification. Despite its simplicity,
our approach shows consistent gain over the state-of-the-art WSSS methods on
both PASCAL VOC and MS-COCO datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tianle Chen and Zheda Mai contributed equally to this work. Our code
  is available at \url{https://github.com/cskyl/SAM_WSSS}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-spectral Entropy Constrained Neural Compression of Solar Imagery <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zafari, Atefeh Khoshkhahtinat, Piyush M. Mehta, Nasser M. Nasrabadi, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Missions studying the dynamic behaviour of the Sun are defined to capture
multi-spectral images of the sun and transmit them to the ground station in a
daily basis. To make transmission efficient and feasible, image compression
systems need to be exploited. Recently successful end-to-end optimized neural
network-based image compression systems have shown great potential to be used
in an ad-hoc manner. In this work we have proposed a transformer-based
multi-spectral neural image compressor to efficiently capture redundancies both
intra/inter-wavelength. To unleash the locality of window-based self attention
mechanism, we propose an inter-window aggregated token multi head self
attention. Additionally to make the neural compressor autoencoder shift
invariant, a randomly shifted window attention mechanism is used which makes
the transformer blocks insensitive to translations in their input domain. We
demonstrate that the proposed approach not only outperforms the conventional
compression algorithms but also it is able to better decorrelates images along
the multiple wavelengths compared to single spectral compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE 22$^{nd}$ International Conference on Machine
  Learning and Applications 2023 (ICMLA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive
  Impairment in older adults using facial videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05292v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05292v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Sun, Hiroko H. Dodge, Mohammad H. Mahoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep machine learning models including Convolutional Neural Networks (CNN)
have been successful in the detection of Mild Cognitive Impairment (MCI) using
medical images, questionnaires, and videos. This paper proposes a novel
Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to
distinguish MCI from those with normal cognition by analyzing facial features.
The data comes from the I-CONECT, a behavioral intervention trial aimed at
improving cognitive function by providing frequent video chats. MC-ViViT
extracts spatiotemporal features of videos in one branch and augments
representations by the MC module. The I-CONECT dataset is challenging as the
dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which
impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy
and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE
loss to address the imbalanced problem. Our experimental results on the
I-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a
high accuracy of 90.63% accuracy on some of the interview videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 tables, 7 figures, 9 equations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessor360: Multi-sequence Network for Blind Omnidirectional Image
  Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10983v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10983v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhe Wu, Shuwei Shi, Haoming Cai, Mingdeng Cao, Jing Xiao, Yinqiang Zheng, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind Omnidirectional Image Quality Assessment (BOIQA) aims to objectively
assess the human perceptual quality of omnidirectional images (ODIs) without
relying on pristine-quality image information. It is becoming more significant
with the increasing advancement of virtual reality (VR) technology. However,
the quality assessment of ODIs is severely hampered by the fact that the
existing BOIQA pipeline lacks the modeling of the observer's browsing process.
To tackle this issue, we propose a novel multi-sequence network for BOIQA
called Assessor360, which is derived from the realistic multi-assessor ODI
quality assessment procedure. Specifically, we propose a generalized Recursive
Probability Sampling (RPS) method for the BOIQA task, combining content and
details information to generate multiple pseudo-viewport sequences from a given
starting point. Additionally, we design a Multi-scale Feature Aggregation (MFA)
module with a Distortion-aware Block (DAB) to fuse distorted and semantic
features of each viewport. We also devise Temporal Modeling Module (TMM) to
learn the viewport transition in the temporal domain. Extensive experimental
results demonstrate that Assessor360 outperforms state-of-the-art methods on
multiple OIQA datasets. The code and models are available at
https://github.com/TianheWu/Assessor360.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Alzheimer's Progression Modeling using Cross-Domain
  Self-Supervised Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08559v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08559v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saba Dadsetan, Mohsen Hejrati, Shandong Wu, Somaye Hashemifar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing successful artificial intelligence systems in practice depends on
both robust deep learning models and large, high-quality data. However,
acquiring and labeling data can be prohibitively expensive and time-consuming
in many real-world applications, such as clinical disease models.
Self-supervised learning has demonstrated great potential in increasing model
accuracy and robustness in small data regimes. In addition, many clinical
imaging and disease modeling applications rely heavily on regression of
continuous quantities. However, the applicability of self-supervised learning
for these medical-imaging regression tasks has not been extensively studied. In
this study, we develop a cross-domain self-supervised learning approach for
disease prognostic modeling as a regression problem using medical images as
input. We demonstrate that self-supervised pretraining can improve the
prediction of Alzheimer's Disease progression from brain MRI. We also show that
pretraining on extended (but not labeled) brain MRI data outperforms
pretraining on natural images. We further observe that the highest performance
is achieved when both natural images and extended brain-MRI data are used for
pretraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been published at the Transactions on Machine Learning
  Research (TMLR) journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-based methods coupled with specific distributional distances for
  adversarial attack detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dwight Nwaigwe, Lucrezia Carboni, Martial Mermillod, Sophie Achard, Michel Dojat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial neural networks are prone to being fooled by carefully perturbed
inputs which cause an egregious misclassification. These \textit{adversarial}
attacks have been the focus of extensive research. Likewise, there has been an
abundance of research in ways to detect and defend against them. We introduce a
novel approach of detection and interpretation of adversarial attacks from a
graph perspective. For an input image, we compute an associated sparse graph
using the layer-wise relevance propagation algorithm \cite{bach15}.
Specifically, we only keep edges of the neural network with the highest
relevance values. Three quantities are then computed from the graph which are
then compared against those computed from the training set. The result of the
comparison is a classification of the image as benign or adversarial. To make
the comparison, two classification methods are introduced: 1) an explicit
formula based on Wasserstein distance applied to the degree of node and 2) a
logistic regression. Both classification methods produce strong results which
lead us to believe that a graph-based interpretation of adversarial attacks is
valuable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published in Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Smoothing and Thresholding Image Segmentation Framework
  with Weighted Anisotropic-Isotropic Total Variation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.10115v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.10115v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Bui, Yifei Lou, Fredrick Park, Jack Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we design an efficient, multi-stage image segmentation
framework that incorporates a weighted difference of anisotropic and isotropic
total variation (AITV). The segmentation framework generally consists of two
stages: smoothing and thresholding, thus referred to as SaT. In the first
stage, a smoothed image is obtained by an AITV-regularized Mumford-Shah (MS)
model, which can be solved efficiently by the alternating direction method of
multipliers (ADMM) with a closed-form solution of a proximal operator of the
$\ell_1 -\alpha \ell_2$ regularizer. Convergence of the ADMM algorithm is
analyzed. In the second stage, we threshold the smoothed image by $K$-means
clustering to obtain the final segmentation result. Numerical experiments
demonstrate that the proposed segmentation framework is versatile for both
grayscale and color images, efficient in producing high-quality segmentation
results within a few seconds, and robust to input images that are corrupted
with noise, blur, or both. We compare the AITV method with its original convex
TV and nonconvex TV$^p (0<p<1)$ counterparts, showcasing the qualitative and
quantitative advantages of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to Springer CAMC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morphologically-Aware Consensus Computation via Heuristics-based
  IterATive Optimization (MACCHIatO) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitri Hamzaoui, Sarah Montagne, Raphaële Renard-Penna, Nicholas Ayache, Hervé Delingette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraction of consensus segmentations from several binary or
probabilistic masks is important to solve various tasks such as the analysis of
inter-rater variability or the fusion of several neural network outputs. One of
the most widely used methods to obtain such a consensus segmentation is the
STAPLE algorithm. In this paper, we first demonstrate that the output of that
algorithm is heavily impacted by the background size of images and the choice
of the prior. We then propose a new method to construct a binary or a
probabilistic consensus segmentation based on the Fr\'{e}chet means of
carefully chosen distances which makes it totally independent of the image
background size. We provide a heuristic approach to optimize this criterion
such that a voxel's class is fully determined by its voxel-wise distance to the
different masks, the connected component it belongs to and the group of raters
who segmented it. We compared extensively our method on several datasets with
the STAPLE method and the naive segmentation averaging method, showing that it
leads to binary consensus masks of intermediate size between Majority Voting
and STAPLE and to different posterior probabilities than Mask Averaging and
STAPLE methods. Our code is available at
https://gitlab.inria.fr/dhamzaou/jaccardmap .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2023:013</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Action Recognition with Attentive Semantic Units <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Chen, Dapeng Chen, Ruijin Liu, Hao Li, Wei Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-Language Models (VLMs) have significantly advanced action video
recognition. Supervised by the semantics of action labels, recent works adapt
the visual branch of VLMs to learn video representations. Despite the
effectiveness proved by these works, we believe that the potential of VLMs has
yet to be fully harnessed. In light of this, we exploit the semantic units (SU)
hiding behind the action labels and leverage their correlations with
fine-grained items in frames for more accurate action recognition. SUs are
entities extracted from the language descriptions of the entire action set,
including body parts, objects, scenes, and motions. To further enhance the
alignments between visual contents and the SUs, we introduce a multi-region
module (MRA) to the visual branch of the VLM. The MRA allows the perception of
region-aware visual features beyond the original global feature. Our method
adaptively attends to and selects relevant SUs with visual features of frames.
With a cross-modal decoder, the selected SUs serve to decode spatiotemporal
video representations. In summary, the SUs as the medium can boost
discriminative ability and transferability. Specifically, in fully-supervised
learning, our method achieved 87.8% top-1 accuracy on Kinetics-400. In K=2
few-shot experiments, our method surpassed the previous state-of-the-art by
+7.1% and +15.0% on HMDB-51 and UCF-101, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05873v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05873v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhili Liu, Kai Chen, Yifan Zhang, Jianhua Han, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning diffusion models through personalized datasets is an acknowledged
method for improving generation quality across downstream tasks, which,
however, often inadvertently generates unintended concepts such as watermarks
and QR codes, attributed to the limitations in image sources and collecting
methods within specific downstream tasks. Existing solutions suffer from
eliminating these unintentionally learned implicit concepts, primarily due to
the dependency on the model's ability to recognize concepts that it actually
cannot discern. In this work, we introduce Geom-Erasing, a novel approach that
successfully removes the implicit concepts with either an additional accessible
classifier or detector model to encode geometric information of these concepts
into text domain. Moreover, we propose Implicit Concept, a novel image-text
dataset imbued with three implicit concepts (i.e., watermarks, QR codes, and
text) for training and evaluation. Experimental results demonstrate that
Geom-Erasing not only identifies but also proficiently eradicates implicit
concepts, revealing a significant improvement over the existing methods. The
integration of geometric information marks a substantial progression in the
precise removal of implicit concepts in diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EndoMapper dataset of complete calibrated endoscopy procedures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.14240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.14240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Azagra, Carlos Sostres, Ángel Ferrandez, Luis Riazuelo, Clara Tomasini, Oscar León Barbed, Javier Morlana, David Recasens, Victor M. Batlle, Juan J. Gómez-Rodríguez, Richard Elvira, Julia López, Cristina Oriol, Javier Civera, Juan D. Tardós, Ana Cristina Murillo, Angel Lanas, José M. M. Montiel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-assisted systems are becoming broadly used in medicine. In
endoscopy, most research focuses on the automatic detection of polyps or other
pathologies, but localization and navigation of the endoscope are completely
performed manually by physicians. To broaden this research and bring spatial
Artificial Intelligence to endoscopies, data from complete procedures is
needed. This paper introduces the Endomapper dataset, the first collection of
complete endoscopy sequences acquired during regular medical practice, making
secondary use of medical data. Its main purpose is to facilitate the
development and evaluation of Visual Simultaneous Localization and Mapping
(VSLAM) methods in real endoscopy data. The dataset contains more than 24 hours
of video. It is the first endoscopic dataset that includes endoscope
calibration as well as the original calibration videos. Meta-data and
annotations associated with the dataset vary from the anatomical landmarks,
procedure labeling, segmentations, reconstructions, simulated sequences with
ground truth and same patient procedures. The software used in this paper is
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 14 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation and Analysis of Hallucination in Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15126v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15126v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, Jitao Sang, Haoyu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have recently achieved remarkable
success. However, LVLMs are still plagued by the hallucination problem, which
limits the practicality in many scenarios. Hallucination refers to the
information of LVLMs' responses that does not exist in the visual input, which
poses potential risks of substantial consequences. There has been limited work
studying hallucination evaluation in LVLMs. In this paper, we propose
Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based
hallucination evaluation framework. HaELM achieves an approximate 95%
performance comparable to ChatGPT and has additional advantages including low
cost, reproducibility, privacy preservation and local deployment. Leveraging
the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we
analyze the factors contributing to hallucination in LVLMs and offer helpful
suggestions to mitigate the hallucination problem. Our training data and human
annotation hallucination data will be made public soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-driven Open-Vocabulary Keypoint Detection for Animal Body and
  Face 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Kaipeng Zhang, Lumin Xu, Shenqi Lai, Wenqi Shao, Nanning Zheng, Ping Luo, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches for image-based keypoint detection on animal (including
human) body and face are limited to specific keypoints and species. We address
the limitation by proposing the Open-Vocabulary Keypoint Detection (OVKD) task.
It aims to use text prompts to localize arbitrary keypoints of any species. To
accomplish this objective, we propose Open-Vocabulary Keypoint Detection with
Semantic-feature Matching (KDSM), which utilizes both vision and language
models to harness the relationship between text and vision and thus achieve
keypoint detection through associating text prompt with relevant keypoint
features. Additionally, KDSM integrates domain distribution matrix matching and
some special designs to reinforce the relationship between language and vision,
thereby improving the model's generalizability and performance. Extensive
experiments show that our proposed components bring significant performance
improvements, and our overall method achieves impressive results in OVKD.
Remarkably, our method outperforms the state-of-the-art few-shot keypoint
detection methods using a zero-shot fashion. We will make the source code
publicly accessible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-label Image Classification using Adaptive Graph Convolutional
  Networks: from a Single Domain to Multiple Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Indel Pal Singh, Enjie Ghorbel, Oyebade Oyedotun, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an adaptive graph-based approach for multi-label image
classification. Graph-based methods have been largely exploited in the field of
multi-label classification, given their ability to model label correlations.
Specifically, their effectiveness has been proven not only when considering a
single domain but also when taking into account multiple domains. However, the
topology of the used graph is not optimal as it is pre-defined heuristically.
In addition, consecutive Graph Convolutional Network (GCN) aggregations tend to
destroy the feature similarity. To overcome these issues, an architecture for
learning the graph connectivity in an end-to-end fashion is introduced. This is
done by integrating an attention-based mechanism and a similarity-preserving
strategy. The proposed framework is then extended to multiple domains using an
adversarial training scheme. Numerous experiments are reported on well-known
single-domain and multi-domain benchmarks. The results demonstrate that our
approach achieves competitive results in terms of mean Average Precision (mAP)
and model size as compared to the state-of-the-art. The code will be made
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructDET: Diversifying Referring Object Detection with Generalized
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song, Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, Yibing Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose InstructDET, a data-centric method for referring object detection
(ROD) that localizes target objects based on user instructions. While deriving
from referring expressions (REC), the instructions we leverage are greatly
diversified to encompass common user intentions related to object detection.
For one image, we produce tremendous instructions that refer to every single
object and different combinations of multiple objects. Each instruction and its
corresponding object bounding boxes (bbxs) constitute one training data pair.
In order to encompass common detection expressions, we involve emerging
vision-language model (VLM) and large language model (LLM) to generate
instructions guided by text prompts and object bbxs, as the generalizations of
foundation models are effective to produce human-like expressions (e.g.,
describing object property, category, and relationship). We name our
constructed dataset as InDET. It contains images, bbxs and generalized
instructions that are from foundation models. Our InDET is developed from
existing REC datasets and object detection datasets, with the expanding
potential that any image with object bbxs can be incorporated through using our
InstructDET method. By using our InDET dataset, we show that a conventional ROD
model surpasses existing methods on standard REC datasets and our InDET test
set. Our data-centric method InstructDET, with automatic data expansion by
leveraging foundation models, directs a promising field that ROD can be greatly
diversified to execute common object detection instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages (include appendix), technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Prostate Cancer Diagnosis with Deep Learning: A Study using
  mpMRI Segmentation and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil B. Gavade, Neel Kanwal, Priyanka A. Gavade, Rajendra Nerli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prostate cancer (PCa) is a severe disease among men globally. It is important
to identify PCa early and make a precise diagnosis for effective treatment. For
PCa diagnosis, Multi-parametric magnetic resonance imaging (mpMRI) emerged as
an invaluable imaging modality that offers a precise anatomical view of the
prostate gland and its tissue structure. Deep learning (DL) models can enhance
existing clinical systems and improve patient care by locating regions of
interest for physicians. Recently, DL techniques have been employed to develop
a pipeline for segmenting and classifying different cancer types. These studies
show that DL can be used to increase diagnostic precision and give objective
results without variability. This work uses well-known DL models for the
classification and segmentation of mpMRI images to detect PCa. Our
implementation involves four pipelines; Semantic DeepSegNet with ResNet50,
DeepSegNet with recurrent neural network (RNN), U-Net with RNN, and U-Net with
a long short-term memory (LSTM). Each segmentation model is paired with a
different classifier to evaluate the performance using different metrics. The
results of our experiments show that the pipeline that uses the combination of
U-Net and the LSTM model outperforms all other combinations, excelling in both
segmentation and classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CISCON-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two-stage MR Image Segmentation Method for Brain Tumors based on
  Attention Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhu, Jiawei Jiang, Lin Lu, Jin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal magnetic resonance imaging (MRI) can reveal different patterns of
human tissue and is crucial for clinical diagnosis. However, limited by cost,
noise and manual labeling, obtaining diverse and reliable multimodal MR images
remains a challenge. For the same lesion, different MRI manifestations have
great differences in background information, coarse positioning and fine
structure. In order to obtain better generation and segmentation performance, a
coordination-spatial attention generation adversarial network (CASP-GAN) based
on the cycle-consistent generative adversarial network (CycleGAN) is proposed.
The performance of the generator is optimized by introducing the Coordinate
Attention (CA) module and the Spatial Attention (SA) module. The two modules
can make full use of the captured location information, accurately locating the
interested region, and enhancing the generator model network structure. The
ability to extract the structure information and the detailed information of
the original medical image can help generate the desired image with higher
quality. There exist some problems in the original CycleGAN that the training
time is long, the parameter amount is too large, and it is difficult to
converge. In response to this problem, we introduce the Coordinate Attention
(CA) module to replace the Res Block to reduce the number of parameters, and
cooperate with the spatial information extraction network above to strengthen
the information extraction ability. On the basis of CASP-GAN, an attentional
generative cross-modality segmentation (AGCMS) method is further proposed. This
method inputs the modalities generated by CASP-GAN and the real modalities into
the segmentation network for brain tumor segmentation. Experimental results
show that CASP-GAN outperforms CycleGAN and some state-of-the-art methods in
PSNR, SSMI and RMSE in most tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Some contributing authors are not signed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Invariant Learning via Probability of Sufficient and Necessary Causes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12559v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12559v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyue Yang, Zhen Fang, Yonggang Zhang, Yali Du, Furui Liu, Jean-Francois Ton, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) generalization is indispensable for learning models
in the wild, where testing distribution typically unknown and different from
the training. Recent methods derived from causality have shown great potential
in achieving OOD generalization. However, existing methods mainly focus on the
invariance property of causes, while largely overlooking the property of
\textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but
insufficient cause (feature) is invariant to distribution shift, yet it may not
have required accuracy. By contrast, a sufficient yet unnecessary cause
(feature) tends to fit specific data well but may have a risk of adapting to a
new domain. To capture the information of sufficient and necessary causes, we
employ a classical concept, the probability of sufficiency and necessary causes
(PNS), which indicates the probability of whether one is the necessary and
sufficient cause. To associate PNS with OOD generalization, we propose PNS risk
and formulate an algorithm to learn representation with a high PNS value. We
theoretically analyze and prove the generalizability of the PNS risk.
Experiments on both synthetic and real-world benchmarks demonstrate the
effectiveness of the proposed method. The details of the implementation can be
found at the GitHub repository: https://github.com/ymy4323460/CaSN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and
  Subtyping in Whole Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Fourkioti, Matt De Vries, Chris Bakal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual examination of tissue biopsy sections is fundamental for cancer
diagnosis, with pathologists analyzing sections at multiple magnifications to
discern tumor cells and their subtypes. However, existing attention-based
multiple instance learning (MIL) models, used for analyzing Whole Slide Images
(WSIs) in cancer diagnostics, often overlook the contextual information of
tumor and neighboring tiles, leading to misclassifications. To address this, we
propose the Context-Aware Multiple Instance Learning (CAMIL) architecture.
CAMIL incorporates neighbor-constrained attention to consider dependencies
among tiles within a WSI and integrates contextual constraints as prior
knowledge into the MIL model. We evaluated CAMIL on subtyping non-small cell
lung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16) metastasis,
achieving test AUCs of 0.959\% and 0.975\%, respectively, outperforming other
state-of-the-art methods. Additionally, CAMIL enhances model interpretability
by identifying regions of high diagnostic value.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Med-Query: Steerable Parsing of 9-DoF Medical Anatomies with Query
  Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Guo, Jianfeng Zhang, Ke Yan, Le Lu, Minfeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic parsing of human anatomies at instance-level from 3D computed
tomography (CT) scans is a prerequisite step for many clinical applications.
The presence of pathologies, broken structures or limited field-of-view (FOV)
all can make anatomy parsing algorithms vulnerable. In this work, we explore
how to exploit and conduct the prosperous detection-then-segmentation paradigm
in 3D medical data, and propose a steerable, robust, and efficient computing
framework for detection, identification, and segmentation of anatomies in CT
scans. Considering complicated shapes, sizes and orientations of anatomies,
without lose of generality, we present the nine degrees-of-freedom (9-DoF) pose
estimation solution in full 3D space using a novel single-stage,
non-hierarchical forward representation. Our whole framework is executed in a
steerable manner where any anatomy of interest can be directly retrieved to
further boost the inference efficiency. We have validated the proposed method
on three medical imaging parsing tasks of ribs, spine, and abdominal organs.
For rib parsing, CT scans have been annotated at the rib instance-level for
quantitative evaluation, similarly for spine vertebrae and abdominal organs.
Extensive experiments on 9-DoF box detection and rib instance segmentation
demonstrate the effectiveness of our framework (with the identification rate of
97.0% and the segmentation Dice score of 90.9%) in high efficiency, compared
favorably against several strong baselines (e.g., CenterNet, FCOS, and
nnU-Net). For spine identification and segmentation, our method achieves a new
state-of-the-art result on the public CTSpine1K dataset. Last, we report highly
competitive results in multi-organ segmentation at FLARE22 competition. Our
annotations, code and models will be made publicly available at:
https://github.com/alibaba-damo-academy/Med_Query.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>updated version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morphological Image Analysis and Feature Extraction for Reasoning with
  AI-based Defect Detection and Classification Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11643v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11643v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Zhang, Georgina Cosma, Sarah Bugby, Axel Finke, Jason Watkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the use of artificial intelligent (AI) models becomes more prevalent in
industries such as engineering and manufacturing, it is essential that these
models provide transparent reasoning behind their predictions. This paper
proposes the AI-Reasoner, which extracts the morphological characteristics of
defects (DefChars) from images and utilises decision trees to reason with the
DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e.
charts) and textual explanations to provide insights into outputs made by
masked-based defect detection and classification models. It also provides
effective mitigation strategies to enhance data pre-processing and overall
model performance. The AI-Reasoner was tested on explaining the outputs of an
IE Mask R-CNN model using a set of 366 images containing defects. The results
demonstrated its effectiveness in explaining the IE Mask R-CNN model's
predictions. Overall, the proposed AI-Reasoner provides a solution for
improving the performance of AI models in industrial applications that require
defect analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 5 tables; accepted in 2023 IEEE symposium series
  on computational intelligence (SSCI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DDF-HO: Hand-Held Object Reconstruction via Conditional Directed
  Distance Field <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyangguang Zhang, Yan Di, Ruida Zhang, Guangyao Zhai, Fabian Manhardt, Federico Tombari, Xiangyang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing hand-held objects from a single RGB image is an important and
challenging problem. Existing works utilizing Signed Distance Fields (SDF)
reveal limitations in comprehensively capturing the complex hand-object
interactions, since SDF is only reliable within the proximity of the target,
and hence, infeasible to simultaneously encode local hand and object cues. To
address this issue, we propose DDF-HO, a novel approach leveraging Directed
Distance Field (DDF) as the shape representation. Unlike SDF, DDF maps a ray in
3D space, consisting of an origin and a direction, to corresponding DDF values,
including a binary visibility signal determining whether the ray intersects the
objects and a distance value measuring the distance from origin to target in
the given direction. We randomly sample multiple rays and collect local to
global geometric features for them by introducing a novel 2D ray-based feature
aggregation scheme and a 3D intersection-aware hand pose embedding, combining
2D-3D features to model hand-object interactions. Extensive experiments on
synthetic and real-world datasets demonstrate that DDF-HO consistently
outperforms all baseline methods by a large margin, especially under Chamfer
Distance, with about $80\%$ leap forward. Codes are available at
\url{https://github.com/ZhangCYG/DDFHO}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with
  Bird's Eye View based Appearance and Motion Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07336v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07336v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Zhou, Jiapeng Xie, Yan Pan, Jiajie Wu, Chuanzhao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying moving objects is an essential capability for autonomous systems,
as it provides critical information for pose estimation, navigation, collision
avoidance, and static map construction. In this paper, we present MotionBEV, a
fast and accurate framework for LiDAR moving object segmentation, which
segments moving objects with appearance and motion features in the bird's eye
view (BEV) domain. Our approach converts 3D LiDAR scans into a 2D polar BEV
representation to improve computational efficiency. Specifically, we learn
appearance features with a simplified PointNet and compute motion features
through the height differences of consecutive frames of point clouds projected
onto vertical columns in the polar BEV coordinate system. We employ a
dual-branch network bridged by the Appearance-Motion Co-attention Module (AMCM)
to adaptively fuse the spatio-temporal information from appearance and motion
features. Our approach achieves state-of-the-art performance on the
SemanticKITTI-MOS benchmark. Furthermore, to demonstrate the practical
effectiveness of our method, we provide a LiDAR-MOS dataset recorded by a
solid-state LiDAR, which features non-repetitive scanning patterns and a small
field of view.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Breast Cancer Classification Using Transfer ResNet with
  Lightweight Attention Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13150v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13150v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suxing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable results of deep learning in breast cancer image
classification, challenges such as data imbalance and interpretability still
exist and require cross-domain knowledge and collaboration among medical
experts. In this study, we propose a dual-activated lightweight attention
ResNet50 module method-based breast cancer classification method that
effectively addresses challenges such as data imbalance and interpretability.
Our model fuses a pre-trained deep ResNet50 and a lightweight attention
mechanism to accomplish classification by embedding an attention module in
layer 4 of ResNet50 and adding two fully connected layers. For the fully
connected network design, we employ both Leaky ReLU and ReLU activation
functions. On medical histopathology datasets, our model outperforms
conventional models, visual transformers, and large models in terms of
precision, accuracy, recall, F1 score, and GMean. In particular, the model
demonstrates significant robustness and broad applicability when dealing with
the unbalanced breast cancer dataset. Our model is tested on 40X, 100X, 200X,
and 400X images and achieves accuracies of 98.5%, 98.7%, 97.9%, and 94.3%,
respectively. Through an in-depth analysis of loss and accuracy, as well as
Grad-CAM analysis, we comprehensively assessed the model performance and gained
perspective on its training process. In the later stages of training, the
validated losses and accuracies change minimally, showing that the model avoids
overfitting and exhibits good generalization ability. Overall, this study
provides an effective solution for breast cancer image classification with
practical applica
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures,6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Inductive Bias: Knowledge Distillation Beyond Model
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gousia Habib, Tausifa Jan Saleem, Brejesh Lall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of computer vision, Vision Transformers (ViTs)
offer the tantalizing prospect of unified information processing across visual
and textual domains. But due to the lack of inherent inductive biases in ViTs,
they require enormous amount of data for training. To make their applications
practical, we introduce an innovative ensemble-based distillation approach
distilling inductive bias from complementary lightweight teacher models. Prior
systems relied solely on convolution-based teaching. However, this method
incorporates an ensemble of light teachers with different architectural
tendencies, such as convolution and involution, to instruct the student
transformer jointly. Because of these unique inductive biases, instructors can
accumulate a wide range of knowledge, even from readily identifiable stored
datasets, which leads to enhanced student performance. Our proposed framework
also involves precomputing and storing logits in advance, essentially the
unnormalized predictions of the model. This optimization can accelerate the
distillation process by eliminating the need for repeated forward passes during
knowledge distillation, significantly reducing the computational burden and
enhancing efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Environment-biased Feature Ranking for Novelty Detection Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Smeu, Elena Burceanu, Emanuela Haller, Andrei Liviu Nicolicioiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the problem of robust novelty detection, where we aim to detect
novelties in terms of semantic content while being invariant to changes in
other, irrelevant factors. Specifically, we operate in a setup with multiple
environments, where we determine the set of features that are associated more
with the environments, rather than to the content relevant for the task. Thus,
we propose a method that starts with a pretrained embedding and a multi-env
setup and manages to rank the features based on their environment-focus. First,
we compute a per-feature score based on the feature distribution variance
between envs. Next, we show that by dropping the highly scored ones, we manage
to remove spurious correlations and improve the overall performance by up to
6%, both in covariance and sub-population shift cases, both for a real and a
synthetic benchmark, that we introduce for this task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The updated, long version of the paper is available at
  arXiv:2310.03738</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stroke-based Neural Painting and Stylization with Dynamically Predicted
  Painting Region <span class="chip">ACM MM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Hu, Ran Yi, Haokun Zhu, Liang Liu, Jinlong Peng, Yabiao Wang, Chengjie Wang, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke-based rendering aims to recreate an image with a set of strokes. Most
existing methods render complex images using an uniform-block-dividing
strategy, which leads to boundary inconsistency artifacts. To solve the
problem, we propose Compositional Neural Painter, a novel stroke-based
rendering framework which dynamically predicts the next painting region based
on the current canvas, instead of dividing the image plane uniformly into
painting regions. We start from an empty canvas and divide the painting process
into several steps. At each step, a compositor network trained with a phasic RL
strategy first predicts the next painting region, then a painter network
trained with a WGAN discriminator predicts stroke parameters, and a stroke
renderer paints the strokes onto the painting region of the current canvas.
Moreover, we extend our method to stroke-based style transfer with a novel
differentiable distance transform loss, which helps preserve the structure of
the input image during stroke-based stylization. Extensive experiments show our
model outperforms the existing models in both stroke-based neural painting and
stroke-based stylization. Code is available at
https://github.com/sjtuplayer/Compositional_Neural_Painter
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM MM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-training with dual uncertainty for semi-supervised medical image
  segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanhong Qiu, Haitao Gan, Ming Shi, Zhongwei Huang, Zhi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of semi-supervised medical image segmentation, the shortage of
labeled data is the fundamental problem. How to effectively learn image
features from unlabeled images to improve segmentation accuracy is the main
research direction in this field. Traditional self-training methods can
partially solve the problem of insufficient labeled data by generating pseudo
labels for iterative training. However, noise generated due to the model's
uncertainty during training directly affects the segmentation results.
Therefore, we added sample-level and pixel-level uncertainty to stabilize the
training process based on the self-training framework. Specifically, we saved
several moments of the model during pre-training, and used the difference
between their predictions on unlabeled samples as the sample-level uncertainty
estimate for that sample. Then, we gradually add unlabeled samples from easy to
hard during training. At the same time, we added a decoder with different
upsampling methods to the segmentation network and used the difference between
the outputs of the two decoders as pixel-level uncertainty. In short, we
selectively retrained unlabeled samples and assigned pixel-level uncertainty to
pseudo labels to optimize the self-training process. We compared the
segmentation results of our model with five semi-supervised approaches on the
public 2017 ACDC dataset and 2018 Prostate dataset. Our proposed method
achieves better segmentation performance on both datasets under the same
settings, demonstrating its effectiveness, robustness, and potential
transferability to other medical image segmentation tasks. Keywords: Medical
image segmentation, semi-supervised learning, self-training, uncertainty
estimation
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompt-based Ingredient-Oriented All-in-One Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.03063v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.03063v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Gao, Depeng Dang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image restoration aims to recover the high-quality images from their degraded
observations. Since most existing methods have been dedicated into single
degradation removal, they may not yield optimal results on other types of
degradations, which do not satisfy the applications in real world scenarios. In
this paper, we propose a novel data ingredient-oriented approach that leverages
prompt-based learning to enable a single model to efficiently tackle multiple
image degradation tasks. Specifically, we utilize a encoder to capture features
and introduce prompts with degradation-specific information to guide the
decoder in adaptively recovering images affected by various degradations. In
order to model the local invariant properties and non-local information for
high-quality image restoration, we combined CNNs operations and Transformers.
Simultaneously, we made several key designs in the Transformer blocks
(multi-head rearranged attention with prompts and simple-gate feed-forward
network) to reduce computational requirements and selectively determines what
information should be persevered to facilitate efficient recovery of
potentially sharp images. Furthermore, we incorporate a feature fusion
mechanism further explores the multi-scale information to improve the
aggregated features. The resulting tightly interlinked hierarchy architecture,
named as CAPTNet, extensive experiments demonstrate that our method performs
competitively to the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Engineering: A Top-Down Approach to AI Transparency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01405v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01405v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we identify and characterize the emerging area of
representation engineering (RepE), an approach to enhancing the transparency of
AI systems that draws on insights from cognitive neuroscience. RepE places
population-level representations, rather than neurons or circuits, at the
center of analysis, equipping us with novel methods for monitoring and
manipulating high-level cognitive phenomena in deep neural networks (DNNs). We
provide baselines and an initial analysis of RepE techniques, showing that they
offer simple yet effective solutions for improving our understanding and
control of large language models. We showcase how these methods can provide
traction on a wide range of safety-relevant problems, including honesty,
harmlessness, power-seeking, and more, demonstrating the promise of top-down
transparency research. We hope that this work catalyzes further exploration of
RepE and fosters advancements in the transparency and safety of AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/andyzoujm/representation-engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-End Chess Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Athanasios Masouris, Jan van Gemert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chess recognition refers to the task of identifying the chess pieces
configuration from a chessboard image. Contrary to the predominant approach
that aims to solve this task through the pipeline of chessboard detection,
square localization, and piece classification, we rely on the power of deep
learning models and introduce two novel methodologies to circumvent this
pipeline and directly predict the chessboard configuration from the entire
image. In doing so, we avoid the inherent error accumulation of the sequential
approaches and the need for intermediate annotations. Furthermore, we introduce
a new dataset, Chess Recognition Dataset (ChessReD), specifically designed for
chess recognition that consists of 10,800 images and their corresponding
annotations. In contrast to existing synthetic datasets with limited angles,
this dataset comprises a diverse collection of real images of chess formations
captured from various angles using smartphone cameras; a sensor choice made to
ensure real-world applicability. We use this dataset to both train our model
and evaluate and compare its performance to that of the current
state-of-the-art. Our approach in chess recognition on this new benchmark
dataset outperforms related approaches, achieving a board recognition accuracy
of 15.26% ($\approx$7x better than the current state-of-the-art).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AliasNet: Alias Artefact Suppression Network for Accelerated
  Phase-Encode MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlon E. Bran Lorenzana, Shekhar S. Chandra, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse reconstruction is an important aspect of MRI, helping to reduce
acquisition time and improve spatial-temporal resolution. Popular methods are
based mostly on compressed sensing (CS), which relies on the random sampling of
k-space to produce incoherent (noise-like) artefacts. Due to hardware
constraints, 1D Cartesian phase-encode under-sampling schemes are popular for
2D CS-MRI. However, 1D under-sampling limits 2D incoherence between
measurements, yielding structured aliasing artefacts (ghosts) that may be
difficult to remove assuming a 2D sparsity model. Reconstruction algorithms
typically deploy direction-insensitive 2D regularisation for these
direction-associated artefacts. Recognising that phase-encode artefacts can be
separated into contiguous 1D signals, we develop two decoupling techniques that
enable explicit 1D regularisation and leverage the excellent 1D incoherence
characteristics. We also derive a combined 1D + 2D reconstruction technique
that takes advantage of spatial relationships within the image. Experiments
conducted on retrospectively under-sampled brain and knee data demonstrate that
combination of the proposed 1D AliasNet modules with existing 2D deep learned
(DL) recovery techniques leads to an improvement in image quality. We also find
AliasNet enables a superior scaling of performance compared to increasing the
size of the original 2D network layers. AliasNet therefore improves the
regularisation of aliasing artefacts arising from phase-encode under-sampling,
by tailoring the network architecture to account for their expected appearance.
The proposed 1D + 2D approach is compatible with any existing 2D DL recovery
technique deployed for this application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expanding Small-Scale Datasets with Guided Imagination <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13976v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13976v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Daquan Zhou, Bryan Hooi, Kai Wang, Jiashi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The power of DNNs relies heavily on the quantity and quality of training
data. However, collecting and annotating data on a large scale is often
expensive and time-consuming. To address this issue, we explore a new task,
termed dataset expansion, aimed at expanding a ready-to-use small dataset by
automatically creating new labeled samples. To this end, we present a Guided
Imagination Framework (GIF) that leverages cutting-edge generative models like
DALL-E2 and Stable Diffusion (SD) to "imagine" and create informative new data
from the input seed data. Specifically, GIF conducts data imagination by
optimizing the latent features of the seed data in the semantically meaningful
space of the prior model, resulting in the creation of photo-realistic images
with new content. To guide the imagination towards creating informative samples
for model training, we introduce two key criteria, i.e., class-maintained
information boosting and sample diversity promotion. These criteria are
verified to be essential for effective dataset expansion: GIF-SD obtains 13.5%
higher model accuracy on natural image datasets than unguided expansion with
SD. With these essential criteria, GIF successfully expands small datasets in
various scenarios, boosting model accuracy by 36.9% on average over six natural
image datasets and by 13.5% on average over three medical datasets. The source
code is available at https://github.com/Vanint/DatasetExpansion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023. Source code: https://github.com/Vanint/DatasetExpansion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deakin RF-Sensing: Experiments on Correlated Knowledge Distillation for
  Monitoring Human Postures with Radios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14829v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14829v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiva Raj Pokhrel, Jonathan Kua, Deol Satish, Philip Williams, Arkady Zaslavsky, Seng W. Loke, Jinho Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose and develop a simple experimental testbed to study
the feasibility of a novel idea by coupling radio frequency (RF) sensing
technology with Correlated Knowledge Distillation (CKD) theory towards
designing lightweight, near real-time and precise human pose monitoring
systems. The proposed CKD framework transfers and fuses pose knowledge from a
robust "Teacher" model to a parameterized "Student" model, which can be a
promising technique for obtaining accurate yet lightweight pose estimates. To
assure its efficacy, we implemented CKD for distilling logits in our integrated
Software Defined Radio (SDR)-based experimental setup and investigated the
RF-visual signal correlation. Our CKD-RF sensing technique is characterized by
two modes - a camera-fed Teacher Class Network (e.g., images, videos) with an
SDR-fed Student Class Network (e.g., RF signals). Specifically, our CKD model
trains a dual multi-branch teacher and student network by distilling and fusing
knowledge bases. The resulting CKD models are then subsequently used to
identify the multimodal correlation and teach the student branch in reverse.
Instead of simply aggregating their learnings, CKD training comprised multiple
parallel transformations with the two domains, i.e., visual images and RF
signals. Once trained, our CKD model can efficiently preserve privacy and
utilize the multimodal correlated logits from the two different neural networks
for estimating poses without using visual signals/video frames (by using only
the RF signals).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing stability and plasticity in continual learning: the
  readout-decomposition of activation change (RDAC) framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Anthes, Sushrut Thorat, Peter König, Tim C. Kietzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) algorithms strive to acquire new knowledge while
preserving prior information. However, this stability-plasticity trade-off
remains a central challenge. This paper introduces a framework that dissects
this trade-off, offering valuable insights into CL algorithms. The
Readout-Decomposition of Activation Change (RDAC) framework first addresses the
stability-plasticity dilemma and its relation to catastrophic forgetting. It
relates learning-induced activation changes in the range of prior readouts to
the degree of stability and changes in the null space to the degree of
plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the
framework clarifies the stability-plasticity trade-offs of the popular
regularization algorithms Synaptic intelligence (SI), Elastic-weight
consolidation (EWC), and learning without Forgetting (LwF), and replay-based
algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay
preserved stability and plasticity, while SI, EWC, and LwF traded off
plasticity for stability. The inability of the regularization algorithms to
maintain plasticity was linked to them restricting the change of activations in
the null space of the prior readout. Additionally, for one-hidden-layer linear
neural networks, we derived a gradient decomposition algorithm to restrict
activation change only in the range of the prior readouts, to maintain high
stability while not further sacrificing plasticity. Results demonstrate that
the algorithm maintained stability without significant plasticity loss. The
RDAC framework informs the behavior of existing CL algorithms and paves the way
for novel CL approaches. Finally, it sheds light on the connection between
learning-induced activation/representation changes and the stability-plasticity
dilemma, also offering insights into representational drift in biological
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Adversarial Training Cost with Gradient Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09464v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09464v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huihui Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have achieved state-of-the-art performances in various
domains, while they are vulnerable to the inputs with well-crafted but small
perturbations, which are named after adversarial examples (AEs). Among many
strategies to improve the model robustness against AEs, Projected Gradient
Descent (PGD) based adversarial training is one of the most effective methods.
Unfortunately, the prohibitive computational overhead of generating strong
enough AEs, due to the maximization of the loss function, sometimes makes the
regular PGD adversarial training impractical when using larger and more
complicated models. In this paper, we propose that the adversarial loss can be
approximated by the partial sum of Taylor series. Furthermore, we approximate
the gradient of adversarial loss and propose a new and efficient adversarial
training method, adversarial training with gradient approximation (GAAT), to
reduce the cost of building up robust models. Additionally, extensive
experiments demonstrate that this efficiency improvement can be achieved
without any or with very little loss in accuracy on natural and adversarial
examples, which show that our proposed method saves up to 60\% of the training
time with comparable model test accuracy on MNIST, CIFAR-10 and CIFAR-100
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The experiments are insufficient, later will be updated. Withraw this
  manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Persis: A Persian Font Recognition Pipeline Using Convolutional Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Mohammadian, Neda Maleki, Tobias Olsson, Fredrik Ahlgren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What happens if we encounter a suitable font for our design work but do not
know its name? Visual Font Recognition (VFR) systems are used to identify the
font typeface in an image. These systems can assist graphic designers in
identifying fonts used in images. A VFR system also aids in improving the speed
and accuracy of Optical Character Recognition (OCR) systems. In this paper, we
introduce the first publicly available datasets in the field of Persian font
recognition and employ Convolutional Neural Networks (CNN) to address this
problem. The results show that the proposed pipeline obtained 78.0% top-1
accuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the
KAFD dataset. Furthermore, the average time spent in the entire pipeline for
one sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU,
respectively. We conclude that CNN methods can be used to recognize Persian
fonts without the need for additional pre-processing steps such as feature
extraction, binarization, normalization, etc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Fine-Tune Vision Models with SGD 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ananya Kumar, Ruoqi Shen, Sebastien Bubeck, Suriya Gunasekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SGD and AdamW are the two most used optimizers for fine-tuning large neural
networks in computer vision. When the two methods perform the same, SGD is
preferable because it uses less memory (12 bytes/parameter with momentum and 8
bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite
of downstream tasks, especially those with distribution shifts, we find that
fine-tuning with AdamW performs substantially better than SGD on modern Vision
Transformer and ConvNeXt models. We find that large gaps in performance between
SGD and AdamW occur when the fine-tuning gradients in the first "embedding"
layer are much larger than in the rest of the model. Our analysis suggests an
easy fix that works consistently across datasets and models: freezing the
embedding layer (less than 1% of the parameters) leads to SGD with or without
momentum performing slightly better than AdamW while using less memory (e.g.,
on ViT-L, SGD uses 33% less GPU memory). Our insights result in
state-of-the-art accuracies on five popular distribution shift benchmarks:
WILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-grained Audio-Visual Joint Representations for Multimodal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual large language models (LLM) have drawn significant attention,
yet the fine-grained combination of both input streams is rather
under-explored, which is challenging but necessary for LLMs to understand
general video inputs. To this end, a fine-grained audio-visual joint
representation (FAVOR) learning framework for multimodal LLMs is proposed in
this paper, which extends a text-based LLM to simultaneously perceive speech
and audio events in the audio input stream and images or videos in the visual
input stream, at the frame level. To fuse the audio and visual feature streams
into joint representations and to align the joint space with the LLM input
embedding space, we propose a causal Q-Former structure with a causal attention
module to enhance the capture of causal relations of the audio-visual frames
across time. An audio-visual evaluation benchmark (AVEB) is also proposed which
comprises six representative single-modal tasks with five cross-modal tasks
reflecting audio-visual co-reasoning abilities. While achieving competitive
single-modal performance on audio, speech and image tasks in AVEB, FAVOR
achieved over 20% accuracy improvements on the video question-answering task
when fine-grained information or temporal causal reasoning is required. FAVOR,
in addition, demonstrated remarkable video comprehension and reasoning
abilities on tasks that are unprecedented by other multimodal LLMs. An
interactive demo of FAVOR is available at
https://github.com/BriansIDP/AudioVisualLLM.git, and the training code and
model checkpoints will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyperLips: Hyper Control Lips with High Resolution Decoder for Talking
  Face Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaosen Chen, Yu Yao, Zhiqiang Li, Wei Wang, Yanru Zhang, Han Yang, Xuming Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking face generation has a wide range of potential applications in the
field of virtual digital humans. However, rendering high-fidelity facial video
while ensuring lip synchronization is still a challenge for existing
audio-driven talking face generation approaches. To address this issue, we
propose HyperLips, a two-stage framework consisting of a hypernetwork for
controlling lips and a high-resolution decoder for rendering high-fidelity
faces. In the first stage, we construct a base face generation network that
uses the hypernetwork to control the encoding latent code of the visual face
information over audio. First, FaceEncoder is used to obtain latent code by
extracting features from the visual face information taken from the video
source containing the face frame.Then, HyperConv, which weighting parameters
are updated by HyperNet with the audio features as input, will modify the
latent code to synchronize the lip movement with the audio. Finally,
FaceDecoder will decode the modified and synchronized latent code into visual
face content. In the second stage, we obtain higher quality face videos through
a high-resolution decoder. To further improve the quality of face generation,
we trained a high-resolution decoder, HRDecoder, using face images and detected
sketches generated from the first stage as input.Extensive quantitative and
qualitative experiments show that our method outperforms state-of-the-art work
with more realistic, high-fidelity, and lip synchronization. Project page:
https://semchan.github.io/HyperLips/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12493v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12493v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilin Lu, Yanzhu Liu, Adams Wai-Kin Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven diffusion models have exhibited impressive generative
capabilities, enabling various image editing tasks. In this paper, we propose
TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the
power of text-driven diffusion models for cross-domain image-guided
composition. This task aims to seamlessly integrate user-provided objects into
a specific visual context. Current diffusion-based methods often involve costly
instance-based optimization or finetuning of pretrained models on customized
datasets, which can potentially undermine their rich prior. In contrast,
TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain
image-guided composition without requiring additional training, finetuning, or
optimization. Moreover, we introduce the exceptional prompt, which contains no
information, to facilitate text-driven diffusion models in accurately inverting
real images into latent representations, forming the basis for compositing. Our
experiments show that equipping Stable Diffusion with the exceptional prompt
outperforms state-of-the-art inversion methods on various datasets (CelebA-HQ,
COCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile
visual domains. Code is available at https://github.com/Shilin-LU/TF-ICON
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-Teller: Enhancing Cross-Modal Generation with Fusion and
  Decoupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haogeng Liu, Qihang Fan, Tingkai Liu, Linjie Yang, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Video-Teller, a video-language foundation model that
leverages multi-modal fusion and fine-grained modality alignment to
significantly enhance the video-to-text generation task. Video-Teller boosts
the training efficiency by utilizing frozen pretrained vision and language
modules. It capitalizes on the robust linguistic capabilities of large language
models, enabling the generation of both concise and elaborate video
descriptions. To effectively integrate visual and auditory information,
Video-Teller builds upon the image-based BLIP-2 model and introduces a cascaded
Q-Former which fuses information across frames and ASR texts. To better guide
video summarization, we introduce a fine-grained modality alignment objective,
where the cascaded Q-Former's output embedding is trained to align with the
caption/summary embedding created by a pretrained text auto-encoder.
Experimental results demonstrate the efficacy of our proposed video-language
foundation model in accurately comprehending videos and generating coherent and
precise language descriptions. It is worth noting that the fine-grained
alignment enhances the model's capabilities (4% improvement of CIDEr score on
MSR-VTT) with only 13% extra parameters in training and zero additional cost in
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Discovery of 3D Hierarchical Structure with Generative
  Diffusion Features <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurislam Tursynbek, Marc Niethammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by recent findings that generative diffusion models learn
semantically meaningful representations, we use them to discover the intrinsic
hierarchical structure in biomedical 3D images using unsupervised segmentation.
We show that features of diffusion models from different stages of a
U-Net-based ladder-like architecture capture different hierarchy levels in 3D
biomedical images. We design three losses to train a predictive unsupervised
segmentation network that encourages the decomposition of 3D volumes into
meaningful nested subvolumes that represent a hierarchy. First, we pretrain 3D
diffusion models and use the consistency of their features across subvolumes.
Second, we use the visual consistency between subvolumes. Third, we use the
invariance to photometric augmentations as a regularizer. Our models achieve
better performance than prior unsupervised structure discovery approaches on
challenging biologically-inspired synthetic datasets and on a real-world brain
tumor MRI dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current deep networks are very data-hungry and benefit from training on
largescale datasets, which are often time-consuming to collect and annotate. By
contrast, synthetic data can be generated infinitely using generative models
such as DALL-E and diffusion models, with minimal effort and cost. In this
paper, we present DatasetDM, a generic dataset generation model that can
produce diverse synthetic images and the corresponding high-quality perception
annotations (e.g., segmentation masks, and depth). Our method builds upon the
pre-trained diffusion model and extends text-guided image synthesis to
perception data generation. We show that the rich latent code of the diffusion
model can be effectively decoded as accurate perception annotations using a
decoder module. Training the decoder only needs less than 1% (around 100
images) manually labeled images, enabling the generation of an infinitely large
annotated dataset. Then these synthetic data can be used for training various
perception models for downstream tasks. To showcase the power of the proposed
approach, we generate datasets with rich dense pixel-wise labels for a wide
range of downstream tasks, including semantic segmentation, instance
segmentation, and depth estimation. Notably, it achieves 1) state-of-the-art
results on semantic segmentation and instance segmentation; 2) significantly
more robust on domain generalization than using the real data alone; and
state-of-the-art results in zero-shot segmentation setting; and 3) flexibility
for efficient application and novel task composition (e.g., image editing). The
project website and code can be found at
https://weijiawu.github.io/DatasetDM_page/ and
https://github.com/showlab/DatasetDM, respectively
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ General Rotation Invariance Learning for Point Clouds via Weight-Feature
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09907v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09907v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Xie, Yibo Yang, Wenxiao Wang, Binbin Lin, Deng Cai, Xiaofei He, Ronghua Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to 2D images, 3D point clouds are much more sensitive to rotations.
We expect the point features describing certain patterns to keep invariant to
the rotation transformation. There are many recent SOTA works dedicated to
rotation-invariant learning for 3D point clouds. However, current
rotation-invariant methods lack generalizability on the point clouds in the
open scenes due to the reliance on the global distribution, \ie the global
scene and backgrounds. Considering that the output activation is a function of
the pattern and its orientation, we need to eliminate the effect of the
orientation.In this paper, inspired by the idea that the network weights can be
considered a set of points distributed in the same 3D space as the input
points, we propose Weight-Feature Alignment (WFA) to construct a local
Invariant Reference Frame (IRF) via aligning the features with the principal
axes of the network weights. Our WFA algorithm provides a general solution for
the point clouds of all scenes. WFA ensures the model achieves the target that
the response activity is a necessary and sufficient condition of the pattern
matching degree. Practically, we perform experiments on the point clouds of
both single objects and open large-range scenes. The results suggest that our
method almost bridges the gap between rotation invariance learning and normal
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ You Only Look at Once for Real-time and Generic Multi-Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayuan Wang, Q. M. Jonathan Wu, Ning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High precision, lightweight, and real-time responsiveness are three essential
requirements for implementing autonomous driving. In this study, we present an
adaptive, real-time, and lightweight multi-task model designed to concurrently
address object detection, drivable area segmentation, and lane line
segmentation tasks. Specifically, we developed an end-to-end multi-task model
with a unified and streamlined segmentation structure. We introduced a
learnable parameter that adaptively concatenate features in segmentation necks,
using the same loss function for all segmentation tasks. This eliminates the
need for customizations and enhances the model's generalization capabilities.
We also introduced a segmentation head composed only of a series of
convolutional layers, which reduces the inference time. We achieved competitive
results on the BDD100k dataset, particularly in visualization outcomes. The
performance results show a mAP50 of 81.1% for object detection, a mIoU of 91.0%
for drivable area segmentation, and an IoU of 28.8% for lane line segmentation.
Additionally, we introduced real-world scenarios to evaluate our model's
performance in a real scene, which significantly outperforms competitors. This
demonstrates that our model not only exhibits competitive performance but is
also more flexible and faster than existing multi-task models. The source codes
and pre-trained models are released at
https://github.com/JiayuanWang-JW/YOLOv8-multi-task
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Domain-Aware Detection Head with Prompt Tuning <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05718v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05718v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Li, Rui Zhang, Hantao Yao, Xinkai Song, Yifan Hao, Yongwei Zhao, Ling Li, Yunji Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptive object detection (DAOD) aims to generalize detectors trained
on an annotated source domain to an unlabelled target domain. However, existing
methods focus on reducing the domain bias of the detection backbone by
inferring a discriminative visual encoder, while ignoring the domain bias in
the detection head. Inspired by the high generalization of vision-language
models (VLMs), applying a VLM as the robust detection backbone following a
domain-aware detection head is a reasonable way to learn the discriminative
detector for each domain, rather than reducing the domain bias in traditional
methods. To achieve the above issue, we thus propose a novel DAOD framework
named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies
the learnable domain-adaptive prompt to generate the dynamic detection head for
each domain. Formally, the domain-adaptive prompt consists of the
domain-invariant tokens, domain-specific tokens, and the domain-related textual
description along with the class label. Furthermore, two constraints between
the source and target domains are applied to ensure that the domain-adaptive
prompt can capture the domains-shared and domain-specific knowledge. A prompt
ensemble strategy is also proposed to reduce the effect of prompt disturbance.
Comprehensive experiments over multiple cross-domain adaptation tasks
demonstrate that using the domain-adaptive prompt can produce an effectively
domain-related detection head for boosting domain-adaptive object detection.
Our code is available at https://github.com/Therock90421/DA-Pro.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 1st Place Solution of Egocentric 3D Hand Pose Estimation Challenge 2023
  Technical Report:A Concise Pipeline for Egocentric Hand Pose Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhishan Zhou, Zhi Lv, Shihao Zhou, Minqiang Zou, Tong Wu, Mochen Yu, Yao Tang, Jiajun Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report introduce our work on Egocentric 3D Hand Pose Estimation
workshop. Using AssemblyHands, this challenge focuses on egocentric 3D hand
pose estimation from a single-view image. In the competition, we adopt ViT
based backbones and a simple regressor for 3D keypoints prediction, which
provides strong model baselines. We noticed that Hand-objects occlusions and
self-occlusions lead to performance degradation, thus proposed a non-model
method to merge multi-view results in the post-process stage. Moreover, We
utilized test time augmentation and model ensemble to make further improvement.
We also found that public dataset and rational preprocess are beneficial. Our
method achieved 12.21mm MPJPE on test dataset, achieve the first place in
Egocentric 3D Hand Pose Estimation challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\texttt{GradICON}$: Approximate Diffeomorphisms via Gradient Inverse
  Consistency <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05897v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05897v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Tian, Hastings Greer, François-Xavier Vialard, Roland Kwitt, Raúl San José Estépar, Richard Jarrett Rushmore, Nikolaos Makris, Sylvain Bouix, Marc Niethammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach to learning regular spatial transformations between
image pairs in the context of medical image registration. Contrary to
optimization-based registration techniques and many modern learning-based
methods, we do not directly penalize transformation irregularities but instead
promote transformation regularity via an inverse consistency penalty. We use a
neural network to predict a map between a source and a target image as well as
the map when swapping the source and target images. Different from existing
approaches, we compose these two resulting maps and regularize deviations of
the $\bf{Jacobian}$ of this composition from the identity matrix. This
regularizer -- $\texttt{GradICON}$ -- results in much better convergence when
training registration models compared to promoting inverse consistency of the
composition of maps directly while retaining the desirable implicit
regularization effects of the latter. We achieve state-of-the-art registration
performance on a variety of real-world medical image datasets using a single
set of hyperparameters and a single non-dataset-specific training protocol.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 16 figures, CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion
  for Polyp Localization in Colonoscopy Images <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju-Hyeon Nam, Seo-Hyeong Park, Nur Suriza Syazwany, Yerim Jung, Yu-Han Im, Sang-Chul Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polyp segmentation is crucial for preventing colorectal cancer a common type
of cancer. Deep learning has been used to segment polyps automatically, which
reduces the risk of misdiagnosis. Localizing small polyps in colonoscopy images
is challenging because of its complex characteristics, such as color,
occlusion, and various shapes of polyps. To address this challenge, a novel
frequency-based fully convolutional neural network, Multi-Frequency Feature
Fusion Polyp Segmentation Network (M3FPolypSegNet) was proposed to decompose
the input image into low/high/full-frequency components to use the
characteristics of each component. We used three independent multi-frequency
encoders to map multiple input images into a high-dimensional feature space. In
the Frequency-ASPP Scalable Attention Module (F-ASPP SAM), ASPP was applied
between each frequency component to preserve scale information. Subsequently,
scalable attention was applied to emphasize polyp regions in a high-dimensional
feature space. Finally, we designed three multi-task learning (i.e., region,
edge, and distance) in four decoder blocks to learn the structural
characteristics of the region. The proposed model outperformed various
segmentation models with performance gains of 6.92% and 7.52% on average for
all metrics on CVC-ClinicDB and BKAI-IGH-NeoPolyp, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5pages. 2023 IEEE International Conference on Image Processing
  (ICIP). IEEE, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symmetrical Linguistic Feature Distillation with CLIP for Scene Text
  Recognition <span class="chip">ACM MM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiao Wang, Hongtao Xie, Yuxin Wang, Jianjun Xu, Boqiang Zhang, Yongdong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the potential of the Contrastive Language-Image
Pretraining (CLIP) model in scene text recognition (STR), and establish a novel
Symmetrical Linguistic Feature Distillation framework (named CLIP-OCR) to
leverage both visual and linguistic knowledge in CLIP. Different from previous
CLIP-based methods mainly considering feature generalization on visual
encoding, we propose a symmetrical distillation strategy (SDS) that further
captures the linguistic knowledge in the CLIP text encoder. By cascading the
CLIP image encoder with the reversed CLIP text encoder, a symmetrical structure
is built with an image-to-text feature flow that covers not only visual but
also linguistic information for distillation.Benefiting from the natural
alignment in CLIP, such guidance flow provides a progressive optimization
objective from vision to language, which can supervise the STR feature
forwarding process layer-by-layer.Besides, a new Linguistic Consistency Loss
(LCL) is proposed to enhance the linguistic capability by considering
second-order statistics during the optimization. Overall, CLIP-OCR is the first
to design a smooth transition between image and text for the STR task.Extensive
experiments demonstrate the effectiveness of CLIP-OCR with 93.8% average
accuracy on six popular STR benchmarks.Code will be available at
https://github.com/wzx99/CLIPOCR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffNAS: Bootstrapping Diffusion Models by Prompting for Better
  Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently exhibited remarkable performance on synthetic
data. After a diffusion path is selected, a base model, such as UNet, operates
as a denoising autoencoder, primarily predicting noises that need to be
eliminated step by step. Consequently, it is crucial to employ a model that
aligns with the expected budgets to facilitate superior synthetic performance.
In this paper, we meticulously analyze the diffusion model and engineer a base
model search approach, denoted "DiffNAS". Specifically, we leverage GPT-4 as a
supernet to expedite the search, supplemented with a search memory to enhance
the results. Moreover, we employ RFID as a proxy to promptly rank the
experimental outcomes produced by GPT-4. We also adopt a rapid-convergence
training strategy to boost search efficiency. Rigorous experimentation
corroborates that our algorithm can augment the search efficiency by 2 times
under GPT-based scenarios, while also attaining a performance of 2.82 with 0.37
improvement in FID on CIFAR10 relative to the benchmark IDDPM algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Uncertainty Aided Framework for Learning based Liver $T_1ρ$
  Mapping and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02736v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02736v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoxing Huang, Vincent Wai Sun Wong, Queenie Chan, Winnie Chiu Wing Chu, Weitian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Quantitative $T_1\rho$ imaging has potential for assessment of
biochemical alterations of liver pathologies. Deep learning methods have been
employed to accelerate quantitative $T_1\rho$ imaging. To employ artificial
intelligence-based quantitative imaging methods in complicated clinical
environment, it is valuable to estimate the uncertainty of the predicated
$T_1\rho$ values to provide the confidence level of the quantification results.
The uncertainty should also be utilized to aid the post-hoc quantitative
analysis and model learning tasks. Approach: To address this need, we propose a
parametric map refinement approach for learning-based $T_1\rho$ mapping and
train the model in a probabilistic way to model the uncertainty. We also
propose to utilize the uncertainty map to spatially weight the training of an
improved $T_1\rho$ mapping network to further improve the mapping performance
and to remove pixels with unreliable $T_1\rho$ values in the region of
interest. The framework was tested on a dataset of 51 patients with different
liver fibrosis stages. Main results: Our results indicate that the
learning-based map refinement method leads to a relative mapping error of less
than 3% and provides uncertainty estimation simultaneously. The estimated
uncertainty reflects the actual error level, and it can be used to further
reduce relative $T_1\rho$ mapping error to 2.60% as well as removing unreliable
pixels in the region of interest effectively. Significance: Our studies
demonstrate the proposed approach has potential to provide a learning-based
quantitative MRI system for trustworthy $T_1\rho$ mapping of the liver.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting CLIP's Image Representation via Text-Based Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the CLIP image encoder by analyzing how individual model
components affect the final representation. We decompose the image
representation as a sum across individual image patches, model layers, and
attention heads, and use CLIP's text representation to interpret the summands.
Interpreting the attention heads, we characterize each head's role by
automatically finding text representations that span its output space, which
reveals property-specific roles for many heads (e.g. location or shape). Next,
interpreting the image patches, we uncover an emergent spatial localization
within CLIP. Finally, we use this understanding to remove spurious features
from CLIP and to create a strong zero-shot image segmenter. Our results
indicate that a scalable understanding of transformer models is attainable and
can be used to repair and improve models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code:
  https://yossigandelsman.github.io/clip_decomposition/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyuan Lyu, Chengquan Zhang, Shanshan Liu, Meina Qiao, Yangliu Xu, Liang Wu, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text images contain both visual and linguistic information. However, existing
pre-training techniques for text recognition mainly focus on either visual
representation learning or linguistic knowledge learning. In this paper, we
propose a novel approach MaskOCR to unify vision and language pre-training in
the classical encoder-decoder recognition framework. We adopt the masked image
modeling approach to pre-train the feature encoder using a large set of
unlabeled real text images, which allows us to learn strong visual
representations. In contrast to introducing linguistic knowledge with an
additional language model, we directly pre-train the sequence decoder.
Specifically, we transform text data into synthesized text images to unify the
data modalities of vision and language, and enhance the language modeling
capability of the sequence decoder using a proposed masked image-language
modeling scheme. Significantly, the encoder is frozen during the pre-training
phase of the sequence decoder. Experimental results demonstrate that our
proposed method achieves superior performance on benchmark datasets, including
Chinese and English text images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Digital-Twin Localization via An RGBD-based Transformer Network
  and A Comprehensive Evaluation on a Mobile Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixun Huang, Keling Yao, Seth Z. Zhao, Chuanyu Pan, Tianjian Xu, Weiyu Feng, Allen Y. Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The potential of digital-twin technology, involving the creation of precise
digital replicas of physical objects, to reshape AR experiences in 3D object
tracking and localization scenarios is significant. However, enabling robust 3D
object tracking in dynamic mobile AR environments remains a formidable
challenge. These scenarios often require a more robust pose estimator capable
of handling the inherent sensor-level measurement noise. In this paper,
recognizing the challenges of comprehensive solutions in existing literature,
we propose a transformer-based 6DoF pose estimator designed to achieve
state-of-the-art accuracy under real-world noisy data. To systematically
validate the new solution's performance against the prior art, we also
introduce a novel RGBD dataset called Digital Twin Tracking Dataset v2 (DTTD2),
which is focused on digital-twin object tracking scenarios. Expanded from an
existing DTTD v1 (DTTD1), the new dataset adds digital-twin data captured using
a cutting-edge mobile RGBD sensor suite on Apple iPhone 14 Pro, expanding the
applicability of our approach to iPhone sensor data. Through extensive
experimentation and in-depth analysis, we illustrate the effectiveness of our
methods under significant depth data errors, surpassing the performance of
existing baselines. Code and dataset are made publicly available at:
https://github.com/augcog/DTTD2
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Streaming Video Temporal Action Segmentation In Real Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.13808v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.13808v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujun Wen, Yunheng Li, Zhuben Dong, Lin Feng, Wanxiao Yang, Shenlan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal action segmentation (TAS) is a critical step toward long-term video
understanding. Recent studies follow a pattern that builds models based on
features instead of raw video picture information. However, we claim those
models are trained complicatedly and limit application scenarios. It is hard
for them to segment human actions of video in real time because they must work
after the full video features are extracted. As the real-time action
segmentation task is different from TAS task, we define it as streaming video
real-time temporal action segmentation (SVTAS) task. In this paper, we propose
a real-time end-to-end multi-modality model for SVTAS task. More specifically,
under the circumstances that we cannot get any future information, we segment
the current human action of streaming video chunk in real time. Furthermore,
the model we propose combines the last steaming video chunk feature extracted
by language model with the current image feature extracted by image model to
improve the quantity of real-time temporal action segmentation. To the best of
our knowledge, it is the first multi-modality real-time temporal action
segmentation model. Under the same evaluation criteria as full video temporal
action segmentation, our model segments human action in real time with less
than 40% of state-of-the-art model computation and achieves 90% of the accuracy
of the full video state-of-the-art model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ISKE2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Cross-Dataset Performance of Distracted Driving Detection With
  Score-Softmax Classifier 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Duan, Zixuan Liu, Jiahao Xia, Minghai Zhang, Jiacai Liao, Libo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks enable real-time monitoring of in-vehicle driver,
facilitating the timely prediction of distractions, fatigue, and potential
hazards. This technology is now integral to intelligent transportation systems.
Recent research has exposed unreliable cross-dataset end-to-end driver behavior
recognition due to overfitting, often referred to as ``shortcut learning",
resulting from limited data samples. In this paper, we introduce the
Score-Softmax classifier, which addresses this issue by enhancing inter-class
independence and Intra-class uncertainty. Motivated by human rating patterns,
we designed a two-dimensional supervisory matrix based on marginal Gaussian
distributions to train the classifier. Gaussian distributions help amplify
intra-class uncertainty while ensuring the Score-Softmax classifier learns
accurate knowledge. Furthermore, leveraging the summation of independent
Gaussian distributed random variables, we introduced a multi-channel
information fusion method. This strategy effectively resolves the
multi-information fusion challenge for the Score-Softmax classifier.
Concurrently, we substantiate the necessity of transfer learning and
multi-dataset combination. We conducted cross-dataset experiments using the
SFD, AUCDD-V1, and 100-Driver datasets, demonstrating that Score-Softmax
improves cross-dataset performance without modifying the model architecture.
This provides a new approach for enhancing neural network generalization.
Additionally, our information fusion approach outperforms traditional methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jet tagging algorithm of graph network with HaarPooling message passing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13869v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13869v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Ma, Feiyi Liu, Wei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently methods of graph neural networks (GNNs) have been applied to solving
the problems in high energy physics (HEP) and have shown its great potential
for quark-gluon tagging with graph representation of jet events. In this paper,
we introduce an approach of GNNs combined with a HaarPooling operation to
analyze the events, called HaarPooling Message Passing neural network (HMPNet).
In HMPNet, HaarPooling not only extracts the features of graph, but embeds
additional information obtained by clustering of k-means of different particle
features. We construct Haarpooling from five different features: absolute
energy $\log E$, transverse momentum $\log p_T$, relative coordinates
$(\Delta\eta,\Delta\phi)$, the mixed ones $(\log E, \log p_T)$ and $(\log E,
\log p_T, \Delta\eta,\Delta\phi)$. The results show that an appropriate
selection of information for HaarPooling enhances the accuracy of quark-gluon
tagging, as adding extra information of $\log P_T$ to the HMPNet outperforms
all the others, whereas adding relative coordinates information
$(\Delta\eta,\Delta\phi)$ is not very effective. This implies that by adding
effective particle features from HaarPooling can achieve much better results
than solely pure message passing neutral network (MPNN) can do, which
demonstrates significant improvement of feature extraction via the pooling
process. Finally we compare the HMPNet study, ordering by $p_T$, with other
studies and prove that the HMPNet is also a good choice of GNN algorithms for
jet tagging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Map Vectorization for Autonomous Driving: A Rasterization
  Perspective <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongjie Zhang, Jiahao Lin, Shuang Wu, Yilin Song, Zhipeng Luo, Yang Xue, Shijian Lu, Zuoguan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vectorized high-definition (HD) map is essential for autonomous driving,
providing detailed and precise environmental information for advanced
perception and planning. However, current map vectorization methods often
exhibit deviations, and the existing evaluation metric for map vectorization
lacks sufficient sensitivity to detect these deviations. To address these
limitations, we propose integrating the philosophy of rasterization into map
vectorization. Specifically, we introduce a new rasterization-based evaluation
metric, which has superior sensitivity and is better suited to real-world
autonomous driving scenarios. Furthermore, we propose MapVR (Map Vectorization
via Rasterization), a novel framework that applies differentiable rasterization
to vectorized outputs and then performs precise and geometry-aware supervision
on rasterized HD maps. Notably, MapVR designs tailored rasterization strategies
for various geometric shapes, enabling effective adaptation to a wide range of
map elements. Experiments show that incorporating rasterization into map
vectorization greatly enhances performance with no extra computational cost
during inference, leading to more accurate map perception and ultimately
promoting safer autonomous driving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[NeurIPS 2023]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Class-Incremental Learning using Diffusion Model for Distillation and
  Replay <span class="chip">ICCV
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning aims to learn new classes in an incremental
fashion without forgetting the previously learned ones. Several research works
have shown how additional data can be used by incremental models to help
mitigate catastrophic forgetting. In this work, following the recent
breakthrough in text-to-image generative models and their wide distribution, we
propose the use of a pretrained Stable Diffusion model as a source of
additional data for class-incremental learning. Compared to competitive methods
that rely on external, often unlabeled, datasets of real images, our approach
can generate synthetic samples belonging to the same classes as the previously
encountered images. This allows us to use those additional data samples not
only in the distillation loss but also for replay in the classification loss.
Experiments on the competitive benchmarks CIFAR100, ImageNet-Subset, and
ImageNet demonstrate how this new approach can be used to further improve the
performance of state-of-the-art methods for class-incremental learning on large
scale datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Best paper award at 1st Workshop on Visual Continual Learning, ICCV
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem
  Formulation and Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Korawat Charoenpitaks, Van-Quang Nguyen, Masanori Suganuma, Masahiro Takahashi, Ryoma Niihara, Takayuki Okatani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of predicting hazards that drivers may
encounter while driving a car. We formulate it as a task of anticipating
impending accidents using a single input image captured by car dashcams. Unlike
existing approaches to driving hazard prediction that rely on computational
simulations or anomaly detection from videos, this study focuses on high-level
inference from static images. The problem needs predicting and reasoning about
future events based on uncertain observations, which falls under visual
abductive reasoning. To enable research in this understudied area, a new
dataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is
created. The dataset consists of 15K dashcam images of street scenes, and each
image is associated with a tuple containing car speed, a hypothesized hazard
description, and visual entities present in the scene. These are annotated by
human annotators, who identify risky scenes and provide descriptions of
potential accidents that could occur a few seconds later. We present several
baseline methods and evaluate their performance on our dataset, identifying
remaining issues and discussing future directions. This study contributes to
the field by introducing a novel problem formulation and dataset, enabling
researchers to explore the potential of multi-modal AI for driving hazard
prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main Paper: 10 pages, Supplementary Materials: 25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rank-N-Contrast: Learning Continuous Representations for Regression <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Zha, Peng Cao, Jeany Son, Yuzhe Yang, Dina Katabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep regression models typically learn in an end-to-end fashion without
explicitly emphasizing a regression-aware representation. Consequently, the
learned representations exhibit fragmentation and fail to capture the
continuous nature of sample orders, inducing suboptimal results across a wide
range of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a
framework that learns continuous representations for regression by contrasting
samples against each other based on their rankings in the target space. We
demonstrate, theoretically and empirically, that RNC guarantees the desired
order of learned representations in accordance with the target orders, enjoying
not only better performance but also significantly improved robustness,
efficiency, and generalization. Extensive experiments using five real-world
regression datasets that span computer vision, human-computer interaction, and
healthcare verify that RNC achieves state-of-the-art performance, highlighting
its intriguing properties including better data efficiency, robustness to
spurious targets and data corruptions, and generalization to distribution
shifts. Code is available at: https://github.com/kaiwenzha/Rank-N-Contrast.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 Spotlight. The first two authors contributed equally to
  this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RetSeg: Retention-based Colorectal Polyps Segmentation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled ELKarazle, Valliappan Raman, Caslon Chua, Patrick Then
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have revolutionized medical imaging analysis,
showcasing superior efficacy compared to conventional Convolutional Neural
Networks (CNNs) in vital tasks such as polyp classification, detection, and
segmentation. Leveraging attention mechanisms to focus on specific image
regions, ViTs exhibit contextual awareness in processing visual data,
culminating in robust and precise predictions, even for intricate medical
images. Moreover, the inherent self-attention mechanism in Transformers
accommodates varying input sizes and resolutions, granting an unprecedented
flexibility absent in traditional CNNs. However, Transformers grapple with
challenges like excessive memory usage and limited training parallelism due to
self-attention, rendering them impractical for real-time disease detection on
resource-constrained devices. In this study, we address these hurdles by
investigating the integration of the recently introduced retention mechanism
into polyp segmentation, introducing RetSeg, an encoder-decoder network
featuring multi-head retention blocks. Drawing inspiration from Retentive
Networks (RetNet), RetSeg is designed to bridge the gap between precise polyp
segmentation and resource utilization, particularly tailored for colonoscopy
images. We train and validate RetSeg for polyp segmentation employing two
publicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we
showcase RetSeg's promising performance across diverse public datasets,
including CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While
our work represents an early-stage exploration, further in-depth studies are
imperative to advance these promising findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version with a PDF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Side-Tuning for Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weifeng Lin, Ziheng Wu, Jiayu Chen, Wentao Yang, Mingxin Huang, Jun Huang, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained Vision Transformers (ViT) has consistently
demonstrated promising performance in the realm of visual recognition. However,
adapting large pre-trained models to various tasks poses a significant
challenge. This challenge arises from the need for each model to undergo an
independent and comprehensive fine-tuning process, leading to substantial
computational and memory demands. While recent advancements in
Parameter-efficient Transfer Learning (PETL) have demonstrated their ability to
achieve superior performance compared to full fine-tuning with a smaller subset
of parameter updates, they tend to overlook dense prediction tasks such as
object detection and segmentation. In this paper, we introduce Hierarchical
Side-Tuning (HST), a novel PETL approach that enables ViT transfer to various
downstream tasks effectively. Diverging from existing methods that exclusively
fine-tune parameters within input spaces or certain modules connected to the
backbone, we tune a lightweight and hierarchical side network (HSN) that
leverages intermediate activations extracted from the backbone and generates
multi-scale features to make predictions. To validate HST, we conducted
extensive experiments encompassing diverse visual tasks, including
classification, object detection, instance segmentation, and semantic
segmentation. Notably, our method achieves state-of-the-art average Top-1
accuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters.
When applied to object detection tasks on COCO testdev benchmark, HST even
surpasses full fine-tuning and obtains better performance with 49.7 box AP and
43.2 mask AP using Cascade Mask R-CNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Critical Look at Classic Test-Time Adaptation Methods in Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang'an Yi, Haotian Chen, Yifan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation (TTA) aims to adapt a model, initially trained on
training data, to potential distribution shifts in the test data. Most existing
TTA studies, however, focus on classification tasks, leaving a notable gap in
the exploration of TTA for semantic segmentation. This pronounced emphasis on
classification might lead numerous newcomers and engineers to mistakenly assume
that classic TTA methods designed for classification can be directly applied to
segmentation. Nonetheless, this assumption remains unverified, posing an open
question. To address this, we conduct a systematic, empirical study to disclose
the unique challenges of segmentation TTA, and to determine whether classic TTA
strategies can effectively address this task. Our comprehensive results have
led to three key observations. First, the classic batch norm updating strategy,
commonly used in classification TTA, only brings slight performance
improvement, and in some cases it might even adversely affect the results. Even
with the application of advanced distribution estimation techniques like batch
renormalization, the problem remains unresolved. Second, the teacher-student
scheme does enhance training stability for segmentation TTA in the presence of
noisy pseudo-labels. However, it cannot directly result in performance
improvement compared to the original model without TTA. Third, segmentation TTA
suffers a severe long-tailed imbalance problem, which is substantially more
complex than that in TTA for classification. This long-tailed challenge
significantly affects segmentation TTA performance, even when the accuracy of
pseudo-labels is high. In light of these observations, we conclude that TTA for
segmentation presents significant challenges, and simply using classic TTA
methods cannot address this problem well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coping with Change: Learning Invariant and Minimum Sufficient
  Representations for Fine-Grained Visual Categorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04893v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04893v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Ye, Shujian Yu, Wenjin Hou, Yu Wang, Xinge You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-grained visual categorization (FGVC) is a challenging task due to
similar visual appearances between various species. Previous studies always
implicitly assume that the training and test data have the same underlying
distributions, and that features extracted by modern backbone architectures
remain discriminative and generalize well to unseen test data. However, we
empirically justify that these conditions are not always true on benchmark
datasets. To this end, we combine the merits of invariant risk minimization
(IRM) and information bottleneck (IB) principle to learn invariant and minimum
sufficient (IMS) representations for FGVC, such that the overall model can
always discover the most succinct and consistent fine-grained features. We
apply the matrix-based R{\'e}nyi's $\alpha$-order entropy to simplify and
stabilize the training of IB; we also design a ``soft" environment partition
scheme to make IRM applicable to FGVC task. To the best of our knowledge, we
are the first to address the problem of FGVC from a generalization perspective
and develop a new information-theoretic solution accordingly. Extensive
experiments demonstrate the consistent performance gain offered by our IMS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript accepted by CVIU, code is available at Github</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Grid-based Method for Removing Overlaps of Dimensionality Reduction
  Scatterplot Layouts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1903.06262v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1903.06262v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gladys M. Hilasaca, Wilson E. Marcílio-Jr, Danilo M. Eler, Rafael M. Martins, Fernando V. Paulovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality Reduction (DR) scatterplot layouts have become a ubiquitous
visualization tool for analyzing multidimensional datasets. Despite their
popularity, such scatterplots suffer from occlusion, especially when
informative glyphs are used to represent data instances, potentially
obfuscating critical information for the analysis under execution. Different
strategies have been devised to address this issue, either producing
overlap-free layouts that lack the powerful capabilities of contemporary DR
techniques in uncovering interesting data patterns or eliminating overlaps as a
post-processing strategy. Despite the good results of post-processing
techniques, most of the best methods typically expand or distort the
scatterplot area, thus reducing glyphs' size (sometimes) to unreadable
dimensions, defeating the purpose of removing overlaps. This paper presents
Distance Grid (DGrid), a novel post-processing strategy to remove overlaps from
DR layouts that faithfully preserves the original layout's characteristics and
bounds the minimum glyph sizes. We show that DGrid surpasses the
state-of-the-art in overlap removal (through an extensive comparative
evaluation considering multiple different metrics) while also being one of the
fastest techniques, especially for large datasets. A user study with 51
participants also shows that DGrid is consistently ranked among the top
techniques for preserving the original scatterplots' visual characteristics and
the aesthetics of the final results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures. A preprint version of a publication at IEEE
  Transactions on Visualization and Computer Graphics (TVCG), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A
  Reproducibility Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05174v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05174v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariya Hendriksen, Svitlana Vakulenko, Ernst Kuiper, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most approaches to cross-modal retrieval (CMR) focus either on object-centric
datasets, meaning that each document depicts or describes a single object, or
on scene-centric datasets, meaning that each image depicts or describes a
complex scene that involves multiple objects and relations between them. We
posit that a robust CMR model should generalize well across both dataset types.
Despite recent advances in CMR, the reproducibility of the results and their
generalizability across different dataset types has not been studied before. We
address this gap and focus on the reproducibility of the state-of-the-art CMR
results when evaluated on object-centric and scene-centric datasets. We select
two state-of-the-art CMR models with different architectures: (i) CLIP; and
(ii) X-VLM. Additionally, we select two scene-centric datasets, and three
object-centric datasets, and determine the relative performance of the selected
models on these datasets. We focus on reproducibility, replicability, and
generalizability of the outcomes of previously published CMR experiments. We
discover that the experiments are not fully reproducible and replicable.
Besides, the relative performance results partially generalize across
object-centric and scene-centric datasets. On top of that, the scores obtained
on object-centric datasets are much lower than the scores obtained on
scene-centric datasets. For reproducibility and transparency we make our source
code and the trained models publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, accepted as a reproducibility paper at ECIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MyStyle++: A Controllable Personalized Generative Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04865v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04865v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libing Zeng, Lele Chen, Yi Xu, Nima Kalantari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an approach to obtain a personalized generative
prior with explicit control over a set of attributes. We build upon MyStyle, a
recently introduced method, that tunes the weights of a pre-trained StyleGAN
face generator on a few images of an individual. This system allows
synthesizing, editing, and enhancing images of the target individual with high
fidelity to their facial features. However, MyStyle does not demonstrate
precise control over the attributes of the generated images. We propose to
address this problem through a novel optimization system that organizes the
latent space in addition to tuning the generator. Our key contribution is to
formulate a loss that arranges the latent codes, corresponding to the input
images, along a set of specific directions according to their attributes. We
demonstrate that our approach, dubbed MyStyle++, is able to synthesize, edit,
and enhance images of an individual with great control over the attributes,
while preserving the unique facial characteristics of that individual.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMOOT: Saliency Guided Mask Optimized Online Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Karkehabadi, Houman Homayoun, Avesta Sasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks are powerful tools for understanding complex patterns
and making decisions. However, their black-box nature impedes a complete
understanding of their inner workings. Saliency-Guided Training (SGT) methods
try to highlight the prominent features in the model's training based on the
output to alleviate this problem. These methods use back-propagation and
modified gradients to guide the model toward the most relevant features while
keeping the impact on the prediction accuracy negligible. SGT makes the model's
final result more interpretable by masking input partially. In this way,
considering the model's output, we can infer how each segment of the input
affects the output. In the particular case of image as the input, masking is
applied to the input pixels. However, the masking strategy and number of pixels
which we mask, are considered as a hyperparameter. Appropriate setting of
masking strategy can directly affect the model's training. In this paper, we
focus on this issue and present our contribution. We propose a novel method to
determine the optimal number of masked images based on input, accuracy, and
model loss during the training. The strategy prevents information loss which
leads to better accuracy values. Also, by integrating the model's performance
in the strategy formula, we show that our model represents the salient features
more meaningful. Our experimental results demonstrate a substantial improvement
in both model accuracy and the prominence of saliency, thereby affirming the
effectiveness of our proposed solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Inverse Problems with Latent Diffusion Models via Hard Data
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, Liyue Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently emerged as powerful generative priors for
solving inverse problems. However, training diffusion models in the pixel space
are both data-intensive and computationally demanding, which restricts their
applicability as priors for high-dimensional real-world data such as medical
images. Latent diffusion models, which operate in a much lower-dimensional
space, offer a solution to these challenges. However, incorporating latent
diffusion models to solve inverse problems remains a challenging problem due to
the nonlinearity of the encoder and decoder. To address these issues, we
propose \textit{ReSample}, an algorithm that can solve general inverse problems
with pre-trained latent diffusion models. Our algorithm incorporates data
consistency by solving an optimization problem during the reverse sampling
process, a concept that we term as hard data consistency. Upon solving this
optimization problem, we propose a novel resampling scheme to map the
measurement-consistent sample back onto the noisy data manifold and
theoretically demonstrate its benefits. Lastly, we apply our algorithm to solve
a wide range of linear and nonlinear inverse problems in both natural and
medical images, demonstrating that our approach outperforms existing
state-of-the-art approaches, including those based on pixel-space diffusion
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Implicit Denoising Diffusion Models (SIDDMs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12511v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12511v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwu Xu, Mingming Gong, Shaoan Xie, Wei Wei, Matthias Grundmann, Kayhan Batmanghelich, Tingbo Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the proliferation of generative models, achieving fast sampling
during inference without compromising sample diversity and quality remains
challenging. Existing models such as Denoising Diffusion Probabilistic Models
(DDPM) deliver high-quality, diverse samples but are slowed by an inherently
high number of iterative steps. The Denoising Diffusion Generative Adversarial
Networks (DDGAN) attempted to circumvent this limitation by integrating a GAN
model for larger jumps in the diffusion process. However, DDGAN encountered
scalability limitations when applied to large datasets. To address these
limitations, we introduce a novel approach that tackles the problem by matching
implicit and explicit factors. More specifically, our approach involves
utilizing an implicit model to match the marginal distributions of noisy data
and the explicit conditional distribution of the forward diffusion. This
combination allows us to effectively match the joint denoising distributions.
Unlike DDPM but similar to DDGAN, we do not enforce a parametric distribution
for the reverse step, enabling us to take large steps during inference. Similar
to the DDPM but unlike DDGAN, we take advantage of the exact form of the
diffusion process. We demonstrate that our proposed method obtains comparable
generative performance to diffusion-based models and vastly superior results to
models with a small number of sampling steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Elucidating the Exposure Bias in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15321v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15321v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, Itir Onal Ertugrul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated impressive generative capabilities, but
their \textit{exposure bias} problem, described as the input mismatch between
training and sampling, lacks in-depth exploration. In this paper, we
systematically investigate the exposure bias problem in diffusion models by
first analytically modelling the sampling distribution, based on which we then
attribute the prediction error at each sampling step as the root cause of the
exposure bias issue. Furthermore, we discuss potential solutions to this issue
and propose an intuitive metric for it. Along with the elucidation of exposure
bias, we propose a simple, yet effective, training-free method called Epsilon
Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly
moves the sampling trajectory closer to the vector field learned in the
training phase by scaling down the network output (Epsilon), mitigating the
input mismatch between training and sampling. Experiments on various diffusion
frameworks (ADM, DDPM/DDIM, EDM, LDM), unconditional and conditional settings,
and deterministic vs. stochastic sampling verify the effectiveness of our
method. Remarkably, our ADM-ES, as a SOTA stochastic sampler, obtains 2.17 FID
on CIFAR-10 under 100-step unconditional generation. The code is available at
\url{https://github.com/forever208/ADM-ES} and
\url{https://github.com/forever208/EDM-ES}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Equivariant Transfer Learning from Pretrained Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourya Basu, Pulkit Katdare, Prasanna Sattigeri, Vijil Chenthamarakshan, Katherine Driggs-Campbell, Payel Das, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient transfer learning algorithms are key to the success of foundation
models on diverse downstream tasks even with limited data. Recent works of Basu
et al. (2023) and Kaba et al. (2022) propose group averaging (equitune) and
optimization-based methods, respectively, over features from group-transformed
inputs to obtain equivariant outputs from non-equivariant neural networks.
While Kaba et al. (2022) are only concerned with training from scratch, we find
that equitune performs poorly on equivariant zero-shot tasks despite good
finetuning results. We hypothesize that this is because pretrained models
provide better quality features for certain transformations than others and
simply averaging them is deleterious. Hence, we propose {\lambda}-equitune that
averages the features using importance weights, {\lambda}s. These weights are
learned directly from the data using a small neural network, leading to
excellent zero-shot and finetuned results that outperform equitune. Further, we
prove that {\lambda}-equitune is equivariant and a universal approximator of
equivariant functions. Additionally, we show that the method of Kaba et al.
(2022) used with appropriate loss functions, which we call equizero, also gives
excellent zero-shot and finetuned performance. Both equitune and equizero are
special cases of {\lambda}-equitune. To show the simplicity and generality of
our method, we validate on a wide range of diverse applications and models such
as 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in
natural language generation (NLG), 4) compositional generalization in
languages, and 5) image classification using pretrained CNNs such as Resnet and
Alexnet.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-09T00:00:00Z">2023-10-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">44</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Decision Theory: Safe Autonomous Decisions from Imperfect
  Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jordan Lekeufack, Anastasios A. Angelopoulos, Andrea Bajcsy, Michael I. Jordan, Jitendra Malik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Conformal Decision Theory, a framework for producing safe
autonomous decisions despite imperfect machine learning predictions. Examples
of such decisions are ubiquitous, from robot planning algorithms that rely on
pedestrian predictions, to calibrating autonomous manufacturing to exhibit high
throughput and low error, to the choice of trusting a nominal policy versus
switching to a safe backup policy at run-time. The decisions produced by our
algorithms are safe in the sense that they come with provable statistical
guarantees of having low risk without any assumptions on the world model
whatsoever; the observations need not be I.I.D. and can even be adversarial.
The theory extends results from conformal prediction to calibrate decisions
directly, without requiring the construction of prediction sets. Experiments
demonstrate the utility of our approach in robot motion planning around humans,
automated stock trading, and robot manufacturin
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAIL: Task-specific Adapters for Imitation Learning with Large
  Pretrained Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuxin Liu, Jesse Zhang, Kavosh Asadi, Yao Liu, Ding Zhao, Shoham Sabach, Rasool Fakoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The full potential of large pretrained models remains largely untapped in
control domains like robotics. This is mainly because of the scarcity of data
and the computational challenges associated with training or fine-tuning these
large models for such applications. Prior work mainly emphasizes effective
pretraining of large models for decision-making, with little exploration into
how to perform data-efficient continual adaptation of these models for new
tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters
for Imitation Learning), a framework for efficient adaptation to new control
tasks. Inspired by recent advancements in parameter-efficient fine-tuning in
language domains, we explore efficient fine-tuning techniques -- e.g.,
Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to
adapt large pretrained models for new tasks with limited demonstration data.
Our extensive experiments in large-scale language-conditioned manipulation
tasks comparing prevalent parameter-efficient fine-tuning techniques and
adaptation baselines suggest that TAIL with LoRA can achieve the best
post-adaptation performance with only 1\% of the trainable parameters of full
fine-tuning, while avoiding catastrophic forgetting and preserving adaptation
plasticity in continual learning settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Multi-Fidelity Impedance Tuning for Human-Robot Cooperative
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Lau, Vaibhav Srivastava, Shaunak D. Bopardikar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine how a human-robot interaction (HRI) system may be designed when
input-output data from previous experiments are available. In particular, we
consider how to select an optimal impedance in the assistance design for a
cooperative manipulation task with a new operator. Due to the variability
between individuals, the design parameters that best suit one operator of the
robot may not be the best parameters for another one. However, by incorporating
historical data using a linear auto-regressive (AR-1) Gaussian process, the
search for a new operator's optimal parameters can be accelerated. We lay out a
framework for optimizing the human-robot cooperative manipulation that only
requires input-output data. We establish how the AR-1 model improves the bound
on the regret and numerically simulate a human-robot cooperative manipulation
task to show the regret improvement. Further, we show how our approach's
input-output nature provides robustness against modeling error through an
additional numerical study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures. Submitted to the 2024 ACC on September 29, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DTPP: Differentiable Joint Conditional Prediction and Cost Evaluation
  for Tree Policy Planning in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Huang, Peter Karkus, Boris Ivanovic, Yuxiao Chen, Marco Pavone, Chen Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion prediction and cost evaluation are vital components in the
decision-making system of autonomous vehicles. However, existing methods often
ignore the importance of cost learning and treat them as separate modules. In
this study, we employ a tree-structured policy planner and propose a
differentiable joint training framework for both ego-conditioned prediction and
cost models, resulting in a direct improvement of the final planning
performance. For conditional prediction, we introduce a query-centric
Transformer model that performs efficient ego-conditioned motion prediction.
For planning cost, we propose a learnable context-aware cost function with
latent interaction features, facilitating differentiable joint learning. We
validate our proposed approach using the real-world nuPlan dataset and its
associated planning test platform. Our framework not only matches
state-of-the-art planning methods but outperforms other learning-based methods
in planning quality, while operating more efficiently in terms of runtime. We
show that joint training delivers significantly better performance than
separate training of the two modules. Additionally, we find that
tree-structured policy planning outperforms the conventional single-stage
planning approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Learning-Based Framework for Safe Human-Robot Collaboration with
  Multiple Backup Control Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil C. Janwani, Ersin Daş, Thomas Touma, Skylar X. Wei, Tamas G. Molnar, Joel W. Burdick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring robot safety in complex environments is a difficult task due to
actuation limits, such as torque bounds. This paper presents a safety-critical
control framework that leverages learning-based switching between multiple
backup controllers to formally guarantee safety under bounded control inputs
while satisfying driver intention. By leveraging backup controllers designed to
uphold safety and input constraints, backup control barrier functions (BCBFs)
construct implicitly defined control invariance sets via a feasible quadratic
program (QP). However, BCBF performance largely depends on the design and
conservativeness of the chosen backup controller, especially in our setting of
human-driven vehicles in complex, e.g, off-road, conditions. While
conservativeness can be reduced by using multiple backup controllers,
determining when to switch is an open problem. Consequently, we develop a
broadcast scheme that estimates driver intention and integrates BCBFs with
multiple backup strategies for human-robot interaction. An LSTM classifier uses
data inputs from the robot, human, and safety algorithms to continually choose
a backup controller in real-time. We demonstrate our method's efficacy on a
dual-track robot in obstacle avoidance scenarios. Our framework guarantees
robot safety while adhering to driver intention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Open-Loop Baseline for Reinforcement Learning Locomotion Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonin Raffin, Olivier Sigaud, Jens Kober, Alin Albu-Schäffer, João Silvério, Freek Stulp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In search of the simplest baseline capable of competing with Deep
Reinforcement Learning on locomotion tasks, we propose a biologically inspired
model-free open-loop strategy. Drawing upon prior knowledge and harnessing the
elegance of simple oscillators to generate periodic joint motions, it achieves
respectable performance in five different locomotion environments, with a
number of tunable parameters that is a tiny fraction of the thousands typically
required by RL algorithms. Unlike RL methods, which are prone to performance
degradation when exposed to sensor noise or failure, our open-loop oscillators
exhibit remarkable robustness due to their lack of reliance on sensors.
Furthermore, we showcase a successful transfer from simulation to reality using
an elastic quadruped, all without the need for randomization or reward
engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>video: https://b2drop.eudat.eu/s/ykDPMM7F9KFyLgi</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FeatSense -- A Feature-based Registration Algorithm with GPU-accelerated
  TSDF-Mapping Backend for NVIDIA Jetson Boards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Gaal, Thomas Wiemann, Alexander Mock, Mario Porrmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents FeatSense, a feature-based GPU-accelerated SLAM system
for high resolution LiDARs, combined with a map generation algorithm for
real-time generation of large Truncated Signed Distance Fields (TSDFs) on
embedded hardware. FeatSense uses LiDAR point cloud features for odometry
estimation and point cloud registration. The registered point clouds are
integrated into a global Truncated Signed Distance Field (TSDF) representation.
FeatSense is intended to run on embedded systems with integrated
GPU-accelerator like NVIDIA Jetson boards. In this paper, we present a
real-time capable TSDF-SLAM system specially tailored for close coupled CPU/GPU
systems. The implementation is evaluated in various structured and unstructured
environments and benchmarked against existing reference datasets. The main
contribution of this paper is the ability to register up to 128 scan lines of
an Ouster OS1-128 LiDAR at 10Hz on a NVIDIA AGX Xavier while achieving a TSDF
map generation speedup by a factor of 100 compared to previous work on the same
power budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining the simulation-to-reality gap of a wheel loader digging in
  deformable terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koji Aoshima, Martin Servin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate how well a wheel loader simulator can replicate a real one
when performing bucket filling in a pile of gravel. The comparisons are made
using field test time series of the vehicle motion and actuation forces, loaded
mass, and total work. The vehicle was modeled as a rigid multibody system with
frictional contacts, driveline, and linear actuators. For the soil, we tested
discrete element models of different resolutions, with and without multiscale
acceleration. The spatio-temporal resolution ranged between 50-400 mm and 2-500
ms, and the computational speed was between 1/10,000 to 5 times faster than
real-time. The simulation-to-reality gap was found to be around 10% and
exhibited a weak dependence on the level of fidelity, i.e. accessible with
real-time simulation and faster. Furthermore, the sensitivity of an optimized
force feedback controller under transfer between different simulation domains
was investigated. The domain bias was observed to cause a performance reduction
of 5% despite the domain gap being about 15%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D tomatoes' localisation with monocular cameras using histogram filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandro Costa Magalhães, Filipe Neves dos Santos, António Paulo Moreira, Jorge Dias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performing tasks in agriculture, such as fruit monitoring or harvesting,
requires perceiving the objects' spatial position. RGB-D cameras are limited
under open-field environments due to lightning interferences. Therefore, in
this study, we approach the use of Histogram Filters (Bayesian Discrete
Filters) to estimate the position of tomatoes in the tomato plant. Two kernel
filters were studied: the square kernel and the Gaussian kernel. The
implemented algorithm was essayed in simulation, with and without Gaussian
noise and random noise, and in a testbed at laboratory conditions. The
algorithm reported a mean absolute error lower than 10 mm in simulation and 20
mm in the testbed at laboratory conditions with an assessing distance of about
0.5 m. So, the results are viable for real environments and should be improved
at closer distances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects
  on Production Lines <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Kuang, Qin Han, Danshi Li, Qiyu Dai, Lian Ding, Dong Sun, Hanlin Zhao, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present STOPNet, a framework for 6-DoF object suction
detection on production lines, with a focus on but not limited to transparent
objects, which is an important and challenging problem in robotic systems and
modern industry. Current methods requiring depth input fail on transparent
objects due to depth cameras' deficiency in sensing their geometry, while we
proposed a novel framework to reconstruct the scene on the production line
depending only on RGB input, based on multiview stereo. Compared to existing
works, our method not only reconstructs the whole 3D scene in order to obtain
high-quality 6-DoF suction poses in real time but also generalizes to novel
environments, novel arrangements and novel objects, including challenging
transparent objects, both in simulation and the real world. Extensive
experiments in simulation and the real world show that our method significantly
surpasses the baselines and has better generalizability, which caters to
practical industrial needs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review. ICRA 2024 submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DecAP: Decaying Action Priors for Accelerated Learning of Torque-Based
  Legged Locomotion Policies <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Sood, Ge Sun, Peizhuo Li, Guillaume Sartoretti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal Control for legged robots has gone through a paradigm shift from
position-based to torque-based control, owing to the latter's compliant and
robust nature. In parallel to this shift, the community has also turned to Deep
Reinforcement Learning (DRL) as a promising approach to directly learn
locomotion policies for complex real-life tasks. However, most end-to-end DRL
approaches still operate in position space, mainly because learning in torque
space is often sample-inefficient and does not consistently converge to natural
gaits. To address these challenges, we introduce Decaying Action Priors
(DecAP), a novel three-stage framework to learn and deploy torque policies for
legged locomotion. In the first stage, we generate our own imitation data by
training a position policy, eliminating the need for expert knowledge in
designing optimal controllers. The second stage incorporates decaying action
priors to enhance the exploration of torque-based policies aided by imitation
rewards. We show that our approach consistently outperforms imitation learning
alone and is significantly robust to the scaling of these rewards. Finally, our
third stage facilitates safe sim-to-real transfer by directly deploying our
learned torques, alongside low-gain PID control from our trained position
policy. We demonstrate the generality of our approach by training torque-based
locomotion policies for a biped, a quadruped, and a hexapod robot in
simulation, and experimentally demonstrate our learned policies on a quadruped
(Unitree Go1).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE International Conference on Robotics and
  Automation (ICRA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement learning for freeform robot design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhan Li, David Matthews, Sam Kriegman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the necessity of morphological adaptation in animals, a growing
body of work has attempted to expand robot training to encompass physical
aspects of a robot's design. However, reinforcement learning methods capable of
optimizing the 3D morphology of a robot have been restricted to reorienting or
resizing the limbs of a predetermined and static topological genus. Here we
show policy gradients for designing freeform robots with arbitrary external and
internal structure. This is achieved through actions that deposit or remove
bundles of atomic building blocks to form higher-level nonparametric
macrostructures such as appendages, organs and cavities. Although results are
provided for open loop control only, we discuss how this method could be
adapted for closed loop control and sim2real transfer to physical machines in
future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael G. Adam, Sebastian Eger, Martin Piccolrovazzi, Maged Iskandar, Joern Vogel, Alexander Dietrich, Seongjien Bien, Jon Skerlj, Abdeldjallil Naceri, Eckehard Steinbach, Alin Albu-Schaeffer, Sami Haddadin, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As labor shortage increases in the health sector, the demand for assistive
robotics grows. However, the needed test data to develop those robots is
scarce, especially for the application of active 3D object detection, where no
real data exists at all. This short paper counters this by introducing such an
annotated dataset of real environments. The captured environments represent
areas which are already in use in the field of robotic health care research. We
further provide ground truth data within one room, for assessing SLAM
algorithms running directly on a health care robot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collision Avoidance for Autonomous Surface Vessels using Novel
  Artificial Potential Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kailas Jadhav, Anantha Raj Pandi, Abhilash Somayajula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the demand for transportation through waterways continues to rise, the
number of vessels plying the waters has correspondingly increased. This has
resulted in a greater number of accidents and collisions between ships, some of
which lead to significant loss of life and financial losses. Research has shown
that human error is a major factor responsible for such incidents. The maritime
industry is constantly exploring newer approaches to autonomy to mitigate this
issue. This study presents the use of novel Artificial Potential Fields (APFs)
to perform obstacle and collision avoidance in marine environments. This study
highlights the advantage of harmonic functions over traditional functions in
modeling potential fields. With a modification, the method is extended to
effectively avoid dynamic obstacles while adhering to COLREGs. Improved
performance is observed as compared to the traditional potential fields and
also against the popular velocity obstacle approach. A comprehensive
statistical analysis is also performed through Monte Carlo simulations in
different congested environments that emulate real traffic conditions to
demonstrate robustness of the approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 30 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry-Aware Safety-Critical Local Reactive Controller for Robot
  Navigation in Unknown and Cluttered Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Li, Xindong Tang, Kai Chen, Chunxin Zheng, Haichao Liu, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a safety-critical local reactive controller that enables
the robot to navigate in unknown and cluttered environments. In particular, the
trajectory tracking task is formulated as a constrained polynomial optimization
problem. Then, safety constraints are imposed on the control variables invoking
the notion of polynomial positivity certificates in conjunction with their
Sum-of-Squares (SOS) approximation, thereby confining the robot motion inside
the locally extracted convex free region. It is noteworthy that, in the process
of devising the proposed safety constraints, the geometry of the robot can be
approximated using any shape that can be characterized with a set of polynomial
functions. The optimization problem is further convexified into a semidefinite
program (SDP) leveraging truncated multi-sequences (tms) and moment relaxation,
which favorably facilitates the effective use of off-the-shelf conic
programming solvers, such that real-time performance is attainable. Various
robot navigation tasks are investigated to demonstrate the effectiveness of the
proposed approach in terms of safety and tracking performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Visual Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Li, Zonglin Lyu, Mingxuan Lu, Chao Chen, Michael Milford, Chen Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition (VPR) capabilities enable autonomous robots to
navigate complex environments by discovering the environment's topology based
on visual input. Most research efforts focus on enhancing the accuracy and
robustness of single-robot VPR but often encounter issues such as occlusion due
to individual viewpoints. Despite a number of research on multi-robot
metric-based localization, there is a notable gap in research concerning more
robust and efficient place-based localization with a multi-robot system. This
work proposes collaborative VPR, where multiple robots share abstracted visual
features to enhance place recognition capabilities. We also introduce a novel
collaborative VPR framework based on similarity-regularized information fusion,
reducing irrelevant noise while harnessing valuable data from collaborators.
This framework seamlessly integrates with well-established single-robot VPR
techniques and supports end-to-end training with a weakly-supervised
contrastive loss. We conduct experiments in urban, rural, and indoor scenes,
achieving a notable improvement over single-agent VPR in urban environments
(~12\%), along with consistent enhancements in rural (~3\%) and indoor (~1\%)
scenarios. Our work presents a promising solution to the pressing challenges of
VPR, representing a substantial step towards safe and robust autonomous
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://ai4ce.github.io/CoVPR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Problem, One Solution: Unifying Robot and Environment Design
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Baumgärtner, Gajanan Kanagalingam, Alexander Puchtaand Jürgen Fleischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task-specific optimization of robotic systems has long been divided into
the optimization of the robot and the optimization of the environment. In this
letter, we argue that these two problems are interdependent and should be
treated as such. To this end, we present a unified problem formulation that
enables for the simultaneous optimization of both the robot kinematics and the
environment. We demonstrate the effectiveness of our approach by jointly
optimizing a robotic milling system. To compare our approach to the state of
the art we also optimize the robot kinematics and environment separately. The
results show that our approach outperforms the state of the art and that
simultaneous optimization leads to a much better solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud
  Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunge Bai, Ruijie Fu, Xiang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art techniques for monocular camera reconstruction predominantly
rely on the Structure from Motion (SfM) pipeline. However, such methods often
yield reconstruction outcomes that lack crucial scale information, and over
time, accumulation of images leads to inevitable drift issues. In contrast,
mapping methods based on LiDAR scans are popular in large-scale urban scene
reconstruction due to their precise distance measurements, a capability
fundamentally absent in visual-based approaches. Researchers have made attempts
to utilize concurrent LiDAR and camera measurements in pursuit of precise
scaling and color details within mapping outcomes. However, the outcomes are
subject to extrinsic calibration and time synchronization precision. In this
paper, we propose a novel cost-effective reconstruction pipeline that utilizes
a pre-established LiDAR map as a fixed constraint to effectively address the
inherent scale challenges present in monocular camera reconstruction. To our
knowledge, our method is the first to register images onto the point cloud map
without requiring synchronous capture of camera and LiDAR data, granting us the
flexibility to manage reconstruction detail levels across various areas of
interest. To facilitate further research in this domain, we have released
Colmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that
enables precise fine-scale registration of images to the point cloud map.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Replication of Multi-agent Reinforcement Learning for the "Hide and
  Seek" Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haider Kamal, Muaz A. Niazi, Hammad Afzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning generates policies based on reward functions and
hyperparameters. Slight changes in these can significantly affect results. The
lack of documentation and reproducibility in Reinforcement learning research
makes it difficult to replicate once-deduced strategies. While previous
research has identified strategies using grounded maneuvers, there is limited
work in more complex environments. The agents in this study are simulated
similarly to Open Al's hider and seek agents, in addition to a flying
mechanism, enhancing their mobility, and expanding their range of possible
actions and strategies. This added functionality improves the Hider agents to
develop a chasing strategy from approximately 2 million steps to 1.6 million
steps and hiders
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ethics of Artificial Intelligence and Robotics in the Architecture,
  Engineering, and Construction Industry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ci-Jyun Liang, Thai-Hoa Le, Youngjib Ham, Bharadwaj R. K. Mantha, Marvin H. Cheng, Jacob J. Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) and robotics research and implementation emerged
in the architecture, engineering, and construction (AEC) industry to positively
impact project efficiency and effectiveness concerns such as safety,
productivity, and quality. This shift, however, warrants the need for ethical
considerations of AI and robotics adoption due to its potential negative
impacts on aspects such as job security, safety, and privacy. Nevertheless,
this did not receive sufficient attention, particularly within the academic
community. This research systematically reviews AI and robotics research
through the lens of ethics in the AEC community for the past five years. It
identifies nine key ethical issues namely job loss, data privacy, data
security, data transparency, decision-making conflict, acceptance and trust,
reliability and safety, fear of surveillance, and liability, by summarizing
existing literature and filtering it further based on its AEC relevance.
Furthermore, thirteen research topics along the process were identified based
on existing AEC studies that had direct relevance to the theme of ethics in
general and their parallels are further discussed. Finally, the current
challenges and knowledge gaps are discussed and seven specific future research
directions are recommended. This study not only signifies more stakeholder
awareness of this important topic but also provides imminent steps towards
safer and more efficient realization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>109 pages, 5 figures, submitted to Automation in Construction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPS Attack Detection and Mitigation for Safe Autonomous Driving using
  Image and Map based Lateral Direction Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingming Chen, Peng Liu, Guoqiang Li, Zhenpo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accuracy and robustness of vehicle localization are critical for
achieving safe and reliable high-level autonomy. Recent results show that GPS
is vulnerable to spoofing attacks, which is one major threat to autonomous
driving. In this paper, a novel anomaly detection and mitigation method against
GPS attacks that utilizes onboard camera and high-precision maps is proposed to
ensure accurate vehicle localization. First, lateral direction localization in
driving lanes is calculated by camera-based lane detection and map matching
respectively. Then, a real-time detector for GPS spoofing attack is developed
to evaluate the localization data. When the attack is detected, a multi-source
fusion-based localization method using Unscented Kalman filter is derived to
mitigate GPS attack and improve the localization accuracy. The proposed method
is validated in various scenarios in Carla simulator and open-source public
dataset to demonstrate its effectiveness in timely GPS attack detection and
data recovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wu, Xiuwei Xu, Ziwei Wang, Chong Xia, Linqing Zhao, Jiwen Lu, Haibin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel network framework for indoor 3D object
detection to handle variable input frame numbers in practical scenarios.
Existing methods only consider fixed frames of input data for a single
detector, such as monocular RGB-D images or point clouds reconstructed from
dense multi-view RGB-D images. While in practical application scenes such as
robot navigation and manipulation, the raw input to the 3D detectors is the
RGB-D images with variable frame numbers instead of the reconstructed scene
point cloud. However, the previous approaches can only handle fixed frame input
data and have poor performance with variable frame input. In order to
facilitate 3D object detection methods suitable for practical tasks, we present
a novel 3D detection framework named AnyView for our practical applications,
which generalizes well across different numbers of input frames with a single
model. To be specific, we propose a geometric learner to mine the local
geometric features of each input RGB-D image frame and implement local-global
feature interaction through a designed spatial mixture module. Meanwhile, we
further utilize a dynamic token strategy to adaptively adjust the number of
extracted features for each frame, which ensures consistent global feature
density and further enhances the generalization after fusion. Extensive
experiments on the ScanNet dataset show our method achieves both great
generalizability and high detection accuracy with a simple and clean
architecture containing a similar amount of parameters with the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAT-RRT: Motion Planning that Admits Contact One Link at a Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nataliya Nechyporenko, Caleb Escobedo, Shreyas Kadekodi, Alessandro Roncone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current motion planning approaches rely on binary collision checking to
evaluate the validity of a state and thereby dictate where the robot is allowed
to move. This approach leaves little room for robots to engage in contact with
an object, as is often necessary when operating in densely cluttered spaces. In
this work, we propose an alternative method that considers contact states as
high-cost states that the robot should avoid but can traverse if necessary to
complete a task. More specifically, we introduce Contact Admissible
Transition-based Rapidly exploring Random Trees (CAT-RRT), a planner that uses
a novel per-link cost heuristic to find a path by traversing high-cost obstacle
regions. Through extensive testing, we find that state-of-the-art optimization
planners tend to over-explore low-cost states, which leads to slow and
inefficient convergence to contact regions. Conversely, CAT-RRT searches both
low and high-cost regions simultaneously with an adaptive thresholding
mechanism carried out at each robot link. This leads to paths with a balance
between efficiency, path length, and contact cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Robot Gym: Benchmarking Reinforcement Learning in Human-Robot
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Thumm, Felix Trost, Matthias Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (RL) has shown promising results in robot motion
planning with first attempts in human-robot collaboration (HRC). However, a
fair comparison of RL approaches in HRC under the constraint of guaranteed
safety is yet to be made. We, therefore, present human-robot gym, a benchmark
for safe RL in HRC. Our benchmark provides eight challenging, realistic HRC
tasks in a modular simulation framework. Most importantly, human-robot gym
includes a safety shield that provably guarantees human safety. We are,
thereby, the first to provide a benchmark to train RL agents that adhere to the
safety specifications of real-world HRC. This bridges a critical gap between
theoretic RL research and its real-world deployment. Our evaluation of six
environments led to three key results: (a) the diverse nature of the tasks
offered by human-robot gym creates a challenging benchmark for state-of-the-art
RL methods, (b) incorporating expert knowledge in the RL training in the form
of an action-based reward can outperform the expert, and (c) our agents
negligibly overfit to training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion Memory: Leveraging Past Experiences to Accelerate Future Motion
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dibyendu Das, Yuanjie Lu, Erion Plaku, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When facing a new motion-planning problem, most motion planners solve it from
scratch, e.g., via sampling and exploration or starting optimization from a
straight-line path. However, most motion planners have to experience a variety
of planning problems throughout their lifetimes, which are yet to be leveraged
for future planning. In this paper, we present a simple but efficient method
called Motion Memory, which allows different motion planners to accelerate
future planning using past experiences. Treating existing motion planners as
either a closed or open box, we present a variety of ways that Motion Memory
can contribute to reduce the planning time when facing a new planning problem.
We provide extensive experiment results with three different motion planners on
three classes of planning problems with over 30,000 problem instances and show
that planning speed can be significantly reduced by up to 89% with the proposed
Motion Memory technique and with increasing past planning experiences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory-Consistent Neural Networks for Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, James Weimer, Insup Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning considerably simplifies policy synthesis compared to
alternative approaches by exploiting access to expert demonstrations. For such
imitation policies, errors away from the training samples are particularly
critical. Even rare slip-ups in the policy action outputs can compound quickly
over time, since they lead to unfamiliar future states where the policy is
still more likely to err, eventually causing task failures. We revisit simple
supervised ``behavior cloning'' for conveniently training the policy from
nothing more than pre-recorded demonstrations, but carefully design the model
class to counter the compounding error phenomenon. Our ``memory-consistent
neural network'' (MCNN) outputs are hard-constrained to stay within clearly
specified permissible regions anchored to prototypical ``memory'' training
samples. We provide a guaranteed upper bound for the sub-optimality gap induced
by MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP,
Transformer, and Diffusion backbones, spanning dexterous robotic manipulation
and driving, proprioceptive inputs and visual inputs, and varying sizes and
types of demonstration data, we find large and consistent gains in performance,
validating that MCNNs are better-suited than vanilla deep neural networks for
imitation learning applications. Website:
https://sites.google.com/view/mcnn-imitation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages (9 main pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing Robust Walking Gaits via Discrete-Time Barrier Functions
  with Application to Multi-Contact Exoskeleton Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maegan Tucker, Kejun Li, Aaron D. Ames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successfully achieving bipedal locomotion remains challenging due to
real-world factors such as model uncertainty, random disturbances, and
imperfect state estimation. In this work, we propose the use of discrete-time
barrier functions to certify hybrid forward invariance of reduced step-to-step
dynamics. The size of these invariant sets can then be used as a metric for
locomotive robustness. We demonstrate an application of this metric towards
synthesizing robust nominal walking gaits using a simulation-in-the-loop
approach. This procedure produces reference motions with step-to-step dynamics
that are maximally forward-invariant with respect to the reduced representation
of choice. The results demonstrate robust locomotion for both flat-foot walking
and multi-contact walking on the Atalante lower-body exoskeleton.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropy Based Multi-robot Active <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Farhan Ahmed, Matteo Maragliano, Vincent Frémont, Carmine Tommaso Recchiuto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we present an efficient multi-robot active SLAM framework
that involves a frontier-sharing method for maximum exploration of an unknown
environment. It encourages the robots to spread into the environment while
weighting the goal frontiers with the pose graph SLAM uncertainly and path
entropy. Our approach works on a limited number of frontier points and weights
the goal frontiers with a utility function that encapsulates both the SLAM and
map uncertainties, thus providing an efficient and not computationally
expensive solution. Our approach has been tested on publicly available
simulation environments and on real robots. An accumulative 31% more coverage
than similar state-of-the-art approaches has been obtained, proving the
capability of our approach for efficient environment exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Robot Task Assignment and Path Finding for Time-Sensitive Missions
  with Online Task Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Thorne, Brett T. Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Executing time-sensitive multi-robot missions involves two distinct problems:
Multi-Robot Task Assignment (MRTA) and Multi-Agent Path Finding (MAPF).
Computing safe paths that complete every task and minimize the time to mission
completion, or makespan, is a significant computational challenge even for
small teams. In many missions, tasks can be generated during execution which is
typically handled by either recomputing task assignments and paths from
scratch, or by modifying existing plans using approximate approaches. While
performing task reassignment and path finding from scratch produces
theoretically optimal results, the computational load makes it too expensive
for online implementation. In this work, we present Time-Sensitive Online Task
Assignment and Navigation (TSOTAN), a framework which can quickly incorporate
online generated tasks while guaranteeing bounded suboptimal task assignment
makespans. It does this by assessing the quality of partial task reassignments
and only performing a complete reoptimization when the makespan exceeds a user
specified suboptimality bound. Through experiments in 2D environments we
demonstrate TSOTAN's ability to produce quality solutions with computation
times suitable for online implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layout Sequence Prediction From Noisy Mobile Modality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Zhang, Yi Xu, Hongsheng Lu, Takayuki Shimizu, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction plays a vital role in understanding pedestrian movement
for applications such as autonomous driving and robotics. Current trajectory
prediction models depend on long, complete, and accurately observed sequences
from visual modalities. Nevertheless, real-world situations often involve
obstructed cameras, missed objects, or objects out of sight due to
environmental factors, leading to incomplete or noisy trajectories. To overcome
these limitations, we propose LTrajDiff, a novel approach that treats objects
obstructed or out of sight as equally important as those with fully visible
trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount
out-of-sight constraints, albeit introducing new challenges such as modality
fusion, noisy data, and the absence of spatial layout and object size
information. We employ a denoising diffusion model to predict precise layout
sequences from noisy mobile data using a coarse-to-fine diffusion strategy,
incorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model
predicts layout sequences by implicitly inferring object size and projection
status from a single reference timestamp or significantly obstructed sequences.
Achieving SOTA results in randomly obstructed experiments and extremely short
input experiments, our model illustrates the effectiveness of leveraging noisy
mobile data. In summary, our approach offers a promising solution to the
challenges faced by layout sequence and trajectory prediction models in
real-world settings, paving the way for utilizing sensor data from mobile
phones to accurately predict pedestrian bounding box trajectories. To the best
of our knowledge, this is the first work that addresses severely obstructed and
extremely short layout sequences by combining vision with noisy mobile
modality, making it the pioneering work in the field of layout sequence
trajectory prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 31st ACM International Conference on Multimedia
  2023 (MM 23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exoskeleton-Mediated Physical Human-Human Interaction for a Sit-to-Stand
  Rehabilitation Task <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Vianello, Emek Barış Küçüktabak, Matthew Short, Clément Lhoste, Lorenzo Amato, Kevin Lynch, Jose Pons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sit-to-Stand (StS) is a fundamental daily activity that can be challenging
for stroke survivors due to strength, motor control, and proprioception
deficits in their lower limbs. Existing therapies involve repetitive StS
exercises, but these can be physically demanding for therapists while assistive
devices may limit patient participation and hinder motor learning. To address
these challenges, this work proposes the use of two lower-limb exoskeletons to
mediate physical interaction between therapists and patients during a StS
rehabilitative task. This approach offers several advantages, including
improved therapist-patient interaction, safety enforcement, and performance
quantification. The whole body control of the two exoskeletons transmits online
feedback between the two users, but at the same time assists in movement and
ensures balance, and thus helping subjects with greater difficulty. In this
study we present the architecture of the framework, presenting and discussing
some technical choices made in the design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, submitted to 2024 IEEE The International
  Conference on Robotics and Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Agility: A Momentum Aware Trajectory Optimisation Framework
  using Full-Centroidal Dynamics & Implicit Inverse Kinematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aristotelis Papatheodorou, Wolfgang Merkt, Alexander L. Mitchell, Ioannis Havoutis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online planning and execution of acrobatic maneuvers pose significant
challenges in legged locomotion. Their underlying combinatorial nature, along
with the current hardware's limitations constitute the main obstacles in
unlocking the true potential of legged-robots. This letter tries to expose the
intricacies of these optimal control problems in a tangible way, directly
applicable to the creation of more efficient online trajectory optimisation
frameworks. By analysing the fundamental principles that shape the behaviour of
the system, the dynamics themselves can be exploited to surpass its hardware
limitations. More specifically, a trajectory optimisation formulation is
proposed that exploits the system's high-order nonlinearities, such as the
nonholonomy of the angular momentum, and phase-space symmetries in order to
produce feasible high-acceleration maneuvers. By leveraging the full-centroidal
dynamics of the quadruped ANYmal C and directly optimising its footholds and
contact forces, the framework is capable of producing efficient motion plans
with low computational overhead. The feasibility of the produced trajectories
is ensured by taking into account the configuration-dependent inertial
properties of the robot during the planning process, while its robustness is
increased by supplying the full analytic derivatives & hessians to the solver.
Finally, a significant portion of the discussion is centred around the
deployment of the proposed framework on the ANYmal C platform, while its true
capabilities are demonstrated through real-world experiments, with the
successful execution of high-acceleration motion scenarios like the squat-jump.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DyST: Towards Dynamic Neural Scene Representations on Real-World Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Seitzer, Sjoerd van Steenkiste, Thomas Kipf, Klaus Greff, Mehdi S. M. Sajjadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual understanding of the world goes beyond the semantics and flat
structure of individual images. In this work, we aim to capture both the 3D
structure and dynamics of real-world scenes from monocular real-world videos.
Our Dynamic Scene Transformer (DyST) model leverages recent work in neural
scene representation to learn a latent decomposition of monocular real-world
videos into scene content, per-view scene dynamics, and camera pose. This
separation is achieved through a novel co-training scheme on monocular videos
and our new synthetic dataset DySO. DyST learns tangible latent representations
for dynamic scenes that enable view generation with separate control over the
camera and the content of the scene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://dyst-paper.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Review of control algorithms for mobile robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andres-David Suarez-Gomez, Andres A. Hernandez Ortega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a comprehensive review of control algorithms used in
mobile robotics, a field in constant evolution. Mobile robotics has seen
significant advances in recent years, driven by the demand for applications in
various sectors, such as industrial automation, space exploration, and medical
care. The review focuses on control algorithms that address specific challenges
in navigation, localization, mapping, and path planning in changing and unknown
environments. Classical approaches, such as PID control and methods based on
classical control theory, as well as modern techniques, including deep learning
and model-based planning, are discussed in detail. In addition, practical
applications and remaining challenges in implementing these algorithms in
real-world mobile robots are highlighted. Ultimately, this review provides a
comprehensive overview of the diversity and complexity of control algorithms in
mobile robotics, helping researchers and practitioners to better understand the
options available to address specific problems in this exciting area of study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, in Spanish</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Hypernetworks are Surprisingly Strong in Meta-RL <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Beck, Risto Vuorio, Zheng Xiong, Shimon Whiteson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (RL) is notoriously impractical to deploy due to
sample inefficiency. Meta-RL directly addresses this sample inefficiency by
learning to perform few-shot learning when a distribution of related tasks is
available for meta-training. While many specialized meta-RL methods have been
proposed, recent work suggests that end-to-end learning in conjunction with an
off-the-shelf sequential model, such as a recurrent network, is a surprisingly
strong baseline. However, such claims have been controversial due to limited
supporting evidence, particularly in the face of prior work establishing
precisely the opposite. In this paper, we conduct an empirical investigation.
While we likewise find that a recurrent network can achieve strong performance,
we demonstrate that the use of hypernetworks is crucial to maximizing their
potential. Surprisingly, when combined with hypernetworks, the recurrent
baselines that are far simpler than existing specialized methods actually
achieve the strongest performance of all methods evaluated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Low-Cost Lane-Following Algorithm for Cyber-Physical Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archit Gupta, Arvind Easwaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Duckiebots are low-cost mobile robots that are widely used in the fields of
research and education. Although there are existing self-driving algorithms for
the Duckietown platform, they are either too complex or perform too poorly to
navigate a multi-lane track. Moreover, it is essential to give memory and
computational resources to a Duckiebot so it can perform additional tasks such
as out-of-distribution input detection. In order to satisfy these constraints,
we built a low-cost autonomous driving algorithm capable of driving on a
two-lane track. The algorithm uses traditional computer vision techniques to
identify the central lane on the track and obtain the relevant steering angle.
The steering is then controlled by a PID controller that smoothens the movement
of the Duckiebot. The performance of the algorithm was compared to that of the
NeurIPS 2018 AI Driving Olympics (AIDO) finalists, and it outperformed all but
one finalists. The two main contributions of our algorithm are its low
computational requirements and very quick set-up, with ongoing efforts to make
it more reliable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Redundancy for UWB Anomaly Detection in Infrastructure-Free
  Multi-Robot Relative Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Salimpour, Paola Torrico Morón, Xianjia Yu, Tomi Westerlund, Jorge Peña Queralta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultra-wideband (UWB) localization methods have emerged as a cost-effective
and accurate solution for GNSS-denied environments. There is a significant
amount of previous research in terms of resilience of UWB ranging, with
non-line-of-sight and multipath detection methods. However, little attention
has been paid to resilience against disturbances in relative localization
systems involving multiple nodes. This paper presents an approach to detecting
range anomalies in UWB ranging measurements from the perspective of multi-robot
cooperative localization. We introduce an approach to exploiting redundancy for
relative localization in multi-robot systems, where the position of each node
is calculated using different subsets of available data. This enables us to
effectively identify nodes that present ranging anomalies and eliminate their
effect within the cooperative localization scheme. We analyze anomalies created
by timing errors in the ranging process, e.g., owing to malfunctioning
hardware. However, our method is generic and can be extended to other types of
ranging anomalies. Our approach results in a more resilient cooperative
localization framework with a negligible impact in terms of the computational
workload.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniScene: Multi-Camera Unified Pre-training via 3D Scene Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18829v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18829v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Min, Liang Xiao, Dawei Zhao, Yiming Nie, Bin Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-camera 3D perception has emerged as a prominent research field in
autonomous driving, offering a viable and cost-effective alternative to
LiDAR-based solutions. The existing multi-camera algorithms primarily rely on
monocular 2D pre-training. However, the monocular 2D pre-training overlooks the
spatial and temporal correlations among the multi-camera system. To address
this limitation, we propose the first multi-camera unified pre-training
framework, called UniScene, which involves initially reconstructing the 3D
scene as the foundational stage and subsequently fine-tuning the model on
downstream tasks. Specifically, we employ Occupancy as the general
representation for the 3D scene, enabling the model to grasp geometric priors
of the surrounding world through pre-training. A significant benefit of
UniScene is its capability to utilize a considerable volume of unlabeled
image-LiDAR pairs for pre-training purposes. The proposed multi-camera unified
pre-training framework demonstrates promising results in key tasks such as
multi-camera 3D object detection and surrounding semantic scene completion.
When compared to monocular pre-training methods on the nuScenes dataset,
UniScene shows a significant improvement of about 2.0% in mAP and 2.0% in NDS
for multi-camera 3D object detection, as well as a 3% increase in mIoU for
surrounding semantic scene completion. By adopting our unified pre-training
method, a 25% reduction in 3D training annotation costs can be achieved,
offering significant practical value for the implementation of real-world
autonomous driving. Codes are publicly available at
https://github.com/chaytonmin/UniScene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Optimal Control via Heaviside Step-Function Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Pfeiffer, Quang-Cuong Pham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Least-squares programming is a popular tool in robotics due to its simplicity
and availability of open-source solvers. However, certain problems like sparse
programming in the $\ell_0$- or $\ell_1$-norm for time-optimal control are not
equivalently solvable. In this work, we propose a non-linear hierarchical
least-squares programming (NL-HLSP) for time-optimal control of non-linear
discrete dynamic systems. We use a continuous approximation of the heaviside
step function with an additional term that avoids vanishing gradients. We use a
simple discretization method by keeping states and controls piece-wise constant
between discretization steps. This way, we obtain a comparatively easily
implementable NL-HLSP in contrast to direct transcription approaches of optimal
control. We show that the NL-HLSP indeed recovers the discrete time-optimal
control in the limit for resting goal points. We confirm the results in
simulation for linear and non-linear control scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Explicable Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akkamahadevi Hanni, Andrew Boateng, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human expectations stem from their knowledge about the others and the world.
Where human-AI interaction is concerned, such knowledge may be inconsistent
with the ground truth, resulting in the AI agent not meeting its expectations
and degraded team performance. Explicable planning was previously introduced as
a novel planning approach to reconciling human expectations and the agent's
optimal behavior for more interpretable decision-making. One critical issue
that remains unaddressed is safety in explicable planning since it can lead to
explicable behaviors that are unsafe. We propose Safe Explicable Planning (SEP)
to extend the prior work to support the specification of a safety bound. The
objective of SEP is to search for behaviors that are close to the human's
expectations while satisfying the bound on the agent's return, the safety
criterion chosen in this work. We show that the problem generalizes
multi-objective optimization and our formulation introduces a Pareto set. Under
such a formulation, we propose a novel exact method that returns the Pareto set
of safe explicable policies, a more efficient greedy method that returns one of
the Pareto optimal policies, and approximate solutions for them based on the
aggregation of states to further scalability. Formal proofs are provided to
validate the desired theoretical properties of the exact and greedy methods. We
evaluate our methods both in simulation and with physical robot experiments.
Results confirm the validity and efficacy of our methods for safe explicable
planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sizhe Wei, Yuxi Wei, Yue Hu, Yifan Lu, Yiqi Zhong, Siheng Chen, Ya Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative perception can substantially boost each agent's perception
ability by facilitating communication among multiple agents. However, temporal
asynchrony among agents is inevitable in the real world due to communication
delays, interruptions, and clock misalignments. This issue causes information
mismatch during multi-agent fusion, seriously shaking the foundation of
collaboration. To address this issue, we propose CoBEVFlow, an
asynchrony-robust collaborative perception system based on bird's eye view
(BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align
asynchronous collaboration messages sent by multiple agents. To model the
motion in a scene, we propose BEV flow, which is a collection of the motion
vector corresponding to each spatial location. Based on BEV flow, asynchronous
perceptual features can be reassigned to appropriate positions, mitigating the
impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle
asynchronous collaboration messages sent at irregular, continuous time stamps
without discretization; and (ii) with BEV flow, CoBEVFlow only transports the
original perceptual features, instead of generating new perceptual features,
avoiding additional noises. To validate CoBEVFlow's efficacy, we create
IRregular V2V(IRV2V), the first synthetic collaborative perception dataset with
various temporal asynchronies that simulate different real-world scenarios.
Extensive experiments conducted on both IRV2V and the real-world dataset
DAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is
robust in extremely asynchronous settings. The code is available at
https://github.com/MediaBrain-SJTU/CoBEVFlow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures. Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coordination of multiple mobile manipulators for ordered sorting of
  cluttered objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeeho Ahn, Seabin Lee, Changjoo Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a coordination method for multiple mobile manipulators to sort
objects in clutter. We consider the object rearrangement problem in which the
objects must be sorted into different groups in a particular order. In clutter,
the order constraints could not be easily satisfied since some objects occlude
other objects so the occluded ones are not directly accessible to the robots.
Those objects occluding others need to be moved more than once to make the
occluded objects accessible. Such rearrangement problems fall into the class of
nonmonotone rearrangement problems which are computationally intractable. While
the nonmonotone problems with order constraints are harder, involving with
multiple robots requires another computation for task allocation. The proposed
method first finds a sequence of objects to be sorted using a search such that
the order constraint in each group is satisfied. The search can solve
nonmonotone instances that require temporal relocation of some objects to
access the next object to be sorted. Once a complete sorting sequence is found,
the objects in the sequence are assigned to multiple mobile manipulators using
a greedy allocation method. We develop four versions of the method with
different search strategies. In the experiments, we show that our method can
find a sorting sequence quickly (e.g., 4.6 sec with 20 objects sorted into five
groups) even though the solved instances include hard nonmonotone ones. The
extensive tests and the experiments in simulation show the ability of the
method to solve the real-world sorting problem using multiple mobile
manipulators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at iROS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Network-PSO-based Velocity Control Algorithm for Landing UAVs on
  a Boat 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li-Fan Wu, Zihan Wang, Mo Rastgaar, Nina Mahmoudian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise landing of Unmanned Aerial Vehicles (UAVs) onto moving platforms like
Autonomous Surface Vehicles (ASVs) is both important and challenging,
especially in GPS-denied environments, for collaborative navigation of
heterogeneous vehicles. UAVs need to land within a confined space onboard ASV
to get energy replenishment, while ASV is subject to translational and
rotational disturbances due to wind and water flow. Current solutions either
rely on high-level waypoint navigation, which struggles to robustly land on
varied-speed targets, or necessitate laborious manual tuning of controller
parameters, and expensive sensors for target localization. Therefore, we
propose an adaptive velocity control algorithm that leverages Particle Swarm
Optimization (PSO) and Neural Network (NN) to optimize PID parameters across
varying flight altitudes and distinct speeds of a moving boat. The cost
function of PSO includes the status change rates of UAV and proximity to the
target. The NN further interpolates the PSO-founded PID parameters. The
proposed method implemented on a water strider hexacopter design, not only
ensures accuracy but also increases robustness. Moreover, this NN-PSO can be
readily adapted to suit various mission requirements. Its ability to achieve
precise landings extends its applicability to scenarios, including but not
limited to rescue missions, package deliveries, and workspace inspections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flexible Attention-Based Multi-Policy Fusion for Efficient Deep
  Reinforcement Learning <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zih-Yun Chiu, Yi-Lin Tuan, William Yang Wang, Michael C. Yip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) agents have long sought to approach the
efficiency of human learning. Humans are great observers who can learn by
aggregating external knowledge from various sources, including observations
from others' policies of attempting a task. Prior studies in RL have
incorporated external knowledge policies to help agents improve sample
efficiency. However, it remains non-trivial to perform arbitrary combinations
and replacements of those policies, an essential feature for generalization and
transferability. In this work, we present Knowledge-Grounded RL (KGRL), an RL
paradigm fusing multiple knowledge policies and aiming for human-like
efficiency and flexibility. We propose a new actor architecture for KGRL,
Knowledge-Inclusive Attention Network (KIAN), which allows free knowledge
rearrangement due to embedding-based attentive action prediction. KIAN also
addresses entropy imbalance, a problem arising in maximum entropy KGRL that
hinders an agent from efficiently exploring the environment, through a new
design of policy distributions. The experimental results demonstrate that KIAN
outperforms alternative methods incorporating external knowledge policies and
achieves efficient and flexible learning. Our implementation is available at
https://github.com/Pascalson/KGRL.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">141</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video
  editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, Sen He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video editing aims to edit the visual appearance of a source video
conditional on textual prompts. A major challenge in this task is to ensure
that all frames in the edited video are visually consistent. Most recent works
apply advanced text-to-image diffusion models to this task by inflating 2D
spatial attention in the U-Net into spatio-temporal attention. Although
temporal context can be added through spatio-temporal attention, it may
introduce some irrelevant information for each patch and therefore cause
inconsistency in the edited video. In this paper, for the first time, we
introduce optical flow into the attention module in the diffusion model's U-Net
to address the inconsistency issue for text-to-video editing. Our method,
FLATTEN, enforces the patches on the same flow path across different frames to
attend to each other in the attention module, thus improving the visual
consistency in the edited videos. Additionally, our method is training-free and
can be seamlessly integrated into any diffusion-based text-to-video editing
methods and improve their visual consistency. Experiment results on existing
text-to-video editing benchmarks show that our proposed method achieves the new
state-of-the-art performance. In particular, our method excels in maintaining
the visual consistency in the edited videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://flatten-video-editing.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimPLR: A Simple and Plain Transformer for Object Detection and
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy-Kien Nguyen, Martin R. Oswald, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to detect objects in images at varying scales has played a
pivotal role in the design of modern object detectors. Despite considerable
progress in removing handcrafted components using transformers, multi-scale
feature maps remain a key factor for their empirical success, even with a plain
backbone like the Vision Transformer (ViT). In this paper, we show that this
reliance on feature pyramids is unnecessary and a transformer-based detector
with scale-aware attention enables the plain detector `SimPLR' whose backbone
and detection head both operate on single-scale features. The plain
architecture allows SimPLR to effectively take advantages of self-supervised
learning and scaling approaches with ViTs, yielding strong performance compared
to multi-scale counterparts. We demonstrate through our experiments that when
scaling to larger backbones, SimPLR indicates better performance than
end-to-end detectors (Mask2Former) and plain-backbone detectors (ViTDet), while
consistently being faster. The code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic
  Clothing Driven by Sparse RGB-D Input <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donglai Xiang, Fabian Prada, Zhe Cao, Kaiwen Guo, Chenglei Wu, Jessica Hodgins, Timur Bagautdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clothing is an important part of human appearance but challenging to model in
photorealistic avatars. In this work we present avatars with dynamically moving
loose clothing that can be faithfully driven by sparse RGB-D inputs as well as
body and face motion. We propose a Neural Iterative Closest Point (N-ICP)
algorithm that can efficiently track the coarse garment shape given sparse
depth input. Given the coarse tracking results, the input RGB-D images are then
remapped to texel-aligned features, which are fed into the drivable avatar
models to faithfully reconstruct appearance details. We evaluate our method
against recent image-driven synthesis baselines, and conduct a comprehensive
analysis of the N-ICP algorithm. We demonstrate that our method can generalize
to a novel testing environment, while preserving the ability to produce
high-fidelity and faithful clothing dynamics and appearance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2023 Conference Paper. Project website:
  https://xiangdonglai.github.io/www-sa23-drivable-clothing/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting CLIP's Image Representation via Text-Based Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the CLIP image encoder by analyzing how individual model
components affect the final representation. We decompose the image
representation as a sum across individual image patches, model layers, and
attention heads, and use CLIP's text representation to interpret the summands.
Interpreting the attention heads, we characterize each head's role by
automatically finding text representations that span its output space, which
reveals property-specific roles for many heads (e.g. location or shape). Next,
interpreting the image patches, we uncover an emergent spatial localization
within CLIP. Finally, we use this understanding to remove spurious features
from CLIP and to create a strong zero-shot image segmenter. Our results
indicate that a scalable understanding of transformer models is attainable and
can be used to repair and improve models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and code:
  https://yossigandelsman.github.io/clip_decomposition/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Streaming Anchor Loss: Augmenting Supervision with Temporal Significance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Utkarsh,  Sarawgi, John Berkowitz, Vineet Garg, Arnav Kundu, Minsik Cho, Sai Srujana Buddi, Saurabh Adya, Ahmed Tewfik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Streaming neural network models for fast frame-wise responses to various
speech and sensory signals are widely adopted on resource-constrained
platforms. Hence, increasing the learning capacity of such streaming models
(i.e., by adding more parameters) to improve the predictive power may not be
viable for real-world tasks. In this work, we propose a new loss, Streaming
Anchor Loss (SAL), to better utilize the given learning capacity by encouraging
the model to learn more from essential frames. More specifically, our SAL and
its focal variations dynamically modulate the frame-wise cross entropy loss
based on the importance of the corresponding frames so that a higher loss
penalty is assigned for frames within the temporal proximity of semantically
critical events. Therefore, our loss ensures that the model training focuses on
predicting the relatively rare but task-relevant frames. Experimental results
with standard lightweight convolutional and recurrent streaming networks on
three different speech based detection tasks demonstrate that SAL enables the
model to learn the overall task more effectively with improved accuracy and
latency, without any additional data, model parameters, or architectural
changes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review for ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Chest X-Ray Report Generation from Longitudinal
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni, Jeffrey Dalton, Alison Q O'Neil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology reports are detailed text descriptions of the content of medical
scans. Each report describes the presence/absence and location of relevant
clinical findings, commonly including comparison with prior exams of the same
patient to describe how they evolved. Radiology reporting is a time-consuming
process, and scan results are often subject to delays. One strategy to speed up
reporting is to integrate automated reporting systems, however clinical
deployment requires high accuracy and interpretability. Previous approaches to
automated radiology reporting generally do not provide the prior study as
input, precluding comparison which is required for clinical accuracy in some
types of scans, and offer only unreliable methods of interpretability.
Therefore, leveraging an existing visual input format of anatomical tokens, we
introduce two novel aspects: (1) longitudinal representation learning -- we
input the prior scan as an additional input, proposing a method to align,
concatenate and fuse the current and prior visual information into a joint
longitudinal representation which can be provided to the multimodal report
generation model; (2) sentence-anatomy dropout -- a training strategy for
controllability in which the report generator model is trained to predict only
sentences from the original report which correspond to the subset of anatomical
regions given as input. We show through in-depth experiments on the MIMIC-CXR
dataset how the proposed approach achieves state-of-the-art results while
enabling anatomy-wise controllable report generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Findings of EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhili Liu, Kai Chen, Yifan Zhang, Jianhua Han, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning diffusion models through personalized datasets is an acknowledged
method for improving generation quality across downstream tasks, which,
however, often inadvertently generates unintended concepts such as watermarks
and QR codes, attributed to the limitations in image sources and collecting
methods within specific downstream tasks. Existing solutions suffer from
eliminating these unintentionally learned implicit concepts, primarily due to
the dependency on the model's ability to recognize concepts that it actually
cannot discern. In this work, we introduce \methodname, a novel approach that
successfully removes the implicit concepts with either an additional accessible
classifier or detector model to encode geometric information of these concepts
into text domain. Moreover, we propose \textit{Implicit Concept}, a novel
image-text dataset imbued with three implicit concepts (\ie, watermarks, QR
codes, and text) for training and evaluation. Experimental results demonstrate
that \methodname not only identifies but also proficiently eradicates implicit
concepts, revealing a significant improvement over the existing methods. The
integration of geometric information marks a substantial progression in the
precise removal of implicit concepts in diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViCor: Bridging Visual Understanding and Commonsense Reasoning with
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiwen Zhou, Kwonjoon Lee, Teruhisa Misu, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our work, we explore the synergistic capabilities of pre-trained
vision-and-language models (VLMs) and large language models (LLMs) for visual
commonsense reasoning (VCR). We categorize the problem of VCR into visual
commonsense understanding (VCU) and visual commonsense inference (VCI). For
VCU, which involves perceiving the literal visual content, pre-trained VLMs
exhibit strong cross-dataset generalization. On the other hand, in VCI, where
the goal is to infer conclusions beyond image content, VLMs face difficulties.
We find that a baseline where VLMs provide perception results (image captions)
to LLMs leads to improved performance on VCI. However, we identify a challenge
with VLMs' passive perception, which often misses crucial context information,
leading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we
suggest a collaborative approach where LLMs, when uncertain about their
reasoning, actively direct VLMs to concentrate on and gather relevant visual
elements to support potential commonsense inferences. In our method, named
ViCor, pre-trained LLMs serve as problem classifiers to analyze the problem
category, VLM commanders to leverage VLMs differently based on the problem
classification, and visual commonsense reasoners to answer the question. VLMs
will perform visual recognition and understanding. We evaluate our framework on
two VCR benchmark datasets and outperform all other methods that do not require
in-domain supervised fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-wise Invariant Learning for Panoptic Scene Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Li, You Qin, Wei Ji, Yuxiao Zhou, Roger Zimmermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoptic Scene Graph Generation (PSG) involves the detection of objects and
the prediction of their corresponding relationships (predicates). However, the
presence of biased predicate annotations poses a significant challenge for PSG
models, as it hinders their ability to establish a clear decision boundary
among different predicates. This issue substantially impedes the practical
utility and real-world applicability of PSG models. To address the intrinsic
bias above, we propose a novel framework to infer potentially biased
annotations by measuring the predicate prediction risks within each
subject-object pair (domain), and adaptively transfer the biased annotations to
consistent ones by learning invariant predicate representation embeddings.
Experiments show that our method significantly improves the performance of
benchmark models, achieving a new state-of-the-art performance, and shows great
generalization and effectiveness on PSG dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2307.15567</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-grained Audio-Visual Joint Representations for Multimodal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual large language models (LLM) have drawn significant attention,
yet the fine-grained combination of both input streams is rather
under-explored, which is challenging but necessary for LLMs to understand
general video inputs. To this end, a fine-grained audio-visual joint
representation (FAVOR) learning framework for multimodal LLMs is proposed in
this paper, which extends a text-based LLM to simultaneously perceive speech
and audio events in the audio input stream and images or videos in the visual
input stream, at the frame level. To fuse the audio and visual feature streams
into joint representations and to align the joint space with the LLM input
embedding space, we propose a causal Q-Former structure with a causal attention
module to enhance the capture of causal relations of the audio-visual frames
across time. An audio-visual evaluation benchmark (AVEB) is also proposed which
comprises six representative single-modal tasks with five cross-modal tasks
reflecting audio-visual co-reasoning abilities. While achieving competitive
single-modal performance on audio, speech and image tasks in AVEB, FAVOR
achieved over 20% accuracy improvements on the video question-answering task
when fine-grained information or temporal causal reasoning is required. FAVOR,
in addition, demonstrated remarkable video comprehension and reasoning
abilities on tasks that are unprecedented by other multimodal LLMs. An
interactive demo of FAVOR is available at
https://github.com/the-anonymous-bs/FAVOR.git, and the training code and model
checkpoints will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rephrase, Augment, Reason: Visual Grounding of Questions for
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An increasing number of vision-language tasks can be handled with little to
no training, i.e., in a zero and few-shot manner, by marrying large language
models (LLMs) to vision encoders, resulting in large vision-language models
(LVLMs). While this has huge upsides, such as not requiring training data or
custom architectures, how an input is presented to a LVLM can have a major
impact on zero-shot model performance. In particular, inputs phrased in an
underspecified way can result in incorrect answers due to factors like missing
visual information, complex implicit reasoning, or linguistic ambiguity.
Therefore, adding visually grounded information to the input as a preemptive
clarification should improve model performance by reducing underspecification,
e.g., by localizing objects and disambiguating references. Similarly, in the
VQA setting, changing the way questions are framed can make them easier for
models to answer. To this end, we present Rephrase, Augment and Reason
(RepARe), a gradient-free framework that extracts salient details about the
image using the underlying LVLM as a captioner and reasoner, in order to
propose modifications to the original question. We then use the LVLM's
confidence over a generated answer as an unsupervised scoring function to
select the rephrased question most likely to improve zero-shot performance.
Focusing on two visual question answering tasks, we show that RepARe can result
in a 3.85% (absolute) increase in zero-shot performance on VQAv2 and a 6.41%
point increase on A-OKVQA. Additionally, we find that using gold answers for
oracle question candidate selection achieves a substantial gain in VQA accuracy
by up to 14.41%. Through extensive analysis, we demonstrate that outputs from
RepARe increase syntactic complexity, and effectively utilize vision-language
interaction and the frozen language model in LVLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 4 figures, Code: https://github.com/archiki/RepARe</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Real-time Method for Inserting Virtual Objects into Neural Radiance
  Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyang Ye, Hongzhi Wu, Xin Tong, Kun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first real-time method for inserting a rigid virtual object
into a neural radiance field, which produces realistic lighting and shadowing
effects, as well as allows interactive manipulation of the object. By
exploiting the rich information about lighting and geometry in a NeRF, our
method overcomes several challenges of object insertion in augmented reality.
For lighting estimation, we produce accurate, robust and 3D spatially-varying
incident lighting that combines the near-field lighting from NeRF and an
environment lighting to account for sources not covered by the NeRF. For
occlusion, we blend the rendered virtual object with the background scene using
an opacity map integrated from the NeRF. For shadows, with a precomputed field
of spherical signed distance field, we query the visibility term for any point
around the virtual object, and cast soft, detailed shadows onto 3D surfaces.
Compared with state-of-the-art techniques, our approach can insert virtual
object into scenes with superior fidelity, and has a great potential to be
further applied to augmented reality systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting the Temporal Modeling in Spatio-Temporal Predictive Learning
  under A Unified View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Tan, Jue Wang, Zhangyang Gao, Siyuan Li, Lirong Wu, Jun Xia, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal predictive learning plays a crucial role in self-supervised
learning, with wide-ranging applications across a diverse range of fields.
Previous approaches for temporal modeling fall into two categories:
recurrent-based and recurrent-free methods. The former, while meticulously
processing frames one by one, neglect short-term spatio-temporal information
redundancies, leading to inefficiencies. The latter naively stack frames
sequentially, overlooking the inherent temporal dependencies. In this paper, we
re-examine the two dominant temporal modeling approaches within the realm of
spatio-temporal predictive learning, offering a unified perspective. Building
upon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive
learning), an innovative framework that reconciles the recurrent-based and
recurrent-free methods by integrating both micro-temporal and macro-temporal
scales. Extensive experiments on a wide range of spatio-temporal predictive
learning demonstrate that USTEP achieves significant improvements over existing
temporal modeling approaches, thereby establishing it as a robust solution for
a wide range of spatio-temporal applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Convergent Data-Driven Convex-Nonconvex Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zakhar Shumaylov, Jeremy Budd, Subhadip Mukherjee, Carola-Bibiane Schönlieb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An emerging new paradigm for solving inverse problems is via the use of deep
learning to learn a regularizer from data. This leads to high-quality results,
but often at the cost of provable guarantees. In this work, we show how
well-posedness and convergent regularization arises within the convex-nonconvex
(CNC) framework for inverse problems. We introduce a novel input weakly convex
neural network (IWCNN) construction to adapt the method of learned adversarial
regularization to the CNC framework. Empirically we show that our method
overcomes numerical issues of previous adversarial methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages + 3 pages appendices; preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Language-guided Adaptive Hyper-modality Representation for
  Multimodal Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhang, Yu Wang, Guanghao Yin, Kejun Liu, Yuanyuan Liu, Tianshu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential sentiment-irrelevant and conflicting information across modalities
may hinder the performance from being further improved. To alleviate this, we
present Adaptive Language-guided Multimodal Transformer (ALMT), which
incorporates an Adaptive Hyper-modality Learning (AHL) module to learn an
irrelevance/conflict-suppressing representation from visual and audio features
under the guidance of language features at different scales. With the obtained
hyper-modality representation, the model can obtain a complementary and joint
representation through multimodal fusion for effective MSA. In practice, ALMT
achieves state-of-the-art performance on several popular datasets (e.g., MOSI,
MOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and
necessity of our irrelevance/conflict suppression mechanism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint object detection and re-identification for 3D obstacle
  multi-camera systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irene Cortés, Jorge Beltrán, Arturo de la Escalera, Fernando García
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the field of autonomous driving has witnessed remarkable
advancements, driven by the integration of a multitude of sensors, including
cameras and LiDAR systems, in different prototypes. However, with the
proliferation of sensor data comes the pressing need for more sophisticated
information processing techniques. This research paper introduces a novel
modification to an object detection network that uses camera and lidar
information, incorporating an additional branch designed for the task of
re-identifying objects across adjacent cameras within the same vehicle while
elevating the quality of the baseline 3D object detection outcomes. The
proposed methodology employs a two-step detection pipeline: initially, an
object detection network is employed, followed by a 3D box estimator that
operates on the filtered point cloud generated from the network's detections.
Extensive experimental evaluations encompassing both 2D and 3D domains validate
the effectiveness of the proposed approach and the results underscore the
superiority of this method over traditional Non-Maximum Suppression (NMS)
techniques, with an improvement of more than 5\% in the car category in the
overlapping areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ultimate goal of Dataset Distillation is to synthesize a small synthetic
dataset such that a model trained on this synthetic set will perform equally
well as a model trained on the full, real dataset. Until now, no method of
Dataset Distillation has reached this completely lossless goal, in part due to
the fact that previous methods only remain effective when the total number of
synthetic samples is extremely small. Since only so much information can be
contained in such a small number of samples, it seems that to achieve truly
loss dataset distillation, we must develop a distillation method that remains
effective as the size of the synthetic dataset grows. In this work, we present
such an algorithm and elucidate why existing methods fail to generate larger,
high-quality synthetic sets. Current state-of-the-art methods rely on
trajectory-matching, or optimizing the synthetic data to induce similar
long-term training dynamics as the real data. We empirically find that the
training stage of the trajectories we choose to match (i.e., early or late)
greatly affects the effectiveness of the distilled dataset. Specifically, early
trajectories (where the teacher network learns easy patterns) work well for a
low-cardinality synthetic set since there are fewer examples wherein to
distribute the necessary information. Conversely, late trajectories (where the
teacher network learns hard patterns) provide better signals for larger
synthetic sets since there are now enough samples to represent the necessary
complex patterns. Based on our findings, we propose to align the difficulty of
the generated patterns with the size of the synthetic dataset. In doing so, we
successfully scale trajectory matching-based methods to larger synthetic
datasets, achieving lossless dataset distillation for the very first time. Code
and distilled datasets are available at https://gzyaftermath.github.io/DATM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First lossless dataset distillation method</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DANet: Enhancing Small Object Detection through an Efficient Deformable
  Attention Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Sohag Mia, Abdullah Al Bary Voban, Abu Bakor Hayat Arnob, Abdu Naim, Md Kawsar Ahmed, Md Shariful Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and accurate detection of small objects in manufacturing settings,
such as defects and cracks, is crucial for ensuring product quality and safety.
To address this issue, we proposed a comprehensive strategy by synergizing
Faster R-CNN with cutting-edge methods. By combining Faster R-CNN with Feature
Pyramid Network, we enable the model to efficiently handle multi-scale features
intrinsic to manufacturing environments. Additionally, Deformable Net is used
that contorts and conforms to the geometric variations of defects, bringing
precision in detecting even the minuscule and complex features. Then, we
incorporated an attention mechanism called Convolutional Block Attention Module
in each block of our base ResNet50 network to selectively emphasize informative
features and suppress less useful ones. After that we incorporated RoI Align,
replacing RoI Pooling for finer region-of-interest alignment and finally the
integration of Focal Loss effectively handles class imbalance, crucial for rare
defect occurrences. The rigorous evaluation of our model on both the NEU-DET
and Pascal VOC datasets underscores its robust performance and generalization
capabilities. On the NEU-DET dataset, our model exhibited a profound
understanding of steel defects, achieving state-of-the-art accuracy in
identifying various defects. Simultaneously, when evaluated on the Pascal VOC
dataset, our model showcases its ability to detect objects across a wide
spectrum of categories within complex and small scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D tomatoes' localisation with monocular cameras using histogram filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandro Costa Magalhães, Filipe Neves dos Santos, António Paulo Moreira, Jorge Dias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performing tasks in agriculture, such as fruit monitoring or harvesting,
requires perceiving the objects' spatial position. RGB-D cameras are limited
under open-field environments due to lightning interferences. Therefore, in
this study, we approach the use of Histogram Filters (Bayesian Discrete
Filters) to estimate the position of tomatoes in the tomato plant. Two kernel
filters were studied: the square kernel and the Gaussian kernel. The
implemented algorithm was essayed in simulation, with and without Gaussian
noise and random noise, and in a testbed at laboratory conditions. The
algorithm reported a mean absolute error lower than 10 mm in simulation and 20
mm in the testbed at laboratory conditions with an assessing distance of about
0.5 m. So, the results are viable for real environments and should be improved
at closer distances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing the power of Neural Collapse for Transferability Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhe Ding, Bo Jiang, Lijun Sheng, Aihua Zheng, Jian Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transferability estimation aims to provide heuristics for quantifying how
suitable a pre-trained model is for a specific downstream task, without
fine-tuning them all. Prior studies have revealed that well-trained models
exhibit the phenomenon of Neural Collapse. Based on a widely used neural
collapse metric in existing literature, we observe a strong correlation between
the neural collapse of pre-trained models and their corresponding fine-tuned
models. Inspired by this observation, we propose a novel method termed Fair
Collapse (FaCe) for transferability estimation by comprehensively measuring the
degree of neural collapse in the pre-trained model. Typically, FaCe comprises
two different terms: the variance collapse term, which assesses the class
separation and within-class compactness, and the class fairness term, which
quantifies the fairness of the pre-trained model towards each class. We
investigate FaCe on a variety of pre-trained classification models across
different network architectures, source datasets, and training loss functions.
Results show that FaCe yields state-of-the-art performance on different tasks
including image classification, semantic segmentation, and text classification,
which demonstrate the effectiveness and generalization of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) are the dominant models for generative
tasks in language, they do not perform as well as diffusion models on image and
video generation. To effectively use LLMs for visual generation, one crucial
component is the visual tokenizer that maps pixel-space inputs to discrete
tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a
video tokenizer designed to generate concise and expressive tokens for both
videos and images using a common token vocabulary. Equipped with this new
tokenizer, we show that LLMs outperform diffusion models on standard image and
video generation benchmarks including ImageNet and Kinetics. In addition, we
demonstrate that our tokenizer surpasses the previously top-performing video
tokenizer on two more tasks: (1) video compression comparable to the
next-generation video codec (VCC) according to human evaluations, and (2)
learning effective representations for action recognition tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyperLips: Hyper Control Lips with High Resolution Decoder for Talking
  Face Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaosen Chen, Yu Yao, Zhiqiang Li, Wei Wang, Yanru Zhang, Han Yang, Xuming Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking face generation has a wide range of potential applications in the
field of virtual digital humans. However, rendering high-fidelity facial video
while ensuring lip synchronization is still a challenge for existing
audio-driven talking face generation approaches. To address this issue, we
propose HyperLips, a two-stage framework consisting of a hypernetwork for
controlling lips and a high-resolution decoder for rendering high-fidelity
faces.In the first stage, we construct a base face generation network that uses
the hypernetwork to control the encoding latent code of the visual face
information over audio. First, FaceEncoder is used to obtain latent code by
extracting features from the visual face information taken from the video
source containing the face frame.Then, HyperConv, which weighting parameters
are updated by HyperNet with the audio features as input, will modify the
latent code to synchronize the lip movement with the audio. Finally,
FaceDecoder will decode the modified and synchronized latent code into visual
face content. In the second stage, we obtain higher quality face videos through
a high-resolution decoder. To further improve the quality of face generation,
we trained a high-resolution decoder, HRDecoder, using face images and detected
sketches generated from the first stage as input.Extensive quantitative and
qualitative experiments show that our method outperforms state-of-the-art work
with more realistic, high-fidelity, and lip synchronization. Project page:
https://semchan.github.io/HyperLips/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational
  Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gulcin Baykal, Melih Kandemir, Gozde Unal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Codebook collapse is a common problem in training deep generative models with
discrete representation spaces like Vector Quantized Variational Autoencoders
(VQ-VAEs). We observe that the same problem arises for the alternatively
designed discrete variational autoencoders (dVAEs) whose encoder directly
learns a distribution over the codebook embeddings to represent the data. We
hypothesize that using the softmax function to obtain a probability
distribution causes the codebook collapse by assigning overconfident
probabilities to the best matching codebook elements. In this paper, we propose
a novel way to incorporate evidential deep learning (EDL) instead of softmax to
combat the codebook collapse problem of dVAE. We evidentially monitor the
significance of attaining the probability distribution over the codebook
embeddings, in contrast to softmax usage. Our experiments using various
datasets show that our model, called EdVAE, mitigates codebook collapse while
improving the reconstruction performance, and enhances the codebook usage
compared to dVAE and VQ-VAE based models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects
  on Production Lines <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Kuang, Qin Han, Danshi Li, Qiyu Dai, Lian Ding, Dong Sun, Hanlin Zhao, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present STOPNet, a framework for 6-DoF object suction
detection on production lines, with a focus on but not limited to transparent
objects, which is an important and challenging problem in robotic systems and
modern industry. Current methods requiring depth input fail on transparent
objects due to depth cameras' deficiency in sensing their geometry, while we
proposed a novel framework to reconstruct the scene on the production line
depending only on RGB input, based on multiview stereo. Compared to existing
works, our method not only reconstructs the whole 3D scene in order to obtain
high-quality 6-DoF suction poses in real time but also generalizes to novel
environments, novel arrangements and novel objects, including challenging
transparent objects, both in simulation and the real world. Extensive
experiments in simulation and the real world show that our method significantly
surpasses the baselines and has better generalizability, which caters to
practical industrial needs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review. ICRA 2024 submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni3DETR: Unified 3D Detection Transformer <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wang, Yali Li, Xi Chen, Hengshuang Zhao, Shengjin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing point cloud based 3D detectors are designed for the particular
scene, either indoor or outdoor ones. Because of the substantial differences in
object distribution and point density within point clouds collected from
various environments, coupled with the intricate nature of 3D metrics, there is
still a lack of a unified network architecture that can accommodate diverse
scenes. In this paper, we propose Uni3DETR, a unified 3D detector that
addresses indoor and outdoor 3D detection within the same framework.
Specifically, we employ the detection transformer with point-voxel interaction
for object prediction, which leverages voxel features and points for
cross-attention and behaves resistant to the discrepancies from data. We then
propose the mixture of query points, which sufficiently exploits global
information for dense small-range indoor scenes and local information for
large-range sparse outdoor ones. Furthermore, our proposed decoupled IoU
provides an easy-to-optimize training target for localization by disentangling
the xy and z space. Extensive experiments validate that Uni3DETR exhibits
excellent performance consistently on both indoor and outdoor 3D detection. In
contrast to previous specialized detectors, which may perform well on some
particular datasets but suffer a substantial degradation on different scenes,
Uni3DETR demonstrates the strong generalization ability under heterogeneous
conditions (Fig. 1).
  Codes are available at
\href{https://github.com/zhenyuw16/Uni3DETR}{https://github.com/zhenyuw16/Uni3DETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining recurrent and residual learning for deforestation monitoring
  using multitemporal SAR images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carla Nascimento Neves, Raul Queiroz Feitosa, Mabel X. Ortega Adarme, Gilson Antonio Giraldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With its vast expanse, exceeding that of Western Europe by twice, the Amazon
rainforest stands as the largest forest of the Earth, holding immense
importance in global climate regulation. Yet, deforestation detection from
remote sensing data in this region poses a critical challenge, often hindered
by the persistent cloud cover that obscures optical satellite data for much of
the year. Addressing this need, this paper proposes three deep-learning models
tailored for deforestation monitoring, utilizing SAR (Synthetic Aperture Radar)
multitemporal data moved by its independence on atmospheric conditions.
Specifically, the study proposes three novel recurrent fully convolutional
network architectures-namely, RRCNN-1, RRCNN-2, and RRCNN-3, crafted to enhance
the accuracy of deforestation detection. Additionally, this research explores
replacing a bitemporal with multitemporal SAR sequences, motivated by the
hypothesis that deforestation signs quickly fade in SAR images over time. A
comprehensive assessment of the proposed approaches was conducted using a
Sentinel-1 multitemporal sequence from a sample site in the Brazilian
rainforest. The experimental analysis confirmed that analyzing a sequence of
SAR images over an observation period can reveal deforestation spots
undetectable in a pair of images. Notably, experimental results underscored the
superiority of the multitemporal approach, yielding approximately a five
percent enhancement in F1-Score across all tested network architectures.
Particularly the RRCNN-1 achieved the highest accuracy and also boasted half
the processing time of its closest counterpart.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 19 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Climate-sensitive Urban Planning through Optimization of Tree Placements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Schrodi, Ferdinand Briegel, Max Argus, Andreas Christen, Thomas Brox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change is increasing the intensity and frequency of many extreme
weather events, including heatwaves, which results in increased thermal
discomfort and mortality rates. While global mitigation action is undoubtedly
necessary, so is climate adaptation, e.g., through climate-sensitive urban
planning. Among the most promising strategies is harnessing the benefits of
urban trees in shading and cooling pedestrian-level environments. Our work
investigates the challenge of optimal placement of such trees. Physical
simulations can estimate the radiative and thermal impact of trees on human
thermal comfort but induce high computational costs. This rules out
optimization of tree placements over large areas and considering effects over
longer time scales. Hence, we employ neural networks to simulate the point-wise
mean radiant temperatures--a driving factor of outdoor human thermal
comfort--across various time scales, spanning from daily variations to extended
time scales of heatwave events and even decades. To optimize tree placements,
we harness the innate local effect of trees within the iterated local search
framework with tailored adaptations. We show the efficacy of our approach
across a wide spectrum of study areas and time scales. We believe that our
approach is a step towards empowering decision-makers, urban designers and
planners to proactively and effectively assess the potential of urban trees to
mitigate heat stress.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Rainfall Variability and Water Extent of Selected Hydropower
  Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical
  Countries, Sri Lanka and Vietnam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Punsisi Rajakaruna, Surajit Ghosh, Bunyod Holmatov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a comprehensive remote sensing analysis of rainfall
patterns and selected hydropower reservoir water extent in two tropical monsoon
countries, Vietnam and Sri Lanka. The aim is to understand the relationship
between remotely sensed rainfall data and the dynamic changes (monthly) in
reservoir water extent. The analysis utilizes high-resolution optical imagery
and Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water
bodies during different weather conditions, especially during the monsoon
season. The average annual rainfall for both countries is determined, and
spatiotemporal variations in monthly average rainfall are examined at regional
and reservoir basin levels using the Climate Hazards Group InfraRed
Precipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents
are derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected
(GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are
pre-processed and corrected using terrain correction and refined Lee filter. An
automated thresholding algorithm, OTSU, distinguishes water and land, taking
advantage of both VV and VH polarization data. The connected pixel count
threshold is applied to enhance result accuracy. The results indicate a clear
relationship between rainfall patterns and reservoir water extent, with
increased precipitation during the monsoon season leading to higher water
extents in the later months. This study contributes to understanding how
rainfall variability impacts reservoir water resources in tropical monsoon
regions. The preliminary findings can inform water resource management
strategies and support these countries' decision-making processes related to
hydropower generation, flood management, and irrigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anchor-Intermediate Detector: Decoupling and Coupling Bounding Boxes for
  Accurate Object Detection <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilong Lv, Min Li, Yujie He, Shaopeng Li, Zhuzhen He, Aitao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anchor-based detectors have been continuously developed for object detection.
However, the individual anchor box makes it difficult to predict the boundary's
offset accurately. Instead of taking each bounding box as a closed individual,
we consider using multiple boxes together to get prediction boxes. To this end,
this paper proposes the \textbf{Box Decouple-Couple(BDC) strategy} in the
inference, which no longer discards the overlapping boxes, but decouples the
corner points of these boxes. Then, according to each corner's score, we couple
the corner points to select the most accurate corner pairs. To meet the BDC
strategy, a simple but novel model is designed named the
\textbf{Anchor-Intermediate Detector(AID)}, which contains two head networks,
i.e., an anchor-based head and an anchor-free \textbf{Corner-aware head}. The
corner-aware head is able to score the corners of each bounding box to
facilitate the coupling between corner points. Extensive experiments on MS COCO
show that the proposed anchor-intermediate detector respectively outperforms
their baseline RetinaNet and GFL method by $\sim$2.4 and $\sim$1.2 AP on the MS
COCO test-dev dataset without any bells and whistles. Code is available at:
https://github.com/YilongLv/AID.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted 29 September, 2023; originally announced October 2023.
  Accepted by ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViTs are Everywhere: A Comprehensive Study Showcasing Vision
  Transformers in Different Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Sohag Mia, Abu Bakor Hayat Arnob, Abdu Naim+, Abdullah Al Bary Voban, Md Shariful Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer design is the de facto standard for natural language processing
tasks. The success of the transformer design in natural language processing has
lately piqued the interest of researchers in the domain of computer vision.
When compared to Convolutional Neural Networks (CNNs), Vision Transformers
(ViTs) are becoming more popular and dominant solutions for many vision
problems. Transformer-based models outperform other types of networks, such as
convolutional and recurrent neural networks, in a range of visual benchmarks.
We evaluate various vision transformer models in this work by dividing them
into distinct jobs and examining their benefits and drawbacks. ViTs can
overcome several possible difficulties with convolutional neural networks
(CNNs). The goal of this survey is to show the first use of ViTs in CV. In the
first phase, we categorize various CV applications where ViTs are appropriate.
Image classification, object identification, image segmentation, video
transformer, image denoising, and NAS are all CV applications. Our next step
will be to analyze the state-of-the-art in each area and identify the models
that are currently available. In addition, we outline numerous open research
difficulties as well as prospective research possibilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Token Left Behind: Efficient Vision Transformer via Dynamic Token
  Idling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuwei Xu, Changlin Li, Yudong Chen, Xiaojun Chang, Jiajun Liu, Sen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have demonstrated outstanding performance in
computer vision tasks, yet their high computational complexity prevents their
deployment in computing resource-constrained environments. Various token
pruning techniques have been introduced to alleviate the high computational
burden of ViTs by dynamically dropping image tokens. However, some undesirable
pruning at early stages may result in permanent loss of image information in
subsequent layers, consequently hindering model performance. To address this
problem, we propose IdleViT, a dynamic token-idle-based method that achieves an
excellent trade-off between performance and efficiency. Specifically, in each
layer, IdleViT selects a subset of the image tokens to participate in
computations while keeping the rest of the tokens idle and directly passing
them to this layer's output. By allowing the idle tokens to be re-selected in
the following layers, IdleViT mitigates the negative impact of improper pruning
in the early stages. Furthermore, inspired by the normalized graph cut, we
devise a token cut loss on the attention map as regularization to improve
IdleViT's token selection ability. Our method is simple yet effective and can
be extended to pyramid ViTs since no token is completely dropped. Extensive
experimental results on various ViT architectures have shown that IdleViT can
diminish the complexity of pretrained ViTs by up to 33\% with no more than
0.2\% accuracy decrease on ImageNet, after finetuning for only 30 epochs.
Notably, when the keep ratio is 0.5, IdleViT outperforms the state-of-the-art
EViT on DeiT-S by 0.5\% higher accuracy and even faster inference speed. The
source code is available in the supplementary material.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AJCAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Manifold Structured Data Priors for Improved MR
  Fingerprinting Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Li, Yuping Ji, Yue Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating tissue parameter maps with high accuracy and precision from highly
undersampled measurements presents one of the major challenges in MR
fingerprinting (MRF). Many existing works project the recovered voxel
fingerprints onto the Bloch manifold to improve reconstruction performance.
However, little research focuses on exploiting the latent manifold structure
priors among fingerprints. To fill this gap, we propose a novel MRF
reconstruction framework based on manifold structured data priors. Since it is
difficult to directly estimate the fingerprint manifold structure, we model the
tissue parameters as points on a low-dimensional parameter manifold. We reveal
that the fingerprint manifold shares the same intrinsic topology as the
parameter manifold, although being embedded in different Euclidean spaces. To
exploit the non-linear and non-local redundancies in MRF data, we divide the
MRF data into spatial patches, and the similarity measurement among data
patches can be accurately obtained using the Euclidean distance between the
corresponding patches in the parameter manifold. The measured similarity is
then used to construct the graph Laplacian operator, which represents the
fingerprint manifold structure. Thus, the fingerprint manifold structure is
introduced in the reconstruction framework by using the low-dimensional
parameter manifold. Additionally, we incorporate the locally low-rank prior in
the reconstruction framework to further utilize the local correlations within
each patch for improved reconstruction performance. We also adopt a
GPU-accelerated NUFFT library to accelerate reconstruction in non-Cartesian
sampling scenarios. Experimental results demonstrate that our method can
achieve significantly improved reconstruction performance with reduced
computational time over the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures, will submit to IEEE Transactions on Medical
  Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosing Catastrophe: Large parts of accuracy loss in continual
  learning can be accounted for by readout misalignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Anthes, Sushrut Thorat, Peter König, Tim C. Kietzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike primates, training artificial neural networks on changing data
distributions leads to a rapid decrease in performance on old tasks. This
phenomenon is commonly referred to as catastrophic forgetting. In this paper,
we investigate the representational changes that underlie this performance
decrease and identify three distinct processes that together account for the
phenomenon. The largest component is a misalignment between hidden
representations and readout layers. Misalignment occurs due to learning on
additional tasks and causes internal representations to shift. Representational
geometry is partially conserved under this misalignment and only a small part
of the information is irrecoverably lost. All types of representational changes
scale with the dimensionality of hidden representations. These insights have
implications for deep learning applications that need to be continuously
updated, but may also aid aligning ANN models to the rather robust biological
vision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 1 figure; published at the 2023 Conference on Cognitive
  Computational Neuroscience</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plug n' Play: Channel Shuffle Module for Enhancing Tiny Vision
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuwei Xu, Sen Wang, Yudong Chen, Jiajun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have demonstrated remarkable performance in
various computer vision tasks. However, the high computational complexity
hinders ViTs' applicability on devices with limited memory and computing
resources. Although certain investigations have delved into the fusion of
convolutional layers with self-attention mechanisms to enhance the efficiency
of ViTs, there remains a knowledge gap in constructing tiny yet effective ViTs
solely based on the self-attention mechanism. Furthermore, the straightforward
strategy of reducing the feature channels in a large but outperforming ViT
often results in significant performance degradation despite improved
efficiency. To address these challenges, we propose a novel channel shuffle
module to improve tiny-size ViTs, showing the potential of pure self-attention
models in environments with constrained computing resources. Inspired by the
channel shuffle design in ShuffleNetV2 \cite{ma2018shufflenet}, our module
expands the feature channels of a tiny ViT and partitions the channels into two
groups: the \textit{Attended} and \textit{Idle} groups. Self-attention
computations are exclusively employed on the designated \textit{Attended}
group, followed by a channel shuffle operation that facilitates information
exchange between the two groups. By incorporating our module into a tiny ViT,
we can achieve superior performance while maintaining a comparable
computational complexity to the vanilla model. Specifically, our proposed
channel shuffle module consistently improves the top-1 accuracy on the
ImageNet-1K dataset for various tiny ViT models by up to 2.8\%, with the
changes in model complexity being less than 0.03 GMACs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High Accuracy and Cost-Saving Active Learning 3D WD-UNet for Airway
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyi Wang, Yang Nan, Simon Walsh, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel Deep Active Learning (DeepAL) model-3D Wasserstein
Discriminative UNet (WD-UNet) for reducing the annotation effort of medical 3D
Computed Tomography (CT) segmentation. The proposed WD-UNet learns in a
semi-supervised way and accelerates learning convergence to meet or exceed the
prediction metrics of supervised learning models. Our method can be embedded
with different Active Learning (AL) strategies and different network
structures. The model is evaluated on 3D lung airway CT scans for medical
segmentation and show that the use of uncertainty metric, which is parametrized
as an input of query strategy, leads to more accurate prediction results than
some state-of-the-art Deep Learning (DL) supervised models, e.g.,3DUNet and 3D
CEUNet. Compared to the above supervised DL methods, our WD-UNet not only saves
the cost of annotation for radiologists but also saves computational resources.
WD-UNet uses a limited amount of annotated data (35% of the total) to achieve
better predictive metrics with a more efficient deep learning model algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locality-Aware Generalizable Implicit Neural Representation} 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doyup Lee, Chiheon Kim, Minsu Cho, Wook-Shin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizable implicit neural representation (INR) enables a single
continuous function, i.e., a coordinate-based neural network, to represent
multiple data instances by modulating its weights or intermediate features
using latent codes. However, the expressive power of the state-of-the-art
modulation is limited due to its inability to localize and capture fine-grained
details of data entities such as specific pixels and rays. To address this
issue, we propose a novel framework for generalizable INR that combines a
transformer encoder with a locality-aware INR decoder. The transformer encoder
predicts a set of latent tokens from a data instance to encode local
information into each latent token. The locality-aware INR decoder extracts a
modulation vector by selectively aggregating the latent tokens via
cross-attention for a coordinate input and then predicts the output by
progressively decoding with coarse-to-fine modulation through multiple
frequency bandwidths. The selective token aggregation and the multi-band
feature modulation enable us to learn locality-aware representation in spatial
and spectral aspects, respectively. Our framework significantly outperforms
previous generalizable INRs and validates the usefulness of the locality-aware
latents for downstream tasks such as image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASM: Adaptive Sample Mining for In-The-Wild Facial Expression
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Zhang, Xiao Sun, Liuwei An, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the similarity between facial expression categories, the presence of
compound facial expressions, and the subjectivity of annotators, facial
expression recognition (FER) datasets often suffer from ambiguity and noisy
labels. Ambiguous expressions are challenging to differentiate from expressions
with noisy labels, which hurt the robustness of FER models. Furthermore, the
difficulty of recognition varies across different expression categories,
rendering a uniform approach unfair for all expressions. In this paper, we
introduce a novel approach called Adaptive Sample Mining (ASM) to dynamically
address ambiguity and noise within each expression category. First, the
Adaptive Threshold Learning module generates two thresholds, namely the clean
and noisy thresholds, for each category. These thresholds are based on the mean
class probabilities at each training epoch. Next, the Sample Mining module
partitions the dataset into three subsets: clean, ambiguity, and noise, by
comparing the sample confidence with the clean and noisy thresholds. Finally,
the Tri-Regularization module employs a mutual learning strategy for the
ambiguity subset to enhance discrimination ability, and an unsupervised
learning strategy for the noise subset to mitigate the impact of noisy labels.
Extensive experiments prove that our method can effectively mine both ambiguity
and noise, and outperform SOTA methods on both synthetic noisy and original
datasets. The supplement material is available at
https://github.com/zzzzzzyang/ASM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Multi-head Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Piotr Koniusz, Tom Gedeon, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrastive learning, two views of an original image generated by
different augmentations are considered as a positive pair whose similarity is
required to be high. Moreover, two views of two different images are considered
as a negative pair, and their similarity is encouraged to be low. Normally, a
single similarity measure given by a single projection head is used to evaluate
positive and negative sample pairs, respectively. However, due to the various
augmentation strategies and varying intra-sample similarity, augmented views
from the same image are often not similar. Moreover, due to inter-sample
similarity, augmented views of two different images may be more similar than
augmented views from the same image. As such, enforcing a high similarity for
positive pairs and a low similarity for negative pairs may not always be
achievable, and in the case of some pairs, forcing so may be detrimental to the
performance. To address this issue, we propose to use multiple projection
heads, each producing a separate set of features. Our loss function for
pre-training emerges from a solution to the maximum likelihood estimation over
head-wise posterior distributions of positive samples given observations. The
loss contains the similarity measure over positive and negative pairs, each
re-weighted by an individual adaptive temperature that is regularized to
prevent ill solutions. Our adaptive multi-head contrastive learning (AMCL) can
be applied to and experimentally improves several popular contrastive learning
methods such as SimCLR, MoCo and Barlow Twins. Such improvement is consistent
under various backbones and linear probing epoches and is more significant when
multiple augmentation methods are used.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael G. Adam, Sebastian Eger, Martin Piccolrovazzi, Maged Iskandar, Joern Vogel, Alexander Dietrich, Seongjien Bien, Jon Skerlj, Abdeldjallil Naceri, Eckehard Steinbach, Alin Albu-Schaeffer, Sami Haddadin, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As labor shortage increases in the health sector, the demand for assistive
robotics grows. However, the needed test data to develop those robots is
scarce, especially for the application of active 3D object detection, where no
real data exists at all. This short paper counters this by introducing such an
annotated dataset of real environments. The captured environments represent
areas which are already in use in the field of robotic health care research. We
further provide ground truth data within one room, for assessing SLAM
algorithms running directly on a health care robot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceptual Artifacts Localization for Image Synthesis Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingzhi Zhang, Zhengjie Xu, Connelly Barnes, Yuqian Zhou, Qing Liu, He Zhang, Sohrab Amirghodsi, Zhe Lin, Eli Shechtman, Jianbo Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in deep generative models have facilitated the creation
of photo-realistic images across various tasks. However, these generated images
often exhibit perceptual artifacts in specific regions, necessitating manual
correction. In this study, we present a comprehensive empirical examination of
Perceptual Artifacts Localization (PAL) spanning diverse image synthesis
endeavors. We introduce a novel dataset comprising 10,168 generated images,
each annotated with per-pixel perceptual artifact labels across ten synthesis
tasks. A segmentation model, trained on our proposed dataset, effectively
localizes artifacts across a range of tasks. Additionally, we illustrate its
proficiency in adapting to previously unseen models using minimal training
samples. We further propose an innovative zoom-in inpainting pipeline that
seamlessly rectifies perceptual artifacts in the generated images. Through our
experimental analyses, we elucidate several practical downstream applications,
such as automated artifact rectification, non-referential image quality
evaluation, and abnormal region detection in images. The dataset and code are
released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple and Robust Framework for Cross-Modality Medical Image
  Segmentation applied to Vision Transformers <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Bastico, David Ryckelynck, Laurent Corté, Yannick Tillier, Etienne Decencière
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When it comes to clinical images, automatic segmentation has a wide variety
of applications and a considerable diversity of input domains, such as
different types of Magnetic Resonance Images (MRIs) and Computerized Tomography
(CT) scans. This heterogeneity is a challenge for cross-modality algorithms
that should equally perform independently of the input image type fed to them.
Often, segmentation models are trained using a single modality, preventing
generalization to other types of input data without resorting to transfer
learning techniques. Furthermore, the multi-modal or cross-modality
architectures proposed in the literature frequently require registered images,
which are not easy to collect in clinical environments, or need additional
processing steps, such as synthetic image generation. In this work, we propose
a simple framework to achieve fair image segmentation of multiple modalities
using a single conditional model that adapts its normalization layers based on
the input type, trained with non-registered interleaved mixed data. We show
that our framework outperforms other cross-modality segmentation methods, when
applied to the same 3D UNet baseline model, on the Multi-Modality Whole Heart
Segmentation Challenge. Furthermore, we define the Conditional Vision
Transformer (C-ViT) encoder, based on the proposed cross-modality framework,
and we show that it brings significant improvements to the resulting
segmentation, up to 6.87\% of Dice accuracy, with respect to its baseline
reference. The code to reproduce our experiments and the trained model weights
are available at https://github.com/matteo-bastico/MI-Seg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted in International Conference on Computer
  Vision Workshops (ICCVW) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth
  Estimation under Adverse Weather Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyuan Wang, Chunyu Lin, Lang Nie, Shujun Huang, Yao Zhao, Xing Pan, Rui Ai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation models have shown promising performance on clear scenes but
fail to generalize to adverse weather conditions due to illumination
variations, weather particles, etc. In this paper, we propose WeatherDepth, a
self-supervised robust depth estimation model with curriculum contrastive
learning, to tackle performance degradation in complex weather conditions.
Concretely, we first present a progressive curriculum learning scheme with
three simple-to-complex curricula to gradually adapt the model from clear to
relative adverse, and then to adverse weather scenes. It encourages the model
to gradually grasp beneficial depth cues against the weather effect, yielding
smoother and better domain adaption. Meanwhile, to prevent the model from
forgetting previous curricula, we integrate contrastive learning into different
curricula. Drawn the reference knowledge from the previous course, our strategy
establishes a depth consistency constraint between different courses towards
robust depth estimation in diverse weather. Besides, to reduce manual
intervention and better adapt to different models, we designed an adaptive
curriculum scheduler to automatically search for the best timing for course
switching. In the experiment, the proposed solution is proven to be easily
incorporated into various architectures and demonstrates state-of-the-art
(SoTA) performance on both synthetic and real weather datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion
  for Polyp Localization in Colonoscopy Images <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju-Hyeon Nam, Seo-Hyeong Park, Nur Suriza Syazwany, Yerim Jung, Yu-Han Im, Sang-Chul Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polyp segmentation is crucial for preventing colorectal cancer a common type
of cancer. Deep learning has been used to segment polyps automatically, which
reduces the risk of misdiagnosis. Localizing small polyps in colonoscopy images
is challenging because of its complex characteristics, such as color,
occlusion, and various shapes of polyps. To address this challenge, a novel
frequency-based fully convolutional neural network, Multi-Frequency Feature
Fusion Polyp Segmentation Network (M3FPolypSegNet) was proposed to decompose
the input image into low/high/full-frequency components to use the
characteristics of each component. We used three independent multi-frequency
encoders to map multiple input images into a high-dimensional feature space. In
the Frequency-ASPP Scalable Attention Module (F-ASPP SAM), ASPP was applied
between each frequency component to preserve scale information. Subsequently,
scalable attention was applied to emphasize polyp regions in a high-dimensional
feature space. Finally, we designed three multi-task learning (i.e., region,
edge, and distance) in four decoder blocks to learn the structural
characteristics of the region. The proposed model outperformed various
segmentation models with performance gains of 6.92% and 7.52% on average for
all metrics on CVC-ClinicDB and BKAI-IGH-NeoPolyp, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5pages. 2023 IEEE International Conference on Image Processing
  (ICIP). IEEE, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bi-directional Deformation for Parameterization of Neural Implicit
  Surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baixin Xu, Jiangbei Hu, Fei Hou, Kwan-Yee Lin, Wayne Wu, Chen Qian, Ying He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing capabilities of neural rendering have increased the demand for
new techniques that enable the intuitive editing of 3D objects, particularly
when they are represented as neural implicit surfaces. In this paper, we
present a novel neural algorithm to parameterize neural implicit surfaces to
simple parametric domains, such as spheres, cubes or polycubes, where 3D
radiance field can be represented as a 2D field, thereby facilitating
visualization and various editing tasks. Technically, our method computes a
bi-directional deformation between 3D objects and their chosen parametric
domains, eliminating the need for any prior information. We adopt a forward
mapping of points on the zero level set of the 3D object to a parametric
domain, followed by a backward mapping through inverse deformation. To ensure
the map is bijective, we employ a cycle loss while optimizing the smoothness of
both deformations. Additionally, we leverage a Laplacian regularizer to
effectively control angle distortion and offer the flexibility to choose from a
range of parametric domains for managing area distortion. Designed for
compatibility, our framework integrates seamlessly with existing neural
rendering pipelines, taking multi-view images as input to reconstruct 3D
geometry and compute the corresponding texture map. We also introduce a simple
yet effective technique for intrinsic radiance decomposition, facilitating both
view-independent material editing and view-dependent shading editing. Our
method allows for the immediate rendering of edited textures through volume
rendering, without the need for network re-training. Moreover, our approach
supports the co-parameterization of multiple objects and enables texture
transfer between them. We demonstrate the effectiveness of our method on images
of human heads and man-made objects. We will make the source code publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAVs and Neural Networks for search and rescue missions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hartmut Surmann, Artur Leinweber, Gerhard Senkowski, Julien Meine, Dominik Slomma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a method for detecting objects of interest,
including cars, humans, and fire, in aerial images captured by unmanned aerial
vehicles (UAVs) usually during vegetation fires. To achieve this, we use
artificial neural networks and create a dataset for supervised learning. We
accomplish the assisted labeling of the dataset through the implementation of
an object detection pipeline that combines classic image processing techniques
with pretrained neural networks. In addition, we develop a data augmentation
pipeline to augment the dataset with automatically labeled images. Finally, we
evaluate the performance of different neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 56th International Symposium on Robotics (ISR Europe) |
  September 26-27, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proposal-based Temporal Action Localization with Point-level Supervision <span class="chip">BMVC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Yin, Yifei Huang, Ryosuke Furuta, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-level supervised temporal action localization (PTAL) aims at
recognizing and localizing actions in untrimmed videos where only a single
point (frame) within every action instance is annotated in training data.
Without temporal annotations, most previous works adopt the multiple instance
learning (MIL) framework, where the input video is segmented into
non-overlapped short snippets, and action classification is performed
independently on every short snippet. We argue that the MIL framework is
suboptimal for PTAL because it operates on separated short snippets that
contain limited temporal information. Therefore, the classifier only focuses on
several easy-to-distinguish snippets instead of discovering the whole action
instance without missing any relevant snippets. To alleviate this problem, we
propose a novel method that localizes actions by generating and evaluating
action proposals of flexible duration that involve more comprehensive temporal
information. Moreover, we introduce an efficient clustering algorithm to
efficiently generate dense pseudo labels that provide stronger supervision, and
a fine-grained contrastive loss to further refine the quality of pseudo labels.
Experiments show that our proposed method achieves competitive or superior
performance to the state-of-the-art methods and some fully-supervised methods
on four benchmarks: ActivityNet 1.3, THUMOS 14, GTEA, and BEOID datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud
  Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunge Bai, Ruijie Fu, Xiang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art techniques for monocular camera reconstruction predominantly
rely on the Structure from Motion (SfM) pipeline. However, such methods often
yield reconstruction outcomes that lack crucial scale information, and over
time, accumulation of images leads to inevitable drift issues. In contrast,
mapping methods based on LiDAR scans are popular in large-scale urban scene
reconstruction due to their precise distance measurements, a capability
fundamentally absent in visual-based approaches. Researchers have made attempts
to utilize concurrent LiDAR and camera measurements in pursuit of precise
scaling and color details within mapping outcomes. However, the outcomes are
subject to extrinsic calibration and time synchronization precision. In this
paper, we propose a novel cost-effective reconstruction pipeline that utilizes
a pre-established LiDAR map as a fixed constraint to effectively address the
inherent scale challenges present in monocular camera reconstruction. To our
knowledge, our method is the first to register images onto the point cloud map
without requiring synchronous capture of camera and LiDAR data, granting us the
flexibility to manage reconstruction detail levels across various areas of
interest. To facilitate further research in this domain, we have released
Colmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that
enables precise fine-scale registration of images to the point cloud map.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Object Detection with Uncurated Unlabeled Data for
  Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nanqing Liu, Xun Xu, Yingjie Gao, Heng-Chao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotating remote sensing images (RSIs) presents a notable challenge due to
its labor-intensive nature. Semi-supervised object detection (SSOD) methods
tackle this issue by generating pseudo-labels for the unlabeled data, assuming
that all classes found in the unlabeled dataset are also represented in the
labeled data. However, real-world situations introduce the possibility of
out-of-distribution (OOD) samples being mixed with in-distribution (ID) samples
within the unlabeled dataset. In this paper, we delve into techniques for
conducting SSOD directly on uncurated unlabeled data, which is termed Open-Set
Semi-Supervised Object Detection (OSSOD). Our approach commences by employing
labeled in-distribution data to dynamically construct a class-wise feature bank
(CFB) that captures features specific to each class. Subsequently, we compare
the features of predicted object bounding boxes with the corresponding entries
in the CFB to calculate OOD scores. We design an adaptive threshold based on
the statistical properties of the CFB, allowing us to filter out OOD samples
effectively. The effectiveness of our proposed method is substantiated through
extensive experiments on two widely used remote sensing object detection
datasets: DIOR and DOTA. These experiments showcase the superior performance
and efficacy of our approach for OSSOD on RSIs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry-Guided Ray Augmentation for Neural Surface Reconstruction with
  Sparse Views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Yao, Chen Wang, Tong Wu, Chuming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel method for 3D scene and object
reconstruction from sparse multi-view images. Different from previous methods
that leverage extra information such as depth or generalizable features across
scenes, our approach leverages the scene properties embedded in the multi-view
inputs to create precise pseudo-labels for optimization without any prior
training. Specifically, we introduce a geometry-guided approach that improves
surface reconstruction accuracy from sparse views by leveraging spherical
harmonics to predict the novel radiance while holistically considering all
color observations for a point in the scene. Also, our pipeline exploits proxy
geometry and correctly handles the occlusion in generating the pseudo-labels of
radiance, which previous image-warping methods fail to avoid. Our method,
dubbed Ray Augmentation (RayAug), achieves superior results on DTU and Blender
datasets without requiring prior training, demonstrating its effectiveness in
addressing the problem of sparse view reconstruction. Our pipeline is flexible
and can be integrated into other implicit neural reconstruction methods for
sparse views.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sentence-level Prompts Benefit Composed Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Xinxing Xu, Yong Liu, Salman Khan, Fahad Khan, Wangmeng Zuo, Rick Siow Mong Goh, Chun-Mei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed image retrieval (CIR) is the task of retrieving specific images by
using a query that involves both a reference image and a relative caption. Most
existing CIR models adopt the late-fusion strategy to combine visual and
language features. Besides, several approaches have also been suggested to
generate a pseudo-word token from the reference image, which is further
integrated into the relative caption for CIR. However, these pseudo-word-based
prompting methods have limitations when target image encompasses complex
changes on reference image, e.g., object removal and attribute modification. In
this work, we demonstrate that learning an appropriate sentence-level prompt
for the relative caption (SPRC) is sufficient for achieving effective composed
image retrieval. Instead of relying on pseudo-word-based prompts, we propose to
leverage pretrained V-L models, e.g., BLIP-2, to generate sentence-level
prompts. By concatenating the learned sentence-level prompt with the relative
caption, one can readily use existing text-based image retrieval models to
enhance CIR performance. Furthermore, we introduce both image-text contrastive
loss and text prompt alignment loss to enforce the learning of suitable
sentence-level prompts. Experiments show that our proposed method performs
favorably against the state-of-the-art CIR methods on the Fashion-IQ and CIRR
datasets. The source code and pretrained model are publicly available at
https://github.com/chunmeifeng/SPRC
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaFuse: Adaptive Medical Image Fusion Based on Spatial-Frequential
  Cross Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianming Gu, Lihui Wang, Zeyu Deng, Ying Cao, Xingyu Huang, Yue-min Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal medical image fusion is essential for the precise clinical
diagnosis and surgical navigation since it can merge the complementary
information in multi-modalities into a single image. The quality of the fused
image depends on the extracted single modality features as well as the fusion
rules for multi-modal information. Existing deep learning-based fusion methods
can fully exploit the semantic features of each modality, they cannot
distinguish the effective low and high frequency information of each modality
and fuse them adaptively. To address this issue, we propose AdaFuse, in which
multimodal image information is fused adaptively through frequency-guided
attention mechanism based on Fourier transform. Specifically, we propose the
cross-attention fusion (CAF) block, which adaptively fuses features of two
modalities in the spatial and frequency domains by exchanging key and query
values, and then calculates the cross-attention scores between the spatial and
frequency features to further guide the spatial-frequential information fusion.
The CAF block enhances the high-frequency features of the different modalities
so that the details in the fused images can be retained. Moreover, we design a
novel loss function composed of structure loss and content loss to preserve
both low and high frequency information. Extensive comparison experiments on
several datasets demonstrate that the proposed method outperforms
state-of-the-art methods in terms of both visual quality and quantitative
metrics. The ablation experiments also validate the effectiveness of the
proposed loss and fusion strategy. Our code is publicly available at
https://github.com/xianming-gu/AdaFuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Lai, Xinghong Liu, Tao Zhou, Yi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal domain adaptation aims to align the classes and reduce the feature
gap between the same category of the source and target domains. The target
private category is set as the unknown class during the adaptation process, as
it is not included in the source domain. However, most existing methods
overlook the intra-class structure within a category, especially in cases where
there exists significant concept shift between the samples belonging to the
same category. When samples with large concept shift are forced to be pushed
together, it may negatively affect the adaptation performance. Moreover, from
the interpretability aspect, it is unreasonable to align visual features with
significant differences, such as fighter jets and civil aircraft, into the same
category. Unfortunately, due to such semantic ambiguity and annotation cost,
categories are not always classified in detail, making it difficult for the
model to perform precise adaptation. To address these issues, we propose a
novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the
differences between samples belonging to the same category and mine sub-classes
when there exists significant concept shift between them. By doing so, our
model learns a more reasonable feature space that enhances the transferability
and reflects the inherent differences among samples annotated as the same
category. We evaluate the effectiveness of our MemSPM method over multiple
scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art
performance on four benchmarks in most cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Fair and Comprehensive Comparisons for Image-Based 3D Object
  Detection <span class="chip">ICCV23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhu Ma, Yongtao Wan, Yinmin Zhang, Zhiyi Xia, Yuan Meng, Zhihui Wang, Haojie Li, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we build a modular-designed codebase, formulate strong training
recipes, design an error diagnosis toolbox, and discuss current methods for
image-based 3D object detection. In particular, different from other highly
mature tasks, e.g., 2D object detection, the community of image-based 3D object
detection is still evolving, where methods often adopt different training
recipes and tricks resulting in unfair evaluations and comparisons. What is
worse, these tricks may overwhelm their proposed designs in performance, even
leading to wrong conclusions. To address this issue, we build a module-designed
codebase and formulate unified training standards for the community.
Furthermore, we also design an error diagnosis toolbox to measure the detailed
characterization of detection models. Using these tools, we analyze current
methods in-depth under varying settings and provide discussions for some open
questions, e.g., discrepancies in conclusions on KITTI-3D and nuScenes
datasets, which have led to different dominant methods for these datasets. We
hope that this work will facilitate future research in image-based 3D object
detection. Our codes will be released at
\url{https://github.com/OpenGVLab/3dodi}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV23, code will be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RetSeg: Retention-based Colorectal Polyps Segmentation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled ELKarazle, Valliappan Raman, Caslon Chua, Patrick Then
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have revolutionized medical imaging analysis,
showcasing superior efficacy compared to conventional Convolutional Neural
Networks (CNNs) in vital tasks such as polyp classification, detection, and
segmentation. Leveraging attention mechanisms to focus on specific image
regions, ViTs exhibit contextual awareness in processing visual data,
culminating in robust and precise predictions, even for intricate medical
images. Moreover, the inherent self-attention mechanism in Transformers
accommodates varying input sizes and resolutions, granting an unprecedented
flexibility absent in traditional CNNs. However, Transformers grapple with
challenges like excessive memory usage and limited training parallelism due to
self-attention, rendering them impractical for real-time disease detection on
resource-constrained devices. In this study, we address these hurdles by
investigating the integration of the recently introduced retention mechanism
into polyp segmentation, introducing RetSeg, an encoder-decoder network
featuring multi-head retention blocks. Drawing inspiration from Retentive
Networks (RetNet), RetSeg is designed to bridge the gap between precise polyp
segmentation and resource utilization, particularly tailored for colonoscopy
images. We train and validate RetSeg for polyp segmentation employing two
publicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we
showcase RetSeg's promising performance across diverse public datasets,
including CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While
our work represents an early-stage exploration, further in-depth studies are
imperative to advance these promising findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AngioMoCo: Learning-based Motion Correction in Cerebral Digital
  Subtraction Angiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruisheng Su, Matthijs van der Sluijs, Sandra Cornelissen, Wim van Zwam, Aad van der Lugt, Wiro Niessen, Danny Ruijters, Theo van Walsum, Adrian Dalca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cerebral X-ray digital subtraction angiography (DSA) is the standard imaging
technique for visualizing blood flow and guiding endovascular treatments. The
quality of DSA is often negatively impacted by body motion during acquisition,
leading to decreased diagnostic value. Time-consuming iterative methods address
motion correction based on non-rigid registration, and employ sparse key points
and non-rigidity penalties to limit vessel distortion. Recent methods alleviate
subtraction artifacts by predicting the subtracted frame from the corresponding
unsubtracted frame, but do not explicitly compensate for motion-induced
misalignment between frames. This hinders the serial evaluation of blood flow,
and often causes undesired vasculature and contrast flow alterations, leading
to impeded usability in clinical practice. To address these limitations, we
present AngioMoCo, a learning-based framework that generates motion-compensated
DSA sequences from X-ray angiography. AngioMoCo integrates contrast extraction
and motion correction, enabling differentiation between patient motion and
intensity changes caused by contrast flow. This strategy improves registration
quality while being substantially faster than iterative elastix-based methods.
We demonstrate AngioMoCo on a large national multi-center dataset (MR CLEAN
Registry) of clinically acquired angiographic images through comprehensive
qualitative and quantitative analyses. AngioMoCo produces high-quality
motion-compensated DSA, removing motion artifacts while preserving contrast
flow. Code is publicly available at https://github.com/RuishengSu/AngioMoCo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-aware Temporal Channel-wise Attention for Cardiac Function
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanqi Chen, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac function assessment aims at predicting left ventricular ejection
fraction (LVEF) given an echocardiogram video, which requests models to focus
on the changes in the left ventricle during the cardiac cycle. How to assess
cardiac function accurately and automatically from an echocardiogram video is a
valuable topic in intelligent assisted healthcare. Existing video-based methods
do not pay much attention to the left ventricular region, nor the left
ventricular changes caused by motion. In this work, we propose a
semi-supervised auxiliary learning paradigm with a left ventricular
segmentation task, which contributes to the representation learning for the
left ventricular region. To better model the importance of motion information,
we introduce a temporal channel-wise attention (TCA) module to excite those
channels used to describe motion. Furthermore, we reform the TCA module with
semantic perception by taking the segmentation map of the left ventricle as
input to focus on the motion patterns of the left ventricle. Finally, to reduce
the difficulty of direct LVEF regression, we utilize an anchor-based
classification and regression method to predict LVEF. Our approach achieves
state-of-the-art performance on the Stanford dataset with an improvement of
0.22 MAE, 0.26 RMSE, and 1.9% $R^2$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ISBI 2022 (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Divide and Ensemble: Progressively Learning for the Unknown 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hu Zhang, Xin Shen, Heming Du, Huiqiang Chen, Chen Liu, Hongwei Sheng, Qingzheng Xu, MD Wahiduzzaman Khan, Qingtao Yu, Tianqing Zhu, Scott Chapman, Zi Huang, Xin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the wheat nutrient deficiencies classification challenge, we present the
DividE and EnseMble (DEEM) method for progressive test data predictions. We
find that (1) test images are provided in the challenge; (2) samples are
equipped with their collection dates; (3) the samples of different dates show
notable discrepancies. Based on the findings, we partition the dataset into
discrete groups by the dates and train models on each divided group. We then
adopt the pseudo-labeling approach to label the test data and incorporate those
with high confidence into the training set. In pseudo-labeling, we leverage
models ensemble with different architectures to enhance the reliability of
predictions. The pseudo-labeling and ensembled model training are iteratively
conducted until all test samples are labeled. Finally, the separated models for
each group are unified to obtain the model for the whole dataset. Our method
achieves an average of 93.6\% Top-1 test accuracy~(94.0\% on WW2020 and 93.2\%
on WR2021) and wins the 1$st$ place in the Deep Nutrient Deficiency
Challenge~\footnote{https://cvppa2023.github.io/challenges/}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GradientSurf: Gradient-Domain Neural Surface Reconstruction from RGB
  Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Crane He Chen, Joerg Liebelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes GradientSurf, a novel algorithm for real time surface
reconstruction from monocular RGB video. Inspired by Poisson Surface
Reconstruction, the proposed method builds on the tight coupling between
surface, volume, and oriented point cloud and solves the reconstruction problem
in gradient-domain. Unlike Poisson Surface Reconstruction which finds an
offline solution to the Poisson equation by solving a linear system after the
scanning process is finished, our method finds online solutions from partial
scans with a neural network incrementally where the Poisson layer is designed
to supervise both local and global reconstruction. The main challenge that
existing methods suffer from when reconstructing from RGB signal is a lack of
details in the reconstructed surface. We hypothesize this is due to the
spectral bias of neural networks towards learning low frequency geometric
features. To address this issue, the reconstruction problem is cast onto
gradient domain, where zeroth-order and first-order energies are minimized. The
zeroth-order term penalizes location of the surface. The first-order term
penalizes the difference between the gradient of reconstructed implicit
function and the vector field formulated from oriented point clouds sampled at
adaptive local densities. For the task of indoor scene reconstruction, visual
and quantitative experimental results show that the proposed method
reconstructs surfaces with more details in curved regions and higher fidelity
for small objects than previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient
  Vision Transformers <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, Kaiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector-quantized image modeling has shown great potential in synthesizing
high-quality images. However, generating high-resolution images remains a
challenging task due to the quadratic computational overhead of the
self-attention process. In this study, we seek to explore a more efficient
two-stage framework for high-resolution image generation with improvements in
the following three aspects. (1) Based on the observation that the first
quantization stage has solid local property, we employ a local attention-based
quantization model instead of the global attention mechanism used in previous
methods, leading to better efficiency and reconstruction quality. (2) We
emphasize the importance of multi-grained feature interaction during image
generation and introduce an efficient attention mechanism that combines global
attention (long-range semantic consistency within the whole image) and local
attention (fined-grained details). This approach results in faster generation
speed, higher generation fidelity, and improved resolution. (3) We propose a
new generation pipeline incorporating autoencoding training and autoregressive
generation strategy, demonstrating a better paradigm for image synthesis.
Extensive experiments demonstrate the superiority of our approach in
high-quality and high-resolution image reconstruction and generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted to ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAMEL2: Enhancing weakly supervised learning for histopathology images
  by incorporating the significance ratio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gang Xu, Shuhao Wang, Lingyu Zhao, Xiao Chen, Tongwei Wang, Lang Wang, Zhenwei Luo, Dahan Wang, Zewen Zhang, Aijun Liu, Wei Ba, Zhigang Song, Huaiyin Shi, Dingrong Zhong, Jianpeng Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathology image analysis plays a crucial role in cancer diagnosis.
However, training a clinically applicable segmentation algorithm requires
pathologists to engage in labour-intensive labelling. In contrast, weakly
supervised learning methods, which only require coarse-grained labels at the
image level, can significantly reduce the labeling efforts. Unfortunately,
while these methods perform reasonably well in slide-level prediction, their
ability to locate cancerous regions, which is essential for many clinical
applications, remains unsatisfactory. Previously, we proposed CAMEL, which
achieves comparable results to those of fully supervised baselines in
pixel-level segmentation. However, CAMEL requires 1,280x1,280 image-level
binary annotations for positive WSIs. Here, we present CAMEL2, by introducing a
threshold of the cancerous ratio for positive bags, it allows us to better
utilize the information, consequently enabling us to scale up the image-level
setting from 1,280x1,280 to 5,120x5,120 while maintaining the accuracy. Our
results with various datasets, demonstrate that CAMEL2, with the help of
5,120x5,120 image-level binary annotations, which are easy to annotate,
achieves comparable performance to that of a fully supervised baseline in both
instance- and slide-level classifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Side-Tuning for Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weifeng Lin, Ziheng Wu, Jiayu Chen, Wentao Yang, Mingxin Huang, Jun Huang, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained Vision Transformers (ViT) has consistently
demonstrated promising performance in the realm of visual recognition. However,
adapting large pre-trained models to various tasks poses a significant
challenge. This challenge arises from the need for each model to undergo an
independent and comprehensive fine-tuning process, leading to substantial
computational and memory demands. While recent advancements in
Parameter-efficient Transfer Learning (PETL) have demonstrated their ability to
achieve superior performance compared to full fine-tuning with a smaller subset
of parameter updates, they tend to overlook dense prediction tasks such as
object detection and segmentation. In this paper, we introduce Hierarchical
Side-Tuning (HST), a novel PETL approach that enables ViT transfer to various
downstream tasks effectively. Diverging from existing methods that exclusively
fine-tune parameters within input spaces or certain modules connected to the
backbone, we tune a lightweight and hierarchical side network (HSN) that
leverages intermediate activations extracted from the backbone and generates
multi-scale features to make predictions. To validate HST, we conducted
extensive experiments encompassing diverse visual tasks, including
classification, object detection, instance segmentation, and semantic
segmentation. Notably, our method achieves state-of-the-art average Top-1
accuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters.
When applied to object detection tasks on COCO testdev benchmark, HST even
surpasses full fine-tuning and obtains better performance with 49.7 box AP and
43.2 mask AP using Cascade Mask R-CNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Full-Convolutional Siamese Tracker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Yunfeng, Wang Bo, Li Ye, Liu Zhuoyan, Wu Xueyi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although single object trackers have achieved advanced performance, their
large-scale network models make it difficult to apply them on the platforms
with limited resources. Moreover, existing lightweight trackers only achieve
balance between 2-3 points in terms of parameters, performance, Flops and FPS.
To achieve the balance among all 4 points, this paper propose a lightweight
full-convolutional Siamese tracker called lightFC. LightFC employs a noval
efficient cross-correlation module (ECM) and a noval efficient rep-center head
(ERH) to enhance the nonlinear expressiveness of the convoluational tracking
pipeline. The ECM adopts an architecture of attention-like module and fuses
local spatial and channel features from the pixel-wise correlation fusion
features and enhance model nonlinearity with an inversion activation block.
Additionally, skip-connections and the reuse of search area features are
introduced by the ECM to improve its performance. The ERH reasonably introduces
reparameterization technology and channel attention to enhance the nonlinear
expressiveness of the center head. Comprehensive experiments show that LightFC
achieves a good balance between performance, parameters, Flops and FPS. The
precision score of LightFC outperforms MixFormerV2-S by 3.7 \% and 6.5 \% on
LaSOT and TNL2K, respectively, while using 5x fewer parameters and 4.6x fewer
Flops. Besides, LightFC runs 2x faster than MixFormerV2-S on CPUs. Our code and
raw results can be found at https://github.com/LiYunfengLYF/LightFC
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Impostor: Editing Neural Radiance Fields with Explicit Shape
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Liu, Jinxu Xiang, Bowen Zhao, Ran Zhang, Jingyi Yu, Changxi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) have significantly advanced the generation of
highly realistic and expressive 3D scenes. However, the task of editing NeRF,
particularly in terms of geometry modification, poses a significant challenge.
This issue has obstructed NeRF's wider adoption across various applications. To
tackle the problem of efficiently editing neural implicit fields, we introduce
Neural Impostor, a hybrid representation incorporating an explicit tetrahedral
mesh alongside a multigrid implicit field designated for each tetrahedron
within the explicit mesh. Our framework bridges the explicit shape manipulation
and the geometric editing of implicit fields by utilizing multigrid barycentric
coordinate encoding, thus offering a pragmatic solution to deform, composite,
and generate neural implicit fields while maintaining a complex volumetric
appearance. Furthermore, we propose a comprehensive pipeline for editing neural
implicit fields based on a set of explicit geometric editing operations. We
show the robustness and adaptability of our system through diverse examples and
experiments, including the editing of both synthetic objects and real captured
data. Finally, we demonstrate the authoring process of a hybrid
synthetic-captured object utilizing a variety of editing operations,
underlining the transformative potential of Neural Impostor in the field of 3D
content creation and manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Pacific Graphics 2023 and Computer Graphics Forum</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Three-Stage Cascade Framework for Blurry Video Frame Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Lei, Zaoming Yan, Tingting Wang, Faming Fang, Guixu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blurry video frame interpolation (BVFI) aims to generate high-frame-rate
clear videos from low-frame-rate blurry videos, is a challenging but important
topic in the computer vision community. Blurry videos not only provide spatial
and temporal information like clear videos, but also contain additional motion
information hidden in each blurry frame. However, existing BVFI methods usually
fail to fully leverage all valuable information, which ultimately hinders their
performance. In this paper, we propose a simple end-to-end three-stage
framework to fully explore useful information from blurry videos. The frame
interpolation stage designs a temporal deformable network to directly sample
useful information from blurry inputs and synthesize an intermediate frame at
an arbitrary time interval. The temporal feature fusion stage explores the
long-term temporal information for each target frame through a bi-directional
recurrent deformable alignment network. And the deblurring stage applies a
transformer-empowered Taylor approximation network to recursively recover the
high-frequency details. The proposed three-stage framework has clear task
assignment for each module and offers good expandability, the effectiveness of
which are demonstrated by various experimental results. We evaluate our model
on four benchmarks, including the Adobe240 dataset, GoPro dataset, YouTube240
dataset and Sony dataset. Quantitative and qualitative results indicate that
our model outperforms existing SOTA methods. Besides, experiments on real-world
blurry videos also indicate the good generalization ability of our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IPDreamer: Appearance-Controllable 3D Object Generation with Image
  Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang, Jianzhuang Liu, Baochang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-3D generation have been remarkable, with methods
such as DreamFusion leveraging large-scale text-to-image diffusion-based models
to supervise 3D generation. These methods, including the variational score
distillation proposed by ProlificDreamer, enable the synthesis of detailed and
photorealistic textured meshes. However, the appearance of 3D objects generated
by these methods is often random and uncontrollable, posing a challenge in
achieving appearance-controllable 3D objects. To address this challenge, we
introduce IPDreamer, a novel approach that incorporates image prompts to
provide specific and comprehensive appearance information for 3D object
generation. Our results demonstrate that IPDreamer effectively generates
high-quality 3D objects that are consistent with both the provided text and
image prompts, demonstrating its promising capability in
appearance-controllable 3D object generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Prostate Cancer Diagnosis with Deep Learning: A Study using
  mpMRI Segmentation and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil B. Gavade, Neel Kanwal, Priyanka A. Gavade, Rajendra Nerli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prostate cancer (PCa) is a severe disease among men globally. It is important
to identify PCa early and make a precise diagnosis for effective treatment. For
PCa diagnosis, Multi-parametric magnetic resonance imaging (mpMRI) emerged as
an invaluable imaging modality that offers a precise anatomical view of the
prostate gland and its tissue structure. Deep learning (DL) models can enhance
existing clinical systems and improve patient care by locating regions of
interest for physicians. Recently, DL techniques have been employed to develop
a pipeline for segmenting and classifying different cancer types. These studies
show that DL can be used to increase diagnostic precision and give objective
results without variability. This work uses well-known DL models for the
classification and segmentation of mpMRI images to detect PCa. Our
implementation involves four pipelines; Semantic DeepSegNet with ResNet50,
DeepSegNet with recurrent neural network (RNN), U-Net with RNN, and U-Net with
a long short-term memory (LSTM). Each segmentation model is paired with a
different classifier to evaluate the performance using different metrics. The
results of our experiments show that the pipeline that uses the combination of
U-Net and the LSTM model outperforms all other combinations, excelling in both
segmentation and classification tasks
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CISCON-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SocialCircle: Learning the Angle-based Social Interaction Representation
  for Pedestrian Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conghao Wong, Beihao Xia, Xinge You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing and forecasting trajectories of agents like pedestrians and cars in
complex scenes has become more and more significant in many intelligent systems
and applications. The diversity and uncertainty in socially interactive
behaviors among a rich variety of agents make this task more challenging than
other deterministic computer vision tasks. Researchers have made a lot of
efforts to quantify the effects of these interactions on future trajectories
through different mathematical models and network structures, but this problem
has not been well solved. Inspired by marine animals that localize the
positions of their companions underwater through echoes, we build a new
anglebased trainable social representation, named SocialCircle, for
continuously reflecting the context of social interactions at different angular
orientations relative to the target agent. We validate the effect of the
proposed SocialCircle by training it along with several newly released
trajectory prediction models, and experiments show that the SocialCircle not
only quantitatively improves the prediction performance, but also qualitatively
helps better consider social interactions when forecasting pedestrian
trajectories in a way that is consistent with human intuitions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rotation Matters: Generalized Monocular 3D Object Detection for Various
  Camera Systems <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SungHo Moon, JinWoo Bae, SungHoon Im
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on monocular 3D object detection is being actively studied, and as a
result, performance has been steadily improving. However, 3D object detection
performance is significantly reduced when applied to a camera system different
from the system used to capture the training datasets. For example, a 3D
detector trained on datasets from a passenger car mostly fails to regress
accurate 3D bounding boxes for a camera mounted on a bus. In this paper, we
conduct extensive experiments to analyze the factors that cause performance
degradation. We find that changing the camera pose, especially camera
orientation, relative to the road plane caused performance degradation. In
addition, we propose a generalized 3D object detection method that can be
universally applied to various camera systems. We newly design a compensation
module that corrects the estimated 3D bounding box location and heading
direction. The proposed module can be applied to most of the recent 3D object
detection networks. It increases AP3D score (KITTI moderate, IoU $> 70\%$)
about 6-to-10-times above the baselines without additional training. Both
quantitative and qualitative results show the effectiveness of the proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPRw 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ C^2M-DoT: Cross-modal consistent multi-view medical report generation
  with domain transfer network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhi Wang, Xiangtao Wang, Jie Zhou, Thomas Lukasiewicz, Zhenghua Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical scenarios, multiple medical images with different views are
usually generated simultaneously, and these images have high semantic
consistency. However, most existing medical report generation methods only
consider single-view data. The rich multi-view mutual information of medical
images can help generate more accurate reports, however, the dependence of
multi-view models on multi-view data in the inference stage severely limits
their application in clinical practice. In addition, word-level optimization
based on numbers ignores the semantics of reports and medical images, and the
generated reports often cannot achieve good performance. Therefore, we propose
a cross-modal consistent multi-view medical report generation with a domain
transfer network (C^2M-DoT). Specifically, (i) a semantic-based multi-view
contrastive learning medical report generation framework is adopted to utilize
cross-view information to learn the semantic representation of lesions; (ii) a
domain transfer network is further proposed to ensure that the multi-view
report generation model can still achieve good inference performance under
single-view input; (iii) meanwhile, optimization using a cross-modal
consistency loss facilitates the generation of textual reports that are
semantically consistent with medical images. Extensive experimental studies on
two public benchmark datasets demonstrate that C^2M-DoT substantially
outperforms state-of-the-art baselines in all metrics. Ablation studies also
confirmed the validity and necessity of each component in C^2M-DoT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Neural Collapse for a Large Number of Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen Jiang, Jinxin Zhou, Peng Wang, Qing Qu, Dustin Mixon, Chong You, Zhihui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural collapse provides an elegant mathematical characterization of learned
last layer representations (a.k.a. features) and classifier weights in deep
classification models. Such results not only provide insights but also motivate
new techniques for improving practical deep models. However, most of the
existing empirical and theoretical studies in neural collapse focus on the case
that the number of classes is small relative to the dimension of the feature
space. This paper extends neural collapse to cases where the number of classes
are much larger than the dimension of feature space, which broadly occur for
language models, retrieval systems, and face recognition applications. We show
that the features and classifier exhibit a generalized neural collapse
phenomenon, where the minimum one-vs-rest margins is maximized.We provide
empirical study to verify the occurrence of generalized neural collapse in
practical deep neural networks. Moreover, we provide theoretical study to show
that the generalized neural collapse provably occurs under unconstrained
feature model with spherical constraint, under certain technical conditions on
feature dimension and number of classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Infrared Small Target Detection Using Double-Weighted Multi-Granularity
  Patch Tensor Model With Tensor-Train Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guiyu Zhang, Qunbo Lv, Zui Tao, Baoyu Zhu, Zheng Tan, Yuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared small target detection plays an important role in the remote sensing
fields. Therefore, many detection algorithms have been proposed, in which the
infrared patch-tensor (IPT) model has become a mainstream tool due to its
excellent performance. However, most IPT-based methods face great challenges,
such as inaccurate measure of the tensor low-rankness and poor robustness to
complex scenes, which will leadto poor detection performance. In order to solve
these problems, this paper proposes a novel double-weighted multi-granularity
infrared patch tensor (DWMGIPT) model. First, to capture different granularity
information of tensor from multiple modes, a multi-granularity infrared patch
tensor (MGIPT) model is constructed by collecting nonoverlapping patches and
tensor augmentation based on the tensor train (TT) decomposition. Second, to
explore the latent structure of tensor more efficiently, we utilize the
auto-weighted mechanism to balance the importance of information at different
granularity. Then, the steering kernel (SK) is employed to extract local
structure prior, which suppresses background interference such as strong edges
and noise. Finally, an efficient optimization algorithm based on the
alternating direction method of multipliers (ADMM) is presented to solve the
model. Extensive experiments in various challenging scenes show that the
proposed algorithm is robust to noise and different scenes. Compared with the
other eight state-of-the-art methods, different evaluation metrics demonstrate
that our method achieves better detection performance in various complex
scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anyview: Generalizable Indoor 3D Object Detection with Variable Frames 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wu, Xiuwei Xu, Ziwei Wang, Chong Xia, Linqing Zhao, Jiwen Lu, Haibin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel network framework for indoor 3D object
detection to handle variable input frame numbers in practical scenarios.
Existing methods only consider fixed frames of input data for a single
detector, such as monocular RGB-D images or point clouds reconstructed from
dense multi-view RGB-D images. While in practical application scenes such as
robot navigation and manipulation, the raw input to the 3D detectors is the
RGB-D images with variable frame numbers instead of the reconstructed scene
point cloud. However, the previous approaches can only handle fixed frame input
data and have poor performance with variable frame input. In order to
facilitate 3D object detection methods suitable for practical tasks, we present
a novel 3D detection framework named AnyView for our practical applications,
which generalizes well across different numbers of input frames with a single
model. To be specific, we propose a geometric learner to mine the local
geometric features of each input RGB-D image frame and implement local-global
feature interaction through a designed spatial mixture module. Meanwhile, we
further utilize a dynamic token strategy to adaptively adjust the number of
extracted features for each frame, which ensures consistent global feature
density and further enhances the generalization after fusion. Extensive
experiments on the ScanNet dataset show our method achieves both great
generalizability and high detection accuracy with a simple and clean
architecture containing a similar amount of parameters with the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Critical Look at Classic Test-Time Adaptation Methods in Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang'an Yi, Haotian Chen, Yifan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time adaptation (TTA) aims to adapt a model, initially trained on
training data, to potential distribution shifts in the test data. Most existing
TTA studies, however, focus on classification tasks, leaving a notable gap in
the exploration of TTA for semantic segmentation. This pronounced emphasis on
classification might lead numerous newcomers and engineers to mistakenly assume
that classic TTA methods designed for classification can be directly applied to
segmentation. Nonetheless, this assumption remains unverified, posing an open
question. To address this, we conduct a systematic, empirical study to disclose
the unique challenges of segmentation TTA, and to determine whether classic TTA
strategies can effectively address this task. Our comprehensive results have
led to three key observations. First, the classic batch norm updating strategy,
commonly used in classification TTA, only brings slight performance
improvement, and in some cases it might even adversely affect the results. Even
with the application of advanced distribution estimation techniques like batch
renormalization, the problem remains unresolved. Second, the teacher-student
scheme does enhance training stability for segmentation TTA in the presence of
noisy pseudo-labels. However, it cannot directly result in performance
improvement compared to the original model without TTA. Third, segmentation TTA
suffers a severe long-tailed imbalance problem, which is substantially more
complex than that in TTA for classification. This long-tailed challenge
significantly affects segmentation TTA performance, even when the accuracy of
pseudo-labels is high.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negative Object Presence Evaluation (NOPE) to Measure Object
  Hallucination in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object hallucination poses a significant challenge in vision-language (VL)
models, often leading to the generation of nonsensical or unfaithful responses
with non-existent objects. However, the absence of a general measurement for
evaluating object hallucination in VL models has hindered our understanding and
ability to mitigate this issue. In this work, we present NOPE (Negative Object
Presence Evaluation), a novel benchmark designed to assess object hallucination
in VL models through visual question answering (VQA). We propose a
cost-effective and scalable approach utilizing large language models to
generate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.
We extensively investigate the performance of 10 state-of-the-art VL models in
discerning the non-existence of objects in visual questions, where the ground
truth answers are denoted as NegP (e.g., "none"). Additionally, we evaluate
their standard performance on visual questions on 9 other VQA datasets. Through
our experiments, we demonstrate that no VL model is immune to the vulnerability
of object hallucination, as all models achieve accuracy below 10\% on NegP.
Furthermore, we uncover that lexically diverse visual questions, question types
with large scopes, and scene-relevant objects capitalize the risk of object
hallucination in VL models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What do larger image classifiers memorise? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Lukasik, Vaishnavh Nagarajan, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of modern neural networks has prompted study of the connection
between memorisation and generalisation: overparameterised models generalise
well, despite being able to perfectly fit (memorise) completely random labels.
To carefully study this issue, Feldman proposed a metric to quantify the degree
of memorisation of individual training examples, and empirically computed the
corresponding memorisation profile of a ResNet on image classification
bench-marks. While an exciting first glimpse into what real-world models
memorise, this leaves open a fundamental question: do larger neural models
memorise more? We present a comprehensive empirical analysis of this question
on image classification benchmarks. We find that training examples exhibit an
unexpectedly diverse set of memorisation trajectories across model sizes: most
samples experience decreased memorisation under larger models, while the rest
exhibit cap-shaped or increasing memorisation. We show that various proxies for
the Feldman memorization score fail to capture these fundamental trends.
Lastly, we find that knowledge distillation, an effective and popular model
compression technique, tends to inhibit memorisation, while also improving
generalisation. Specifically, memorisation is mostly inhibited on examples with
increasing memorisation trajectories, thus pointing at how distillation
improves generalisation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GReAT: A Graph Regularized Adversarial Training Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samet Bayram, Kenneth Barner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a regularization method called GReAT, Graph Regularized
Adversarial Training, to improve deep learning models' classification
performance. Adversarial examples are a well-known challenge in machine
learning, where small, purposeful perturbations to input data can mislead
models. Adversarial training, a powerful and one of the most effective defense
strategies, involves training models with both regular and adversarial
examples. However, it often neglects the underlying structure of the data. In
response, we propose GReAT, a method that leverages data graph structure to
enhance model robustness. GReAT deploys the graph structure of the data into
the adversarial training process, resulting in more robust models that better
generalize its testing performance and defend against adversarial attacks.
Through extensive evaluation on benchmark datasets, we demonstrate GReAT's
effectiveness compared to state-of-the-art classification methods, highlighting
its potential in improving deep learning models' classification performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages including references. 7 figures and 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Lightweight Video Anomaly Detection Model with Weak Supervision and
  Adaptive Instance Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Wang, Jiaogen Zhou, Jihong Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video anomaly detection is to determine whether there are any abnormal
events, behaviors or objects in a given video, which enables effective and
intelligent public safety management. As video anomaly labeling is both
time-consuming and expensive, most existing works employ unsupervised or weakly
supervised learning methods. This paper focuses on weakly supervised video
anomaly detection, in which the training videos are labeled whether or not they
contain any anomalies, but there is no information about which frames the
anomalies are located. However, the uncertainty of weakly labeled data and the
large model size prevent existing methods from wide deployment in real
scenarios, especially the resource-limit situations such as edge-computing. In
this paper, we develop a lightweight video anomaly detection model. On the one
hand, we propose an adaptive instance selection strategy, which is based on the
model's current status to select confident instances, thereby mitigating the
uncertainty of weakly labeled data and subsequently promoting the model's
performance. On the other hand, we design a lightweight multi-level temporal
correlation attention module and an hourglass-shaped fully connected layer to
construct the model, which can reduce the model parameters to only 0.56\% of
the existing methods (e.g. RTFM). Our extensive experiments on two public
datasets UCF-Crime and ShanghaiTech show that our model can achieve comparable
or even superior AUC score compared to the state-of-the-art methods, with a
significantly reduced number of model parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edge Computing-Enabled Road Condition Monitoring: System Development and
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulateef Daud, Mark Amo-Boateng, Neema Jakisa Owor, Armstrong Aboah, Yaw Adu-Gyamfi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time pavement condition monitoring provides highway agencies with timely
and accurate information that could form the basis of pavement maintenance and
rehabilitation policies. Existing technologies rely heavily on manual data
processing, are expensive and therefore, difficult to scale for frequent,
networklevel pavement condition monitoring. Additionally, these systems require
sending large packets of data to the cloud which requires large storage space,
are computationally expensive to process, and results in high latency. The
current study proposes a solution that capitalizes on the widespread
availability of affordable Micro Electro-Mechanical System (MEMS) sensors, edge
computing and internet connection capabilities of microcontrollers, and
deployable machine learning (ML) models to (a) design an Internet of Things
(IoT)-enabled device that can be mounted on axles of vehicles to stream live
pavement condition data (b) reduce latency through on-device processing and
analytics of pavement condition sensor data before sending to the cloud
servers. In this study, three ML models including Random Forest, LightGBM and
XGBoost were trained to predict International Roughness Index (IRI) at every
0.1-mile segment. XGBoost had the highest accuracy with an RMSE and MAPE of
16.89in/mi and 20.3%, respectively. In terms of the ability to classify the IRI
of pavement segments based on ride quality according to MAP-21 criteria, our
proposed device achieved an average accuracy of 96.76% on I-70EB and 63.15% on
South Providence. Overall, our proposed device demonstrates significant
potential in providing real-time pavement condition data to State Highway
Agencies (SHA) and Department of Transportation (DOTs) with a satisfactory
level of accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Feature Norm for Out-of-Distribution Detection <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoo Park, Jacky Chen Long Chai, Jaeho Yoon, Andrew Beng Jin Teoh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A neural network trained on a classification dataset often exhibits a higher
vector norm of hidden layer features for in-distribution (ID) samples, while
producing relatively lower norm values on unseen instances from
out-of-distribution (OOD). Despite this intriguing phenomenon being utilized in
many applications, the underlying cause has not been thoroughly investigated.
In this study, we demystify this very phenomenon by scrutinizing the
discriminative structures concealed in the intermediate layers of a neural
network. Our analysis leads to the following discoveries: (1) The feature norm
is a confidence value of a classifier hidden in the network layer, specifically
its maximum logit. Hence, the feature norm distinguishes OOD from ID in the
same manner that a classifier confidence does. (2) The feature norm is
class-agnostic, thus it can detect OOD samples across diverse discriminative
models. (3) The conventional feature norm fails to capture the deactivation
tendency of hidden layer neurons, which may lead to misidentification of ID
samples as OOD instances. To resolve this drawback, we propose a novel
negative-aware norm (NAN) that can capture both the activation and deactivation
tendencies of hidden layer neurons. We conduct extensive experiments on NAN,
demonstrating its efficacy and compatibility with existing OOD detectors, as
well as its capability in label-free environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiPS: Discriminative Pseudo-Label Sampling with Self-Supervised
  Transformers for Weakly Supervised Object Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakeeb Murtaza, Soufiane Belharbi, Marco Pedersoli, Aydin Sarraf, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised vision transformers (SSTs) have shown great potential to
yield rich localization maps that highlight different objects in an image.
However, these maps remain class-agnostic since the model is unsupervised. They
often tend to decompose the image into multiple maps containing different
objects while being unable to distinguish the object of interest from
background noise objects. In this paper, Discriminative Pseudo-label Sampling
(DiPS) is introduced to leverage these class-agnostic maps for
weakly-supervised object localization (WSOL), where only image-class labels are
available. Given multiple attention maps, DiPS relies on a pre-trained
classifier to identify the most discriminative regions of each attention map.
This ensures that the selected ROIs cover the correct image object while
discarding the background ones, and, as such, provides a rich pool of diverse
and discriminative proposals to cover different parts of the object.
Subsequently, these proposals are used as pseudo-labels to train our new
transformer-based WSOL model designed to perform classification and
localization tasks. Unlike standard WSOL methods, DiPS optimizes performance in
both tasks by using a transformer encoder and a dedicated output head for each
task, each trained using dedicated loss functions. To avoid overfitting a
single proposal and promote better object coverage, a single proposal is
randomly selected among the top ones for a training image at each training
step. Experimental results on the challenging CUB, ILSVRC, OpenImages, and
TelDrone datasets indicate that our architecture, in combination with our
transformer-based proposals, can yield better localization performance than
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HydraViT: Adaptive Multi-Branch Transformer for Multi-Label Disease
  Classification from Chest X-ray Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Şaban Öztürk, M. Yiğit Turalı, Tolga Çukur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-ray is an essential diagnostic tool in the identification of chest
diseases given its high sensitivity to pathological abnormalities in the lungs.
However, image-driven diagnosis is still challenging due to heterogeneity in
size and location of pathology, as well as visual similarities and
co-occurrence of separate pathology. Since disease-related regions often occupy
a relatively small portion of diagnostic images, classification models based on
traditional convolutional neural networks (CNNs) are adversely affected given
their locality bias. While CNNs were previously augmented with attention maps
or spatial masks to guide focus on potentially critical regions, learning
localization guidance under heterogeneity in the spatial distribution of
pathology is challenging. To improve multi-label classification performance,
here we propose a novel method, HydraViT, that synergistically combines a
transformer backbone with a multi-branch output module with learned weighting.
The transformer backbone enhances sensitivity to long-range context in X-ray
images, while using the self-attention mechanism to adaptively focus on
task-critical regions. The multi-branch output module dedicates an independent
branch to each disease label to attain robust learning across separate disease
classes, along with an aggregated branch across labels to maintain sensitivity
to co-occurrence relationships among pathology. Experiments demonstrate that,
on average, HydraViT outperforms competing attention-guided methods by 1.2%,
region-guided methods by 1.4%, and semantic-guided methods by 1.0% in
multi-label classification performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layout Sequence Prediction From Noisy Mobile Modality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Zhang, Yi Xu, Hongsheng Lu, Takayuki Shimizu, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction plays a vital role in understanding pedestrian movement
for applications such as autonomous driving and robotics. Current trajectory
prediction models depend on long, complete, and accurately observed sequences
from visual modalities. Nevertheless, real-world situations often involve
obstructed cameras, missed objects, or objects out of sight due to
environmental factors, leading to incomplete or noisy trajectories. To overcome
these limitations, we propose LTrajDiff, a novel approach that treats objects
obstructed or out of sight as equally important as those with fully visible
trajectories. LTrajDiff utilizes sensor data from mobile phones to surmount
out-of-sight constraints, albeit introducing new challenges such as modality
fusion, noisy data, and the absence of spatial layout and object size
information. We employ a denoising diffusion model to predict precise layout
sequences from noisy mobile data using a coarse-to-fine diffusion strategy,
incorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model
predicts layout sequences by implicitly inferring object size and projection
status from a single reference timestamp or significantly obstructed sequences.
Achieving SOTA results in randomly obstructed experiments and extremely short
input experiments, our model illustrates the effectiveness of leveraging noisy
mobile data. In summary, our approach offers a promising solution to the
challenges faced by layout sequence and trajectory prediction models in
real-world settings, paving the way for utilizing sensor data from mobile
phones to accurately predict pedestrian bounding box trajectories. To the best
of our knowledge, this is the first work that addresses severely obstructed and
extremely short layout sequences by combining vision with noisy mobile
modality, making it the pioneering work in the field of layout sequence
trajectory prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 31st ACM International Conference on Multimedia
  2023 (MM 23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Factorized Tensor Networks for Multi-Task and Multi-Domain Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Garg, Nebiyou Yismaw, Rakib Hyder, Ashley Prater-Bennette, M. Salman Asif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task and multi-domain learning methods seek to learn multiple
tasks/domains, jointly or one after another, using a single unified network.
The key challenge and opportunity is to exploit shared information across tasks
and domains to improve the efficiency of the unified network. The efficiency
can be in terms of accuracy, storage cost, computation, or sample complexity.
In this paper, we propose a factorized tensor network (FTN) that can achieve
accuracy comparable to independent single-task/domain networks with a small
number of additional parameters. FTN uses a frozen backbone network from a
source model and incrementally adds task/domain-specific low-rank tensor
factors to the shared frozen network. This approach can adapt to a large number
of target domains and tasks without catastrophic forgetting. Furthermore, FTN
requires a significantly smaller number of task-specific parameters compared to
existing methods. We performed experiments on widely used multi-domain and
multi-task datasets. We show the experiments on convolutional-based
architecture with different backbones and on transformer-based architecture. We
observed that FTN achieves similar accuracy as single-task/domain methods while
using only a fraction of additional parameters per task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-driven Prompt Generation for Vision-Language Models in Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Qiu, Xingyu Li, Chaithanya Kumar Mummadi, Madan Ravi Ganesh, Zhenzhen Li, Lu Peng, Wan-Yi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning for vision-language models, e.g., CoOp, has shown great
success in adapting CLIP to different downstream tasks, making it a promising
solution for federated learning due to computational reasons. Existing prompt
learning techniques replace hand-crafted text prompts with learned vectors that
offer improvements on seen classes, but struggle to generalize to unseen
classes. Our work addresses this challenge by proposing Federated Text-driven
Prompt Generation (FedTPG), which learns a unified prompt generation network
across multiple remote clients in a scalable manner. The prompt generation
network is conditioned on task-related text input, thus is context-aware,
making it suitable to generalize for both seen and unseen classes. Our
comprehensive empirical evaluations on nine diverse image classification
datasets show that our method is superior to existing federated prompt learning
methods, that achieve overall better generalization on both seen and unseen
classes and is also generalizable to unseen datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QR-Tag: Angular Measurement and Tracking with a QR-Design Marker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simeng Qiu, Hadi Amata, Wolfgang Heidrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directional information measurement has many applications in domains such as
robotics, virtual and augmented reality, and industrial computer vision.
Conventional methods either require pre-calibration or necessitate controlled
environments. The state-of-the-art MoireTag approach exploits the Moire effect
and QR-design to continuously track the angular shift precisely. However, it is
still not a fully QR code design. To overcome the above challenges, we propose
a novel snapshot method for discrete angular measurement and tracking with
scannable QR-design patterns that are generated by binary structures printed on
both sides of a glass plate. The QR codes, resulting from the parallax effect
due to the geometry alignment between two layers, can be readily measured as
angular information using a phone camera. The simulation results show that the
proposed non-contact object tracking framework is computationally efficient
with high accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Developing and Refining a Multifunctional Facial Recognition System for
  Older Adults with Cognitive Impairments: A Journey Towards Enhanced Quality
  of Life 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era where the global population is aging significantly, cognitive
impairments among the elderly have become a major health concern. The need for
effective assistive technologies is clear, and facial recognition systems are
emerging as promising tools to address this issue. This document discusses the
development and evaluation of a new Multifunctional Facial Recognition System
(MFRS), designed specifically to assist older adults with cognitive
impairments. The MFRS leverages face_recognition [1], a powerful open-source
library capable of extracting, identifying, and manipulating facial features.
Our system integrates the face recognition and retrieval capabilities of
face_recognition, along with additional functionalities to capture images and
record voice memos. This combination of features notably enhances the system's
usability and versatility, making it a more user-friendly and universally
applicable tool for end-users. The source code for this project can be accessed
at https://github.com/Li-8023/Multi-function-face-recognition.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Diagnostic Precision: Leveraging Machine Learning Techniques
  for Accurate Detection of Covid-19, Pneumonia, and Tuberculosis in Chest
  X-Ray Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kulkarni, Guruprasad Parasnis, Harish Balasubramanian, Vansh Jain, Anmol Chokshi, Reena Sonkusare
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lung diseases such as COVID-19, tuberculosis (TB), and pneumonia continue to
be serious global health concerns that affect millions of people worldwide. In
medical practice, chest X-ray examinations have emerged as the norm for
diagnosing diseases, particularly chest infections such as COVID-19. Paramedics
and scientists are working intensively to create a reliable and precise
approach for early-stage COVID-19 diagnosis in order to save lives. But with a
variety of symptoms, medical diagnosis of these disorders poses special
difficulties. It is essential to address their identification and timely
diagnosis in order to successfully treat and prevent these illnesses. In this
research, a multiclass classification approach using state-of-the-art methods
for deep learning and image processing is proposed. This method takes into
account the robustness and efficiency of the system in order to increase
diagnostic precision of chest diseases. A comparison between a brand-new
convolution neural network (CNN) and several transfer learning pre-trained
models including VGG19, ResNet, DenseNet, EfficientNet, and InceptionNet is
recommended. Publicly available and widely used research datasets like Shenzen,
Montogomery, the multiclass Kaggle dataset and the NIH dataset were used to
rigorously test the model. Recall, precision, F1-score, and Area Under Curve
(AUC) score are used to evaluate and compare the performance of the proposed
model. An AUC value of 0.95 for COVID-19, 0.99 for TB, and 0.98 for pneumonia
is obtained using the proposed network. Recall and precision ratings of 0.95,
0.98, and 0.97, respectively, likewise met high standards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 18 figures, Under review in Discover Artificial
  Intelligence Journal by Springer Nature</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting Vision-Based Human Pose Estimation with Rotation Matrix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Vazan, Fatemeh Sadat Masoumi, Ruizhi Ou, Reza Rawassizadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fitness applications are commonly used to monitor activities within the gym,
but they often fail to automatically track indoor activities inside the gym.
This study proposes a model that utilizes pose estimation combined with a novel
data augmentation method, i.e., rotation matrix. We aim to enhance the
classification accuracy of activity recognition based on pose estimation data.
Through our experiments, we experiment with different classification algorithms
along with image augmentation approaches. Our findings demonstrate that the SVM
with SGD optimization, using data augmentation with the Rotation Matrix, yields
the most accurate results, achieving a 96% accuracy rate in classifying five
physical activities. Conversely, without implementing the data augmentation
techniques, the baseline accuracy remains at a modest 64%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DyST: Towards Dynamic Neural Scene Representations on Real-World Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Seitzer, Sjoerd van Steenkiste, Thomas Kipf, Klaus Greff, Mehdi S. M. Sajjadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual understanding of the world goes beyond the semantics and flat
structure of individual images. In this work, we aim to capture both the 3D
structure and dynamics of real-world scenes from monocular real-world videos.
Our Dynamic Scene Transformer (DyST) model leverages recent work in neural
scene representation to learn a latent decomposition of monocular real-world
videos into scene content, per-view scene dynamics, and camera pose. This
separation is achieved through a novel co-training scheme on monocular videos
and our new synthetic dataset DySO. DyST learns tangible latent representations
for dynamic scenes that enable view generation with separate control over the
camera and the content of the scene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://dyst-paper.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBEVFusion: Cooperative Perception with LiDAR-Camera Bird's-Eye View
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghao Qiao, Farhana Zulkernine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Vehicles (AVs) use multiple sensors to gather information about
their surroundings. By sharing sensor data between Connected Autonomous
Vehicles (CAVs), the safety and reliability of these vehicles can be improved
through a concept known as cooperative perception. However, recent approaches
in cooperative perception only share single sensor information such as cameras
or LiDAR. In this research, we explore the fusion of multiple sensor data
sources and present a framework, called CoBEVFusion, that fuses LiDAR and
camera data to create a Bird's-Eye View (BEV) representation. The CAVs process
the multi-modal data locally and utilize a Dual Window-based Cross-Attention
(DWCA) module to fuse the LiDAR and camera features into a unified BEV
representation. The fused BEV feature maps are shared among the CAVs, and a 3D
Convolutional Neural Network is applied to aggregate the features from the
CAVs. Our CoBEVFusion framework was evaluated on the cooperative perception
dataset OPV2V for two perception tasks: BEV semantic segmentation and 3D object
detection. The results show that our DWCA LiDAR-camera fusion model outperforms
perception models with single-modal data and state-of-the-art BEV fusion
models. Our overall cooperative perception architecture, CoBEVFusion, also
achieves comparable performance with other cooperative perception models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Booster: a Benchmark for Depth from Images of Specular and Transparent
  Surfaces <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierluigi Zama Ramirez, Alex Costanzino, Fabio Tosi, Matteo Poggi, Samuele Salti, Stefano Mattoccia, Luigi Di Stefano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating depth from images nowadays yields outstanding results, both in
terms of in-domain accuracy and generalization. However, we identify two main
challenges that remain open in this field: dealing with non-Lambertian
materials and effectively processing high-resolution images. Purposely, we
propose a novel dataset that includes accurate and dense ground-truth labels at
high resolution, featuring scenes containing several specular and transparent
surfaces. Our acquisition pipeline leverages a novel deep space-time stereo
framework, enabling easy and accurate labeling with sub-pixel precision. The
dataset is composed of 606 samples collected in 85 different scenes, each
sample includes both a high-resolution pair (12 Mpx) as well as an unbalanced
stereo pair (Left: 12 Mpx, Right: 1.1 Mpx), typical of modern mobile devices
that mount sensors with different resolutions. Additionally, we provide
manually annotated material segmentation masks and 15K unlabeled samples. The
dataset is composed of a train set and two test sets, the latter devoted to the
evaluation of stereo and monocular depth estimation networks. Our experiments
highlight the open challenges and future research directions in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extension of the paper "Open Challenges in Deep Stereo: the Booster
  Dataset" presented at CVPR 2022. Accepted at TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanwen Yue, Sabarinath Mahadevan, Jonas Schult, Francis Engelmann, Bastian Leibe, Konrad Schindler, Theodora Kontogianni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During interactive segmentation, a model and a user work together to
delineate objects of interest in a 3D point cloud. In an iterative process, the
model assigns each data point to an object (or the background), while the user
corrects errors in the resulting segmentation and feeds them back into the
model. The current best practice formulates the problem as binary
classification and segments objects one at a time. The model expects the user
to provide positive clicks to indicate regions wrongly assigned to the
background and negative clicks on regions wrongly assigned to the object.
Sequentially visiting objects is wasteful since it disregards synergies between
objects: a positive click for a given object can, by definition, serve as a
negative click for nearby objects. Moreover, a direct competition between
adjacent objects can speed up the identification of their common boundary. We
introduce AGILE3D, an efficient, attention-based model that (1) supports
simultaneous segmentation of multiple 3D objects, (2) yields more accurate
segmentation masks with fewer user clicks, and (3) offers faster inference. Our
core idea is to encode user clicks as spatial-temporal queries and enable
explicit interactions between click queries as well as between them and the 3D
scene through a click attention module. Every time new clicks are added, we
only need to run a lightweight decoder that produces updated segmentation
masks. In experiments with four different 3D point cloud datasets, AGILE3D sets
a new state-of-the-art. Moreover, we also verify its practicality in real-world
setups with real user studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ywyue.github.io/AGILE3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Low-Cost Lane-Following Algorithm for Cyber-Physical Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archit Gupta, Arvind Easwaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Duckiebots are low-cost mobile robots that are widely used in the fields of
research and education. Although there are existing self-driving algorithms for
the Duckietown platform, they are either too complex or perform too poorly to
navigate a multi-lane track. Moreover, it is essential to give memory and
computational resources to a Duckiebot so it can perform additional tasks such
as out-of-distribution input detection. In order to satisfy these constraints,
we built a low-cost autonomous driving algorithm capable of driving on a
two-lane track. The algorithm uses traditional computer vision techniques to
identify the central lane on the track and obtain the relevant steering angle.
The steering is then controlled by a PID controller that smoothens the movement
of the Duckiebot. The performance of the algorithm was compared to that of the
NeurIPS 2018 AI Driving Olympics (AIDO) finalists, and it outperformed all but
one finalists. The two main contributions of our algorithm are its low
computational requirements and very quick set-up, with ongoing efforts to make
it more reliable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Equivariant Similarity for Vision-Language Foundation Models <span class="chip">ICCV'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the concept of equivariance in vision-language foundation
models (VLMs), focusing specifically on the multimodal similarity function that
is not only the major training objective but also the core delivery to support
downstream tasks. Unlike the existing image-text similarity objective which
only categorizes matched pairs as similar and unmatched pairs as dissimilar,
equivariance also requires similarity to vary faithfully according to the
semantic changes. This allows VLMs to generalize better to nuanced and unseen
multimodal compositions. However, modeling equivariance is challenging as the
ground truth of semantic change is difficult to collect. For example, given an
image-text pair about a dog, it is unclear to what extent the similarity
changes when the pixel is changed from dog to cat? To this end, we propose
EqSim, a regularization loss that can be efficiently calculated from any two
matched training pairs and easily pluggable into existing image-text retrieval
fine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we
present a new challenging benchmark EqBen. Compared to the existing evaluation
sets, EqBen is the first to focus on "visual-minimal change". Extensive
experiments show the lack of equivariance in current VLMs and validate the
effectiveness of EqSim. Code is available at https://github.com/Wangt-CN/EqBen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV'23 (Oral); Add evaluation on MLLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recent Advancements in Machine Learning For Cybercrime Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lavanya Elluri, Varun Mandalapu, Piyush Vyas, Nirmalya Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cybercrime is a growing threat to organizations and individuals worldwide,
with criminals using sophisticated techniques to breach security systems and
steal sensitive data. This paper aims to comprehensively survey the latest
advancements in cybercrime prediction, highlighting the relevant research. For
this purpose, we reviewed more than 150 research articles and discussed 50 most
recent and appropriate ones. We start the review with some standard methods
cybercriminals use and then focus on the latest machine and deep learning
techniques, which detect anomalous behavior and identify potential threats. We
also discuss transfer learning, which allows models trained on one dataset to
be adapted for use on another dataset. We then focus on active and
reinforcement learning as part of early-stage algorithmic research in
cybercrime prediction. Finally, we discuss critical innovations, research gaps,
and future research opportunities in Cybercrime prediction. This paper presents
a holistic view of cutting-edge developments and publicly available datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Journal of Computer Information Systems, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TransHP: Image Classification with Hierarchical Prompting <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06385v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06385v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Wang, Yifan Sun, Wei Li, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores a hierarchical prompting mechanism for the hierarchical
image classification (HIC) task. Different from prior HIC methods, our
hierarchical prompting is the first to explicitly inject ancestor-class
information as a tokenized hint that benefits the descendant-class
discrimination. We think it well imitates human visual recognition, i.e.,
humans may use the ancestor class as a prompt to draw focus on the subtle
differences among descendant classes. We model this prompting mechanism into a
Transformer with Hierarchical Prompting (TransHP). TransHP consists of three
steps: 1) learning a set of prompt tokens to represent the coarse (ancestor)
classes, 2) on-the-fly predicting the coarse class of the input image at an
intermediate block, and 3) injecting the prompt token of the predicted coarse
class into the intermediate feature. Though the parameters of TransHP maintain
the same for all input images, the injected coarse-class prompt conditions
(modifies) the subsequent feature extraction and encourages a dynamic focus on
relatively subtle differences among the descendant classes. Extensive
experiments show that TransHP improves image classification on accuracy (e.g.,
improving ViT-B/16 by +2.83% ImageNet classification accuracy), training data
efficiency (e.g., +12.69% improvement under 10% ImageNet training data), and
model explainability. Moreover, TransHP also performs favorably against prior
HIC methods, showing that TransHP well exploits the hierarchical information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning based projection domain metal segmentation for metal
  artifact reduction in cone beam computed tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshit Agrawal, Ari Hietanen, Simo Särkkä
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metal artifact correction is a challenging problem in cone beam computed
tomography (CBCT) scanning. Metal implants inserted into the anatomy cause
severe artifacts in reconstructed images. Widely used inpainting-based metal
artifact reduction (MAR) methods require segmentation of metal traces in the
projections as a first step, which is a challenging task. One approach is to
use a deep learning method to segment metals in the projections. However, the
success of deep learning methods is limited by the availability of realistic
training data. It is laborious and time consuming to get reliable ground truth
annotations due to unclear implant boundaries and large numbers of projections.
We propose to use X-ray simulations to generate synthetic metal segmentation
training dataset from clinical CBCT scans. We compare the effect of simulations
with different numbers of photons and also compare several training strategies
to augment the available data. We compare our model's performance on real
clinical scans with conventional region growing threshold-based MAR, moving
metal artifact reduction method, and a recent deep learning method. We show
that simulations with relatively small number of photons are suitable for the
metal segmentation task and that training the deep learning model with full
size and cropped projections together improves the robustness of the model. We
show substantial improvement in the image quality affected by severe motion,
voxel size under-sampling, and out-of-FOV metals. Our method can be easily
integrated into the existing projection-based MAR pipeline to get improved
image quality. This method can provide a novel paradigm to accurately segment
metals in CBCT projections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Generated Images as Data Source: The Dawn of Synthetic Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01830v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01830v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuhao Yang, Fangneng Zhan, Kunhao Liu, Muyu Xu, Shijian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of visual intelligence is intrinsically tethered to the
availability of data. In parallel, generative Artificial Intelligence (AI) has
unlocked the potential to create synthetic images that closely resemble
real-world photographs, which prompts a compelling inquiry: how visual
intelligence benefit from the advance of generative AI? This paper explores the
innovative concept of harnessing these AI-generated images as a new data
source, reshaping traditional model paradigms in visual intelligence. In
contrast to real data, AI-generated data sources exhibit remarkable advantages,
including unmatched abundance and scalability, the rapid generation of vast
datasets, and the effortless simulation of edge cases. Built on the success of
generative AI models, we examines the potential of their generated data in a
range of applications, from training machine learning models to simulating
scenarios for computational modeling, testing, and validation. We probe the
technological foundations that support this groundbreaking use of generative
AI, engaging in an in-depth discussion on the ethical, legal, and practical
considerations that accompany this transformative paradigm shift. Through an
exhaustive survey of current technologies and applications, this paper presents
a comprehensive view of the synthetic era in visual intelligence. A project
associated with this paper can be found at https://github.com/mwxely/AIGS .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLCA: Slow Learner with Classifier Alignment for Continual Learning on a
  Pre-trained Model <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05118v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05118v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, Yunchao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of continual learning is to improve the performance of recognition
models in learning sequentially arrived data. Although most existing works are
established on the premise of learning from scratch, growing efforts have been
devoted to incorporating the benefits of pre-training. However, how to
adaptively exploit the pre-trained knowledge for each incremental task while
maintaining its generalizability remains an open question. In this work, we
present an extensive analysis for continual learning on a pre-trained model
(CLPM), and attribute the key challenge to a progressive overfitting problem.
Observing that selectively reducing the learning rate can almost resolve this
issue in the representation layer, we propose a simple but extremely effective
approach named Slow Learner with Classifier Alignment (SLCA), which further
improves the classification layer by modeling the class-wise distributions and
aligning the classification layers in a post-hoc fashion. Across a variety of
scenarios, our proposal provides substantial improvements for CLPM (e.g., up to
49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split
CUB-200 and Split Cars-196, respectively), and thus outperforms
state-of-the-art approaches by a large margin. Based on such a strong baseline,
critical factors and promising directions are analyzed in-depth to facilitate
subsequent research. Code has been made available at:
https://github.com/GengDavid/SLCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023, code released</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Supervised Training with Autoencoders for Visual Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.11723v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.11723v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Bauer, Shinichi Nakajima, Klaus-Robert Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep autoencoders provide an effective tool for learning non-linear
dimensionality reduction in an unsupervised way. Recently, they have been used
for the task of anomaly detection in the visual domain. By optimizing for the
reconstruction error using anomaly-free examples, the common belief is that a
corresponding network should fail to accurately reconstruct anomalous regions
in the application phase. This goal is typically addressed by controlling the
capacity of the network, either by reducing the size of the bottleneck layer or
by enforcing sparsity constraints on the activations. However, neither of these
techniques does explicitly penalize reconstruction of anomalous signals often
resulting in poor detection. We tackle this problem by adapting a
self-supervised learning regime that allows the use of discriminative
information during training but focuses on the data manifold of normal
examples. We emphasize that inference with our approach is very efficient
during training and prediction requiring a single forward pass for each input
image. Our experiments on the MVTec AD dataset demonstrate high detection and
localization performance. On the texture-subset, in particular, our approach
consistently outperforms recent anomaly detection methods by a significant
margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized
  Tokenizer of a Large-Scale Generative Model <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Xu, Enver Sangineto, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the progress made in the style transfer task, most previous work
focus on transferring only relatively simple features like color or texture,
while missing more abstract concepts such as overall art expression or
painter-specific traits. However, these abstract semantics can be captured by
models like DALL-E or CLIP, which have been trained using huge datasets of
images and textual documents. In this paper, we propose StylerDALLE, a style
transfer method that exploits both of these models and uses natural language to
describe abstract art styles. Specifically, we formulate the language-guided
style transfer task as a non-autoregressive token sequence translation, i.e.,
from input content image to output stylized image, in the discrete latent space
of a large-scale pretrained vector-quantized tokenizer, e.g., the discrete
variational auto-encoder (dVAE) of DALL-E. To incorporate style information, we
propose a Reinforcement Learning strategy with CLIP-based language supervision
that ensures stylization and content preservation simultaneously. Experimental
results demonstrate the superiority of our method, which can effectively
transfer art styles using language instructions at different granularities.
Code is available at https://github.com/zipengxuc/StylerDALLE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InternLM-XComposer: A Vision-Language Large Model for Advanced
  Text-image Comprehension and Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15112v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15112v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose InternLM-XComposer, a vision-language large model that enables
advanced image-text comprehension and composition. The innovative nature of our
model is highlighted by three appealing properties: 1) Interleaved Text-Image
Composition: InternLM-XComposer can effortlessly generate coherent and
contextual articles that seamlessly integrate images, providing a more engaging
and immersive reading experience. Simply provide a title, and our system will
generate the corresponding manuscript. It can intelligently identify the areas
in the text where images would enhance the content and automatically insert the
most appropriate visual candidates. 2) Comprehension with Rich Multilingual
Knowledge: The text-image comprehension is empowered by training on extensive
multi-modal multilingual concepts with carefully crafted strategies, resulting
in a deep understanding of visual content. 3) State-of-the-art Performance: Our
model consistently achieves state-of-the-art results across various mainstream
benchmarks for vision-language foundational models, including MME Benchmark,
MMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark).
Collectively, InternLM-XComposer seamlessly blends advanced text-image
comprehension and composition, revolutionizing vision-language interaction and
offering new insights and opportunities. The InternLM-XComposer model series
with 7B parameters are publicly available at
https://github.com/InternLM/InternLM-XComposer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/InternLM/InternLM-XComposer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Connections between Operator-splitting Methods and Deep Neural Networks
  with Applications in Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Xue-Cheng Tai, Raymond Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural network is a powerful tool for many tasks. Understanding why it
is so successful and providing a mathematical explanation is an important
problem and has been one popular research direction in past years. In the
literature of mathematical analysis of deep neural networks, a lot of works is
dedicated to establishing representation theories. How to make connections
between deep neural networks and mathematical algorithms is still under
development. In this paper, we give an algorithmic explanation for deep neural
networks, especially in their connections with operator splitting. We show that
with certain splitting strategies, operator-splitting methods have the same
structure as networks. Utilizing this connection and the Potts model for image
segmentation, two networks inspired by operator-splitting methods are proposed.
The two networks are essentially two operator-splitting algorithms solving the
Potts model. Numerical experiments are presented to demonstrate the
effectiveness of the proposed networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zhao, Zengxin Qi, Sheng Wang, Qian Wang, Xuehai Wu, Ying Mao, Lichi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation methods are generally designed as fully-supervised
to guarantee model performance, which require a significant amount of expert
annotated samples that are high-cost and laborious. Semi-supervised image
segmentation can alleviate the problem by utilizing a large number of unlabeled
images along with limited labeled images. However, learning a robust
representation from numerous unlabeled images remains challenging due to
potential noise in pseudo labels and insufficient class separability in feature
space, which undermines the performance of current semi-supervised segmentation
approaches. To address the issues above, we propose a novel semi-supervised
segmentation method named as Rectified Contrastive Pseudo Supervision (RCPS),
which combines a rectified pseudo supervision and voxel-level contrastive
learning to improve the effectiveness of semi-supervised segmentation.
Particularly, we design a novel rectification strategy for the pseudo
supervision method based on uncertainty estimation and consistency
regularization to reduce the noise influence in pseudo labels. Furthermore, we
introduce a bidirectional voxel contrastive loss to the network to ensure
intra-class consistency and inter-class contrast in feature space, which
increases class separability in the segmentation. The proposed RCPS
segmentation method has been validated on two public datasets and an in-house
clinical dataset. Experimental results reveal that the proposed method yields
better segmentation performance compared with the state-of-the-art methods in
semi-supervised medical image segmentation. The source code is available at
https://github.com/hsiangyuzhao/RCPS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep
  Ensembles are More Efficient than Single Models <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08010v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08010v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxuan Xia, Christos-Savvas Bouganis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Ensembles are a simple, reliable, and effective method of improving both
the predictive performance and uncertainty estimates of deep learning
approaches. However, they are widely criticised as being computationally
expensive, due to the need to deploy multiple independent models. Recent work
has challenged this view, showing that for predictive accuracy, ensembles can
be more computationally efficient (at inference) than scaling single models
within an architecture family. This is achieved by cascading ensemble members
via an early-exit approach. In this work, we investigate extending these
efficiency gains to tasks related to uncertainty estimation. As many such
tasks, e.g. selective classification, are binary classification, our key novel
insight is to only pass samples within a window close to the binary decision
boundary to later cascade stages. Experiments on ImageNet-scale data across a
number of network architectures and uncertainty tasks show that the proposed
window-based early-exit approach is able to achieve a superior
uncertainty-computation trade-off compared to scaling single models. For
example, a cascaded EfficientNet-B2 ensemble is able to achieve similar
coverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.
We also find that cascades/ensembles give more reliable improvements on OOD
data vs scaling models up. Code for this work is available at:
https://github.com/Guoxoug/window-early-exit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2023 (camera-ready version, 9 pages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08685v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08685v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhui Xiao, Xiaoshan Yang, Fang Peng, Ming Yan, Yaowei Wang, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Grounding (VG) is a crucial topic in the field of vision and language,
which involves locating a specific region described by expressions within an
image. To reduce the reliance on manually labeled data, unsupervised methods
have been developed to locate regions using pseudo-labels. However, the
performance of existing unsupervised methods is highly dependent on the quality
of pseudo-labels and these methods always encounter issues with limited
diversity. In order to utilize vision and language pre-trained models to
address the grounding problem, and reasonably take advantage of pseudo-labels,
we propose CLIP-VG, a novel method that can conduct self-paced curriculum
adapting of CLIP with pseudo-language labels. We propose a simple yet efficient
end-to-end network architecture to realize the transfer of CLIP to the visual
grounding. Based on the CLIP-based architecture, we further propose
single-source and multi-source curriculum adapting algorithms, which can
progressively find more reliable pseudo-labels to learn an optimal model,
thereby achieving a balance between reliability and diversity for the
pseudo-language labels. Our method outperforms the current state-of-the-art
unsupervised method by a significant margin on RefCOCO/+/g datasets in both
single-source and multi-source scenarios, with improvements ranging from 6.78%
to 10.67% and 11.39% to 14.87%, respectively. Furthermore, our approach even
outperforms existing weakly supervised methods. The code and models are
available at https://github.com/linhuixiao/CLIP-VG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transaction on Multimedia (2023), Paper page:
  https://ieeexplore.ieee.org/abstract/document/10269126. Code will be released
  at https://github.com/linhuixiao/CLIP-VG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vital Videos: A dataset of face videos with PPG and blood pressure
  ground truths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pieter-Jan Toye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We collected a large dataset consisting of nearly 900 unique participants.
For every participant we recorded two 30 second uncompressed videos,
synchronized PPG waveforms and a single blood pressure measurement. Gender, age
and skin color were also registered for every participant. The dataset includes
roughly equal numbers of males and females, as well as participants of all
ages. While the skin color distribution could have been more balanced, the
dataset contains individuals from every skin color. The data was collected in a
diverse set of locations to ensure a wide variety of backgrounds and lighting
conditions. In an effort to assist in the research and development of remote
vital sign measurement we are now opening up access to this dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Hours to Seconds: Towards 100x Faster Quantitative Phase Imaging
  via Differentiable Microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11521v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11521v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Udith Haputhanthri, Kithmini Herath, Ramith Hettiarachchi, Hasindu Kariyawasam, Azeem Ahmad, Balpreet S. Ahluwalia, Chamira U. S. Edussooriya, Dushan N. Wadduwage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With applications ranging from metabolomics to histopathology, quantitative
phase microscopy (QPM) is a powerful label-free imaging modality. Despite
significant advances in fast multiplexed imaging sensors and
deep-learning-based inverse solvers, the throughput of QPM is currently limited
by the speed of electronic hardware. Complementarily, to improve throughput
further, here we propose to acquire images in a compressed form such that more
information can be transferred beyond the existing electronic hardware
bottleneck. To this end, we present a learnable optical
compression-decompression framework that learns content-specific features. The
proposed differentiable quantitative phase microscopy ($\partial \mu$) first
uses learnable optical feature extractors as image compressors. The intensity
representation produced by these networks is then captured by the imaging
sensor. Finally, a reconstruction network running on electronic hardware
decompresses the QPM images. In numerical experiments, the proposed system
achieves compression of $\times$ 64 while maintaining the SSIM of $\sim 0.90$
and PSNR of $\sim 30$ dB on cells. The results demonstrated by our experiments
open up a new pathway for achieving end-to-end optimized (i.e., optics and
electronic) compact QPM systems that may provide unprecedented throughput
improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency Regularization for Generalizable Source-free Domain
  Adaptation <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longxiang Tang, Kai Li, Chunming He, Yulun Zhang, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain adaptation (SFDA) aims to adapt a well-trained source
model to an unlabelled target domain without accessing the source dataset,
making it applicable in a variety of real-world scenarios. Existing SFDA
methods ONLY assess their adapted models on the target training set, neglecting
the data from unseen but identically distributed testing sets. This oversight
leads to overfitting issues and constrains the model's generalization ability.
In this paper, we propose a consistency regularization framework to develop a
more generalizable SFDA method, which simultaneously boosts model performance
on both target training and testing datasets. Our method leverages soft
pseudo-labels generated from weakly augmented images to supervise strongly
augmented images, facilitating the model training process and enhancing the
generalization ability of the adapted model. To leverage more potentially
useful supervision, we present a sampling-based pseudo-label selection
strategy, taking samples with severer domain shift into consideration.
Moreover, global-oriented calibration methods are introduced to exploit global
class distribution and feature cluster information, further improving the
adaptation process. Extensive experiments demonstrate our method achieves
state-of-the-art performance on several SFDA benchmarks, and exhibits
robustness on unseen testing datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2023 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point
  Clouds with Masked Occupancy Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09900v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09900v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Min, Xinli Xu, Dawei Zhao, Liang Xiao, Yiming Nie, Bin Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current perception models in autonomous driving heavily rely on large-scale
labelled 3D data, which is both costly and time-consuming to annotate. This
work proposes a solution to reduce the dependence on labelled 3D training data
by leveraging pre-training on large-scale unlabeled outdoor LiDAR point clouds
using masked autoencoders (MAE). While existing masked point autoencoding
methods mainly focus on small-scale indoor point clouds or pillar-based
large-scale outdoor LiDAR data, our approach introduces a new self-supervised
masked occupancy pre-training method called Occupancy-MAE, specifically
designed for voxel-based large-scale outdoor LiDAR point clouds. Occupancy-MAE
takes advantage of the gradually sparse voxel occupancy structure of outdoor
LiDAR point clouds and incorporates a range-aware random masking strategy and a
pretext task of occupancy prediction. By randomly masking voxels based on their
distance to the LiDAR and predicting the masked occupancy structure of the
entire 3D surrounding scene, Occupancy-MAE encourages the extraction of
high-level semantic information to reconstruct the masked voxel using only a
small number of visible voxels. Extensive experiments demonstrate the
effectiveness of Occupancy-MAE across several downstream tasks. For 3D object
detection, Occupancy-MAE reduces the labelled data required for car detection
on the KITTI dataset by half and improves small object detection by
approximately 2% in AP on the Waymo dataset. For 3D semantic segmentation,
Occupancy-MAE outperforms training from scratch by around 2% in mIoU. For
multi-object tracking, Occupancy-MAE enhances training from scratch by
approximately 1% in terms of AMOTA and AMOTP. Codes are publicly available at
https://github.com/chaytonmin/Occupancy-MAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TIV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniScene: Multi-Camera Unified Pre-training via 3D Scene Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18829v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18829v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Min, Liang Xiao, Dawei Zhao, Yiming Nie, Bin Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-camera 3D perception has emerged as a prominent research field in
autonomous driving, offering a viable and cost-effective alternative to
LiDAR-based solutions. The existing multi-camera algorithms primarily rely on
monocular 2D pre-training. However, the monocular 2D pre-training overlooks the
spatial and temporal correlations among the multi-camera system. To address
this limitation, we propose the first multi-camera unified pre-training
framework, called UniScene, which involves initially reconstructing the 3D
scene as the foundational stage and subsequently fine-tuning the model on
downstream tasks. Specifically, we employ Occupancy as the general
representation for the 3D scene, enabling the model to grasp geometric priors
of the surrounding world through pre-training. A significant benefit of
UniScene is its capability to utilize a considerable volume of unlabeled
image-LiDAR pairs for pre-training purposes. The proposed multi-camera unified
pre-training framework demonstrates promising results in key tasks such as
multi-camera 3D object detection and surrounding semantic scene completion.
When compared to monocular pre-training methods on the nuScenes dataset,
UniScene shows a significant improvement of about 2.0% in mAP and 2.0% in NDS
for multi-camera 3D object detection, as well as a 3% increase in mIoU for
surrounding semantic scene completion. By adopting our unified pre-training
method, a 25% reduction in 3D training annotation costs can be achieved,
offering significant practical value for the implementation of real-world
autonomous driving. Codes are publicly available at
https://github.com/chaytonmin/UniScene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with
  Uncertainty <span class="chip">ACM MM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02722v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02722v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhang, Weihua Chen, Yichen Lu, Tao Huang, Xiuyu Sun, Jian Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation is an effective paradigm for boosting the performance
of pocket-size model, especially when multiple teacher models are available,
the student would break the upper limit again. However, it is not economical to
train diverse teacher models for the disposable distillation. In this paper, we
introduce a new concept dubbed Avatars for distillation, which are the
inference ensemble models derived from the teacher. Concretely, (1) For each
iteration of distillation training, various Avatars are generated by a
perturbation transformation. We validate that Avatars own higher upper limit of
working capacity and teaching ability, aiding the student model in learning
diverse and receptive knowledge perspectives from the teacher model. (2) During
the distillation, we propose an uncertainty-aware factor from the variance of
statistical differences between the vanilla teacher and Avatars, to adjust
Avatars' contribution on knowledge transfer adaptively. Avatar Knowledge
Distillation AKD is fundamentally different from existing methods and refines
with the innovative view of unequal training. Comprehensive experiments
demonstrate the effectiveness of our Avatars mechanism, which polishes up the
state-of-the-art distillation methods for dense prediction without more extra
computational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object
Detection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On The Coherence of Quantitative Evaluation of Visual Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10764v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10764v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Vandersmissen, Jose Oramas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have shown an increased development of methods for justifying
the predictions of neural networks through visual explanations. These
explanations usually take the form of heatmaps which assign a saliency (or
relevance) value to each pixel of the input image that expresses how relevant
the pixel is for the prediction of a label.
  Complementing this development, evaluation methods have been proposed to
assess the "goodness" of such explanations. On the one hand, some of these
methods rely on synthetic datasets. However, this introduces the weakness of
having limited guarantees regarding their applicability on more realistic
settings. On the other hand, some methods rely on metrics for objective
evaluation. However the level to which some of these evaluation methods perform
with respect to each other is uncertain.
  Taking this into account, we conduct a comprehensive study on a subset of the
ImageNet-1k validation set where we evaluate a number of different
commonly-used explanation methods following a set of evaluation methods. We
complement our study with sanity checks on the studied evaluation methods as a
means to investigate their reliability and the impact of characteristics of the
explanations on the evaluation methods.
  Results of our study suggest that there is a lack of coherency on the grading
provided by some of the considered evaluation methods. Moreover, we have
identified some characteristics of the explanations, e.g. sparsity, which can
have a significant effect on the performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Network Initialization for Medical AI Models Using
  Large-Scale, Unlabeled Natural Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroosh Tayebi Arasteh, Leo Misera, Jakob Nikolas Kather, Daniel Truhn, Sven Nebelung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training datasets, like ImageNet, have become the gold standard in
medical image analysis. However, the emergence of self-supervised learning
(SSL), which leverages unlabeled data to learn robust features, presents an
opportunity to bypass the intensive labeling process. In this study, we
explored if SSL for pre-training on non-medical images can be applied to chest
radiographs and how it compares to supervised pre-training on non-medical
images and on medical images. We utilized a vision transformer and initialized
its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL
pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on
chest radiographs from the MIMIC-CXR database. We tested our approach on over
800,000 chest radiographs from six large global datasets, diagnosing more than
20 different imaging findings. Our SSL pre-training on curated images not only
outperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in
certain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest
that selecting the right pre-training strategy, especially with SSL, can be
pivotal for improving artificial intelligence (AI)'s diagnostic accuracy in
medical imaging. By demonstrating the promise of SSL in chest radiograph
analysis, we underline a transformative shift towards more efficient and
accurate AI models in medical imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attentive Mask CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yang, Weiquan Huang, Yixuan Wei, Houwen Peng, Xinyang Jiang, Huiqiang Jiang, Fangyun Wei, Yin Wang, Han Hu, Lili Qiu, Yuqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image token removal is an efficient augmentation strategy for reducing the
cost of computing image features. However, this efficient augmentation strategy
has been found to adversely affect the accuracy of CLIP-based training. We
hypothesize that removing a large portion of image tokens may improperly
discard the semantic content associated with a given text description, thus
constituting an incorrect pairing target in CLIP training. To address this
issue, we propose an attentive token removal approach for CLIP training, which
retains tokens with a high semantic correlation to the text description. The
correlation scores are computed in an online fashion using the EMA version of
the visual encoder. Our experiments show that the proposed attentive masking
approach performs better than the previous method of random token removal for
CLIP training. The approach also makes it efficient to apply multiple
augmentation views to the image, as well as introducing instance contrastive
learning tasks between these views into the CLIP framework. Compared to other
CLIP improvements that combine different pre-training targets such as SLIP and
MaskCLIP, our method is not only more effective, but also much more efficient.
Specifically, using ViT-B and YFCC-15M dataset, our approach achieves $43.9\%$
top-1 accuracy on ImageNet-1K zero-shot classification, as well as $62.7/42.1$
and $38.0/23.2$ I2T/T2I retrieval accuracy on Flickr30K and MS COCO, which are
$+1.1\%$, $+5.5/+0.9$, and $+4.4/+1.3$ higher than the SLIP method, while being
$2.30\times$ faster. An efficient version of our approach running $1.16\times$
faster than the plain CLIP model achieves significant gains of $+5.3\%$,
$+11.3/+8.0$, and $+9.5/+4.9$ on these benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DSFFNet: Dual-Side Feature Fusion Network for 3D Pose Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jue Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To solve the problem of pose distortion in the forward propagation of pose
features in existing methods, this pa-per proposes a Dual-Side Feature Fusion
Network for pose transfer (DSFFNet). Firstly, a fixed-length pose code is
extracted from the source mesh by a pose encoder and combined with the target
vertices to form a mixed feature; Then, a Feature Fusion Adaptive Instance
Normalization module (FFAdaIN) is designed, which can process both pose and
identity features simultaneously, so that the pose features can be compensated
in layer-by-layer for-ward propagation, thus solving the pose distortion
problem; Finally, using the mesh decoder composed of this module, the pose are
gradually transferred to the target mesh. Experimental results on SMPL, SMAL,
FAUST and MultiGarment datasets show that DSFFNet successfully solves the pose
distortion problem while maintaining a smaller network structure with stronger
pose transfer capability and faster convergence speed, and can adapt to meshes
with different numbers of vertices. Code is available at
https://github.com/YikiDragon/DSFFNet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Chinese language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fusing VHR Post-disaster Aerial Imagery and LiDAR Data for Roof
  Classification in the Caribbean <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16177v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16177v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabelle Tingzon, Nuala Margaret Cowan, Pierre Chrzanowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and up-to-date information on building characteristics is essential
for vulnerability assessment; however, the high costs and long timeframes
associated with conducting traditional field surveys can be an obstacle to
obtaining critical exposure datasets needed for disaster risk management. In
this work, we leverage deep learning techniques for the automated
classification of roof characteristics from very high-resolution orthophotos
and airborne LiDAR data obtained in Dominica following Hurricane Maria in 2017.
We demonstrate that the fusion of multimodal earth observation data performs
better than using any single data source alone. Using our proposed methods, we
achieve F1 scores of 0.93 and 0.92 for roof type and roof material
classification, respectively. This work is intended to help governments produce
more timely building information to improve resilience and disaster response in
the Caribbean.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023 Workshop on Artificial Intelligence for Humanitarian
  Assistance and Disaster Response</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy Assessment on Reconstructed Images: Are Existing Evaluation
  Metrics Faithful to Human Perception? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiao Sun, Nidham Gazagnadou, Vivek Sharma, Lingjuan Lyu, Hongdong Li, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly used
to evaluate model privacy risk under reconstruction attacks. Under these
metrics, reconstructed images that are determined to resemble the original one
generally indicate more privacy leakage. Images determined as overall
dissimilar, on the other hand, indicate higher robustness against attack.
However, there is no guarantee that these metrics well reflect human opinions,
which, as a judgement for model privacy leakage, are more trustworthy. In this
paper, we comprehensively study the faithfulness of these hand-crafted metrics
to human perception of privacy information from the reconstructed images. On 5
datasets ranging from natural images, faces, to fine-grained classes, we use 4
existing attack methods to reconstruct images from many different
classification models and, for each reconstructed image, we ask multiple human
annotators to assess whether this image is recognizable. Our studies reveal
that the hand-crafted metrics only have a weak correlation with the human
evaluation of privacy leakage and that even these metrics themselves often
contradict each other. These observations suggest risks of current metrics in
the community. To address this potential risk, we propose a learning-based
measure called SemSim to evaluate the Semantic Similarity between the original
and reconstructed images. SemSim is trained with a standard triplet loss, using
an original image as an anchor, one of its recognizable reconstructed images as
a positive sample, and an unrecognizable one as a negative. By training on
human annotations, SemSim exhibits a greater reflection of privacy leakage on
the semantic level. We show that SemSim has a significantly higher correlation
with human judgment compared with existing metrics. Moreover, this strong
correlation generalizes to unseen datasets, models and attack methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model
  Generalization Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04414v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04414v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiao Sun, Xingjian Leng, Zijian Wang, Yang Yang, Zi Huang, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing model performance in various unseen environments is a critical
research problem in the machine learning community. To study this problem, it
is important to construct a testbed with out-of-distribution test sets that
have broad coverage of environmental discrepancies. However, existing testbeds
typically either have a small number of domains or are synthesized by image
corruptions, hindering algorithm design that demonstrates real-world
effectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of
180 datasets collected by prompting image search engines and diffusion models
in various ways. Generally sized between 300 and 8,000 images, the datasets
contain natural images, cartoons, certain colors, or objects that do not
naturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen
the understanding of two generalization tasks: domain generalization and model
accuracy prediction in various out-of-distribution environments. We conduct
extensive benchmarking and comparison experiments and show that CIFAR-10-W
offers new and interesting insights inherent to these tasks. We also discuss
other fields that would benefit from CIFAR-10-W.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Blumenstiel, Johannes Jakubik, Hilde Kühne, Michael Vössing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While semantic segmentation has seen tremendous improvements in the past,
there are still significant labeling efforts necessary and the problem of
limited generalization to classes that have not been present during training.
To address this problem, zero-shot semantic segmentation makes use of large
self-supervised vision-language models, allowing zero-shot transfer to unseen
classes. In this work, we build a benchmark for Multi-domain Evaluation of
Semantic Segmentation (MESS), which allows a holistic analysis of performance
across a wide range of domain-specific datasets such as medicine, engineering,
earth monitoring, biology, and agriculture. To do this, we reviewed 120
datasets, developed a taxonomy, and classified the datasets according to the
developed taxonomy. We select a representative subset consisting of 22 datasets
and propose it as the MESS benchmark. We evaluate eight recently published
models on the proposed MESS benchmark and analyze characteristics for the
performance of zero-shot transfer models. The toolkit is available at
https://github.com/blumenstiel/MESS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023 Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Divide & Bind Your Attention for Improved Generative Semantic Nursing <span class="chip">BMVC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeng Li, Margret Keuper, Dan Zhang, Anna Khoreva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging large-scale text-to-image generative models, e.g., Stable Diffusion
(SD), have exhibited overwhelming results with high fidelity. Despite the
magnificent progress, current state-of-the-art models still struggle to
generate images fully adhering to the input prompt. Prior work, Attend &
Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming
to optimize cross-attention during inference time to better incorporate the
semantics. It demonstrates promising results in generating simple prompts,
e.g., ``a cat and a dog''. However, its efficacy declines when dealing with
more complex prompts, and it does not explicitly address the problem of
improper attribute binding. To address the challenges posed by complex prompts
or scenarios involving multiple entities and to achieve improved attribute
binding, we propose Divide & Bind. We introduce two novel loss objectives for
GSN: a novel attendance loss and a binding loss. Our approach stands out in its
ability to faithfully synthesize desired objects with improved attribute
alignment from complex prompts and exhibits superior performance across
multiple evaluation benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BMVC 2023 as Oral. Code:
  https://github.com/boschresearch/Divide-and-Bind and project page:
  https://sites.google.com/view/divide-and-bind</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Road Disease Detection based on Latent Domain Background Feature
  Separation and Suppression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juwu Zheng, Jiangtao Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Road disease detection is challenging due to the the small proportion of road
damage in target region and the diverse background,which introduce lots of
domain information.Besides, disease categories have high similarity,makes the
detection more difficult. In this paper, we propose a new LDBFSS(Latent Domain
Background Feature Separation and Suppression) network which could perform
background information separation and suppression without domain supervision
and contrastive enhancement of object features.We combine our LDBFSS network
with YOLOv5 model to enhance disease features for better road disease
detection. As the components of LDBFSS network, we first design a latent domain
discovery module and a domain adversarial learning module to obtain pseudo
domain labels through unsupervised method, guiding domain discriminator and
model to train adversarially to suppress background information. In addition,
we introduce a contrastive learning module and design k-instance contrastive
loss, optimize the disease feature representation by increasing the inter-class
distance and reducing the intra-class distance for object features. We
conducted experiments on two road disease detection datasets, GRDDC and CNRDD,
and compared with other models,which show an increase of nearly 4% on GRDDC
dataset compared with optimal model, and an increase of 4.6% on CNRDD dataset.
Experimental results prove the effectiveness and superiority of our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mining Label Distribution Drift in Unsupervised Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2006.09565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2006.09565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhao Li, Zhengming Ding, Hongfu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptation targets to transfer task-related knowledge
from labeled source domain to unlabeled target domain. Although tremendous
efforts have been made to minimize domain divergence, most existing methods
only partially manage by aligning feature representations from diverse domains.
Beyond the discrepancy in data distribution, the gap between source and target
label distribution, recognized as label distribution drift, is another crucial
factor raising domain divergence, and has been under insufficient exploration.
From this perspective, we first reveal how label distribution drift brings
negative influence. Next, we propose Label distribution Matching Domain
Adversarial Network (LMDAN) to handle data distribution shift and label
distribution drift jointly. In LMDAN, label distribution drift is addressed by
a source sample weighting strategy, which selects samples that contribute to
positive adaptation and avoid adverse effects brought by the mismatched
samples. Experiments show that LMDAN delivers superior performance under
considerable label distribution drift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AJCAI'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debiasing Scores and Prompts of 2D Diffusion for View-consistent
  Text-to-3D Generation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15413v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15413v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Susung Hong, Donghoon Ahn, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing score-distilling text-to-3D generation techniques, despite their
considerable promise, often encounter the view inconsistency problem. One of
the most notable issues is the Janus problem, where the most canonical view of
an object (\textit{e.g}., face or head) appears in other views. In this work,
we explore existing frameworks for score-distilling text-to-3D generation and
identify the main causes of the view inconsistency problem -- the embedded bias
of 2D diffusion models. Based on these findings, we propose two approaches to
debias the score-distillation frameworks for view-consistent text-to-3D
generation. Our first approach, called score debiasing, involves cutting off
the score estimated by 2D diffusion models and gradually increasing the
truncation value throughout the optimization process. Our second approach,
called prompt debiasing, identifies conflicting words between user prompts and
view prompts using a language model, and adjusts the discrepancy between view
prompts and the viewing direction of an object. Our experimental results show
that our methods improve the realism of the generated 3D objects by
significantly reducing artifacts and achieve a good trade-off between
faithfulness to the 2D diffusion models and 3D consistency with little
overhead. Our project page is available
at~\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023. Project Page:
  https://susunghong.github.io/Debiased-Score-Distillation-Sampling/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context Perception Parallel Decoder for Scene Text Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Chenxia Li, Yuning Du, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene text recognition (STR) methods have struggled to attain high accuracy
and fast inference speed. Autoregressive (AR)-based models implement the
recognition in a character-by-character manner, showing superiority in accuracy
but with slow inference speed. Alternatively, parallel decoding (PD)-based
models infer all characters in a single decoding pass, offering faster
inference speed but generally worse accuracy. We first present an empirical
study of AR decoding in STR, and discover that the AR decoder not only models
linguistic context, but also provides guidance on visual context perception.
Consequently, we propose Context Perception Parallel Decoder (CPPD) to predict
the character sequence in a PD pass. CPPD devises a character counting module
to infer the occurrence count of each character, and a character ordering
module to deduce the content-free reading order and placeholders. Meanwhile,
the character prediction task associates the placeholders with characters. They
together build a comprehensive recognition context. We construct a series of
CPPD models and also plug the proposed modules into existing STR decoders.
Experiments on both English and Chinese benchmarks demonstrate that the CPPD
models achieve highly competitive accuracy while running approximately 8x
faster than their AR-based counterparts. Moreover, the plugged models achieve
significant accuracy improvements. Code is at
\href{https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_en/algorithm_rec_cppd_en.md}{this
https URL}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Design of the topology for contrastive visual-textual alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cosine similarity is the common choice for measuring the distance between the
feature representations in contrastive visual-textual alignment learning.
However, empirically a learnable softmax temperature parameter is required when
learning on large-scale noisy training data. In this work, we first discuss the
role of softmax temperature from the embedding space's topological properties.
We argue that the softmax temperature is the key mechanism for contrastive
learning on noisy training data. It acts as a scaling factor of the distance
range (e.g. [-1, 1] for the cosine similarity), and its learned value indicates
the level of noise in the training data. Then, we propose an alternative design
of the topology for the embedding alignment. We make use of multiple class
tokens in the transformer architecture; then map the feature representations
onto an oblique manifold endowed with the negative inner product as the
distance function. With this configuration, we largely improve the zero-shot
classification performance of baseline CLIP models pre-trained on large-scale
datasets by an average of 6.1\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/minogame/clip-mtob</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12493v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12493v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilin Lu, Yanzhu Liu, Adams Wai-Kin Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven diffusion models have exhibited impressive generative
capabilities, enabling various image editing tasks. In this paper, we propose
TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the
power of text-driven diffusion models for cross-domain image-guided
composition. This task aims to seamlessly integrate user-provided objects into
a specific visual context. Current diffusion-based methods often involve costly
instance-based optimization or finetuning of pretrained models on customized
datasets, which can potentially undermine their rich prior. In contrast,
TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain
image-guided composition without requiring additional training, finetuning, or
optimization. Moreover, we introduce the exceptional prompt, which contains no
information, to facilitate text-driven diffusion models in accurately inverting
real images into latent representations, forming the basis for compositing. Our
experiments show that equipping Stable Diffusion with the exceptional prompt
outperforms state-of-the-art inversion methods on various datasets (CelebA-HQ,
COCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile
visual domains. Code is available at https://github.com/Shilin-LU/TF-ICON
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating Discrete Total Curvature with Per Triangle Normal Variation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Crane He Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach for measuring the total curvature at every
triangle of a discrete surface. This method takes advantage of the relationship
between per triangle total curvature and the Dirichlet energy of the Gauss map.
This new tool can be used on both triangle meshes and point clouds and has
numerous applications. In this study, we demonstrate the effectiveness of our
technique by using it for feature-aware mesh decimation, and show that it
outperforms existing curvature-estimation methods from popular libraries such
as Meshlab, Trimesh2, and Libigl. When estimating curvature on point clouds,
our method outperforms popular libraries PCL and CGAL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sizhe Wei, Yuxi Wei, Yue Hu, Yifan Lu, Yiqi Zhong, Siheng Chen, Ya Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative perception can substantially boost each agent's perception
ability by facilitating communication among multiple agents. However, temporal
asynchrony among agents is inevitable in the real world due to communication
delays, interruptions, and clock misalignments. This issue causes information
mismatch during multi-agent fusion, seriously shaking the foundation of
collaboration. To address this issue, we propose CoBEVFlow, an
asynchrony-robust collaborative perception system based on bird's eye view
(BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align
asynchronous collaboration messages sent by multiple agents. To model the
motion in a scene, we propose BEV flow, which is a collection of the motion
vector corresponding to each spatial location. Based on BEV flow, asynchronous
perceptual features can be reassigned to appropriate positions, mitigating the
impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle
asynchronous collaboration messages sent at irregular, continuous time stamps
without discretization; and (ii) with BEV flow, CoBEVFlow only transports the
original perceptual features, instead of generating new perceptual features,
avoiding additional noises. To validate CoBEVFlow's efficacy, we create
IRregular V2V(IRV2V), the first synthetic collaborative perception dataset with
various temporal asynchronies that simulate different real-world scenarios.
Extensive experiments conducted on both IRV2V and the real-world dataset
DAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is
robust in extremely asynchronous settings. The code is available at
https://github.com/MediaBrain-SJTU/CoBEVFlow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures. Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Fang, Xiaotao Hu, Kunming Luo, Ping Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven 3D indoor scene generation could be useful for gaming, film
industry, and AR/VR applications. However, existing methods cannot faithfully
capture the room layout, nor do they allow flexible editing of individual
objects in the room. To address these problems, we present Ctrl-Room, which is
able to generate convincing 3D rooms with designer-style layouts and
high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables
versatile interactive editing operations such as resizing or moving individual
furniture items. Our key insight is to separate the modeling of layouts and
appearance. %how to model the room that takes into account both scene texture
and geometry at the same time. To this end, Our proposed method consists of two
stages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The
`Layout Generation Stage' trains a text-conditional diffusion model to learn
the layout distribution with our holistic scene code parameterization. Next,
the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a
vivid panoramic image of the room guided by the 3D scene layout and text
prompt. In this way, we achieve a high-quality 3D room with convincing layouts
and lively textures. Benefiting from the scene code parameterization, we can
easily edit the generated room model through our mask-guided editing module,
without expensive editing-specific training. Extensive experiments on the
Structured3D dataset demonstrate that our method outperforms existing methods
in producing more reasonable, view-consistent, and editable 3D rooms from
natural language prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Pre-trained Vision and Language Models Answer Visual
  Information-Seeking Questions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11713v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11713v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, Ming-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained vision and language models have demonstrated state-of-the-art
capabilities over existing tasks involving images and texts, including visual
question answering. However, it remains unclear whether these models possess
the capability to answer questions that are not only querying visual content
but knowledge-intensive and information-seeking. In this study, we introduce
InfoSeek, a visual question answering dataset tailored for information-seeking
questions that cannot be answered with only common sense knowledge. Using
InfoSeek, we analyze various pre-trained visual question answering models and
gain insights into their characteristics. Our findings reveal that
state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)
face challenges in answering visual information-seeking questions, but
fine-tuning on the InfoSeek dataset elicits models to use fine-grained
knowledge that was learned during their pre-training. Furthermore, we show that
accurate visual entity recognition can be used to improve performance on
InfoSeek by retrieving relevant documents, showing a significant space for
improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 (main conference); Our dataset and evaluation is available
  at https://open-vision-language.github.io/infoseek/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIMIC: Masked Image Modeling with Image Correspondences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15128v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15128v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalyani Marathe, Mahtab Bigverdi, Nishat Khan, Tuhin Kundu, Aniruddha Kembhavi, Linda G. Shapiro, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense pixel-specific representation learning at scale has been bottlenecked
due to the unavailability of large-scale multi-view datasets. Current methods
for building effective pretraining datasets heavily rely on annotated 3D
meshes, point clouds, and camera parameters from simulated environments,
preventing them from building datasets from real-world data sources where such
metadata is lacking. We propose a pretraining dataset-curation approach that
does not require any additional annotations. Our method allows us to generate
multi-view datasets from both real-world videos and simulated environments at
scale. Specifically, we experiment with two scales: MIMIC-1M with 1.3M and
MIMIC-3M with 3.1M multi-view image pairs. We train multiple models with
different masked image modeling objectives to showcase the following findings:
Representations trained on our automatically generated MIMIC-3M outperform
those learned from expensive crowdsourced datasets (ImageNet-1K) and those
learned from synthetic environments (MULTIVIEW-HABITAT) on two dense geometric
tasks: depth estimation on NYUv2 (1.7%), and surface normals estimation on
Taskonomy (2.05%). For dense tasks which also require object understanding, we
outperform MULTIVIEW-HABITAT, on semantic segmentation on ADE20K (3.89%), pose
estimation on MSCOCO (9.4%), and reduce the gap with models pre-trained on the
object-centric expensive ImageNet-1K. We outperform even when the
representations are frozen, and when downstream training data is limited to
few-shot. Larger dataset (MIMIC-3M) significantly improves performance, which
is promising since our curation method can arbitrarily scale to produce even
larger datasets. MIMIC code, dataset, and pretrained models are open-sourced at
https://github.com/RAIVNLab/MIMIC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Survey on Deep Face Restoration: From Non-blind to Blind and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Li, Mei Wang, Kai Zhang, Juncheng Li, Xiaoming Li, Yuhang Zhang, Guangwei Gao, Weihong Deng, Chia-Wen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face restoration (FR) is a specialized field within image restoration that
aims to recover low-quality (LQ) face images into high-quality (HQ) face
images. Recent advances in deep learning technology have led to significant
progress in FR methods. In this paper, we begin by examining the prevalent
factors responsible for real-world LQ images and introduce degradation
techniques used to synthesize LQ images. We also discuss notable benchmarks
commonly utilized in the field. Next, we categorize FR methods based on
different tasks and explain their evolution over time. Furthermore, we explore
the various facial priors commonly utilized in the restoration process and
discuss strategies to enhance their effectiveness. In the experimental section,
we thoroughly evaluate the performance of state-of-the-art FR methods across
various tasks using a unified benchmark. We analyze their performance from
different perspectives. Finally, we discuss the challenges faced in the field
of FR and propose potential directions for future advancements. The open-source
repository corresponding to this work can be found at https:// github.com/
24wenjie-li/ Awesome-Face-Restoration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Face restoration, Survey, Deep learning, Non-blind/Blind, Joint
  restoration tasks, Facial priors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FusionFormer: A Multi-sensory Fusion in Bird's-Eye-View and Temporal
  Consistent Transformer for 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyong Hu, Hang Zheng, Kun Li, Jianyun Xu, Weibo Mao, Maochun Luo, Lingxuan Wang, Mingxia Chen, Qihao Peng, Kaixuan Liu, Yiru Zhao, Peihan Hao, Minzhe Liu, Kaicheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-sensor modal fusion has demonstrated strong advantages in 3D object
detection tasks. However, existing methods that fuse multi-modal features
require transforming features into the bird's eye view space and may lose
certain information on Z-axis, thus leading to inferior performance. To this
end, we propose a novel end-to-end multi-modal fusion transformer-based
framework, dubbed FusionFormer, that incorporates deformable attention and
residual structures within the fusion encoding module. Specifically, by
developing a uniform sampling strategy, our method can easily sample from 2D
image and 3D voxel features spontaneously, thus exploiting flexible
adaptability and avoiding explicit transformation to the bird's eye view space
during the feature concatenation process. We further implement a residual
structure in our feature encoder to ensure the model's robustness in case of
missing an input modality. Through extensive experiments on a popular
autonomous driving benchmark dataset, nuScenes, our method achieves
state-of-the-art single model performance of 72.6% mAP and 75.1% NDS in the 3D
object detection task without test time augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MV-Map: Offboard HD-Map Generation with Multi-view Consistency <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08851v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08851v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xie, Ziqi Pang, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While bird's-eye-view (BEV) perception models can be useful for building
high-definition maps (HD-Maps) with less human labor, their results are often
unreliable and demonstrate noticeable inconsistencies in the predicted HD-Maps
from different viewpoints. This is because BEV perception is typically set up
in an 'onboard' manner, which restricts the computation and consequently
prevents algorithms from reasoning multiple views simultaneously. This paper
overcomes these limitations and advocates a more practical 'offboard' HD-Map
generation setup that removes the computation constraints, based on the fact
that HD-Maps are commonly reusable infrastructures built offline in data
centers. To this end, we propose a novel offboard pipeline called MV-Map that
capitalizes multi-view consistency and can handle an arbitrary number of frames
with the key design of a 'region-centric' framework. In MV-Map, the target
HD-Maps are created by aggregating all the frames of onboard predictions,
weighted by the confidence scores assigned by an 'uncertainty network'. To
further enhance multi-view consistency, we augment the uncertainty network with
the global 3D structure optimized by a voxelized neural radiance field
(Voxel-NeRF). Extensive experiments on nuScenes show that our MV-Map
significantly improves the quality of HD-Maps, further highlighting the
importance of offboard methods for HD-Map generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RescueNet: A High Resolution UAV Semantic Segmentation Benchmark Dataset
  for Natural Disaster Damage Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.12361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.12361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Rahnemoonfar, Tashnim Chowdhury, Robin Murphy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in computer vision and deep learning techniques have
facilitated notable progress in scene understanding, thereby assisting rescue
teams in achieving precise damage assessment. In this paper, we present
RescueNet, a meticulously curated high-resolution post-disaster dataset that
includes detailed classification and semantic segmentation annotations. This
dataset aims to facilitate comprehensive scene understanding in the aftermath
of natural disasters. RescueNet comprises post-disaster images collected after
Hurricane Michael, obtained using Unmanned Aerial Vehicles (UAVs) from multiple
impacted regions. The uniqueness of RescueNet lies in its provision of
high-resolution post-disaster imagery, accompanied by comprehensive annotations
for each image. Unlike existing datasets that offer annotations limited to
specific scene elements such as buildings, RescueNet provides pixel-level
annotations for all classes, including buildings, roads, pools, trees, and
more. Furthermore, we evaluate the utility of the dataset by implementing
state-of-the-art segmentation models on RescueNet, demonstrating its value in
enhancing existing methodologies for natural disaster damage assessment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AntGPT: Can Large Language Models Help Long-term Action Anticipation
  from Videos? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Zhao, Shijie Wang, Ce Zhang, Changcheng Fu, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we better anticipate an actor's future actions (e.g. mix eggs) by knowing
what commonly happens after his/her current action (e.g. crack eggs)? What if
we also know the longer-term goal of the actor (e.g. making egg fried rice)?
The long-term action anticipation (LTA) task aims to predict an actor's future
behavior from video observations in the form of verb and noun sequences, and it
is crucial for human-machine interaction. We propose to formulate the LTA task
from two perspectives: a bottom-up approach that predicts the next actions
autoregressively by modeling temporal dynamics; and a top-down approach that
infers the goal of the actor and plans the needed procedure to accomplish the
goal. We hypothesize that large language models (LLMs), which have been
pretrained on procedure text data (e.g. recipes, how-tos), have the potential
to help LTA from both perspectives. It can help provide the prior knowledge on
the possible next actions, and infer the goal given the observed part of a
procedure, respectively. To leverage the LLMs, we propose a two-stage
framework, AntGPT. It first recognizes the actions already performed in the
observed videos and then asks an LLM to predict the future actions via
conditioned generation, or to infer the goal and plan the whole procedure by
chain-of-thought prompting. Empirical results on the Ego4D LTA v1 and v2
benchmarks, EPIC-Kitchens-55, as well as EGTEA GAZE+ demonstrate the
effectiveness of our proposed approach. AntGPT achieves state-of-the-art
performance on all above benchmarks, and can successfully infer the goal and
thus perform goal-conditioned "counterfactual" prediction via qualitative
analysis. Code and model will be released at
https://brown-palm.github.io/AntGPT
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inverse Consistency by Construction for Multistep Deep Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00087v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00087v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hastings Greer, Lin Tian, Francois-Xavier Vialard, Roland Kwitt, Sylvain Bouix, Raul San Jose Estepar, Richard Rushmore, Marc Niethammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse consistency is a desirable property for image registration. We
propose a simple technique to make a neural registration network inverse
consistent by construction, as a consequence of its structure, as long as it
parameterizes its output transform by a Lie group. We extend this technique to
multi-step neural registration by composing many such networks in a way that
preserves inverse consistency. This multi-step approach also allows for
inverse-consistent coarse to fine registration. We evaluate our technique on
synthetic 2-D data and four 3-D medical image registration tasks and obtain
excellent registration accuracy while assuring inverse consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Modulate Random Weights: Neuromodulation-inspired Neural
  Networks For Efficient Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04297v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04297v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyung Hong, Theodore P. Pavlic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Continual Learning (CL) approaches have focused on addressing
catastrophic forgetting by leveraging regularization methods, replay buffers,
and task-specific components. However, realistic CL solutions must be shaped
not only by metrics of catastrophic forgetting but also by computational
efficiency and running time. Here, we introduce a novel neural network
architecture inspired by neuromodulation in biological nervous systems to
economically and efficiently address catastrophic forgetting and provide new
avenues for interpreting learned representations. Neuromodulation is a
biological mechanism that has received limited attention in machine learning;
it dynamically controls and fine tunes synaptic dynamics in real time to track
the demands of different behavioral contexts. Inspired by this, our proposed
architecture learns a relatively small set of parameters per task context that
\emph{neuromodulates} the activity of unchanging, randomized weights that
transform the input. We show that this approach has strong learning performance
per task despite the very small number of learnable parameters. Furthermore,
because context vectors are so compact, multiple networks can be stored
concurrently with no interference and little spatial footprint, thus completely
eliminating catastrophic forgetting and accelerating the training process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 13 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-based Analysis of Driver Activity and Driving Performance Under
  the Influence of Alcohol 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08021v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08021v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Greer, Akshay Gopalkrishnan, Sumega Mandadi, Pujitha Gunaratne, Mohan M. Trivedi, Thomas D. Marcotte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  About 30% of all traffic crash fatalities in the United States involve drunk
drivers, making the prevention of drunk driving paramount to vehicle safety in
the US and other locations which have a high prevalence of driving while under
the influence of alcohol. Driving impairment can be monitored through active
use of sensors (when drivers are asked to engage in providing breath samples to
a vehicle instrument or when pulled over by a police officer), but a more
passive and robust mechanism of sensing may allow for wider adoption and
benefit of intelligent systems that reduce drunk driving accidents. This could
assist in identifying impaired drivers before they drive, or early in the
driving process (before a crash or detection by law enforcement). In this
research, we introduce a study which adopts a multi-modal ensemble of visual,
thermal, audio, and chemical sensors to (1) examine the impact of acute alcohol
administration on driving performance in a driving simulator, and (2) identify
data-driven methods for detecting driving under the influence of alcohol. We
describe computer vision and machine learning models for analyzing the driver's
face in thermal imagery, and introduce a pipeline for training models on data
collected from drivers with a range of breath-alcohol content levels, including
discussion of relevant machine learning phenomena which can help in future
experiment design for related studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Withdrawn at the request of industry research collaborators, per
  contract agreement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffusionShield: A Watermark for Copyright Protection against Generative
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingqian Cui, Jie Ren, Han Xu, Pengfei He, Hui Liu, Lichao Sun, Yue Xing, Jiliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Generative Diffusion Models (GDMs) have showcased their remarkable
capabilities in learning and generating images. A large community of GDMs has
naturally emerged, further promoting the diversified applications of GDMs in
various fields. However, this unrestricted proliferation has raised serious
concerns about copyright protection. For example, artists including painters
and photographers are becoming increasingly concerned that GDMs could
effortlessly replicate their unique creative works without authorization. In
response to these challenges, we introduce a novel watermarking scheme,
DiffusionShield, tailored for GDMs. DiffusionShield protects images from
copyright infringement by GDMs through encoding the ownership information into
an imperceptible watermark and injecting it into the images. Its watermark can
be easily learned by GDMs and will be reproduced in their generated images. By
detecting the watermark from generated images, copyright infringement can be
exposed with evidence. Benefiting from the uniformity of the watermarks and the
joint optimization method, DiffusionShield ensures low distortion of the
original image, high watermark detection performance, and the ability to embed
lengthy messages. We conduct rigorous and comprehensive experiments to show the
effectiveness of DiffusionShield in defending against infringement by GDMs and
its superiority over traditional watermarking methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-08T00:00:00Z">2023-10-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSight: An Edge-Cloud Infrastructure-based Perception System for
  Connected Automated Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rusheng Zhang, Depu Meng, Shengyin Shen, Zhengxia Zou, Houqiang Li, Henry X. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As vehicular communication and networking technologies continue to advance,
infrastructure-based roadside perception emerges as a pivotal tool for
connected automated vehicle (CAV) applications. Due to their elevated
positioning, roadside sensors, including cameras and lidars, often enjoy
unobstructed views with diminished object occlusion. This provides them a
distinct advantage over onboard perception, enabling more robust and accurate
detection of road objects. This paper presents MSight, a cutting-edge roadside
perception system specifically designed for CAVs. MSight offers real-time
vehicle detection, localization, tracking, and short-term trajectory
prediction. Evaluations underscore the system's capability to uphold lane-level
accuracy with minimal latency, revealing a range of potential applications to
enhance CAV safety and efficiency. Presently, MSight operates 24/7 at a
two-lane roundabout in the City of Ann Arbor, Michigan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE T-ITS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DELTAHANDS: A Synergistic Dexterous Hand Framework Based on Delta Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilin Si, Kevin Zhang, Oliver Kroemer, F. Zeynep Temel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dexterous robotic manipulation in unstructured environments can aid in
everyday tasks such as cleaning and caretaking. Anthropomorphic robotic hands
are highly dexterous and theoretically well-suited for working in human
domains, but their complex designs and dynamics often make them difficult to
control. By contrast, parallel-jaw grippers are easy to control and are used
extensively in industrial applications, but they lack the dexterity for various
kinds of grasps and in-hand manipulations. In this work, we present DELTAHANDS,
a synergistic dexterous hand framework with Delta robots. The DELTAHANDS are
soft, easy to reconfigure, simple to manufacture with low-cost off-the-shelf
materials, and possess high degrees of freedom that can be easily controlled.
DELTAHANDS' dexterity can be adjusted for different applications by leveraging
actuation synergies, which can further reduce the control complexity, overall
cost, and energy consumption. We characterize the Delta robots' kinematics
accuracy, force profiles, and workspace range to assist with hand design.
Finally, we evaluate the versatility of DELTAHANDS by grasping a diverse set of
objects and by using teleoperation to complete three dexterous manipulation
tasks: cloth folding, cap opening, and cable arrangement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Influence of Camera-LiDAR Configuration on 3D Object Detection for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Li, Hanjiang Hu, Zuxin Liu, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cameras and LiDARs are both important sensors for autonomous driving, playing
critical roles for 3D object detection. Camera-LiDAR Fusion has been a
prevalent solution for robust and accurate autonomous driving perception. In
contrast to the vast majority of existing arts that focus on how to improve the
performance of 3D target detection through cross-modal schemes, deep learning
algorithms, and training tricks, we devote attention to the impact of sensor
configurations on the performance of learning-based methods. To achieve this,
we propose a unified information-theoretic surrogate metric for camera and
LiDAR evaluation based on the proposed sensor perception model. We also design
an accelerated high-quality framework for data acquisition, model training, and
performance evaluation that functions with the CARLA simulator. To show the
correlation between detection performance and our surrogate metrics, We conduct
experiments using several camera-LiDAR placements and parameters inspired by
self-driving companies and research institutions. Extensive experimental
results of representative algorithms on NuScenes dataset validate the
effectiveness of our surrogate metric, demonstrating that sensor configurations
significantly impact point-cloud-image fusion based detection models, which
contribute up to 30% discrepancy in terms of average precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAN-grasp: Using Large Language Models for Semantic Object Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reihaneh Mirjalili, Michael Krawez, Simone Silenzi, Yannik Blei, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose LAN-grasp, a novel approach towards more
appropriate semantic grasping. We use foundation models to provide the robot
with a deeper understanding of the objects, the right place to grasp an object,
or even the parts to avoid. This allows our robot to grasp and utilize objects
in a more meaningful and safe manner. We leverage the combination of a Large
Language Model, a Vision Language Model, and a traditional grasp planner to
generate grasps demonstrating a deeper semantic understanding of the objects.
We first prompt the Large Language Model about which object part is appropriate
for grasping. Next, the Vision Language Model identifies the corresponding part
in the object image. Finally, we generate grasp proposals in the region
proposed by the Vision Language Model. Building on foundation models provides
us with a zero-shot grasp method that can handle a wide range of objects
without the need for further training or fine-tuning. We evaluated our method
in real-world experiments on a custom object data set. We present the results
of a survey that asks the participants to choose an object part appropriate for
grasping. The results show that the grasps generated by our method are
consistently ranked higher by the participants than those generated by a
conventional grasping planner and a recent semantic grasping approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Indoor Localization for an Autonomous Model Car: A Marker-Based
  Multi-Sensor Fusion Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xibo Li, Shruti Patel, David Stronzek-Pfeifer, Christof Büskens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global navigation satellite systems readily provide accurate position
information when localizing a robot outdoors. However, an analogous standard
solution does not exist yet for mobile robots operating indoors. This paper
presents an integrated framework for indoor localization and experimental
validation of an autonomous driving system based on an advanced
driver-assistance system (ADAS) model car. The global pose of the model car is
obtained by fusing information from fiducial markers, inertial sensors and
wheel odometry. In order to achieve robust localization, we investigate and
compare two extensions to the Extended Kalman Filter; first with adaptive noise
tuning and second with Chi-squared test for measurement outlier detection. An
efficient and low-cost ground truth measurement method using a single LiDAR
sensor is also proposed to validate the results. The performance of the
localization algorithms is tested on a complete autonomous driving system with
trajectory planning and model predictive control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepQTest: Testing Autonomous Driving Systems with Reinforcement
  Learning and Real-world Weather Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengjie Lu, Tao Yue, Man Zhang, Shaukat Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving systems (ADSs) are capable of sensing the environment and
making driving decisions autonomously. These systems are safety-critical, and
testing them is one of the important approaches to ensure their safety.
However, due to the inherent complexity of ADSs and the high dimensionality of
their operating environment, the number of possible test scenarios for ADSs is
infinite. Besides, the operating environment of ADSs is dynamic, continuously
evolving, and full of uncertainties, which requires a testing approach adaptive
to the environment. In addition, existing ADS testing techniques have limited
effectiveness in ensuring the realism of test scenarios, especially the realism
of weather conditions and their changes over time. Recently, reinforcement
learning (RL) has demonstrated great potential in addressing challenging
problems, especially those requiring constant adaptations to dynamic
environments. To this end, we present DeepQTest, a novel ADS testing approach
that uses RL to learn environment configurations with a high chance of
revealing abnormal ADS behaviors. Specifically, DeepQTest employs Deep
Q-Learning and adopts three safety and comfort measures to construct the reward
functions. To ensure the realism of generated scenarios, DeepQTest defines a
set of realistic constraints and introduces real-world weather conditions into
the simulated environment. We employed three comparison baselines, i.e.,
random, greedy, and a state-of-the-art RL-based approach DeepCOllision, for
evaluating DeepQTest on an industrial-scale ADS. Evaluation results show that
DeepQTest demonstrated significantly better effectiveness in terms of
generating scenarios leading to collisions and ensuring scenario realism
compared with the baselines. In addition, among the three reward functions
implemented in DeepQTest, Time-To-Collision is recommended as the best design
according to our study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 7 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loco<span class="highlight-title">NeRF</span>: A <span class="highlight-title">NeRF</span>-based Approach for Local Structure from Motion for
  Precise Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Nenashev, Mikhail Kurenkov, Andrei Potapov, Iana Zhura, Maksim Katerishich, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual localization is a critical task in mobile robotics, and researchers
are continuously developing new approaches to enhance its efficiency. In this
article, we propose a novel approach to improve the accuracy of visual
localization using Structure from Motion (SfM) techniques. We highlight the
limitations of global SfM, which suffers from high latency, and the challenges
of local SfM, which requires large image databases for accurate reconstruction.
To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as
opposed to image databases, to cut down on the space required for storage. We
suggest that sampling reference images around the prior query position can lead
to further improvements. We evaluate the accuracy of our proposed method
against ground truth obtained using LIDAR and Advanced Lidar Odometry and
Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM
with COLMAP in the conducted experiments. Our proposed method achieves an
accuracy of 0.068 meters compared to the ground truth, which is slightly lower
than the most advanced method COLMAP, which has an accuracy of 0.022 meters.
However, the size of the database required for COLMAP is 400 megabytes, whereas
the size of our NeRF model is only 160 megabytes. Finally, we perform an
ablation study to assess the impact of using reference images from the NeRF
reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Anomaly Behavior Analysis Framework for Securing Autonomous Vehicle
  Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murad Mehrab Abrar, Salim Hariri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a rapidly growing cyber-physical platform, Autonomous Vehicles (AVs) are
encountering more security challenges as their capabilities continue to expand.
In recent years, adversaries are actively targeting the perception sensors of
autonomous vehicles with sophisticated attacks that are not easily detected by
the vehicles' control systems. This work proposes an Anomaly Behavior Analysis
approach to detect a perception sensor attack against an autonomous vehicle.
The framework relies on temporal features extracted from a physics-based
autonomous vehicle behavior model to capture the normal behavior of vehicular
perception in autonomous driving. By employing a combination of model-based
techniques and machine learning algorithms, the proposed framework
distinguishes between normal and abnormal vehicular perception behavior. To
demonstrate the application of the framework in practice, we performed a depth
camera attack experiment on an autonomous vehicle testbed and generated an
extensive dataset. We validated the effectiveness of the proposed framework
using this real-world data and released the dataset for public access. To our
knowledge, this dataset is the first of its kind and will serve as a valuable
resource for the research community in evaluating their intrusion detection
techniques effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20th ACS/IEEE International Conference on Computer Systems and
  Applications (Accepted for publication)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully Spiking Neural Network for Legged Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyang Jiang, Qiang Zhang, Jingkai Sun, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, legged robots based on deep reinforcement learning have made
remarkable progress. Quadruped robots have demonstrated the ability to complete
challenging tasks in complex environments and have been deployed in real-world
scenarios to assist humans. Simultaneously, bipedal and humanoid robots have
achieved breakthroughs in various demanding tasks. Current reinforcement
learning methods can utilize diverse robot bodies and historical information to
perform actions. However, prior research has not emphasized the speed and
energy consumption of network inference, as well as the biological significance
of the neural networks themselves. Most of the networks employed are
traditional artificial neural networks that utilize multilayer perceptrons
(MLP). In this paper, we successfully apply a novel Spiking Neural Network
(SNN) to process legged robots, achieving outstanding results across a range of
simulated terrains. SNN holds a natural advantage over traditional neural
networks in terms of inference speed and energy consumption, and their
pulse-form processing of body perception signals offers improved biological
interpretability. To the best of our knowledge, this is the first work to
implement SNN in legged robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initial Task Assignment in Multi-Human Multi-Robot Teams: An
  Attention-enhanced Hierarchical Reinforcement Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Wang, Dezhong Zhao, Arjun Gupte, Byung-Cheol Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-human multi-robot teams (MH-MR) obtain tremendous potential in tackling
intricate and massive missions by merging distinct strengths and expertise of
individual members. The inherent heterogeneity of these teams necessitates
advanced initial task assignment (ITA) methods that align tasks with the
intrinsic capabilities of team members from the outset. While existing
reinforcement learning approaches show encouraging results, they might fall
short in addressing the nuances of long-horizon ITA problems, particularly in
settings with large-scale MH-MR teams or multifaceted tasks. To bridge this
gap, we propose an attention-enhanced hierarchical reinforcement learning
approach that decomposes the complex ITA problem into structured sub-problems,
facilitating more efficient allocations. To bolster sub-policy learning, we
introduce a hierarchical cross-attribute attention (HCA) mechanism, encouraging
each sub-policy within the hierarchy to discern and leverage the specific
nuances in the state space that are crucial for its respective decision-making
phase. Through an extensive environmental surveillance case study, we
demonstrate the benefits of our model and the HCA inside.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pick Planning Strategies for Large-Scale Package Manipulation <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Li, Azarakhsh Keipour, Kevin Jamieson, Nicolas Hudson, Sicong Zhao, Charles Swan, Kostas Bekris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating warehouse operations can reduce logistics overhead costs,
ultimately driving down the final price for consumers, increasing the speed of
delivery, and enhancing the resiliency to market fluctuations.
  This extended abstract showcases a large-scale package manipulation from
unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which is
used for picking and singulating up to 6 million packages per day and so far
has manipulated over 2 billion packages. It describes the various heuristic
methods developed over time and their successor, which utilizes a pick success
predictor trained on real production data.
  To the best of the authors' knowledge, this work is the first large-scale
deployment of learned pick quality estimation methods in a real production
system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2023 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), Learning Meets Model-based Methods for Manipulation and
  Grasping Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable
  Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Zhuoran Li, Mingtong Zhang, Katherine Driggs-Campbell, Jiajun Wu, Li Fei-Fei, Yunzhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene representation has been a crucial design choice in robotic manipulation
systems. An ideal representation should be 3D, dynamic, and semantic to meet
the demands of diverse manipulation tasks. However, previous works often lack
all three properties simultaneously. In this work, we introduce D$^3$Fields -
dynamic 3D descriptor fields. These fields capture the dynamics of the
underlying 3D environment and encode both semantic features and instance masks.
Specifically, we project arbitrary 3D points in the workspace onto multi-view
2D visual observations and interpolate features derived from foundational
models. The resulting fused descriptor fields allow for flexible goal
specifications using 2D images with varied contexts, styles, and instances. To
evaluate the effectiveness of these descriptor fields, we apply our
representation to a wide range of robotic manipulation tasks in a zero-shot
manner. Through extensive evaluation in both real-world scenarios and
simulations, we demonstrate that D$^3$Fields are both generalizable and
effective for zero-shot robotic manipulation tasks. In quantitative comparisons
with state-of-the-art dense descriptors, such as Dense Object Nets and DINO,
D$^3$Fields exhibit significantly better generalization abilities and
manipulation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://robopil.github.io/d3fields/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XVO: Generalized <span class="highlight-title">Visual Odometry</span> via Cross-Modal Self-Training <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Lai, Zhongkai Shangguan, Jimuyang Zhang, Eshed Ohn-Bar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose XVO, a semi-supervised learning method for training generalized
monocular Visual Odometry (VO) models with robust off-the-self operation across
diverse datasets and settings. In contrast to standard monocular VO approaches
which often study a known calibration within a single dataset, XVO efficiently
learns to recover relative pose with real-world scale from visual scene
semantics, i.e., without relying on any known camera parameters. We optimize
the motion estimation model via self-training from large amounts of
unconstrained and heterogeneous dash camera videos available on YouTube. Our
key contribution is twofold. First, we empirically demonstrate the benefits of
semi-supervised training for learning a general-purpose direct VO regression
network. Second, we demonstrate multi-modal supervision, including
segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate
generalized representations for the VO task. Specifically, we find audio
prediction task to significantly enhance the semi-supervised learning process
while alleviating noisy pseudo-labels, particularly in highly dynamic and
out-of-domain video data. Our proposed teacher network achieves
state-of-the-art performance on the commonly used KITTI benchmark despite no
multi-frame optimization or knowledge of camera parameters. Combined with the
proposed semi-supervised step, XVO demonstrates off-the-shelf knowledge
transfer across diverse conditions on KITTI, nuScenes, and Argoverse without
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023, Paris https://genxvo.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Generative Adversarial Imitation Learning with Mid-level
  Input Generation for Autonomous Driving on Urban Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04823v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04823v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Claudio Karl Couto, Eric Aislan Antonelo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deriving robust control policies for realistic urban navigation scenarios is
not a trivial task. In an end-to-end approach, these policies must map
high-dimensional images from the vehicle's cameras to low-level actions such as
steering and throttle. While pure Reinforcement Learning (RL) approaches are
based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL)
agents learn from expert demonstrations while interacting with the environment,
which favors GAIL on tasks for which a reward signal is difficult to derive. In
this work, the hGAIL architecture was proposed to solve the autonomous
navigation of a vehicle in an end-to-end approach, mapping sensory perceptions
directly to low-level actions, while simultaneously learning mid-level input
representations of the agent's environment. The proposed hGAIL consists of an
hierarchical Adversarial Imitation Learning architecture composed of two main
modules: the GAN (Generative Adversarial Nets) which generates the Bird's-Eye
View (BEV) representation mainly from the images of three frontal cameras of
the vehicle, and the GAIL which learns to control the vehicle based mainly on
the BEV predictions from the GAN as input.Our experiments have shown that GAIL
exclusively from cameras (without BEV) fails to even learn the task, while
hGAIL, after training, was able to autonomously navigate successfully in all
intersections of the city.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DriveGPT4: Interpretable End-to-end Autonomous Driving via Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K. Wong, Zhenguo Li, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past decade, autonomous driving has experienced rapid development in
both academia and industry. However, its limited interpretability remains a
significant unsolved problem, severely hindering autonomous vehicle
commercialization and further development. Previous approaches utilizing small
language models have failed to address this issue due to their lack of
flexibility, generalization ability, and robustness. Recently, multimodal large
language models (LLMs) have gained considerable attention from the research
community for their capability to process and reason non-text data (e.g.,
images and videos) by text. In this paper, we present DriveGPT4, an
interpretable end-to-end autonomous driving system utilizing LLMs. DriveGPT4 is
capable of interpreting vehicle actions and providing corresponding reasoning,
as well as answering diverse questions posed by human users for enhanced
interaction. Additionally, DriveGPT4 predicts vehicle low-level control signals
in an end-to-end fashion. These capabilities stem from a customized visual
instruction tuning dataset specifically designed for autonomous driving. To the
best of our knowledge, DriveGPT4 is the first work focusing on interpretable
end-to-end autonomous driving. When evaluated on multiple tasks alongside
conventional methods and video understanding LLMs, DriveGPT4 demonstrates
superior qualitative and quantitative performance. Additionally, DriveGPT4 can
be generalized in a zero-shot fashion to accommodate more unseen scenarios. The
project page is available at https://tonyxuqaq.github.io/projects/DriveGPT4/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project page is available at
  https://tonyxuqaq.github.io/projects/DriveGPT4/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerated K-Serial Stable Coalition for Dynamic Capture and Resource
  Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15729v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15729v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Chen, Zili Tang, Meng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coalition is an important mean of multi-robot systems to collaborate on
common tasks. An adaptive coalition strategy is essential for the online
performance in dynamic and unknown environments. In this work, the problem of
territory defense by large-scale heterogeneous robotic teams is considered. The
tasks include exploration, capture of dynamic targets, and perimeter defense
over valuable resources. Since each robot can choose among many tasks, it
remains a challenging problem to coordinate jointly these robots such that the
overall utility is maximized. This work proposes a generic coalition strategy
called K-serial stable coalition algorithm. Different from centralized
approaches, it is distributed and complete, meaning that only local
communication is required and a K-serial Stable solution is ensured.
Furthermore, to accelerate adaptation to dynamic targets and resource
distribution that are only perceived online, a heterogeneous graph attention
network based heuristic is learned to select more appropriate parameters and
promising initial solutions during local optimization. Compared with manual
heuristics or end-to-end predictors, it is shown to both improve online
adaptability and retain the quality guarantee. The proposed methods are
validated via large-scale simulations with 170 robots and hardware experiments
of 13 robots, against several strong baselines such as GreedyNE and FastMaxSum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InsightMapper: A Closer Look at Inner-instance Information for
  Vectorized High-Definition Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhua Xu, Kwan-Yee. K. Wong, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vectorized high-definition (HD) maps contain detailed information about
surrounding road elements, which are crucial for various downstream tasks in
modern autonomous driving vehicles, such as vehicle planning and control.
Recent works have attempted to directly detect the vectorized HD map as a point
set prediction task, resulting in significant improvements in detection
performance. However, these approaches fail to analyze and exploit the
inner-instance correlations between predicted points, impeding further
advancements. To address these challenges, we investigate the utilization of
inner-$\textbf{INS}$tance information for vectorized h$\textbf{IGH}$-definition
mapping through $\textbf{T}$ransformers and introduce InsightMapper. This paper
presents three novel designs within InsightMapper that leverage inner-instance
information in distinct ways, including hybrid query generation, inner-instance
query fusion, and inner-instance feature aggregation. Comparative experiments
are conducted on the NuScenes dataset, showcasing the superiority of our
proposed method. InsightMapper surpasses previous state-of-the-art (SOTA)
methods by 5.78 mAP and 5.12 TOPO, which assess topology correctness.
Simultaneously, InsightMapper maintains high efficiency during both training
and inference phases, resulting in remarkable comprehensive performance. The
project page for this work is available at
https://tonyxuqaq.github.io/InsightMapper/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and demo will be available at
  https://tonyxuqaq.github.io/InsightMapper/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QwenGrasp: A Usage of Large Vision-Language Model for Target-Oriented
  Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Chen, Jian Yang, Zonghan He, Haobin Yang, Qi Zhao, Yuhui Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Target-oriented grasping in unstructured scenes with language control is
essential for intelligent robot arm grasping. The ability for the robot arm to
understand the human language and execute corresponding grasping actions is a
pivotal challenge. In this paper, we propose a combination model called
QwenGrasp which combines a large vision-language model with a 6-DoF grasp
neural network. QwenGrasp is able to conduct a 6-DoF grasping task on the
target object with textual language instruction. We design a complete
experiment with six-dimension instructions to test the QwenGrasp when facing
with different cases. The results show that QwenGrasp has a superior ability to
comprehend the human intention. Even in the face of vague instructions with
descriptive words or instructions with direction information, the target object
can be grasped accurately. When QwenGrasp accepts the instruction which is not
feasible or not relevant to the grasping task, our approach has the ability to
suspend the task execution and provide a proper feedback to humans, improving
the safety. In conclusion, with the great power of large vision-language model,
QwenGrasp can be applied in the open language environment to conduct the
target-oriented grasping task with freely input instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Non-IID Effects in Federated Autonomous Driving with
  Contrastive Divergence Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuong Do, Binh X. Nguyen, Hien Nguyen, Erman Tjiputra, Quang D. Tran, Te-Chuan Chiu, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning has been widely applied in autonomous driving since it
enables training a learning model among vehicles without sharing users' data.
However, data from autonomous vehicles usually suffer from the
non-independent-and-identically-distributed (non-IID) problem, which may cause
negative effects on the convergence of the learning process. In this paper, we
propose a new contrastive divergence loss to address the non-IID problem in
autonomous driving by reducing the impact of divergence factors from
transmitted models during the local learning process of each silo. We also
analyze the effects of contrastive divergence in various autonomous driving
scenarios, under multiple network infrastructures, and with different
centralized/distributed learning schemes. Our intensive experiments on three
datasets demonstrate that our proposed contrastive divergence loss
significantly improves the performance over current state-of-the-art
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Multi Camera-IMU Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.13821v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.13821v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Hartzer, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-inertial navigation systems are powerful in their ability to
accurately estimate localization of mobile systems within complex environments
that preclude the use of global navigation satellite systems. However, these
navigation systems are reliant on accurate and up-to-date temporospatial
calibrations of the sensors being used. As such, online estimators for these
parameters are useful in resilient systems. This paper presents an extension to
existing Kalman Filter based frameworks for estimating and calibrating the
extrinsic parameters of multi-camera IMU systems. In addition to extending the
filter framework to include multiple camera sensors, the measurement model was
reformulated to make use of measurement data that is typically made available
in fiducial detection software. A secondary filter layer was used to estimate
time translation parameters without closed-loop feedback of sensor data.
Experimental calibration results, including the use of cameras with
non-overlapping fields of view, were used to validate the stability and
accuracy of the filter formulation when compared to offline methods. Finally
the generalized filter code has been open-sourced and is available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vehicular Teamwork: Collaborative localization of Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.14106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.14106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Hartzer, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a distributed collaborative localization algorithm based
on an extended kalman filter. This algorithm incorporates Ultra-Wideband (UWB)
measurements for vehicle to vehicle ranging, and shows improvements in
localization accuracy where GPS typically falls short. The algorithm was first
tested in a newly created open-source simulation environment that emulates
various numbers of vehicles and sensors while simultaneously testing multiple
localization algorithms. Predicted error distributions for various algorithms
are quickly producible using the Monte-Carlo method and optimization techniques
within MatLab. The simulation results were validated experimentally in an
outdoor, urban environment. Improvements of localization accuracy over a
typical extended kalman filter ranged from 2.9% to 9.3% over 180 meter test
runs. When GPS was denied, these improvements increased up to 83.3% over a
standard kalman filter. In both simulation and experimentally, the DCL
algorithm was shown to be a good approximation of a full state filter, while
reducing required communication between vehicles. These results are promising
in showing the efficacy of adding UWB ranging sensors to cars for collaborative
and landmark localization, especially in GPS-denied environments. In the
future, additional moving vehicles with additional tags will be tested in other
challenging GPS denied environments.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">76</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Neural Compression for Adaptive Image Offloading under
  Timing Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Wang, Hanyang Liu, Jiaming Qiu, Moran Xu, Roch Guerin, Chenyang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IoT devices are increasingly the source of data for machine learning (ML)
applications running on edge servers. Data transmissions from devices to
servers are often over local wireless networks whose bandwidth is not just
limited but, more importantly, variable. Furthermore, in cyber-physical systems
interacting with the physical environment, image offloading is also commonly
subject to timing constraints. It is, therefore, important to develop an
adaptive approach that maximizes the inference performance of ML applications
under timing constraints and the resource constraints of IoT devices. In this
paper, we use image classification as our target application and propose
progressive neural compression (PNC) as an efficient solution to this problem.
Although neural compression has been used to compress images for different ML
applications, existing solutions often produce fixed-size outputs that are
unsuitable for timing-constrained offloading over variable bandwidth. To
address this limitation, we train a multi-objective rateless autoencoder that
optimizes for multiple compression rates via stochastic taildrop to create a
compression solution that produces features ordered according to their
importance to inference performance. Features are then transmitted in that
order based on available bandwidth, with classification ultimately performed
using the (sub)set of features received by the deadline. We demonstrate the
benefits of PNC over state-of-the-art neural compression approaches and
traditional compression methods on a testbed comprising an IoT device and an
edge server connected over a wireless network with varying bandwidth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE the 44th Real-Time System Symposium (RTSS), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GestSync: Determining who is speaking without a talking head <span class="chip">BMVC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sindhu B Hegde, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce a new synchronisation task, Gesture-Sync:
determining if a person's gestures are correlated with their speech or not. In
comparison to Lip-Sync, Gesture-Sync is far more challenging as there is a far
looser relationship between the voice and body movement than there is between
voice and lip motion. We introduce a dual-encoder model for this task, and
compare a number of input representations including RGB frames, keypoint
images, and keypoint vectors, assessing their performance and advantages. We
show that the model can be trained using self-supervised learning alone, and
evaluate its performance on the LRS3 dataset. Finally, we demonstrate
applications of Gesture-Sync for audio-visual synchronisation, and in
determining who is the speaker in a crowd, without seeing their faces. The
code, datasets and pre-trained models can be found at:
\url{https://www.robots.ox.ac.uk/~vgg/research/gestsync}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in BMVC 2023, 10 pages paper, 7 pages supplementary, 7
  Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Compression and Decompression Framework Based on Latent Diffusion
  Model for Breast Mammography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        InChan Hwang, MinJae Woo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a novel framework for the compression and
decompression of medical images utilizing the Latent Diffusion Model (LDM). The
LDM represents advancement over the denoising diffusion probabilistic model
(DDPM) with a potential to yield superior image quality while requiring fewer
computational resources in the image decompression process. A possible
application of LDM and Torchvision for image upscaling has been explored using
medical image data, serving as an alternative to traditional image compression
and decompression algorithms. The experimental outcomes demonstrate that this
approach surpasses a conventional file compression algorithm, and convolutional
neural network (CNN) models trained with decompressed files perform comparably
to those trained with original image files. This approach also significantly
reduces dataset size so that it can be distributed with a smaller size, and
medical images take up much less space in medical devices. The research
implications extend to noise reduction in lossy compression algorithms and
substitute for complex wavelet-based lossless algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages IEEE conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSight: An Edge-Cloud Infrastructure-based Perception System for
  Connected Automated Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rusheng Zhang, Depu Meng, Shengyin Shen, Zhengxia Zou, Houqiang Li, Henry X. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As vehicular communication and networking technologies continue to advance,
infrastructure-based roadside perception emerges as a pivotal tool for
connected automated vehicle (CAV) applications. Due to their elevated
positioning, roadside sensors, including cameras and lidars, often enjoy
unobstructed views with diminished object occlusion. This provides them a
distinct advantage over onboard perception, enabling more robust and accurate
detection of road objects. This paper presents MSight, a cutting-edge roadside
perception system specifically designed for CAVs. MSight offers real-time
vehicle detection, localization, tracking, and short-term trajectory
prediction. Evaluations underscore the system's capability to uphold lane-level
accuracy with minimal latency, revealing a range of potential applications to
enhance CAV safety and efficiency. Presently, MSight operates 24/7 at a
two-lane roundabout in the City of Ann Arbor, Michigan.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE T-ITS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transforming Pixels into a Masterpiece: AI-Powered Art Restoration using
  a Novel Distributed Denoising CNN (DDCNN) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sankar B., Mukil Saravanan, Kalaivanan Kumar, Siri Dubbaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Art restoration is crucial for preserving cultural heritage, but traditional
methods have limitations in faithfully reproducing original artworks while
addressing issues like fading, staining, and damage. We present an innovative
approach using deep learning, specifically Convolutional Neural Networks
(CNNs), and Computer Vision techniques to revolutionize art restoration. We
start by creating a diverse dataset of deteriorated art images with various
distortions and degradation levels. This dataset trains a Distributed Denoising
CNN (DDCNN) to remove distortions while preserving intricate details. Our
method is adaptable to different distortion types and levels, making it
suitable for various deteriorated artworks, including paintings, sketches, and
photographs. Extensive experiments demonstrate our approach's efficiency and
effectiveness compared to other Denoising CNN models. We achieve a substantial
reduction in distortion, transforming deteriorated artworks into masterpieces.
Quantitative evaluations confirm our method's superiority over traditional
techniques, reshaping the art restoration field and preserving cultural
heritage. In summary, our paper introduces an AI-powered solution that combines
Computer Vision and deep learning with DDCNN to restore artworks accurately,
overcoming limitations and paving the way for future advancements in art
restoration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Emergence of Reproducibility and Consistency in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Liyue Shen, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, diffusion models have emerged as powerful deep generative models,
showcasing cutting-edge performance across various applications such as image
generation, solving inverse problems, and text-to-image synthesis. These models
generate new data (e.g., images) by transforming random noise inputs through a
reverse diffusion process. In this work, we uncover a distinct and prevalent
phenomenon within diffusion models in contrast to most other generative models,
which we refer to as ``consistent model reproducibility''. To elaborate, our
extensive experiments have consistently shown that when starting with the same
initial noise input and sampling with a deterministic solver, diffusion models
tend to produce nearly identical output content. This consistency holds true
regardless of the choices of model architectures and training procedures.
Additionally, our research has unveiled that this exceptional model
reproducibility manifests in two distinct training regimes: (i) ``memorization
regime,'' characterized by a significantly overparameterized model which
attains reproducibility mainly by memorizing the training data; (ii)
``generalization regime,'' in which the model is trained on an extensive
dataset, and its reproducibility emerges with the model's generalization
capabilities. Our analysis provides theoretical justification for the model
reproducibility in ``memorization regime''. Moreover, our research reveals that
this valuable property generalizes to many variants of diffusion models,
including conditional diffusion models, diffusion models for solving inverse
problems, and fine-tuned diffusion models. A deeper understanding of this
phenomenon has the potential to yield more interpretable and controllable data
generative processes based on diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-Preserving Instance Segmentation via Skeleton-Aware Distance
  Transform <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zudi Lin, Donglai Wei, Aarush Gupta, Xingyu Liu, Deqing Sun, Hanspeter Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objects with complex structures pose significant challenges to existing
instance segmentation methods that rely on boundary or affinity maps, which are
vulnerable to small errors around contacting pixels that cause noticeable
connectivity change. While the distance transform (DT) makes instance interiors
and boundaries more distinguishable, it tends to overlook the intra-object
connectivity for instances with varying width and result in over-segmentation.
To address these challenges, we propose a skeleton-aware distance transform
(SDT) that combines the merits of object skeleton in preserving connectivity
and DT in modeling geometric arrangement to represent instances with arbitrary
structures. Comprehensive experiments on histopathology image segmentation
demonstrate that SDT achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023 (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Persis: A Persian Font Recognition Pipeline Using Convolutional Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Mohammadian, Neda Maleki, Tobias Olsson, Fredrik Ahlgren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What happens if we encounter a suitable font for our design work but do not
know its name? Visual Font Recognition (VFR) systems are used to identify the
font typeface in an image. These systems can assist graphic designers in
identifying fonts used in images. A VFR system also aids in improving the speed
and accuracy of Optical Character Recognition (OCR) systems. In this paper, we
introduce the first publicly available datasets in the field of Persian font
recognition and employ Convolutional Neural Networks (CNN) to address this
problem. The results show that the proposed pipeline obtained 78.0% top-1
accuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the
KAFD dataset. Furthermore, the average time spent in the entire pipeline for
one sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU,
respectively. We conclude that CNN methods can be used to recognize Persian
fonts without the need for additional pre-processing steps such as feature
extraction, binarization, normalization, etc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCANet: Scene Complexity Aware Network for Weakly-Supervised Video
  Moment Retrieval <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunjae Yoon, Gwanhyeong Koo, Dahyun Kim, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video moment retrieval aims to localize moments in video corresponding to a
given language query. To avoid the expensive cost of annotating the temporal
moments, weakly-supervised VMR (wsVMR) systems have been studied. For such
systems, generating a number of proposals as moment candidates and then
selecting the most appropriate proposal has been a popular approach. These
proposals are assumed to contain many distinguishable scenes in a video as
candidates. However, existing proposals of wsVMR systems do not respect the
varying numbers of scenes in each video, where the proposals are heuristically
determined irrespective of the video. We argue that the retrieval system should
be able to counter the complexities caused by varying numbers of scenes in each
video. To this end, we present a novel concept of a retrieval system referred
to as Scene Complexity Aware Network (SCANet), which measures the `scene
complexity' of multiple scenes in each video and generates adaptive proposals
responding to variable complexities of scenes in each video. Experimental
results on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR)
achieve state-of-the-art performances and demonstrate the effectiveness of
incorporating the scene complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, Accepted in ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Diffusion Model for Medical Image Standardization and Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Selim, Jie Zhang, Faraneh Fathi, Michael A. Brooks, Ge Wang, Guoqiang Yu, Jin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed tomography (CT) serves as an effective tool for lung cancer
screening, diagnosis, treatment, and prognosis, providing a rich source of
features to quantify temporal and spatial tumor changes. Nonetheless, the
diversity of CT scanners and customized acquisition protocols can introduce
significant inconsistencies in texture features, even when assessing the same
patient. This variability poses a fundamental challenge for subsequent research
that relies on consistent image features. Existing CT image standardization
models predominantly utilize GAN-based supervised or semi-supervised learning,
but their performance remains limited. We present DiffusionCT, an innovative
score-based DDPM model that operates in the latent space to transform disparate
non-standard distributions into a standardized form. The architecture comprises
a U-Net-based encoder-decoder, augmented by a DDPM model integrated at the
bottleneck position. First, the encoder-decoder is trained independently,
without embedding DDPM, to capture the latent representation of the input data.
Second, the latent DDPM model is trained while keeping the encoder-decoder
parameters fixed. Finally, the decoder uses the transformed latent
representation to generate a standardized CT image, providing a more consistent
basis for downstream analysis. Empirical tests on patient CT images indicate
notable improvements in image standardization using DiffusionCT. Additionally,
the model significantly reduces image noise in SPAD images, further validating
the effectiveness of DiffusionCT for advanced imaging tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Semiotics Networks Representing Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Kupeev, Eyal Nitcany
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans perceive objects daily and communicate their perceptions using various
channels. Here, we describe a computational model that track and simulate
objects' perception, and their representations as they pass in communication.
  We describe two key components of our internal representation ('observed' and
'seen') and relate them to familiar computer vision terms (encoding and
decoding). These elements joined together to form semiotic networks, which
simulate awareness in object perception and human communication.
  Nowadays, most neural networks are uninterpretable. On the other hand, our
model is free from this disadvantages. We performed several experiments and
demonstrated the visibility of our model.
  We describe how our network may be used as preprocessing unit to any
classification network. In our experiments the compound network overperforms in
average the classification network at datasets with small training data.
  Future work would leverage our model to gain better understanding of human
communications and personal representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Facial Action Unit Detection Through Jointly Learning Facial
  Landmark Detection and Domain Separation and Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiao Shang, Li Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently how to introduce large amounts of unlabeled facial images in the
wild into supervised Facial Action Unit (AU) detection frameworks has become a
challenging problem. In this paper, we propose a new AU detection framework
where multi-task learning is introduced to jointly learn AU domain separation
and reconstruction and facial landmark detection by sharing the parameters of
homostructural facial extraction modules. In addition, we propose a new feature
alignment scheme based on contrastive learning by simple projectors and an
improved contrastive loss, which adds four additional intermediate supervisors
to promote the feature reconstruction process. Experimental results on two
benchmarks demonstrate our superiority against the state-of-the-art methods for
AU detection in the wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, published to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Cross-Dataset Performance of Distracted Driving Detection With
  Score-Softmax Classifier 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Duan, Zixuan Liu, Jiahao Xia, Minghai Zhang, Jiacai Liao, Libo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks enable real-time monitoring of in-vehicle driver,
facilitating the timely prediction of distractions, fatigue, and potential
hazards. This technology is now integral to intelligent transportation systems.
Recent research has exposed unreliable cross-dataset end-to-end driver behavior
recognition due to overfitting, often referred to as ``shortcut learning",
resulting from limited data samples. In this paper, we introduce the
Score-Softmax classifier, which addresses this issue by enhancing inter-class
independence and Intra-class uncertainty. Motivated by human rating patterns,
we designed a two-dimensional supervisory matrix based on marginal Gaussian
distributions to train the classifier. Gaussian distributions help amplify
intra-class uncertainty while ensuring the Score-Softmax classifier learns
accurate knowledge. Furthermore, leveraging the summation of independent
Gaussian distributed random variables, we introduced a multi-channel
information fusion method. This strategy effectively resolves the
multi-information fusion challenge for the Score-Softmax classifier.
Concurrently, we substantiate the necessity of transfer learning and
multi-dataset combination. We conducted cross-dataset experiments using the
SFD, AUCDD-V1, and 100-Driver datasets, demonstrating that Score-Softmax
improves cross-dataset performance without modifying the model architecture.
This provides a new approach for enhancing neural network generalization.
Additionally, our information fusion approach outperforms traditional methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient
  Partially Relevant Video Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a text query, partially relevant video retrieval (PRVR) seeks to find
untrimmed videos containing pertinent moments in a database. For PRVR, clip
modeling is essential to capture the partial relationship between texts and
videos. Current PRVR methods adopt scanning-based clip construction to achieve
explicit clip modeling, which is information-redundant and requires a large
storage overhead. To solve the efficiency problem of PRVR methods, this paper
proposes GMMFormer, a \textbf{G}aussian-\textbf{M}ixture-\textbf{M}odel based
Trans\textbf{former} which models clip representations implicitly. During frame
interactions, we incorporate Gaussian-Mixture-Model constraints to focus each
frame on its adjacent frames instead of the whole video. Then generated
representations will contain multi-scale clip information, achieving implicit
clip modeling. In addition, PRVR methods ignore semantic differences between
text queries relevant to the same video, leading to a sparse embedding space.
We propose a query diverse loss to distinguish these text queries, making the
embedding space more intensive and contain more semantic information. Extensive
experiments on three large-scale video datasets (\ie, TVR, ActivityNet
Captions, and Charades-STA) demonstrate the superiority and efficiency of
GMMFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. The code will be released</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Discriminative Multi-Modal Learning with Large-Scale
  Pre-Trained Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenzhuang Du, Yue Zhao, Chonghua Liao, Jiacheng You, Jie Fu, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates how to better leverage large-scale pre-trained
uni-modal models to further enhance discriminative multi-modal learning. Even
when fine-tuned with only uni-modal data, these models can outperform previous
multi-modal models in certain tasks. It's clear that their incorporation into
multi-modal learning would significantly improve performance. However,
multi-modal learning with these models still suffers from insufficient learning
of uni-modal features, which weakens the resulting multi-modal model's
generalization ability. While fine-tuning uni-modal models separately and then
aggregating their predictions is straightforward, it doesn't allow for adequate
adaptation between modalities, also leading to sub-optimal results. To this
end, we introduce Multi-Modal Low-Rank Adaptation learning (MMLoRA). By
freezing the weights of uni-modal fine-tuned models, adding extra trainable
rank decomposition matrices to them, and subsequently performing multi-modal
joint training, our method enhances adaptation between modalities and boosts
overall performance. We demonstrate the effectiveness of MMLoRA on three
dataset categories: audio-visual (e.g., AVE, Kinetics-Sound, CREMA-D),
vision-language (e.g., MM-IMDB, UPMC Food101), and RGB-Optical Flow (UCF101).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HOD: A Benchmark Dataset for Harmful Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eungyeom Ha, Heemook Kim, Sung Chul Hong, Dongbin Na
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multi-media data such as images and videos have been rapidly spread
out on various online services such as social network services (SNS). With the
explosive growth of online media services, the number of image content that may
harm users is also growing exponentially. Thus, most recent online platforms
such as Facebook and Instagram have adopted content filtering systems to
prevent the prevalence of harmful content and reduce the possible risk of
adverse effects on users. Unfortunately, computer vision research on detecting
harmful content has not yet attracted attention enough. Users of each platform
still manually click the report button to recognize patterns of harmful content
they dislike when exposed to harmful content. However, the problem with manual
reporting is that users are already exposed to harmful content. To address
these issues, our research goal in this work is to develop automatic harmful
object detection systems for online services. We present a new benchmark
dataset for harmful object detection. Unlike most related studies focusing on a
small subset of object categories, our dataset addresses various categories.
Specifically, our proposed dataset contains more than 10,000 images across 6
categories that might be harmful, consisting of not only normal cases but also
hard cases that are difficult to detect. Moreover, we have conducted extensive
experiments to evaluate the effectiveness of our proposed dataset. We have
utilized the recently proposed state-of-the-art (SOTA) object detection
architectures and demonstrated our proposed dataset can be greatly useful for
the real-time harmful object detection task. The whole source codes and
datasets are publicly accessible at
https://github.com/poori-nuna/HOD-Benchmark-Dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AANet: Aggregation and Alignment Network with Semi-hard Positive Sample
  Mining for Hierarchical Place Recognition <span class="chip">ICRA2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Lu, Lijun Zhang, Shuting Dong, Baifan Chen, Chun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition (VPR) is one of the research hotspots in robotics,
which uses visual information to locate robots. Recently, the hierarchical
two-stage VPR methods have become popular in this field due to the trade-off
between accuracy and efficiency. These methods retrieve the top-k candidate
images using the global features in the first stage, then re-rank the
candidates by matching the local features in the second stage. However, they
usually require additional algorithms (e.g. RANSAC) for geometric consistency
verification in re-ranking, which is time-consuming. Here we propose a
Dynamically Aligning Local Features (DALF) algorithm to align the local
features under spatial constraints. It is significantly more efficient than the
methods that need geometric consistency verification. We present a unified
network capable of extracting global features for retrieving candidates via an
aggregation module and aligning local features for re-ranking via the DALF
alignment module. We call this network AANet. Meanwhile, many works use the
simplest positive samples in triplet for weakly supervised training, which
limits the ability of the network to recognize harder positive pairs. To
address this issue, we propose a Semi-hard Positive Sample Mining (ShPSM)
strategy to select appropriate hard positive images for training more robust
VPR networks. Extensive experiments on four benchmark VPR datasets show that
the proposed AANet can outperform several state-of-the-art methods with less
time consumption. The code is released at https://github.com/Lu-Feng/AANet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Ship Tracking by Robust Similarity metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Zhao, Gongming Wei, Yang Xiao, Xianglei Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-ship tracking (MST) as a core technology has been proven to be applied
to situational awareness at sea and the development of a navigational system
for autonomous ships. Despite impressive tracking outcomes achieved by
multi-object tracking (MOT) algorithms for pedestrian and vehicle datasets,
these models and techniques exhibit poor performance when applied to ship
datasets. Intersection of Union (IoU) is the most popular metric for computing
similarity used in object tracking. The low frame rates and severe image shake
caused by wave turbulence in ship datasets often result in minimal, or even
zero, Intersection of Union (IoU) between the predicted and detected bounding
boxes. This issue contributes to frequent identity switches of tracked objects,
undermining the tracking performance. In this paper, we address the weaknesses
of IoU by incorporating the smallest convex shapes that enclose both the
predicted and detected bounding boxes. The calculation of the tracking version
of IoU (TIoU) metric considers not only the size of the overlapping area
between the detection bounding box and the prediction box, but also the
similarity of their shapes. Through the integration of the TIoU into
state-of-the-art object tracking frameworks, such as DeepSort and ByteTrack, we
consistently achieve improvements in the tracking performance of these
frameworks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ITRE: Low-light Image Enhancement Based on Illumination Transmission
  Ratio Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang, Yihong Wang, Tong Liu, Xiubao Sui, Qian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Noise, artifacts, and over-exposure are significant challenges in the field
of low-light image enhancement. Existing methods often struggle to address
these issues simultaneously. In this paper, we propose a novel Retinex-based
method, called ITRE, which suppresses noise and artifacts from the origin of
the model, prevents over-exposure throughout the enhancement process.
Specifically, we assume that there must exist a pixel which is least disturbed
by low light within pixels of same color. First, clustering the pixels on the
RGB color space to find the Illumination Transmission Ratio (ITR) matrix of the
whole image, which determines that noise is not over-amplified easily. Next, we
consider ITR of the image as the initial illumination transmission map to
construct a base model for refined transmission map, which prevents artifacts.
Additionally, we design an over-exposure module that captures the fundamental
characteristics of pixel over-exposure and seamlessly integrate it into the
base model. Finally, there is a possibility of weak enhancement when
inter-class distance of pixels with same color is too small. To counteract
this, we design a Robust-Guard module that safeguards the robustness of the
image enhancement process. Extensive experiments demonstrate the effectiveness
of our approach in suppressing noise, preventing artifacts, and controlling
over-exposure level simultaneously. Our method performs superiority in
qualitative and quantitative performance evaluations by comparing with
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstructDET: Diversifying Referring Object Detection with Generalized
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song, Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, Yibing Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose InstructDET, a data-centric method for referring object detection
(ROD) that localizes target objects based on user instructions. While deriving
from referring expressions (REC), the instructions we leverage are greatly
diversified to encompass common user intentions related to object detection.
For one image, we produce tremendous instructions that refer to every single
object and different combinations of multiple objects. Each instruction and its
corresponding object bounding boxes (bbxs) constitute one training data pair.
In order to encompass common detection expressions, we involve emerging
vision-language model (VLM) and large language model (LLM) to generate
instructions guided by text prompts and object bbxs, as the generalizations of
foundation models are effective to produce human-like expressions (e.g.,
describing object property, category, and relationship). We name our
constructed dataset as InDET. It contains images, bbxs and generalized
instructions that are from foundation models. Our InDET is developed from
existing REC datasets and object detection datasets, with the expanding
potential that any image with object bbxs can be incorporated through using our
InstructDET method. By using our InDET dataset, we show that a conventional ROD
model surpasses existing methods on standard REC datasets and our InDET test
set. Our data-centric method InstructDET, with automatic data expansion by
leveraging foundation models, directs a promising field that ROD can be greatly
diversified to execute common object detection instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages (include appendix), under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loco<span class="highlight-title">NeRF</span>: A <span class="highlight-title">NeRF</span>-based Approach for Local Structure from Motion for
  Precise Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Nenashev, Mikhail Kurenkov, Andrei Potapov, Iana Zhura, Maksim Katerishich, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual localization is a critical task in mobile robotics, and researchers
are continuously developing new approaches to enhance its efficiency. In this
article, we propose a novel approach to improve the accuracy of visual
localization using Structure from Motion (SfM) techniques. We highlight the
limitations of global SfM, which suffers from high latency, and the challenges
of local SfM, which requires large image databases for accurate reconstruction.
To address these issues, we propose utilizing Neural Radiance Fields (NeRF), as
opposed to image databases, to cut down on the space required for storage. We
suggest that sampling reference images around the prior query position can lead
to further improvements. We evaluate the accuracy of our proposed method
against ground truth obtained using LIDAR and Advanced Lidar Odometry and
Mapping in Real-time (A-LOAM), and compare its storage usage against local SfM
with COLMAP in the conducted experiments. Our proposed method achieves an
accuracy of 0.068 meters compared to the ground truth, which is slightly lower
than the most advanced method COLMAP, which has an accuracy of 0.022 meters.
However, the size of the database required for COLMAP is 400 megabytes, whereas
the size of our NeRF model is only 160 megabytes. Finally, we perform an
ablation study to assess the impact of using reference images from the NeRF
reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry Aware Field-to-field Transformations for 3D Semantic
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Hollidt, Clinton Wang, Polina Golland, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to perform 3D semantic segmentation solely from
2D supervision by leveraging Neural Radiance Fields (NeRFs). By extracting
features along a surface point cloud, we achieve a compact representation of
the scene which is sample-efficient and conducive to 3D reasoning. Learning
this feature space in an unsupervised manner via masked autoencoding enables
few-shot segmentation. Our method is agnostic to the scene parameterization,
working on scenes fit with any type of NeRF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UReader: Universal OCR-free Visually-situated Language Understanding
  with Multimodal Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Alex Lin, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text is ubiquitous in our visual world, conveying crucial information, such
as in documents, websites, and everyday photographs. In this work, we propose
UReader, a first exploration of universal OCR-free visually-situated language
understanding based on the Multimodal Large Language Model (MLLM). By
leveraging the shallow text recognition ability of the MLLM, we only finetuned
1.2% parameters and the training cost is much lower than previous work
following domain-specific pretraining and finetuning paradigms. Concretely,
UReader is jointly finetuned on a wide range of Visually-situated Language
Understanding tasks via a unified instruction format. To enhance the visual
text and semantic understanding, we further apply two auxiliary tasks with the
same format, namely text reading and key points generation tasks. We design a
shape-adaptive cropping module before the encoder-decoder architecture of MLLM
to leverage the frozen low-resolution vision encoder for processing
high-resolution images. Without downstream finetuning, our single model
achieves state-of-the-art ocr-free performance in 8 out of 10 visually-situated
language understanding tasks, across 5 domains: documents, tables, charts,
natural images, and webpage screenshots. Codes and instruction-tuning datasets
will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bidirectional Knowledge Reconfiguration for Lightweight Point Cloud
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peipei Li, Xing Cui, Yibo Hu, Man Zhang, Ting Yao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud analysis faces computational system overhead, limiting its
application on mobile or edge devices. Directly employing small models may
result in a significant drop in performance since it is difficult for a small
model to adequately capture local structure and global shape information
simultaneously, which are essential clues for point cloud analysis. This paper
explores feature distillation for lightweight point cloud models. To mitigate
the semantic gap between the lightweight student and the cumbersome teacher, we
propose bidirectional knowledge reconfiguration (BKR) to distill informative
contextual knowledge from the teacher to the student. Specifically, a top-down
knowledge reconfiguration and a bottom-up knowledge reconfiguration are
developed to inherit diverse local structure information and consistent global
shape knowledge from the teacher, respectively. However, due to the farthest
point sampling in most point cloud models, the intermediate features between
teacher and student are misaligned, deteriorating the feature distillation
performance. To eliminate it, we propose a feature mover's distance (FMD) loss
based on optimal transportation, which can measure the distance between
unordered point cloud features effectively. Extensive experiments conducted on
shape classification, part segmentation, and semantic segmentation benchmarks
demonstrate the universality and superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Multimedia (TMM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-domain Robust Deepfake Bias Expansion Network for Face Forgery
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihua Liu, Lin Li, Chaochao Lin, Said Boumaraf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of deepfake technologies raises significant concerns
about the security of face recognition systems. While existing methods leverage
the clues left by deepfake techniques for face forgery detection, malicious
users may intentionally manipulate forged faces to obscure the traces of
deepfake clues and thereby deceive detection tools. Meanwhile, attaining
cross-domain robustness for data-based methods poses a challenge due to
potential gaps in the training data, which may not encompass samples from all
relevant domains. Therefore, in this paper, we introduce a solution - a
Cross-Domain Robust Bias Expansion Network (BENet) - designed to enhance face
forgery detection. BENet employs an auto-encoder to reconstruct input faces,
maintaining the invariance of real faces while selectively enhancing the
difference between reconstructed fake faces and their original counterparts.
This enhanced bias forms a robust foundation upon which dependable forgery
detection can be built. To optimize the reconstruction results in BENet, we
employ a bias expansion loss infused with contrastive concepts to attain the
aforementioned objective. In addition, to further heighten the amplification of
forged clues, BENet incorporates a Latent-Space Attention (LSA) module. This
LSA module effectively captures variances in latent features between the
auto-encoder's encoder and decoder, placing emphasis on inconsistent
forgery-related information. Furthermore, BENet incorporates a cross-domain
detector with a threshold to determine whether the sample belongs to a known
distribution. The correction of classification results through the cross-domain
detector enables BENet to defend against unknown deepfake attacks from
cross-domain. Extensive experiments demonstrate the superiority of BENet
compared with state-of-the-art methods in intra-database and cross-database
evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Multi-Domain Knowledge Networks for Chest X-ray Report
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihua Liu, Youyuan Xue, Chaochao Lin, Said Boumaraf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated generation of radiology diagnostic reports helps radiologists
make timely and accurate diagnostic decisions while also enhancing clinical
diagnostic efficiency. However, the significant imbalance in the distribution
of data between normal and abnormal samples (including visual and textual
biases) poses significant challenges for a data-driven task like automatically
generating diagnostic radiology reports. Therefore, we propose a Dynamic
Multi-Domain Knowledge(DMDK) network for radiology diagnostic report
generation. The DMDK network consists of four modules: Chest Feature
Extractor(CFE), Dynamic Knowledge Extractor(DKE), Specific Knowledge
Extractor(SKE), and Multi-knowledge Integrator(MKI) module. Specifically, the
CFE module is primarily responsible for extracting the unprocessed visual
medical features of the images. The DKE module is responsible for extracting
dynamic disease topic labels from the retrieved radiology diagnostic reports.
We then fuse the dynamic disease topic labels with the original visual features
of the images to highlight the abnormal regions in the original visual features
to alleviate the visual data bias problem. The SKE module expands upon the
conventional static knowledge graph to mitigate textual data biases and amplify
the interpretability capabilities of the model via domain-specific dynamic
knowledge graphs. The MKI distills all the knowledge and generates the final
diagnostic radiology report. We performed extensive experiments on two widely
used datasets, IU X-Ray and MIMIC-CXR. The experimental results demonstrate the
effectiveness of our method, with all evaluation metrics outperforming previous
state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight In-Context Tuning for Multimodal Unified Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Chen, Shuai Zhang, Boran Han, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) involves reasoning from given contextual examples.
As more modalities comes, this procedure is becoming more challenging as the
interleaved input modalities convolutes the understanding process. This is
exemplified by the observation that multimodal models often struggle to
effectively extrapolate from contextual examples to perform ICL. To address
these challenges, we introduce MultiModal In-conteXt Tuning (M$^2$IXT), a
lightweight module to enhance the ICL capabilities of multimodal unified
models. The proposed M$^2$IXT module perceives an expandable context window to
incorporate various labeled examples of multiple modalities (e.g., text, image,
and coordinates). It can be prepended to various multimodal unified models
(e.g., OFA, Unival, LLaVA) of different architectures and trained via a
mixed-tasks strategy to enable rapid few-shot adaption on multiple tasks and
datasets. When tuned on as little as 50K multimodal data, M$^2$IXT can boost
the few-shot ICL performance significantly (e.g., 18\% relative increase for
OFA), and obtained state-of-the-art results across an array of tasks including
visual question answering, image captioning, visual grounding, and visual
entailment, while being considerably small in terms of model parameters (e.g.,
$\sim$$20\times$ smaller than Flamingo or MMICL), highlighting the flexibility
and effectiveness of M$^2$IXT as a multimodal in-context learner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Representations through Heterogeneous Self-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhong-Yu Li, Bo-Wen Yin, Shanghua Gao, Yongxiang Liu, Li Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating heterogeneous representations from different architectures has
facilitated various vision tasks, e.g., some hybrid networks combine
transformers and convolutions. However, complementarity between such
heterogeneous architectures has not been well exploited in self-supervised
learning. Thus, we propose Heterogeneous Self-Supervised Learning (HSSL), which
enforces a base model to learn from an auxiliary head whose architecture is
heterogeneous from the base model. In this process, HSSL endows the base model
with new characteristics in a representation learning way without structural
changes. To comprehensively understand the HSSL, we conduct experiments on
various heterogeneous pairs containing a base model and an auxiliary head. We
discover that the representation quality of the base model moves up as their
architecture discrepancy grows. This observation motivates us to propose a
search strategy that quickly determines the most suitable auxiliary head for a
specific base model to learn and several simple but effective methods to
enlarge the model discrepancy. The HSSL is compatible with various
self-supervised methods, achieving superior performances on various downstream
tasks, including image classification, semantic segmentation, instance
segmentation, and object detection. Our source code will be made publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OV-PARTS: Towards Open-Vocabulary Part Segmentation <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Wei, Xiaoyu Yue, Wenwei Zhang, Shu Kong, Xihui Liu, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmenting and recognizing diverse object parts is a crucial ability in
applications spanning various computer vision and robotic tasks. While
significant progress has been made in object-level Open-Vocabulary Semantic
Segmentation (OVSS), i.e., segmenting objects with arbitrary text, the
corresponding part-level research poses additional challenges. Firstly, part
segmentation inherently involves intricate boundaries, while limited annotated
data compounds the challenge. Secondly, part segmentation introduces an open
granularity challenge due to the diverse and often ambiguous definitions of
parts in the open world. Furthermore, the large-scale vision and language
models, which play a key role in the open vocabulary setting, struggle to
recognize parts as effectively as objects. To comprehensively investigate and
tackle these challenges, we propose an Open-Vocabulary Part Segmentation
(OV-PARTS) benchmark. OV-PARTS includes refined versions of two publicly
available datasets: Pascal-Part-116 and ADE20K-Part-234. And it covers three
specific tasks: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part
Segmentation, and Few-Shot Part Segmentation, providing insights into
analogical reasoning, open granularity and few-shot adapting abilities of
models. Moreover, we analyze and adapt two prevailing paradigms of existing
object-level OVSS methods for OV-PARTS. Extensive experimental analysis is
conducted to inspire future research in leveraging foundational models for
OV-PARTS. The code and dataset are available at
https://github.com/OpenRobotLab/OV_PARTS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS Dataset and Benchmark Track 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-head mutual Mean-Teaching for semi-supervised medical image
  segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Ruifeng Bian, Wenyi Zhao, Huihua Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised medical image segmentation (SSMIS) has witnessed substantial
advancements by leveraging limited labeled data and abundant unlabeled data.
Nevertheless, existing state-of-the-art methods encounter challenges in
accurately predicting labels for the unlabeled data, resulting in disruptive
noise during training and susceptibility to erroneous information overfitting.
Additionally, applying perturbations to inaccurate predictions further reduces
consistent learning. To address these concerns, a novel \textbf{C}ross-head
\textbf{m}utual \textbf{m}ean-\textbf{t}eaching Network (CMMT-Net) is proposed
to address these issues. The CMMT-Net comprises teacher-student networks and
incorporates strong-weak data augmentation within a shared encoder,
facilitating cross-head co-training by capitalizing on both self-training and
consistent learning. The consistent learning is enhanced by averaging teacher
networks and mutual virtual adversarial training, leading to deterministic and
higher-quality predictions. The diversity of consistency training samples can
be enhanced through the use of Cross-Set CutMix, which also helps mitigate
issues related to distribution mismatch. Notably, CMMT-Net simultaneously
implements data-level, feature-level, and network-level perturbations, boosting
model diversity and generalization performance. The proposed method
consistently outperforms existing SSMIS methods on three publicly available
datasets across various semi-supervised settings. Code and logs will be
available at \url{https://github.com/Leesoon1984/CMMT-Net}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video-CSR: Complex Video Digest Creation for Visual-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingkai Liu, Yunzhe Tao, Haogeng Liu, Qihang Fan, Ding Zhou, Huaibo Huang, Ran He, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel task and human annotated dataset for evaluating the
ability for visual-language models to generate captions and summaries for
real-world video clips, which we call Video-CSR (Captioning, Summarization and
Retrieval). The dataset contains 4.8K YouTube video clips of 20-60 seconds in
duration and covers a wide range of topics and interests. Each video clip
corresponds to 5 independently annotated captions (1 sentence) and summaries
(3-10 sentences). Given any video selected from the dataset and its
corresponding ASR information, we evaluate visual-language models on either
caption or summary generation that is grounded in both the visual and auditory
content of the video. Additionally, models are also evaluated on caption- and
summary-based retrieval tasks, where the summary-based retrieval task requires
the identification of a target video given excerpts of a corresponding summary.
Given the novel nature of the paragraph-length video summarization task, we
perform extensive comparative analyses of different existing evaluation metrics
and their alignment with human preferences. Finally, we propose a foundation
model with competitive generation and retrieval capabilities that serves as a
baseline for the Video-CSR task. We aim for Video-CSR to serve as a useful
evaluation set in the age of large language models and complex multi-modal
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Separable Hidden Unit Contributions for Speaker-Adaptive
  Lip-Reading <span class="chip">BMVC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songtao Luo, Shuang Yang, Shiguang Shan, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel method for speaker adaptation in lip
reading, motivated by two observations. Firstly, a speaker's own
characteristics can always be portrayed well by his/her few facial images or
even a single image with shallow networks, while the fine-grained dynamic
features associated with speech content expressed by the talking face always
need deep sequential networks to represent accurately. Therefore, we treat the
shallow and deep layers differently for speaker adaptive lip reading. Secondly,
we observe that a speaker's unique characteristics ( e.g. prominent oral cavity
and mandible) have varied effects on lip reading performance for different
words and pronunciations, necessitating adaptive enhancement or suppression of
the features for robust lip reading. Based on these two observations, we
propose to take advantage of the speaker's own characteristics to automatically
learn separable hidden unit contributions with different targets for shallow
layers and deep layers respectively. For shallow layers where features related
to the speaker's characteristics are stronger than the speech content related
features, we introduce speaker-adaptive features to learn for enhancing the
speech content features. For deep layers where both the speaker's features and
the speech content features are all expressed well, we introduce the
speaker-adaptive features to learn for suppressing the speech content
irrelevant noise for robust lip reading. Our approach consistently outperforms
existing methods, as confirmed by comprehensive analysis and comparison across
different settings. Besides the evaluation on the popular LRW-ID and GRID
datasets, we also release a new dataset for evaluation, CAS-VSR-S68h, to
further assess the performance in an extreme setting where just a few speakers
are available but the speech content covers a large and diversified range.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to BMVC 2023 20pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-driven Open-Vocabulary Keypoint Detection for Animal Body and
  Face 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Kaipeng Zhang, Lumin Xu, Shenqi Lai, Wenqi Shao, Naning Zheng, Ping Luo, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches for image-based keypoint detection on animal (including
human) body and face are limited to specific keypoints and species. We address
the limitation by proposing the Open-Vocabulary Keypoint Detection (OVKD) task.
It aims to use text prompts to localize arbitrary keypoints of any species. To
accomplish this objective, we propose Open-Vocabulary Keypoint Detection with
Semantic-feature Matching (KDSM), which utilizes both vision and language
models to harness the relationship between text and vision and thus achieve
keypoint detection through associating text prompt with relevant keypoint
features. Additionally, KDSM integrates domain distribution matrix matching and
some special designs to reinforce the relationship between language and vision,
thereby improving the model's generalizability and performance. Extensive
experiments show that our proposed components bring significant performance
improvements, and our overall method achieves impressive results in OVKD.
Remarkably, our method outperforms the state-of-the-art few-shot keypoint
detection methods using a zero-shot fashion. We will make the source code
publicly accessible.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in
  Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raman Dutt, Ondrej Bohdal, Sotirios A. Tsaftaris, Timothy Hospedales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training models with robust group fairness properties is crucial in ethically
sensitive application areas such as medical diagnosis. Despite the growing body
of work aiming to minimise demographic bias in AI, this problem remains
challenging. A key reason for this challenge is the fairness generalisation
gap: High-capacity deep learning models can fit all training data nearly
perfectly, and thus also exhibit perfect fairness during training. In this
case, bias emerges only during testing when generalisation performance differs
across subgroups. This motivates us to take a bi-level optimisation perspective
on fair learning: Optimising the learning strategy based on validation
fairness. Specifically, we consider the highly effective workflow of adapting
pre-trained models to downstream medical imaging tasks using
parameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between
updating more parameters, enabling a better fit to the task of interest vs.
fewer parameters, potentially reducing the generalisation gap. To manage this
tradeoff, we propose FairTune, a framework to optimise the choice of PEFT
parameters with respect to fairness. We demonstrate empirically that FairTune
leads to improved fairness on a range of medical imaging datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Resolution Self-Attention for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Huan Wu, Shi-Chen Zhang, Yun Liu, Le Zhang, Xin Zhan, Daquan Zhou, Jiashi Feng, Ming-Ming Cheng, Liangli Zhen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation tasks naturally require high-resolution information for
pixel-wise segmentation and global context information for class prediction.
While existing vision transformers demonstrate promising performance, they
often utilize high resolution context modeling, resulting in a computational
bottleneck. In this work, we challenge conventional wisdom and introduce the
Low-Resolution Self-Attention (LRSA) mechanism to capture global context at a
significantly reduced computational cost. Our approach involves computing
self-attention in a fixed low-resolution space regardless of the input image's
resolution, with additional 3x3 depth-wise convolutions to capture fine details
in the high-resolution space. We demonstrate the effectiveness of our LRSA
approach by building the LRFormer, a vision transformer with an encoder-decoder
structure. Extensive experiments on the ADE20K, COCO-Stuff, and Cityscapes
datasets demonstrate that LRFormer outperforms state-of-the-art models. The
code will be made available at https://github.com/yuhuan-wu/LRFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 11 tables, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Stage Warped Cloth Learning and Semantic-Contextual Attention
  Feature Fusion for Virtual TryOn 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanhita Pathak, Vinay Kaushik, Brejesh Lall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-based virtual try-on aims to fit an in-shop garment onto a clothed
person image. Garment warping, which aligns the target garment with the
corresponding body parts in the person image, is a crucial step in achieving
this goal. Existing methods often use multi-stage frameworks to handle clothes
warping, person body synthesis and tryon generation separately or rely on noisy
intermediate parser-based labels. We propose a novel single-stage framework
that implicitly learns the same without explicit multi-stage learning. Our
approach utilizes a novel semantic-contextual fusion attention module for
garment-person feature fusion, enabling efficient and realistic cloth warping
and body synthesis from target pose keypoints. By introducing a lightweight
linear attention framework that attends to garment regions and fuses multiple
sampled flow fields, we also address misalignment and artifacts present in
previous methods. To achieve simultaneous learning of warped garment and try-on
results, we introduce a Warped Cloth Learning Module. WCLM uses segmented
warped garments as ground truth, operating within a single-stage paradigm. Our
proposed approach significantly improves the quality and efficiency of virtual
try-on methods, providing users with a more reliable and realistic virtual
try-on experience. We evaluate our method on the VITON dataset and demonstrate
its state-of-the-art performance in terms of both qualitative and quantitative
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Abnormal Health Conditions in Smart Home Using a Drone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pronob Kumar Barman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, detecting aberrant health issues is a difficult process. Falling,
especially among the elderly, is a severe concern worldwide. Falls can result
in deadly consequences, including unconsciousness, internal bleeding, and often
times, death. A practical and optimal, smart approach of detecting falling is
currently a concern. The use of vision-based fall monitoring is becoming more
common among scientists as it enables senior citizens and those with other
health conditions to live independently. For tracking, surveillance, and
rescue, unmanned aerial vehicles use video or image segmentation and object
detection methods. The Tello drone is equipped with a camera and with this
device we determined normal and abnormal behaviors among our participants. The
autonomous falling objects are classified using a convolutional neural network
(CNN) classifier. The results demonstrate that the systems can identify falling
objects with a precision of 0.9948.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building an Open-Vocabulary Video CLIP Model with Better Architectures,
  Optimization and Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuxuan Wu, Zejia Weng, Wujian Peng, Xitong Yang, Ang Li, Larry S. Davis, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant results achieved by Contrastive Language-Image
Pretraining (CLIP) in zero-shot image recognition, limited effort has been made
exploring its potential for zero-shot video recognition. This paper presents
Open-VCLIP++, a simple yet effective framework that adapts CLIP to a strong
zero-shot video classifier, capable of identifying novel actions and events
during testing. Open-VCLIP++ minimally modifies CLIP to capture
spatial-temporal relationships in videos, thereby creating a specialized video
classifier while striving for generalization. We formally demonstrate that
training Open-VCLIP++ is tantamount to continual learning with zero historical
data. To address this problem, we introduce Interpolated Weight Optimization, a
technique that leverages the advantages of weight interpolation during both
training and testing. Furthermore, we build upon large language models to
produce fine-grained video descriptions. These detailed descriptions are
further aligned with video features, facilitating a better transfer of CLIP to
the video domain. Our approach is evaluated on three widely used action
recognition datasets, following a variety of zero-shot evaluation protocols.
The results demonstrate that our method surpasses existing state-of-the-art
techniques by significant margins. Specifically, we achieve zero-shot accuracy
scores of 88.1%, 58.7%, and 81.2% on UCF, HMDB, and Kinetics-600 datasets
respectively, outpacing the best-performing alternative methods by 8.5%, 8.2%,
and 12.3%. We also evaluate our approach on the MSR-VTT video-text retrieval
dataset, where it delivers competitive video-to-text and text-to-video
retrieval performance, while utilizing substantially less fine-tuning data
compared to other methods. Code is released at
https://github.com/wengzejia1/Open-VCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2302.00624</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symmetrical Linguistic Feature Distillation with CLIP for Scene Text
  Recognition <span class="chip">ACM MM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiao Wang, Hongtao Xie, Yuxin Wang, Jianjun Xu, Boqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the potential of the Contrastive Language-Image
Pretraining (CLIP) model in scene text recognition (STR), and establish a novel
Symmetrical Linguistic Feature Distillation framework (named CLIP-OCR) to
leverage both visual and linguistic knowledge in CLIP. Different from previous
CLIP-based methods mainly considering feature generalization on visual
encoding, we propose a symmetrical distillation strategy (SDS) that further
captures the linguistic knowledge in the CLIP text encoder. By cascading the
CLIP image encoder with the reversed CLIP text encoder, a symmetrical structure
is built with an image-to-text feature flow that covers not only visual but
also linguistic information for distillation.Benefiting from the natural
alignment in CLIP, such guidance flow provides a progressive optimization
objective from vision to language, which can supervise the STR feature
forwarding process layer-by-layer.Besides, a new Linguistic Consistency Loss
(LCL) is proposed to enhance the linguistic capability by considering
second-order statistics during the optimization. Overall, CLIP-OCR is the first
to design a smooth transition between image and text for the STR task.Extensive
experiments demonstrate the effectiveness of CLIP-OCR with 93.8% average
accuracy on six popular STR benchmarks.Code will be available at
https://github.com/wzx99/CLIPOCR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemST: Semantically Consistent Multi-Scale Image Translation via
  Structure-Texture Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganning Zhao, Wenhui Cui, Suya You, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised image-to-image (I2I) translation learns cross-domain image
mapping that transfers input from the source domain to output in the target
domain while preserving its semantics. One challenge is that different semantic
statistics in source and target domains result in content discrepancy known as
semantic distortion. To address this problem, a novel I2I method that maintains
semantic consistency in translation is proposed and named SemST in this work.
SemST reduces semantic distortion by employing contrastive learning and
aligning the structural and textural properties of input and output by
maximizing their mutual information. Furthermore, a multi-scale approach is
introduced to enhance translation performance, thereby enabling the
applicability of SemST to domain adaptation in high-resolution images.
Experiments show that SemST effectively mitigates semantic distortion and
achieves state-of-the-art performance. Also, the application of SemST to domain
adaptation (DA) is explored. It is demonstrated by preliminary experiments that
SemST can be utilized as a beneficial pre-training for the semantic
segmentation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisionFM: a Multi-Modal Multi-Task Vision Foundation Model for
  Generalist Ophthalmic Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianing Qiu, Jian Wu, Hao Wei, Peilun Shi, Minqing Zhang, Yunyun Sun, Lin Li, Hanruo Liu, Hongyi Liu, Simeng Hou, Yuyang Zhao, Xuehui Shi, Junfang Xian, Xiaoxia Qu, Sirui Zhu, Lijie Pan, Xiaoniao Chen, Xiaojia Zhang, Shuai Jiang, Kebing Wang, Chenlong Yang, Mingqiang Chen, Sujie Fan, Jianhua Hu, Aiguo Lv, Hui Miao, Li Guo, Shujun Zhang, Cheng Pei, Xiaojuan Fan, Jianqin Lei, Ting Wei, Junguo Duan, Chun Liu, Xiaobo Xia, Siqi Xiong, Junhong Li, Benny Lo, Yih Chung Tham, Tien Yin Wong, Ningli Wang, Wu Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present VisionFM, a foundation model pre-trained with 3.4 million
ophthalmic images from 560,457 individuals, covering a broad range of
ophthalmic diseases, modalities, imaging devices, and demography. After
pre-training, VisionFM provides a foundation to foster multiple ophthalmic
artificial intelligence (AI) applications, such as disease screening and
diagnosis, disease prognosis, subclassification of disease phenotype, and
systemic biomarker and disease prediction, with each application enhanced with
expert-level intelligence and accuracy. The generalist intelligence of VisionFM
outperformed ophthalmologists with basic and intermediate levels in jointly
diagnosing 12 common ophthalmic diseases. Evaluated on a new large-scale
ophthalmic disease diagnosis benchmark database, as well as a new large-scale
segmentation and detection benchmark database, VisionFM outperformed strong
baseline deep neural networks. The ophthalmic image representations learned by
VisionFM exhibited noteworthy explainability, and demonstrated strong
generalizability to new ophthalmic modalities, disease spectrum, and imaging
devices. As a foundation model, VisionFM has a large capacity to learn from
diverse ophthalmic imaging data and disparate datasets. To be commensurate with
this capacity, in addition to the real data used for pre-training, we also
generated and leveraged synthetic ophthalmic imaging data. Experimental results
revealed that synthetic data that passed visual Turing tests, can also enhance
the representation learning capability of VisionFM, leading to substantial
performance gains on downstream ophthalmic AI tasks. Beyond the ophthalmic AI
applications developed, validated, and demonstrated in this work, substantial
further applications can be achieved in an efficient and cost-effective manner
using VisionFM as the foundation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video-Teller: Enhancing Cross-Modal Generation with Fusion and
  Decoupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haogeng Liu, Qihang Fan, Tingkai Liu, Linjie Yang, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Video-Teller, a video-language foundation model that
leverages multi-modal fusion and fine-grained modality alignment to
significantly enhance the video-to-text generation task. Video-Teller boosts
the training efficiency by utilizing frozen pretrained vision and language
modules. It capitalizes on the robust linguistic capabilities of large language
models, enabling the generation of both concise and elaborate video
descriptions. To effectively integrate visual and auditory information,
Video-Teller builds upon the image-based BLIP-2 model and introduces a cascaded
Q-Former which fuses information across frames and ASR texts. To better guide
video summarization, we introduce a fine-grained modality alignment objective,
where the cascaded Q-Former's output embedding is trained to align with the
caption/summary embedding created by a pretrained text auto-encoder.
Experimental results demonstrate the efficacy of our proposed video-language
foundation model in accurately comprehending videos and generating coherent and
precise language descriptions. It is worth noting that the fine-grained
alignment enhances the model's capabilities (4% improvement of CIDEr score on
MSR-VTT) with only 13% extra parameters in training and zero additional cost in
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Semantics for Open Vocabulary Spatio-semantic
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Karlsson, Francisco Lepe-Salazar, Kazuya Takeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-purpose mobile robots need to complete tasks without exact human
instructions. Large language models (LLMs) is a promising direction for
realizing commonsense world knowledge and reasoning-based planning.
Vision-language models (VLMs) transform environment percepts into
vision-language semantics interpretable by LLMs. However, completing complex
tasks often requires reasoning about information beyond what is currently
perceived. We propose latent compositional semantic embeddings z* as a
principled learning-based knowledge representation for queryable
spatio-semantic memories. We mathematically prove that z* can always be found,
and the optimal z* is the centroid for any set Z. We derive a probabilistic
bound for estimating separability of related and unrelated semantics. We prove
that z* is discoverable by iterative optimization by gradient descent from
visual appearance and singular descriptions. We experimentally verify our
findings on four embedding spaces incl. CLIP and SBERT. Our results show that
z* can represent up to 10 semantics encoded by SBERT, and up to 100 semantics
for ideal uniformly distributed high-dimensional embeddings. We demonstrate
that a simple dense VLM trained on the COCO-Stuff dataset can learn z* for 181
overlapping semantics by 42.23 mIoU, while improving conventional
non-overlapping open-vocabulary segmentation performance by +3.48 mIoU compared
with a popular SOTA model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of
  Zoom and Spatial Biases in Image Classification <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05538v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05538v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Reza Taesiri, Giang Nguyen, Sarra Habchi, Cor-Paul Bezemer, Anh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image classifiers are information-discarding machines, by design. Yet, how
these models discard information remains mysterious. We hypothesize that one
way for image classifiers to reach high accuracy is to first zoom to the most
discriminative region in the image and then extract features from there to
predict image labels, discarding the rest of the image. Studying six popular
networks ranging from AlexNet to CLIP, we find that proper framing of the input
image can lead to the correct classification of 98.91% of ImageNet images.
Furthermore, we uncover positional biases in various datasets, especially a
strong center bias in two popular datasets: ImageNet-A and ObjectNet. Finally,
leveraging our insights into the potential of zooming, we propose a test-time
augmentation (TTA) technique that improves classification accuracy by forcing
models to explicitly perform zoom-in operations before making predictions. Our
method is more interpretable, accurate, and faster than MEMO, a
state-of-the-art (SOTA) TTA method. We introduce ImageNet-Hard, a new benchmark
that challenges SOTA classifiers including large vision-language models even
when optimal zooming is allowed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Road Obstacles by Erasing Them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.13633v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.13633v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krzysztof Lis, Sina Honari, Pascal Fua, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicles can encounter a myriad of obstacles on the road, and it is
impossible to record them all beforehand to train a detector. Instead, we
select image patches and inpaint them with the surrounding road texture, which
tends to remove obstacles from those patches. We then use a network trained to
recognize discrepancies between the original patch and the inpainted one, which
signals an erased obstacle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Out of Distribution Performance of State of Art Vision Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10750v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10750v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salman Rahman, Wonkwon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vision transformer (ViT) has advanced to the cutting edge in the visual
recognition task. Transformers are more robust than CNN, according to the
latest research. ViT's self-attention mechanism, according to the claim, makes
it more robust than CNN. Even with this, we discover that these conclusions are
based on unfair experimental conditions and just comparing a few models, which
did not allow us to depict the entire scenario of robustness performance. In
this study, we investigate the performance of 58 state-of-the-art computer
vision models in a unified training setup based not only on attention and
convolution mechanisms but also on neural networks based on a combination of
convolution and attention mechanisms, sequence-based model, complementary
search, and network-based method. Our research demonstrates that robustness
depends on the training setup and model types, and performance varies based on
out-of-distribution type. Our research will aid the community in better
understanding and benchmarking the robustness of computer vision models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>incomplete work - need to complete it</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Context, Less Distraction: Zero-shot Visual Classification by
  Inferring and Conditioning on Contextual Attributes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang An, Sicheng Zhu, Michael-Andrei Panaitescu-Liess, Chaithanya Kumar Mummadi, Furong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models like CLIP are widely used in zero-shot image
classification due to their ability to understand various visual concepts and
natural language descriptions. However, how to fully leverage CLIP's
unprecedented human-like understanding capabilities to achieve better
performance is still an open question. This paper draws inspiration from the
human visual perception process: when classifying an object, humans first infer
contextual attributes (e.g., background and orientation) which help separate
the foreground object from the background, and then classify the object based
on this information. Inspired by it, we observe that providing CLIP with
contextual attributes improves zero-shot image classification and mitigates
reliance on spurious features. We also observe that CLIP itself can reasonably
infer the attributes from an image. With these observations, we propose a
training-free, two-step zero-shot classification method PerceptionCLIP. Given
an image, it first infers contextual attributes (e.g., background) and then
performs object classification conditioning on them. Our experiments show that
PerceptionCLIP achieves better generalization, group robustness, and
interpretability. For example, PerceptionCLIP with ViT-L/14 improves the worst
group accuracy by 16.5% on the Waterbirds dataset and by 3.5% on CelebA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Circumventing Concept Erasure Methods For Text-to-Image Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Pham, Kelly O. Marshall, Niv Cohen, Govind Mittal, Chinmay Hegde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image generative models can produce photo-realistic images for an
extremely broad range of concepts, and their usage has proliferated widely
among the general public. On the flip side, these models have numerous
drawbacks, including their potential to generate images featuring sexually
explicit content, mirror artistic styles without permission, or even
hallucinate (or deepfake) the likenesses of celebrities. Consequently, various
methods have been proposed in order to "erase" sensitive concepts from
text-to-image models. In this work, we examine five recently proposed concept
erasure methods, and show that targeted concepts are not fully excised from any
of these methods. Specifically, we leverage the existence of special learned
word embeddings that can retrieve "erased" concepts from the sanitized models
with no alterations to their weights. Our results highlight the brittleness of
post hoc concept erasure methods, and call into question their use in the
algorithmic toolkit for AI safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable
  Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Zhuoran Li, Mingtong Zhang, Katherine Driggs-Campbell, Jiajun Wu, Li Fei-Fei, Yunzhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene representation has been a crucial design choice in robotic manipulation
systems. An ideal representation should be 3D, dynamic, and semantic to meet
the demands of diverse manipulation tasks. However, previous works often lack
all three properties simultaneously. In this work, we introduce D$^3$Fields -
dynamic 3D descriptor fields. These fields capture the dynamics of the
underlying 3D environment and encode both semantic features and instance masks.
Specifically, we project arbitrary 3D points in the workspace onto multi-view
2D visual observations and interpolate features derived from foundational
models. The resulting fused descriptor fields allow for flexible goal
specifications using 2D images with varied contexts, styles, and instances. To
evaluate the effectiveness of these descriptor fields, we apply our
representation to a wide range of robotic manipulation tasks in a zero-shot
manner. Through extensive evaluation in both real-world scenarios and
simulations, we demonstrate that D$^3$Fields are both generalizable and
effective for zero-shot robotic manipulation tasks. In quantitative comparisons
with state-of-the-art dense descriptors, such as Dense Object Nets and DINO,
D$^3$Fields exhibit significantly better generalization abilities and
manipulation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://robopil.github.io/d3fields/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment
  Anything Model <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02034v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02034v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Wang, Jing Zhang, Bo Du, Minqiang Xu, Lin Liu, Dacheng Tao, Liangpei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of the Segment Anything Model (SAM) demonstrates the significance
of data-centric machine learning. However, due to the difficulties and high
costs associated with annotating Remote Sensing (RS) images, a large amount of
valuable RS data remains unlabeled, particularly at the pixel level. In this
study, we leverage SAM and existing RS object detection datasets to develop an
efficient pipeline for generating a large-scale RS segmentation dataset, dubbed
SAMRS. SAMRS totally possesses 105,090 images and 1,668,241 instances,
surpassing existing high-resolution RS segmentation datasets in size by several
orders of magnitude. It provides object category, location, and instance
information that can be used for semantic segmentation, instance segmentation,
and object detection, either individually or in combination. We also provide a
comprehensive analysis of SAMRS from various aspects. Moreover, preliminary
experiments highlight the importance of conducting segmentation pre-training
with SAMRS to address task discrepancies and alleviate the limitations posed by
limited training data during fine-tuning. The code and dataset will be
available at https://github.com/ViTAE-Transformer/SAMRS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XVO: Generalized <span class="highlight-title">Visual Odometry</span> via Cross-Modal Self-Training <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16772v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16772v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Lai, Zhongkai Shangguan, Jimuyang Zhang, Eshed Ohn-Bar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose XVO, a semi-supervised learning method for training generalized
monocular Visual Odometry (VO) models with robust off-the-self operation across
diverse datasets and settings. In contrast to standard monocular VO approaches
which often study a known calibration within a single dataset, XVO efficiently
learns to recover relative pose with real-world scale from visual scene
semantics, i.e., without relying on any known camera parameters. We optimize
the motion estimation model via self-training from large amounts of
unconstrained and heterogeneous dash camera videos available on YouTube. Our
key contribution is twofold. First, we empirically demonstrate the benefits of
semi-supervised training for learning a general-purpose direct VO regression
network. Second, we demonstrate multi-modal supervision, including
segmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate
generalized representations for the VO task. Specifically, we find audio
prediction task to significantly enhance the semi-supervised learning process
while alleviating noisy pseudo-labels, particularly in highly dynamic and
out-of-domain video data. Our proposed teacher network achieves
state-of-the-art performance on the commonly used KITTI benchmark despite no
multi-frame optimization or knowledge of camera parameters. Combined with the
proposed semi-supervised step, XVO demonstrates off-the-shelf knowledge
transfer across diverse conditions on KITTI, nuScenes, and Argoverse without
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023, Paris https://genxvo.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SIAD: Self-supervised Image Anomaly Detection System <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Li, Chenxi Lan, Xinyi Zhang, Bolin Jiang, Yuqiu Xie, Naiqi Li, Yan Liu, Yaowei Li, Enze Huo, Bin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent trends in AIGC effectively boosted the application of visual
inspection. However, most of the available systems work in a human-in-the-loop
manner and can not provide long-term support to the online application. To make
a step forward, this paper outlines an automatic annotation system called SsaA,
working in a self-supervised learning manner, for continuously making the
online visual inspection in the manufacturing automation scenarios. Benefit
from the self-supervised learning, SsaA is effective to establish a visual
inspection application for the whole life-cycle of manufacturing. In the early
stage, with only the anomaly-free data, the unsupervised algorithms are adopted
to process the pretext task and generate coarse labels for the following data.
Then supervised algorithms are trained for the downstream task. With
user-friendly web-based interfaces, SsaA is very convenient to integrate and
deploy both of the unsupervised and supervised algorithms. So far, the SsaA
system has been adopted for some real-life industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, ICCV 2023 Demo Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictive Coding Based Multiscale Network with Encoder-Decoder LSTM for
  Video Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11642v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11642v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaofan Ling, Junpei Zhong, Weihua Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a multi-scale predictive coding model for future video frames
prediction. Drawing inspiration on the ``Predictive Coding" theories in
cognitive science, it is updated by a combination of bottom-up and top-down
information flows, which can enhance the interaction between different network
levels. However, traditional predictive coding models only predict what is
happening hierarchically rather than predicting the future. To address the
problem, our model employs a multi-scale approach (Coarse to Fine), where the
higher level neurons generate coarser predictions (lower resolution), while the
lower level generate finer predictions (higher resolution). In terms of network
architecture, we directly incorporate the encoder-decoder network within the
LSTM module and share the final encoded high-level semantic information across
different network levels. This enables comprehensive interaction between the
current input and the historical states of LSTM compared with the traditional
Encoder-LSTM-Decoder architecture, thus learning more believable temporal and
spatial dependencies. Furthermore, to tackle the instability in adversarial
training and mitigate the accumulation of prediction errors in long-term
prediction, we propose several improvements to the training strategy. Our
approach achieves good performance on datasets such as KTH, Moving MNIST and
Caltech Pedestrian. Code is available at https://github.com/Ling-CF/MSPN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating images of rare concepts using pre-trained diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, Gal Chechik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models can synthesize high-quality images, but they
have various limitations. Here we highlight a common failure mode of these
models, namely, generating uncommon concepts and structured concepts like hand
palms. We show that their limitation is partly due to the long-tail nature of
their training data: web-crawled data sets are strongly unbalanced, causing
models to under-represent concepts from the tail of the distribution. We
characterize the effect of unbalanced training data on text-to-image models and
offer a remedy. We show that rare concepts can be correctly generated by
carefully selecting suitable generation seeds in the noise space, using a small
reference set of images, a technique that we call SeedSelect. SeedSelect does
not require retraining or finetuning the diffusion model. We assess the
faithfulness, quality and diversity of SeedSelect in creating rare objects and
generating complex formations like hand images, and find it consistently
achieves superior performance. We further show the advantage of SeedSelect in
semantic data augmentation. Generating semantically appropriate images can
successfully improve performance in few-shot recognition benchmarks, for
classes from the head and from the tail of the training data of diffusion
models
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Susceptibility of Continual Learning Against Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05225v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05225v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hikmat Khan, Pir Masoom Shah, Syed Farhan Alam Zaidi, Saif ul Islam, Qasim Zia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent continual learning approaches have primarily focused on mitigating
catastrophic forgetting. Nevertheless, two critical areas have remained
relatively unexplored: 1) evaluating the robustness of proposed methods and 2)
ensuring the security of learned tasks. This paper investigates the
susceptibility of continually learned tasks, including current and previously
acquired tasks, to adversarial attacks. Specifically, we have observed that any
class belonging to any task can be easily targeted and misclassified as the
desired target class of any other task. Such susceptibility or vulnerability of
learned tasks to adversarial attacks raises profound concerns regarding data
integrity and privacy. To assess the robustness of continual learning
approaches, we consider continual learning approaches in all three scenarios,
i.e., task-incremental learning, domain-incremental learning, and
class-incremental learning. In this regard, we explore the robustness of three
regularization-based methods, three replay-based approaches, and one hybrid
technique that combines replay and exemplar approaches. We empirically
demonstrated that in any setting of continual learning, any class, whether
belonging to the current or previously learned tasks, is susceptible to
misclassification. Our observations identify potential limitations of continual
learning approaches against adversarial attacks and highlight that current
continual learning algorithms could not be suitable for deployment in
real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IFT: Image Fusion Transformer for Ghost-free High Dynamic Range Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailing Wang, Wei Li, Yuanyuan Xi, Jie Hu, Hanting Chen, Longyu Li, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-frame high dynamic range (HDR) imaging aims to reconstruct ghost-free
images with photo-realistic details from content-complementary but spatially
misaligned low dynamic range (LDR) images. Existing HDR algorithms are prone to
producing ghosting artifacts as their methods fail to capture long-range
dependencies between LDR frames with large motion in dynamic scenes. To address
this issue, we propose a novel image fusion transformer, referred to as IFT,
which presents a fast global patch searching (FGPS) module followed by a
self-cross fusion module (SCF) for ghost-free HDR imaging. The FGPS searches
the patches from supporting frames that have the closest dependency to each
patch of the reference frame for long-range dependency modeling, while the SCF
conducts intra-frame and inter-frame feature fusion on the patches obtained by
the FGPS with linear complexity to input resolution. By matching similar
patches between frames, objects with large motion ranges in dynamic scenes can
be aligned, which can effectively alleviate the generation of artifacts. In
addition, the proposed FGPS and SCF can be integrated into various deep HDR
methods as efficient plug-in modules. Extensive experiments on multiple
benchmarks show that our method achieves state-of-the-art performance both
quantitatively and qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DriveGPT4: Interpretable End-to-end Autonomous Driving via Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K. Wong, Zhenguo Li, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past decade, autonomous driving has experienced rapid development in
both academia and industry. However, its limited interpretability remains a
significant unsolved problem, severely hindering autonomous vehicle
commercialization and further development. Previous approaches utilizing small
language models have failed to address this issue due to their lack of
flexibility, generalization ability, and robustness. Recently, multimodal large
language models (LLMs) have gained considerable attention from the research
community for their capability to process and reason non-text data (e.g.,
images and videos) by text. In this paper, we present DriveGPT4, an
interpretable end-to-end autonomous driving system utilizing LLMs. DriveGPT4 is
capable of interpreting vehicle actions and providing corresponding reasoning,
as well as answering diverse questions posed by human users for enhanced
interaction. Additionally, DriveGPT4 predicts vehicle low-level control signals
in an end-to-end fashion. These capabilities stem from a customized visual
instruction tuning dataset specifically designed for autonomous driving. To the
best of our knowledge, DriveGPT4 is the first work focusing on interpretable
end-to-end autonomous driving. When evaluated on multiple tasks alongside
conventional methods and video understanding LLMs, DriveGPT4 demonstrates
superior qualitative and quantitative performance. Additionally, DriveGPT4 can
be generalized in a zero-shot fashion to accommodate more unseen scenarios. The
project page is available at https://tonyxuqaq.github.io/projects/DriveGPT4/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project page is available at
  https://tonyxuqaq.github.io/projects/DriveGPT4/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Text-to-Image Attention Prior for Reference-based Multi-view
  Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenjie Cao, Yunuo Cai, Qiaole Dong, Yikai Wang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the domain of multi-view image synthesis, aiming to
create specific image elements or entire scenes while ensuring visual
consistency with reference images. We categorize this task into two approaches:
local synthesis, guided by structural cues from reference images
(Reference-based inpainting, Ref-inpainting), and global synthesis, which
generates entirely new images based solely on reference examples (Novel View
Synthesis, NVS). In recent years, Text-to-Image (T2I) generative models have
gained attention in various domains. However, adapting them for multi-view
synthesis is challenging due to the intricate correlations between reference
and target images. To address these challenges efficiently, we introduce
Attention Reactivated Contextual Inpainting (ARCI), a unified approach that
reformulates both local and global reference-based multi-view synthesis as
contextual inpainting, which is enhanced with pre-existing attention mechanisms
in T2I models. Formally, self-attention is leveraged to learn feature
correlations across different reference views, while cross-attention is
utilized to control the generation through prompt tuning. Our contributions of
ARCI, built upon the StableDiffusion fine-tuned for text-guided inpainting,
include skillfully handling difficult multi-view synthesis tasks with
off-the-shelf T2I models, introducing task and view-specific prompt tuning for
generative control, achieving end-to-end Ref-inpainting, and implementing block
causal masking for autoregressive NVS. We also show the versatility of ARCI by
extending it to multi-view generation for superior consistency with the same
architecture, which has also been validated through extensive experiments.
Codes and models will be released in \url{https://github.com/ewrfcas/ARCI}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We further improved our methods for multi-view synthesis. The project
  page is https://ewrfcas.github.io/ARCI/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InsightMapper: A Closer Look at Inner-instance Information for
  Vectorized High-Definition Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhua Xu, Kwan-Yee. K. Wong, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vectorized high-definition (HD) maps contain detailed information about
surrounding road elements, which are crucial for various downstream tasks in
modern autonomous driving vehicles, such as vehicle planning and control.
Recent works have attempted to directly detect the vectorized HD map as a point
set prediction task, resulting in significant improvements in detection
performance. However, these approaches fail to analyze and exploit the
inner-instance correlations between predicted points, impeding further
advancements. To address these challenges, we investigate the utilization of
inner-$\textbf{INS}$tance information for vectorized h$\textbf{IGH}$-definition
mapping through $\textbf{T}$ransformers and introduce InsightMapper. This paper
presents three novel designs within InsightMapper that leverage inner-instance
information in distinct ways, including hybrid query generation, inner-instance
query fusion, and inner-instance feature aggregation. Comparative experiments
are conducted on the NuScenes dataset, showcasing the superiority of our
proposed method. InsightMapper surpasses previous state-of-the-art (SOTA)
methods by 5.78 mAP and 5.12 TOPO, which assess topology correctness.
Simultaneously, InsightMapper maintains high efficiency during both training
and inference phases, resulting in remarkable comprehensive performance. The
project page for this work is available at
https://tonyxuqaq.github.io/InsightMapper/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and demo will be available at
  https://tonyxuqaq.github.io/InsightMapper/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SARNet: Semantic Augmented Registration of Large-Scale Urban Point
  Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Liu, Jianwei Guo, Dong-Ming Yan, Zhirong Liang, Xiaopeng Zhang, Zhanglin Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Registering urban point clouds is a quite challenging task due to the
large-scale, noise and data incompleteness of LiDAR scanning data. In this
paper, we propose SARNet, a novel semantic augmented registration network aimed
at achieving efficient registration of urban point clouds at city scale.
Different from previous methods that construct correspondences only in the
point-level space, our approach fully exploits semantic features as assistance
to improve registration accuracy. Specifically, we extract per-point semantic
labels with advanced semantic segmentation networks and build a prior semantic
part-to-part correspondence. Then we incorporate the semantic information into
a learning-based registration pipeline, consisting of three core modules: a
semantic-based farthest point sampling module to efficiently filter out
outliers and dynamic objects; a semantic-augmented feature extraction module
for learning more discriminative point descriptors; a semantic-refined
transformation estimation module that utilizes prior semantic matching as a
mask to refine point correspondences by reducing false matching for better
convergence. We evaluate the proposed SARNet extensively by using real-world
data from large regions of urban scenes and comparing it with alternative
methods. The code is available at
https://github.com/WinterCodeForEverything/SARNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Author information changes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompt-based test-time real image dehazing: a novel pipeline 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17389v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17389v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Chen, Zewei He, Ziqian Lu, Zhe-Ming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods attempt to improve models' generalization ability on
real-world hazy images by exploring well-designed training schemes (e.g.,
cycleGAN, prior loss). However, most of them need very complicated training
procedures to achieve satisfactory results. In this work, we present a totally
novel testing pipeline called Prompt-based Test-Time Dehazing (PTTD) to help
generate visually pleasing results of real-captured hazy images during the
inference phase. We experimentally find that given a dehazing model trained on
synthetic data, by fine-tuning the statistics (i.e., mean and standard
deviation) of encoding features, PTTD is able to narrow the domain gap,
boosting the performance of real image dehazing. Accordingly, we first apply a
prompt generation module (PGM) to generate a visual prompt, which is the source
of appropriate statistical perturbations for mean and standard deviation. And
then, we employ the feature adaptation module (FAM) into the existing dehazing
models for adjusting the original statistics with the guidance of the generated
prompt. Note that, PTTD is model-agnostic and can be equipped with various
state-of-the-art dehazing models trained on synthetic hazy-clean pairs.
Extensive experimental results demonstrate that our PTTD is flexible meanwhile
achieves superior performance against state-of-the-art dehazing methods in
real-world scenarios. The source code of our PTTD will be made available at
https://github.com/cecret3350/PTTD-Dehazing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update github link (https://github.com/cecret3350/PTTD-Dehazing)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear-Covariance Loss for End-to-End Learning of 6D Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fulin Liu, Yinlin Hu, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most modern image-based 6D object pose estimation methods learn to predict
2D-3D correspondences, from which the pose can be obtained using a PnP solver.
Because of the non-differentiable nature of common PnP solvers, these methods
are supervised via the individual correspondences. To address this, several
methods have designed differentiable PnP strategies, thus imposing supervision
on the pose obtained after the PnP step. Here, we argue that this conflicts
with the averaging nature of the PnP problem, leading to gradients that may
encourage the network to degrade the accuracy of individual correspondences. To
address this, we derive a loss function that exploits the ground truth pose
before solving the PnP problem. Specifically, we linearize the PnP solver
around the ground-truth pose and compute the covariance of the resulting pose
distribution. We then define our loss based on the diagonal covariance
elements, which entails considering the final pose estimate yet not suffering
from the PnP averaging issue. Our experiments show that our loss consistently
improves the pose estimation accuracy for both dense and sparse correspondence
based methods, achieving state-of-the-art results on both Linemod-Occluded and
YCB-Video.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Queried Object Detection in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Xu, Mengdan Zhang, Chaoyou Fu, Peixian Chen, Xiaoshan Yang, Ke Li, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MQ-Det, an efficient architecture and pre-training strategy
design to utilize both textual description with open-set generalization and
visual exemplars with rich description granularity as category queries, namely,
Multi-modal Queried object Detection, for real-world detection with both
open-vocabulary categories and various granularity. MQ-Det incorporates vision
queries into existing well-established language-queried-only detectors. A
plug-and-play gated class-scalable perceiver module upon the frozen detector is
proposed to augment category text with class-wise visual information. To
address the learning inertia problem brought by the frozen detector, a vision
conditioned masked language prediction strategy is proposed. MQ-Det's simple
yet effective architecture and training strategy design is compatible with most
language-queried object detectors, thus yielding versatile applications.
Experimental results demonstrate that multi-modal queries largely boost
open-world detection. For instance, MQ-Det significantly improves the
state-of-the-art open-set detector GLIP by +7.8% AP on the LVIS benchmark via
multi-modal queries without any downstream finetuning, and averagely +6.3% AP
on 13 few-shot downstream tasks, with merely additional 3% modulating time
required by GLIP. Code is available at https://github.com/YifanXu74/MQ-Det.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Txt2Img-MHN: Remote Sensing Image Generation from Text Using Modern
  Hopfield Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao Xu, Weikang Yu, Pedram Ghamisi, Michael Kopp, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The synthesis of high-resolution remote sensing images based on text
descriptions has great potential in many practical application scenarios.
Although deep neural networks have achieved great success in many important
remote sensing tasks, generating realistic remote sensing images from text
descriptions is still very difficult. To address this challenge, we propose a
novel text-to-image modern Hopfield network (Txt2Img-MHN). The main idea of
Txt2Img-MHN is to conduct hierarchical prototype learning on both text and
image embeddings with modern Hopfield layers. Instead of directly learning
concrete but highly diverse text-image joint feature representations for
different semantics, Txt2Img-MHN aims to learn the most representative
prototypes from text-image embeddings, achieving a coarse-to-fine learning
strategy. These learned prototypes can then be utilized to represent more
complex semantics in the text-to-image generation task. To better evaluate the
realism and semantic consistency of the generated images, we further conduct
zero-shot classification on real remote sensing data using the classification
model trained on synthesized images. Despite its simplicity, we find that the
overall accuracy in the zero-shot classification may serve as a good metric to
evaluate the ability to generate an image from text. Extensive experiments on
the benchmark remote sensing text-image dataset demonstrate that the proposed
Txt2Img-MHN can generate more realistic remote sensing images than existing
methods. Code and pre-trained models are available online
(https://github.com/YonghaoXu/Txt2Img-MHN).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Valley: Video Assistant with Large Language model Enhanced abilitY 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, Zhongyu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), with their remarkable conversational
capabilities, have demonstrated impressive performance across various
applications and have emerged as formidable AI assistants. In view of this, it
raises an intuitive question: Can we harness the power of LLMs to build
multimodal AI assistants for visual applications? Recently, several multi-modal
models have been developed for this purpose. They typically pre-train an
adaptation module to align the semantics of the vision encoder and language
model, followed by fine-tuning on instruction-following data. However, despite
the success of this pipeline in image and language understanding, its
effectiveness in joint video and language understanding has not been widely
explored. In this paper, we aim to develop a novel multi-modal foundation model
capable of comprehending video, image, and language within a general framework.
To achieve this goal, we introduce Valley, a Video Assistant with Large
Language model Enhanced abilitY. The Valley consists of a LLM, a temporal
modeling module, a visual encoder, and a simple projection module designed to
bridge visual and textual modes. To empower Valley with video comprehension and
instruction-following capabilities, we construct a video instruction dataset
and adopt a two-stage tuning procedure to train it. Specifically, we employ
ChatGPT to facilitate the construction of task-oriented conversation data
encompassing various tasks, including multi-shot captions, long video
descriptions, action recognition, causal relationship inference, etc.
Subsequently, we adopt a pre-training-then-instructions-tuned pipeline to align
visual and textual modalities and improve the instruction-following capability
of Valley. Qualitative experiments demonstrate that Valley has the potential to
function as a highly effective video assistant that can make complex video
understanding scenarios easy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09423v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09423v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yang, Hong Shang, Tianyang Shi, Xinghan Chen, Jingkai Zhou, Zhongqian Sun, Wei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research fields of parametric face model and 3D face reconstruction have
been extensively studied. However, a critical question remains unanswered: how
to tailor the face model for specific reconstruction settings. We argue that
reconstruction with multi-view uncalibrated images demands a new model with
stronger capacity. Our study shifts attention from data-dependent 3D Morphable
Models (3DMM) to an understudied human-designed skinning model. We propose
Adaptive Skinning Model (ASM), which redefines the skinning model with more
compact and fully tunable parameters. With extensive experiments, we
demonstrate that ASM achieves significantly improved capacity than 3DMM, with
the additional advantage of model size and easy implementation for new
topology. We achieve state-of-the-art performance with ASM for multi-view
reconstruction on the Florence MICC Coop benchmark. Our quantitative analysis
demonstrates the importance of a high-capacity model for fully exploiting
abundant information from multi-view input in reconstruction. Furthermore, our
model with physical-semantic parameters can be directly utilized for real-world
applications, such as in-game avatar creation. As a result, our work opens up
new research direction for parametric face model and facilitates future
research on multi-view reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning A Coarse-to-Fine Diffusion Transformer for Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08730v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08730v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyan Wang, Qinyu Yang, Cong Wang, Wei Wang, Jinshan Pan, Zhixun Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the remarkable performance of diffusion models in
various vision tasks. However, for image restoration that aims to recover clear
images with sharper details from given degraded observations, diffusion-based
methods may fail to recover promising results due to inaccurate noise
estimation. Moreover, simple constraining noises cannot effectively learn
complex degradation information, which subsequently hinders the model capacity.
To solve the above problems, we propose a coarse-to-fine diffusion Transformer
(C2F-DFT) for image restoration. Specifically, our C2F-DFT contains diffusion
self-attention (DFSA) and diffusion feed-forward network (DFN) within a new
coarse-to-fine training scheme. The DFSA and DFN respectively capture the
long-range diffusion dependencies and learn hierarchy diffusion representation
to facilitate better restoration. In the coarse training stage, our C2F-DFT
estimates noises and then generates the final clean image by a sampling
algorithm. To further improve the restoration quality, we propose a simple yet
effective fine training scheme. It first exploits the coarse-trained diffusion
model with fixed steps to generate restoration results, which then would be
constrained with corresponding ground-truth ones to optimize the models to
remedy the unsatisfactory results affected by inaccurate noise estimation.
Extensive experiments show that C2F-DFT significantly outperforms
diffusion-based restoration method IR-SDE and achieves competitive performance
compared with Transformer-based state-of-the-art methods on $3$ tasks,
including image deraining, image deblurring, and real image denoising. Code is
available at https://github.com/wlydlut/C2F-DFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ U-shaped Transformer: Retain High Frequency Context in Time Series
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingkui Chen, Yiqin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series prediction plays a crucial role in various industrial fields. In
recent years, neural networks with a transformer backbone have achieved
remarkable success in many domains, including computer vision and NLP. In time
series analysis domain, some studies have suggested that even the simplest MLP
networks outperform advanced transformer-based networks on time series forecast
tasks. However, we believe these findings indicate there to be low-rank
properties in time series sequences. In this paper, we consider the low-pass
characteristics of transformers and try to incorporate the advantages of MLP.
We adopt skip-layer connections inspired by Unet into traditional transformer
backbone, thus preserving high-frequency context from input to output, namely
U-shaped Transformer. We introduce patch merge and split operation to extract
features with different scales and use larger datasets to fully make use of the
transformer backbone. Our experiments demonstrate that the model performs at an
advanced level across multiple datasets with relatively low cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.00311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.00311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyuan Lyu, Chengquan Zhang, Shanshan Liu, Meina Qiao, Yangliu Xu, Liang Wu, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text images contain both visual and linguistic information. However, existing
pre-training techniques for text recognition mainly focus on either visual
representation learning or linguistic knowledge learning. In this paper, we
propose a novel approach MaskOCR to unify vision and language pre-training in
the classical encoder-decoder recognition framework. We adopt the masked image
modeling approach to pre-train the feature encoder using a large set of
unlabeled real text images, which allows us to learn strong visual
representations. In contrast to introducing linguistic knowledge with an
additional language model, we directly pre-train the sequence decoder.
Specifically, we transform text data into synthesized text images to unify the
data modalities of vision and language, and enhance the language modeling
capability of the sequence decoder using a proposed masked image-language
modeling scheme. Significantly, the encoder is frozen during the pre-training
phase of the sequence decoder. Experimental results demonstrate that our
proposed method achieves superior performance on benchmark datasets, including
Chinese and English text images. The code for our approach will be made
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using
  Transformer-based Neural Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhang Liu, Yan Zeng, Yifan Qin, Hao Li, Jiakai Zhang, Lan Xu, Jingyi Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryo-electron microscopy (cryo-EM) allows for the high-resolution
reconstruction of 3D structures of proteins and other biomolecules. Successful
reconstruction of both shape and movement greatly helps understand the
fundamental processes of life. However, it is still challenging to reconstruct
the continuous motions of 3D structures from hundreds of thousands of noisy and
randomly oriented 2D cryo-EM images. Recent advancements use Fourier domain
coordinate-based neural networks to continuously model 3D conformations, yet
they often struggle to capture local flexible regions accurately. We propose
CryoFormer, a new approach for continuous heterogeneous cryo-EM reconstruction.
Our approach leverages an implicit feature volume directly in the real domain
as the 3D representation. We further introduce a novel query-based deformation
transformer decoder to improve the reconstruction quality. Our approach is
capable of refining pre-computed pose estimations and locating flexible
regions. In experiments, our method outperforms current approaches on three
public datasets (1 synthetic and 2 experimental) and a new synthetic dataset of
PEDV spike protein. The code and new synthetic dataset will be released for
better reproducibility of our results. Project page:
https://cryoformer.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lookaround Optimizer: $k$ steps around, 1 step average 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangtao Zhang, Shunyu Liu, Jie Song, Tongtian Zhu, Zhengqi Xu, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weight Average (WA) is an active research topic due to its simplicity in
ensembling deep networks and the effectiveness in promoting generalization.
Existing weight average approaches, however, are often carried out along only
one training trajectory in a post-hoc manner (i.e., the weights are averaged
after the entire training process is finished), which significantly degrades
the diversity between networks and thus impairs the effectiveness. In this
paper, inspired by weight average, we propose Lookaround, a straightforward yet
effective SGD-based optimizer leading to flatter minima with better
generalization. Specifically, Lookaround iterates two steps during the whole
training period: the around step and the average step. In each iteration, 1)
the around step starts from a common point and trains multiple networks
simultaneously, each on transformed data by a different data augmentation, and
2) the average step averages these trained networks to get the averaged
network, which serves as the starting point for the next iteration. The around
step improves the functionality diversity while the average step guarantees the
weight locality of these networks during the whole training, which is essential
for WA to work. We theoretically explain the superiority of Lookaround by
convergence analysis, and make extensive experiments to evaluate Lookaround on
popular benchmarks including CIFAR and ImageNet with both CNNs and ViTs,
demonstrating clear superiority over state-of-the-arts. Our code is available
at https://github.com/Ardcy/Lookaround.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Image Residual Learning for Scaling Deeper Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxi Huang, Hongtao Fu, Adrian G. Bors
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deeper Vision Transformers (ViTs) are more challenging to train. We expose a
degradation problem in deeper layers of ViT when using masked image modeling
(MIM) for pre-training. To ease the training of deeper ViTs, we introduce a
self-supervised learning framework called Masked Image Residual Learning
(MIRL), which significantly alleviates the degradation problem, making scaling
ViT along depth a promising direction for performance upgrade. We reformulate
the pre-training objective for deeper layers of ViT as learning to recover the
residual of the masked image. We provide extensive empirical evidence showing
that deeper ViTs can be effectively optimized using MIRL and easily gain
accuracy from increased depth. With the same level of computational complexity
as ViT-Base and ViT-Large, we instantiate 4.5$\times$ and 2$\times$ deeper
ViTs, dubbed ViT-S-54 and ViT-B-48. The deeper ViT-S-54, costing 3$\times$ less
than ViT-Large, achieves performance on par with ViT-Large. ViT-B-48 achieves
86.2% top-1 accuracy on ImageNet. On one hand, deeper ViTs pre-trained with
MIRL exhibit excellent generalization capabilities on downstream tasks, such as
object detection and semantic segmentation. On the other hand, MIRL
demonstrates high pre-training efficiency. With less pre-training time, MIRL
yields competitive performance compared to other approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature-Suppressed Contrast for Self-Supervised Food Pre-training <span class="chip">ACM MM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03272v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03272v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinda Liu, Yaohui Zhu, Linhu Liu, Jiang Tian, Lili Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most previous approaches for analyzing food images have relied on extensively
annotated datasets, resulting in significant human labeling expenses due to the
varied and intricate nature of such images. Inspired by the effectiveness of
contrastive self-supervised methods in utilizing unlabelled data, we explore
leveraging these techniques on unlabelled food images. In contrastive
self-supervised methods, two views are randomly generated from an image by data
augmentations. However, regarding food images, the two views tend to contain
similar informative contents, causing large mutual information, which impedes
the efficacy of contrastive self-supervised learning. To address this problem,
we propose Feature Suppressed Contrast (FeaSC) to reduce mutual information
between views. As the similar contents of the two views are salient or highly
responsive in the feature map, the proposed FeaSC uses a response-aware scheme
to localize salient features in an unsupervised manner. By suppressing some
salient features in one view while leaving another contrast view unchanged, the
mutual information between the two views is reduced, thereby enhancing the
effectiveness of contrast learning for self-supervised food pre-training. As a
plug-and-play module, the proposed method consistently improves BYOL and
SimSiam by 1.70\% $\sim$ 6.69\% classification accuracy on four publicly
available food recognition datasets. Superior results have also been achieved
on downstream segmentation tasks, demonstrating the effectiveness of the
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Role of Neural Collapse in Meta Learning Models for Few-shot
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saaketh Medepalli, Naren Doraiswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-learning frameworks for few-shot learning aims to learn models that can
learn new skills or adapt to new environments rapidly with a few training
examples. This has led to the generalizability of the developed model towards
new classes with just a few labelled samples. However these networks are seen
as black-box models and understanding the representations learnt under
different learning scenarios is crucial. Neural collapse ($\mathcal{NC}$) is a
recently discovered phenomenon which showcases unique properties at the network
proceeds towards zero loss. The input features collapse to their respective
class means, the class means form a Simplex equiangular tight frame (ETF) where
the class means are maximally distant and linearly separable, and the
classifier acts as a simple nearest neighbor classifier. While these phenomena
have been observed in simple classification networks, this study is the first
to explore and understand the properties of neural collapse in meta learning
frameworks for few-shot learning. We perform studies on the Omniglot dataset in
the few-shot setting and study the neural collapse phenomenon. We observe that
the learnt features indeed have the trend of neural collapse, especially as
model size grows, but to do not necessarily showcase the complete collapse as
measured by the $\mathcal{NC}$ properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Domain-Aware Detection Head with Prompt Tuning <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Li, Rui Zhang, Hantao Yao, Xinkai Song, Yifan Hao, Yongwei Zhao, Ling Li, Yunji Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptive object detection (DAOD) aims to generalize detectors trained
on an annotated source domain to an unlabelled target domain. However, existing
methods focus on reducing the domain bias of the detection backbone by
inferring a discriminative visual encoder, while ignoring the domain bias in
the detection head. Inspired by the high generalization of vision-language
models (VLMs), applying a VLM as the robust detection backbone following a
domain-aware detection head is a reasonable way to learn the discriminative
detector for each domain, rather than reducing the domain bias in traditional
methods. To achieve the above issue, we thus propose a novel DAOD framework
named Domain-Aware detection head with Prompt tuning (DA-Pro), which applies
the learnable domain-adaptive prompt to generate the dynamic detection head for
each domain. Formally, the domain-adaptive prompt consists of the
domain-invariant tokens, domain-specific tokens, and the domain-related textual
description along with the class label. Furthermore, two constraints between
the source and target domains are applied to ensure that the domain-adaptive
prompt can capture the domains-shared and domain-specific knowledge. A prompt
ensemble strategy is also proposed to reduce the effect of prompt disturbance.
Comprehensive experiments over multiple cross-domain adaptation tasks
demonstrate that using the domain-adaptive prompt can produce an effectively
domain-related detection head for boosting domain-adaptive object detection.
Our code is available at https://github.com/Therock90421/DA-Pro.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constrained Bundle Adjustment for Structure From Motion Using
  Uncalibrated Multi-Camera Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.04145v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.04145v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debao Huang, Mostafa Elhashash, Rongjun Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structure from motion using uncalibrated multi-camera systems is a
challenging task. This paper proposes a bundle adjustment solution that
implements a baseline constraint respecting that these cameras are static to
each other. We assume these cameras are mounted on a mobile platform,
uncalibrated, and coarsely synchronized. To this end, we propose the baseline
constraint that is formulated for the scenario in which the cameras have
overlapping views. The constraint is incorporated in the bundle adjustment
solution to keep the relative motion of different cameras static. Experiments
were conducted using video frames of two collocated GoPro cameras mounted on a
vehicle with no system calibration. These two cameras were placed capturing
overlapping contents. We performed our bundle adjustment using the proposed
constraint and then produced 3D dense point clouds. Evaluations were performed
by comparing these dense point clouds against LiDAR reference data. We showed
that, as compared to traditional bundle adjustment, our proposed method
achieved an improvement of 29.38%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in ISPRS Congress 2022</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-07T00:00:00Z">2023-10-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via
  Differentiable Physics Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Xiang, Feitong Chen, Qinsi Wang, Yang Gang, Xiang Zhang, Xinghao Zhu, Xingyu Liu, Lin Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability to transfer mastered skills to accomplish a range of similar
yet novel tasks is crucial for intelligent robots. In this work, we introduce
$\textit{Diff-Transfer}$, a novel framework leveraging differentiable physics
simulation to efficiently transfer robotic skills. Specifically,
$\textit{Diff-Transfer}$ discovers a feasible path within the task space that
brings the source task to the target task. At each pair of adjacent points
along this task path, which is two sub-tasks, $\textit{Diff-Transfer}$ adapts
known actions from one sub-task to tackle the other sub-task successfully. The
adaptation is guided by the gradient information from differentiable physics
simulations. We propose a novel path-planning method to generate sub-tasks,
leveraging $Q$-learning with a task-level state and reward. We implement our
framework in simulation experiments and execute four challenging transfer tasks
on robotic manipulation, demonstrating the efficacy of $\textit{Diff-Transfer}$
through comprehensive experiments. Supplementary and Videos are on the website
$~\href{https://sites.google.com/view/difftransfer}{https://sites.google.com/view/difftransfer}$
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AirIMU: Learning Uncertainty Propagation for Inertial Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Qiu, Chen Wang, Xunfei Zhou, Youjie Xia, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty estimation for inertial odometry is the foundation to
achieve optimal fusion in multi-sensor systems, such as visual or LiDAR
inertial odometry. Prior studies often simplify the assumptions regarding the
uncertainty of inertial measurements, presuming fixed covariance parameters and
empirical IMU sensor models. However, the inherent physical limitations and
non-linear characteristics of sensors are difficult to capture. Moreover,
uncertainty may fluctuate based on sensor rates and motion modalities, leading
to variations across different IMUs. To address these challenges, we formulate
a learning-based method that not only encapsulate the non-linearities inherent
to IMUs but also ensure the accurate propagation of covariance in a data-driven
manner. We extend the PyPose library to enable differentiable batched IMU
integration with covariance propagation on manifolds, leading to significant
runtime speedup. To demonstrate our method's adaptability, we evaluate it on
several benchmarks as well as a large-scale helicopter dataset spanning over
262 kilometers. The drift rate of the inertial odometry on these datasets is
reduced by a factor of between 2.2 and 4 times. Our method lays the groundwork
for advanced developments in inertial odometry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft finger dynamic stability and slip by Coulomb friction and bulk
  stiffness <span class="chip">ICRA24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hun Jang, Valentyn Petrichenko, Joonbum Bae, Kevin Haninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft robotic fingers can safely grasp fragile or non-uniform objects, but
their force capacity is limited, especially with less contact area: objects
which are smaller, not round, or where an enclosing grasp is not feasible. To
improve force capacity, this paper considers two types of grip failure, slip
and dynamic rotational stability. For slip, a Coulomb model for soft fingers
based on total normal and tangential force is validated, identifying the effect
of contact area, pressure, and grip position on effective Coulomb coefficient,
normal force and transverse stiffness. For rotational stability, bulk stiffness
of the fingers is used to develop conditions for dynamic stability about the
initial grasp, and a condition for when the rotation leads to slip. Together,
these models suggest contact area improves grip by increasing transverse
stiffness and normal force. The models are validated in a range of grasp
conditions, shown to predict the influence of object radius and finger distance
on grip stability limits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted ICRA24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guardians as You Fall: Active Mode Transition for Safe Falling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikai Wang, Mengdi Xu, Guanya Shi, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in optimal control and reinforcement learning have
enabled quadrupedal robots to perform various agile locomotion tasks over
diverse terrains. During these agile motions, ensuring the stability and
resiliency of the robot is a primary concern to prevent catastrophic falls and
mitigate potential damages. Previous methods primarily focus on recovery
policies after the robot falls. There is no active safe falling solution to the
best of our knowledge. In this paper, we proposed Guardians as You Fall (GYF),
a safe falling/tumbling and recovery framework that can actively tumble and
recover to stable modes to reduce damage in highly dynamic scenarios. The key
idea of GYF is to adaptively traverse different stable modes via active
tumbling before the robot shifts to irrecoverable poses. Via comprehensive
simulation and real-world experiments, we show that GYF significantly reduces
the maximum acceleration and jerk of the robot base compared to the baselines.
In particular, GYF reduces the maximum acceleration and jerk by 20%~73% in
different scenarios in simulation and real-world experiments. GYF offers a new
perspective on safe falling and recovery in locomotion tasks, potentially
enabling much more aggressive explorations of existing agile locomotion skills.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>website: https://sites.google.com/view/guardians-as-you-fall/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Sampling- and Gradient-based Planning for Contact-rich
  Manipulation <span class="chip">ICRA24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Rozzi, Loris Roveda, Kevin Haninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning over discontinuous dynamics is needed for robotics tasks like
contact-rich manipulation, which presents challenges in the numerical stability
and speed of planning methods when either neural network or analytical models
are used. On the one hand, sampling-based planners require higher sample
complexity in high-dimensional problems and cannot describe safety constraints
such as force limits. On the other hand, gradient-based solvers can suffer from
local optima and convergence issues when the Hessian is poorly conditioned. We
propose a planning method with both sampling- and gradient-based elements,
using the Cross-entropy Method to initialize a gradient-based solver, providing
better search over local minima and the ability to handle explicit constraints.
We show the approach allows smooth, stable contact-rich planning for an
impedance-controlled robot making contact with a stiff environment,
benchmarking against gradient-only MPC and CEM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted ICRA24. Video available at https://youtu.be/COqR90392Kw
  Code available at https://gitlab.cc-asp.fraunhofer.de/hanikevi/contact_mpc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Unsupervised Topological <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Sharma, Yash Mehan, Pradyumna Dasu, Sourav Garg, Madhava Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a novel framework for unsupervised topological
clustering resulting in improved loop. In this paper we present a novel
framework for unsupervised topological clustering resulting in improved loop
detection and closure for SLAM. A navigating mobile robot clusters its
traversal into visually similar topologies where each cluster (topology)
contains a set of similar looking images typically observed from spatially
adjacent locations. Each such set of spatially adjacent and visually similar
grouping of images constitutes a topology obtained without any supervision. We
formulate a hierarchical loop discovery strategy that first detects loops at
the level of topologies and subsequently at the level of images between the
looped topologies. We show over a number of traversals across different Habitat
environments that such a hierarchical pipeline significantly improves SOTA
image based loop detection and closure methods. Further, as a consequence of
improved loop detection, we enhance the loop closure and backend SLAM
performance. Such a rendering of a traversal into topological segments is
beneficial for downstream tasks such as navigation that can now build a
topological graph where spatially adjacent topological clusters are connected
by an edge and navigate over such topological graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE ITSC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully Sparse Long Range 3D Object Detection Using Range Experts and
  Multimodal Virtual Points 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajinkya Khoche, Laura Pereira Sánchez, Nazre Batool, Sina Sharif Mansouri, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detection at long-range is crucial for ensuring the safety and
efficiency of self-driving cars, allowing them to accurately perceive and react
to objects, obstacles, and potential hazards from a distance. But most current
state-of-the-art LiDAR based methods are limited by the sparsity of range
sensors, which generates a form of domain gap between points closer to and
farther away from the ego vehicle. Another related problem is the label
imbalance for faraway objects, which inhibits the performance of Deep Neural
Networks at long-range. Although image features could be beneficial for
long-range detections, and some recently proposed multimodal methods
incorporate image features, they do not scale well computationally at long
ranges or are limited by depth estimation accuracy. To address the above
limitations, we propose to combine two LiDAR based 3D detection networks, one
specializing at near to mid-range objects, and one at long-range 3D detection.
To train a detector at long range under a scarce label regime, we further
propose to weigh the loss according to the labelled objects' distance from ego
vehicle. To mitigate the LiDAR sparsity issue, we leverage Multimodal Virtual
Points (MVP), an image based depth completion algorithm, to enrich our data
with virtual points. Our method, combining two range experts trained with MVP,
which we refer to as RangeFSD, achieves state-of-the-art performance on the
Argoverse2 (AV2) dataset, with improvements at long range. The code will be
released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HI-<span class="highlight-title">SLAM</span>: Monocular Real-time Dense Mapping with Hybrid Implicit Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhang, Tiecheng Sun, Sen Wang, Qing Cheng, Norbert Haala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we present a neural field-based real-time monocular mapping
framework for accurate and dense Simultaneous Localization and Mapping (SLAM).
Recent neural mapping frameworks show promising results, but rely on RGB-D or
pose inputs, or cannot run in real-time. To address these limitations, our
approach integrates dense-SLAM with neural implicit fields. Specifically, our
dense SLAM approach runs parallel tracking and global optimization, while a
neural field-based map is constructed incrementally based on the latest SLAM
estimates. For the efficient construction of neural fields, we employ
multi-resolution grid encoding and signed distance function (SDF)
representation. This allows us to keep the map always up-to-date and adapt
instantly to global updates via loop closing. For global consistency, we
propose an efficient Sim(3)-based pose graph bundle adjustment (PGBA) approach
to run online loop closing and mitigate the pose and scale drift. To enhance
depth accuracy further, we incorporate learned monocular depth priors. We
propose a novel joint depth and scale adjustment (JDSA) module to solve the
scale ambiguity inherent in depth priors. Extensive evaluations across
synthetic and real-world datasets validate that our approach outperforms
existing methods in accuracy and map completeness while preserving real-time
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Foundation Models with Quadrotor Control for Visual Tracking
  Beyond Object Categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Saviolo, Pratyaksh Rao, Vivek Radhakrishnan, Jiuhong Xiao, Giuseppe Loianno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual control enables quadrotors to adaptively navigate using real-time
sensory data, bridging perception with action. Yet, challenges persist,
including generalization across scenarios, maintaining reliability, and
ensuring real-time responsiveness. This paper introduces a perception framework
grounded in foundation models for universal object detection and tracking,
moving beyond specific training categories. Integral to our approach is a
multi-layered tracker integrated with the foundation detector, ensuring
continuous target visibility, even when faced with motion blur, abrupt light
shifts, and occlusions. Complementing this, we introduce a model-free
controller tailored for resilient quadrotor visual tracking. Our system
operates efficiently on limited hardware, relying solely on an onboard camera
and an inertial measurement unit. Through extensive validation in diverse
challenging indoor and outdoor environments, we demonstrate our system's
effectiveness and adaptability. In conclusion, our research represents a step
forward in quadrotor visual tracking, moving from task-specific methods to more
versatile and adaptable operations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Current Trends and Advances in Quantum Navigation for Maritime
  Applications: A Comprehensive Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Sambataro, Riccardo Costanzi, Joao Alves, Andrea Caiti, Pietro Paglierani, Roberto Petroccia, Andrea Munafo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive review of the current state of the art in
quantum navigation systems, with a specific focus on their application in
maritime navigation. Quantum technologies have the potential to revolutionise
navigation and positioning systems due to their ability to provide highly
accurate and secure information. The review covers the principles of quantum
navigation and highlights the latest developments in quantum-enhanced sensors,
atomic clocks, and quantum communication protocols. The paper also discusses
the challenges and opportunities of using quantum technologies in maritime
navigation, including the effects that the maritime environment and the
specificity of marine applications can have on the performance of quantum
sensors. Finally, the paper concludes with a discussion on the future of
quantum navigation systems and their potential impact on the maritime industry.
This review aims at providing a valuable resource for researchers and engineers
interested in the development and deployment of quantum navigation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UFD-PRiME: Unsupervised Joint Learning of Optical Flow and Stereo Depth
  through Pixel-Level Rigid Motion Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Yuan, Carlo Tomasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both optical flow and stereo disparities are image matches and can therefore
benefit from joint training. Depth and 3D motion provide geometric rather than
photometric information and can further improve optical flow. Accordingly, we
design a first network that estimates flow and disparity jointly and is trained
without supervision. A second network, trained with optical flow from the first
as pseudo-labels, takes disparities from the first network, estimates 3D rigid
motion at every pixel, and reconstructs optical flow again. A final stage fuses
the outputs from the two networks. In contrast with previous methods that only
consider camera motion, our method also estimates the rigid motions of dynamic
objects, which are of key interest in applications. This leads to better
optical flow with visibly more detailed occlusions and object boundaries as a
result. Our unsupervised pipeline achieves 7.36% optical flow error on the
KITTI-2015 benchmark and outperforms the previous state-of-the-art 9.38% by a
wide margin. It also achieves slightly better or comparable stereo depth
results. Code will be made available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surgical Gym: A high-performance GPU-based platform for reinforcement
  learning with surgical robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Schmidgall, Axel Krieger, Jason Eshraghian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in robot-assisted surgery have resulted in progressively more
precise, efficient, and minimally invasive procedures, sparking a new era of
robotic surgical intervention. This enables doctors, in collaborative
interaction with robots, to perform traditional or minimally invasive surgeries
with improved outcomes through smaller incisions. Recent efforts are working
toward making robotic surgery more autonomous which has the potential to reduce
variability of surgical outcomes and reduce complication rates. Deep
reinforcement learning methodologies offer scalable solutions for surgical
automation, but their effectiveness relies on extensive data acquisition due to
the absence of prior knowledge in successfully accomplishing tasks. Due to the
intensive nature of simulated data collection, previous works have focused on
making existing algorithms more efficient. In this work, we focus on making the
simulator more efficient, making training data much more accessible than
previously possible. We introduce Surgical Gym, an open-source high performance
platform for surgical robot learning where both the physics simulation and
reinforcement learning occur directly on the GPU. We demonstrate between
100-5000x faster training times compared with previous surgical learning
platforms. The code is available at:
https://github.com/SamuelSchmidgall/SurgicalGym.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Terrain-Aware Quadrupedal Locomotion via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Shi, Qingxu Zhu, Lei Han, Wanchao Chi, Tingguang Li, Max Q. -H. Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In nature, legged animals have developed the ability to adapt to challenging
terrains through perception, allowing them to plan safe body and foot
trajectories in advance, which leads to safe and energy-efficient locomotion.
Inspired by this observation, we present a novel approach to train a Deep
Neural Network (DNN) policy that integrates proprioceptive and exteroceptive
states with a parameterized trajectory generator for quadruped robots to
traverse rough terrains. Our key idea is to use a DNN policy that can modify
the parameters of the trajectory generator, such as foot height and frequency,
to adapt to different terrains. To encourage the robot to step on safe regions
and save energy consumption, we propose foot terrain reward and lifting foot
height reward, respectively. By incorporating these rewards, our method can
learn a safer and more efficient terrain-aware locomotion policy that can move
a quadruped robot flexibly in any direction. To evaluate the effectiveness of
our approach, we conduct simulation experiments on challenging terrains,
including stairs, stepping stones, and poles. The simulation results
demonstrate that our approach can successfully direct the robot to traverse
such tough terrains in any direction. Furthermore, we validate our method on a
real legged robot, which learns to traverse stepping stones with gaps over
25.5cm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient State Estimation with Constrained Rao-Blackwellized Particle
  Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Li, Siwei Lyu, Jeff Trinkle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the limitations of the robotic sensors, during a robotic manipulation
task, the acquisition of the object's state can be unreliable and noisy.
Combining an accurate model of multi-body dynamic system with Bayesian
filtering methods has been shown to be able to filter out noise from the
object's observed states. However, efficiency of these filtering methods
suffers from samples that violate the physical constraints, e.g., no
penetration constraint.
  In this paper, we propose a Rao-Blackwellized Particle Filter (RBPF) that
samples the contact states and updates the object's poses using Kalman filters.
This RBPF also enforces the physical constraints on the samples by solving a
quadratic programming problem. By comparing our method with methods that does
not consider physical constraints, we show that our proposed RBPF is not only
able to estimate the object's states, e.g., poses, more accurately but also
able to infer unobserved states, e.g., velocities, with higher precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoCone: An OmniDirectional Robot for Lane-Level Cone Placement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.14103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.14103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Hartzer, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper summarizes the progress in developing a rugged, low-cost,
automated ground cone robot network capable of traffic delineation at
lane-level precision. A holonomic omnidirectional base with a traffic
delineator was developed to allow flexibility in initialization. RTK GPS was
utilized to reduce minimum position error to 2 centimeters. Due to recent
developments, the cost of the platform is now less than $1,600. To minimize the
effects of GPS-denied environments, wheel encoders and an Extended Kalman
Filter were implemented to maintain lane-level accuracy during operation and a
maximum error of 1.97 meters through 50 meters with little to no GPS signal.
Future work includes increasing the operational speed of the platforms,
incorporating lanelet information for path planning, and cross-platform
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRID: A Platform for General Robot Intelligence Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Vemprala, Shuhang Chen, Abhinav Shukla, Dinesh Narayanan, Ashish Kapoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing machine intelligence abilities in robots and autonomous systems is
an expensive and time consuming process. Existing solutions are tailored to
specific applications and are harder to generalize. Furthermore, scarcity of
training data adds a layer of complexity in deploying deep machine learning
models. We present a new platform for General Robot Intelligence Development
(GRID) to address both of these issues. The platform enables robots to learn,
compose and adapt skills to their physical capabilities, environmental
constraints and goals. The platform addresses AI problems in robotics via
foundation models that know the physical world. GRID is designed from the
ground up to be extensible to accommodate new types of robots, vehicles,
hardware platforms and software protocols. In addition, the modular design
enables various deep ML components and existing foundation models to be easily
usable in a wider variety of robot-centric problems. We demonstrate the
platform in various aerial robotics scenarios and demonstrate how the platform
dramatically accelerates development of machine intelligent robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Compliant Contact Primitives for Estimation and Model
  Predictive Control <span class="chip">ICRA24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17476v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17476v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Haninger, Kangwagye Samuel, Filippo Rozzi, Sehoon Oh, Loris Roveda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control techniques like MPC can realize contact-rich manipulation which
exploits dynamic information, maintaining friction limits and safety
constraints. However, contact geometry and dynamics are required to be known.
This information is often extracted from CAD, limiting scalability and the
ability to handle tasks with varying geometry. To reduce the need for a priori
models, we propose a framework for estimating contact models online based on
torque and position measurements. To do this, compliant contact models are
used, connected in parallel to model multi-point contact and constraints such
as a hinge. They are parameterized to be differentiable with respect to all of
their parameters (rest position, stiffness, contact location), allowing the
coupled robot/environment dynamics to be linearized or efficiently used in
gradient-based optimization. These models are then applied for: offline
gradient-based parameter fitting, online estimation via an extended Kalman
filter, and online gradient-based MPC. The proposed approach is validated on
two robots, showing the efficacy of sensorless contact estimation and the
effects of online estimation on MPC performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted ICRA24. Video available at https://youtu.be/CuCTcmn3H-o
  Code available at https://gitlab.cc-asp.fraunhofer.de/hanikevi/contact_mpc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stackelberg Driver Model for Continual Policy Improvement in
  Scenario-Based Closed-Loop Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14235v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14235v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyi Niu, Qimao Chen, Yingyue Li, Jianming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of autonomous vehicles (AVs) has faced hurdles due to the
dominance of rare but critical corner cases within the long-tail distribution
of driving scenarios, which negatively affects their overall performance. To
address this challenge, adversarial generation methods have emerged as a class
of efficient approaches to synthesize safety-critical scenarios for AV testing.
However, these generated scenarios are often underutilized for AV training,
resulting in the potential for continual AV policy improvement remaining
untapped, along with a deficiency in the closed-loop design needed to achieve
it. Therefore, we tailor the Stackelberg Driver Model (SDM) to accurately
characterize the hierarchical nature of vehicle interaction dynamics,
facilitating iterative improvement by engaging background vehicles (BVs) and AV
in a sequential game-like interaction paradigm. With AV acting as the leader
and BVs as followers, this leader-follower modeling ensures that AV would
consistently refine its policy, always taking into account the additional
information that BVs play the best response to challenge AV. Extensive
experiments have shown that our algorithm exhibits superior performance
compared to several baselines especially in higher dimensional scenarios,
leading to substantial advancements in AV capabilities while continually
generating progressively challenging scenarios. Code is available at
https://github.com/BlueCat-de/SDM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferring Foundation Models for Generalizable Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05716v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05716v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiange Yang, Wenhui Tan, Chuhao Jin, Keling Yao, Bei Liu, Jianlong Fu, Ruihua Song, Gangshan Wu, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the generalization capabilities of general-purpose robotic
manipulation agents in the real world has long been a significant challenge.
Existing approaches often rely on collecting large-scale robotic data which is
costly and time-consuming, such as the RT-1 dataset. However, due to
insufficient diversity of data, these approaches typically suffer from limiting
their capability in open-domain scenarios with new objects, and diverse
environments. In this paper, we propose a novel paradigm that effectively
leverages language grounded segmentation mask generated by Internet-scale
foundation models, to address a wide range of pick-and-place robot manipulation
tasks. By integrating the mask modality, which incorporates semantic,
geometric, and temporal correlation priors derived from vision foundation
models, into the end-to-end policy model, our approach can effectively and
robustly perceive object pose and enable sample-efficient generalization
learning, including new object instances, semantic categories, and unseen
backgrounds. We first introduce a series of foundation models to ground natural
language demands across multiple tasks. Secondly, we develop a two-stream 2D
policy model based on imitation learning, which utilizes raw images, object
masks, and robot proprioception to predict robot actions. Extensive real-world
experiments conducted on a Franka Emika robot arm demonstrate the effectiveness
of our proposed paradigm. Demos are shown in YouTube
(https://www.youtube.com/watch?v=MAcUPFBfRIw ).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Oriented Dexterous Grasp Synthesis via Differentiable Grasp Wrench
  Boundary Estimator <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Chen, Yuxing Chen, Jialiang Zhang, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analytical dexterous grasping synthesis is often driven by grasp quality
metrics. However, existing metrics possess many problems, such as being
computationally expensive, physically inaccurate, and non-differentiable.
Moreover, none of them can facilitate the synthesis of non-force-closure
grasps, which account for a significant portion of task-oriented grasping such
as lid screwing and button pushing. The main challenge behind all the above
drawbacks is the difficulty in modeling the complex Grasp Wrench Space (GWS).
In this work, we overcome this challenge by proposing a novel GWS estimator,
thus enabling gradient-based task-oriented dexterous grasp synthesis for the
first time. Our key contribution is a fast, accurate, and differentiable
technique to estimate the GWS boundary with good physical interpretability by
parallel sampling and mapping, which does not require iterative optimization.
Second, based on our differentiable GWS estimator, we derive a task-oriented
energy function to enable gradient-based grasp synthesis and a metric to
evaluate non-force-closure grasps. Finally, we improve the previous dexterous
grasp synthesis pipeline mainly by a novel technique to make nearest-point
calculation differentiable, even on mesh edges and vertices. Extensive
experiments are performed to verify the efficiency and effectiveness of our
methods. Our GWS estimator can run in several milliseconds on GPUs with minimal
memory cost, more than three orders of magnitude faster than the classic
discretization-based method. Using this GWS estimator, we synthesize 0.1
million dexterous grasps to show that our pipeline can significantly outperform
the SOTA method, even in task-unaware force-closure-grasp synthesis. For
task-oriented grasp synthesis, we provide some qualitative results. Our project
page is https://pku-epic.github.io/TaskDexGrasp/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In review. ICRA 2024 submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Gait Generation For Walking, Soft Robotic Quadrupeds <span class="chip">IROS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00498v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00498v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake Ketchum, Sophia Schiffer, Muchen Sun, Pranav Kaarthik, Ryan L. Truby, Todd D. Murphey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gait generation for soft robots is challenging due to the nonlinear dynamics
and high dimensional input spaces of soft actuators. Limitations in soft
robotic control and perception force researchers to hand-craft open loop
controllers for gait sequences, which is a non-trivial process. Moreover, short
soft actuator lifespans and natural variations in actuator behavior limit
machine learning techniques to settings that can be learned on the same time
scales as robot deployment. Lastly, simulation is not always possible, due to
heterogeneity and nonlinearity in soft robotic materials and their dynamics
change due to wear. We present a sample-efficient, simulation free, method for
self-generating soft robot gaits, using very minimal computation. This
technique is demonstrated on a motorized soft robotic quadruped that walks
using four legs constructed from 16 "handed shearing auxetic" (HSA) actuators.
To manage the dimension of the search space, gaits are composed of two
sequential sets of leg motions selected from 7 possible primitives. Pairs of
primitives are executed on one leg at a time; we then select the
best-performing pair to execute while moving on to subsequent legs. This method
-- which uses no simulation, sophisticated computation, or user input --
consistently generates good translation and rotation gaits in as low as 4
minutes of hardware experimentation, outperforming hand-crafted gaits. This is
the first demonstration of completely autonomous gait generation in a soft
robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, 6 Figures, Published at IROS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Review on Tree Detection Methods Using Point Cloud and
  Aerial Imagery from Unmanned Aerial Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Kuang, Hann Woei Ho, Ye Zhou, Shahrel Azmin Suandi, Farzad Ismail
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicles (UAVs) are considered cutting-edge technology with
highly cost-effective and flexible usage scenarios. Although many papers have
reviewed the application of UAVs in agriculture, the review of the application
for tree detection is still insufficient. This paper focuses on tree detection
methods applied to UAV data collected by UAVs. There are two kinds of data, the
point cloud and the images, which are acquired by the Light Detection and
Ranging (LiDAR) sensor and camera, respectively. Among the detection methods
using point-cloud data, this paper mainly classifies these methods according to
LiDAR and Digital Aerial Photography (DAP). For the detection methods using
images directly, this paper reviews these methods by whether or not to use the
Deep Learning (DL) method. Our review concludes and analyses the comparison and
combination between the application of LiDAR-based and DAP-based point cloud
data. The performance, relative merits, and application fields of the methods
are also introduced. Meanwhile, this review counts the number of tree detection
studies using different methods in recent years. From our statics, the
detection task using DL methods on the image has become a mainstream trend as
the number of DL-based detection researches increases to 45% of the total
number of tree detection studies up to 2022. As a result, this review could
help and guide researchers who want to carry out tree detection on specific
forests and for farmers to use UAVs in managing agriculture production.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to Computers and Electronics in
  Agriculture for review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-06T00:00:00Z">2023-10-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">32</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Grasp: from Somewhere to Anywhere 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        François Hélénon, Johann Huber, Faïz Ben Amar, Stéphane Doncieux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic grasping is still a partially solved, multidisciplinary problem where
data-driven techniques play an increasing role. The sparse nature of rewards
make the automatic generation of grasping datasets challenging, especially for
unconventional morphologies or highly actuated end-effectors. Most approaches
for obtaining large-scale datasets rely on numerous human-provided
demonstrations or heavily engineered solutions that do not scale well. Recent
advances in Quality-Diversity (QD) methods have investigated how to learn
object grasping at a specific pose with different robot morphologies. The
present work introduces a pipeline for adapting QD-generated trajectories to
new object poses. Using an RGB-D data stream, the vision pipeline first detects
the targeted object, predicts its 6-DOF pose, and finally tracks it. An
automatically generated reach-and-grasp trajectory can then be adapted by
projecting it relatively to the object frame. Hundreds of trajectories have
been deployed into the real world on several objects and with different robotic
setups: a Franka Research 3 with a parallel gripper and a UR5 with a dexterous
SIH Schunk hand. The transfer ratio obtained when applying transformation to
the object pose matches the one obtained when the object pose matches the
simulation, demonstrating the efficiency of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph learning in robotics: a survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Pistilli, Giuseppe Averta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks for graphs have emerged as a powerful tool for learning
on complex non-euclidean data, which is becoming increasingly common for a
variety of different applications. Yet, although their potential has been
widely recognised in the machine learning community, graph learning is largely
unexplored for downstream tasks such as robotics applications. To fully unlock
their potential, hence, we propose a review of graph neural architectures from
a robotics perspective. The paper covers the fundamentals of graph-based
models, including their architecture, training procedures, and applications. It
also discusses recent advancements and challenges that arise in applied
settings, related for example to the integration of perception,
decision-making, and control. Finally, the paper provides an extensive review
of various robotic applications that benefit from learning on graph structures,
such as bodies and contacts modelling, robotic manipulation, action
recognition, fleet motion planning, and many more. This survey aims to provide
readers with a thorough understanding of the capabilities and limitations of
graph neural architectures in robotics, and to highlight potential avenues for
future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dataset of Anatomical Environments for Medical Robots: Modeling
  Respiratory Deformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inbar Fried, Janine Hoelscher, Jason A. Akulian, Ron Alterovitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomical models of a medical robot's environment can significantly help
guide design and development of a new robotic system. These models can be used
for benchmarking motion planning algorithms, evaluating controllers, optimizing
mechanical design choices, simulating procedures, and even as resources for
data generation. Currently, the time-consuming task of generating these
environments is repeatedly performed by individual research groups and rarely
shared broadly. This not only leads to redundant efforts, but also makes it
challenging to compare systems and algorithms accurately. In this work, we
present a collection of clinically-relevant anatomical environments for medical
robots operating in the lungs. Since anatomical deformation is a fundamental
challenge for medical robots operating in the lungs, we describe a way to model
respiratory deformation in these environments using patient-derived data. We
share the environments and deformation data publicly by adding them to the
Medical Robotics Anatomical Dataset (Med-RAD), our public dataset of anatomical
environments for medical robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Servoing by Recombining Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Argus, Abhijeet Nayak, Martin Büchner, Silvio Galesso, Abhinav Valada, Thomas Brox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based manipulation policies from image inputs often show weak task
transfer capabilities. In contrast, visual servoing methods allow efficient
task transfer in high-precision scenarios while requiring only a few
demonstrations. In this work, we present a framework that formulates the visual
servoing task as graph traversal. Our method not only extends the robustness of
visual servoing, but also enables multitask capability based on a few
task-specific demonstrations. We construct demonstration graphs by splitting
existing demonstrations and recombining them. In order to traverse the
demonstration graph in the inference case, we utilize a similarity function
that helps select the best demonstration for a specific task. This enables us
to compute the shortest path through the graph. Ultimately, we show that
recombining demonstrations leads to higher task-respective success. We present
extensive simulation and real-world experimental results that demonstrate the
efficacy of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>http://compservo.cs.uni-freiburg.de</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms
  Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo El-Hariry, Antoine Richard, Vivek Muralidharan, Baris Can Yalcin, Matthieu Geist, Miguel Olivares-Mendez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This investigation introduces a novel deep reinforcement learning-based suite
to control floating platforms in both simulated and real-world environments.
Floating platforms serve as versatile test-beds to emulate microgravity
environments on Earth. Our approach addresses the system and environmental
uncertainties in controlling such platforms by training policies capable of
precise maneuvers amid dynamic and unpredictable conditions. Leveraging
state-of-the-art deep reinforcement learning techniques, our suite achieves
robustness, adaptability, and good transferability from simulation to reality.
Our Deep Reinforcement Learning (DRL) framework provides advantages such as
fast training times, large-scale testing capabilities, rich visualization
options, and ROS bindings for integration with real-world robotic systems.
Beyond policy development, our suite provides a comprehensive platform for
researchers, offering open-access at
https://github.com/elharirymatteo/RANS/tree/ICRA24.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Safety Objective for the Calibration of the Intelligent Driver
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kingsley Adjenughwure, Arturo Tejada, Pedro F. V. Oliveira, Jeroen Hogema, Gerdien Klunder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intelligent driver model (IDM) is one of the most widely used
car-following (CF) models in recent years. The parameters of this model have
been calibrated using real trajectories obtained from naturalistic driving
,driving simulator experiment and drone data. An important aspect of the model
calibration process is defining the main objective of the calibration. This
objective, influences the objective function and the performance measure for
the calibration. For example, to calibrate CF models, the objective is usually
to minimize the error in measured spacing or speed while important safety
aspects of the models such as the collision avoidance mechanisms are ignored.
For such models, there is no guarantee that the calibrated parameters will
preserve the safety properties of the model since they are not explicitly taken
into account. To explicitly account for the safety properties during
calibration, this paper proposes a simple objective function which minimizes
both the error in the actual measured spacing (as it is currently done) and the
error in the dynamic safety spacing (desired minimum gap) derived from the
collision free property of the IDM model. The proposed objective function is
used to calibrate two variants of the IDM using vehicle trajectories obtained
with drone from a Dutch highway. The calibration performance is then compared
in terms of the error in actual spacing and time gap. The results show that the
proposed safety objective 15 function leads to lower errors in spacing and time
gap compared to when minimizing for only spacing and preserves collision
property of the IDM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be submitted to the Transportation Research Records Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Solving Close Enough Orienteering Problem with Overlapped
  Neighborhoods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuchen Qian, Yanran Wang, David Boyle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Close Enough Traveling Salesman Problem (CETSP) is a well-known variant
of the classic Traveling Salesman Problem whereby the agent may complete its
mission at any point within a target neighborhood. Heuristics based on
overlapped neighborhoods, known as Steiner Zones (SZ), have gained attention in
addressing CETSPs. While SZs offer effective approximations to the original
graph, their inherent overlap imposes constraints on the search space,
potentially conflicting with global optimization objectives. Here we present
the Close Enough Orienteering Problem with Non-uniform Neighborhoods (CEOP-N),
which extends CETSP by introducing variable prize attributes and non-uniform
cost considerations for prize collection. To tackle CEOP-N, we develop a new
approach featuring a Randomized Steiner Zone Discretization (RSZD) scheme
coupled with a hybrid algorithm based on Particle Swarm Optimization (PSO) and
Ant Colony System (ACS) - CRaSZe-AntS. The RSZD scheme identifies sub-regions
for PSO exploration, and ACS determines the discrete visiting sequence. We
evaluate the RSZD's discretization performance on CEOP instances derived from
established CETSP instances, and compare CRaSZe-AntS against the most relevant
state-of-the-art heuristic focused on single-neighborhood optimization for
CEOP. We also compare the performance of the interior search within SZs and the
boundary search on individual neighborhoods in the context of CEOP-N. Our
results show CRaSZe-AntS can yield comparable solution quality with
significantly reduced computation time compared to the single-neighborhood
strategy, where we observe an averaged 140.44% increase in prize collection and
55.18% reduction of execution time. CRaSZe-AntS is thus highly effective in
solving emerging CEOP-N, examples of which include truck-and-drone delivery
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The WayHome: Long-term Motion Prediction on Dynamically Scaled 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kay Scheerer, Thomas Michalke, Juergen Mathes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the key challenges for autonomous vehicles is the ability to
accurately predict the motion of other objects in the surrounding environment,
such as pedestrians or other vehicles. In this contribution, a novel motion
forecasting approach for autonomous vehicles is developed, inspired by the work
of Gilles et al. [1]. We predict multiple heatmaps with a neuralnetwork-based
model for every traffic participant in the vicinity of the autonomous vehicle;
with one heatmap per timestep. The heatmaps are used as input to a novel
sampling algorithm that extracts coordinates corresponding to the most likely
future positions. We experiment with different encoders and decoders, as well
as a comparison of two loss functions. Additionally, a new grid-scaling
technique is introduced, showing further improved performance. Overall, our
approach improves stateof-the-art miss rate performance for the
function-relevant prediction interval of 3 seconds while being competitive in
longer prediction intervals (up to eight seconds). The evaluation is done on
the public 2022 Waymo motion challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Indoor Positioning based on Active Radar Sensing and Passive Reflectors:
  Concepts & Initial Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Schlachter, Zhibin Yu, Naveed Iqbal, Xiaofeng Wu, Sven Hinderer, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To navigate reliably in indoor environments, an industrial autonomous vehicle
must know its position. However, current indoor vehicle positioning
technologies either lack accuracy, usability or are too expensive. Thus, we
propose a novel concept called local reference point assisted active radar
positioning, which is able to overcome these drawbacks. It is based on
distributing passive retroreflectors in the indoor environment such that each
position of the vehicle can be identified by a unique reflection characteristic
regarding the reflectors. To observe these characteristics, the autonomous
vehicle is equipped with an active radar system. On one hand, this paper
presents the basic idea and concept of our new approach towards indoor vehicle
positioning and especially focuses on the crucial placement of the reflectors.
On the other hand, it also provides a proof of concept by conducting a full
system simulation including the placement of the local reference points, the
radar-based distance estimation and the comparison of two different positioning
methods. It successfully demonstrates the feasibility of our proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffPrompter: Differentiable Implicit Visual Prompts for
  Semantic-Segmentation in Adverse Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanket Kalwar, Mihir Ungarala, Shruti Jain, Aaron Monis, Krishna Reddy Konda, Sourav Garg, K Madhava Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation in adverse weather scenarios is a critical task for
autonomous driving systems. While foundation models have shown promise, the
need for specialized adaptors becomes evident for handling more challenging
scenarios. We introduce DiffPrompter, a novel differentiable visual and latent
prompting mechanism aimed at expanding the learning capabilities of existing
adaptors in foundation models. Our proposed $\nabla$HFC image processing block
excels particularly in adverse weather conditions, where conventional methods
often fall short. Furthermore, we investigate the advantages of jointly
training visual and latent prompts, demonstrating that this combined approach
significantly enhances performance in out-of-distribution scenarios. Our
differentiable visual prompts leverage parallel and series architectures to
generate prompts, effectively improving object segmentation tasks in adverse
conditions. Through a comprehensive series of experiments and evaluations, we
provide empirical evidence to support the efficacy of our approach. Project
page at https://diffprompter.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards 6D MCL for LiDARs in 3D TSDF Maps on Embedded Systems with GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Eisoldt, Alexander Mock, Mario Porrmann, Thomas Wiemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monte Carlo Localization is a widely used approach in the field of mobile
robotics. While this problem has been well studied in the 2D case, global
localization in 3D maps with six degrees of freedom has so far been too
computationally demanding. Hence, no mobile robot system has yet been presented
in literature that is able to solve it in real-time. The computationally most
intensive step is the evaluation of the sensor model, but it also offers high
parallelization potential. This work investigates the massive parallelization
of the evaluation of particles in truncated signed distance fields for
three-dimensional laser scanners on embedded GPUs. The implementation on the
GPU is 30 times as fast and more than 50 times more energy efficient compared
to a CPU implementation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Light-LOAM: A Lightweight LiDAR Odometry and Mapping based on
  Graph-Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiquan Yi, Yang Lyu, Lin Hua, Quan Pan, Chunhui Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) plays an important role in robot
autonomy. Reliability and efficiency are the two most valued features for
applying SLAM in robot applications. In this paper, we consider achieving a
reliable LiDAR-based SLAM function in computation-limited platforms, such as
quadrotor UAVs based on graph-based point cloud association. First, contrary to
most works selecting salient features for point cloud registration, we propose
a non-conspicuous feature selection strategy for reliability and robustness
purposes. Then a two-stage correspondence selection method is used to register
the point cloud, which includes a KD-tree-based coarse matching followed by a
graph-based matching method that uses geometric consistency to vote out
incorrect correspondences. Additionally, we propose an odometry approach where
the weight optimizations are guided by vote results from the aforementioned
geometric consistency graph. In this way, the optimization of LiDAR odometry
rapidly converges and evaluates a fairly accurate transformation resulting in
the back-end module efficiently finishing the mapping task. Finally, we
evaluate our proposed framework on the KITTI odometry dataset and real-world
environments. Experiments show that our SLAM system achieves a comparative
level or higher level of accuracy with more balanced computation efficiency
compared with the mainstream LiDAR-based SLAM solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maximizing Performance with Minimal Resources for Real-Time Transition
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeynep Ozge Orhan, Andrea Dal Prete, Anastasia Bolotnikova, Marta Gandolla, Auke Ijspeert, Mohamed Bouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assistive devices, such as exoskeletons and prostheses, have revolutionized
the field of rehabilitation and mobility assistance. Efficiently detecting
transitions between different activities, such as walking, stair ascending and
descending, and sitting, is crucial for ensuring adaptive control and enhancing
user experience. We here present an approach for real-time transition
detection, aimed at optimizing the processing-time performance. By establishing
activity-specific threshold values through trained machine learning models, we
effectively distinguish motion patterns and we identify transition moments
between locomotion modes. This threshold-based method improves real-time
embedded processing time performance by up to 11 times compared to machine
learning approaches. The efficacy of the developed finite-state machine is
validated using data collected from three different measurement systems.
Moreover, experiments with healthy participants were conducted on an active
pelvis orthosis to validate the robustness and reliability of our approach. The
proposed algorithm achieved high accuracy in detecting transitions between
activities. These promising results show the robustness and reliability of the
method, reinforcing its potential for integration into practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for a conference. 7 pages including references, 8 figures,
  3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Doppler-only Single-scan 3D Vehicle Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andres Galeote-Luque, Vladimír Kubelka, Martin Magnusson, Jose-Raul Ruiz-Sarmiento, Javier Gonzalez-Jimenez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel 3D odometry method that recovers the full motion of a
vehicle only from a Doppler-capable range sensor. It leverages the radial
velocities measured from the scene, estimating the sensor's velocity from a
single scan. The vehicle's 3D motion, defined by its linear and angular
velocities, is calculated taking into consideration its kinematic model which
provides a constraint between the velocity measured at the sensor frame and the
vehicle frame.
  Experiments carried out prove the viability of our single-sensor method
compared to mounting an additional IMU. Our method provides the translation of
the sensor, which cannot be reliably determined from an IMU, as well as its
rotation. Its short-term accuracy and fast operation (~5ms) make it a proper
candidate to supply the initialization to more complex localization algorithms
or mapping pipelines. Not only does it reduce the error of the mapper, but it
does so at a comparable level of accuracy as an IMU would. All without the need
to mount and calibrate an extra sensor on the vehicle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-based 3D Collision-distance Estimation Network with Probabilistic
  Graph Rewiring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjae Song, Yeseung Kim, Daehyung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to solve the problem of data-driven collision-distance estimation
given 3-dimensional (3D) geometries. Conventional algorithms suffer from low
accuracy due to their reliance on limited representations, such as point
clouds. In contrast, our previous graph-based model, GraphDistNet, achieves
high accuracy using edge information but incurs higher message-passing costs
with growing graph size, limiting its applicability to 3D geometries. To
overcome these challenges, we propose GDN-R, a novel 3D graph-based estimation
network.GDN-R employs a layer-wise probabilistic graph-rewiring algorithm
leveraging the differentiable Gumbel-top-K relaxation. Our method accurately
infers minimum distances through iterative graph rewiring and updating relevant
embeddings. The probabilistic rewiring enables fast and robust embedding with
respect to unforeseen categories of geometries. Through 41,412 random benchmark
tasks with 150 pairs of 3D objects, we show GDN-R outperforms state-of-the-art
baseline methods in terms of accuracy and generalizability. We also show that
the proposed rewiring improves the update performance reducing the size of the
estimation model. We finally show its batch prediction and auto-differentiation
capabilities for trajectory optimization in both simulated and real-world
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CineTransfer: Controlling a Robot to Imitate Cinematographic Style from
  a Single Example 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Pueyo, Eduardo Montijano, Ana C. Murillo, Mac Schwager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents CineTransfer, an algorithmic framework that drives a robot
to record a video sequence that mimics the cinematographic style of an input
video. We propose features that abstract the aesthetic style of the input
video, so the robot can transfer this style to a scene with visual details that
are significantly different from the input video. The framework builds upon
CineMPC, a tool that allows users to control cinematographic features, like
subjects' position on the image and the depth of field, by manipulating the
intrinsics and extrinsics of a cinematographic camera. However, CineMPC
requires a human expert to specify the desired style of the shot (composition,
camera motion, zoom, focus, etc). CineTransfer bridges this gap, aiming a fully
autonomous cinematographic platform. The user chooses a single input video as a
style guide. CineTransfer extracts and optimizes two important style features,
the composition of the subject in the image and the scene depth of field, and
provides instructions for CineMPC to control the robot to record an output
sequence that matches these features as closely as possible. In contrast with
other style transfer methods, our approach is a lightweight and portable
framework which does not require deep network training or extensive datasets.
Experiments with real and simulated videos demonstrate the system's ability to
analyze and transfer style between recordings, and are available in the
supplementary video.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SlotGNN: Unsupervised Discovery of Multi-Object Representations and
  Visual Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Rezazadeh, Athreyi Badithela, Karthik Desingh, Changhyun Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning multi-object dynamics from visual data using unsupervised techniques
is challenging due to the need for robust, object representations that can be
learned through robot interactions. This paper presents a novel framework with
two new architectures: SlotTransport for discovering object representations
from RGB images and SlotGNN for predicting their collective dynamics from RGB
images and robot interactions. Our SlotTransport architecture is based on slot
attention for unsupervised object discovery and uses a feature transport
mechanism to maintain temporal alignment in object-centric representations.
This enables the discovery of slots that consistently reflect the composition
of multi-object scenes. These slots robustly bind to distinct objects, even
under heavy occlusion or absence. Our SlotGNN, a novel unsupervised graph-based
dynamics model, predicts the future state of multi-object scenes. SlotGNN
learns a graph representation of the scene using the discovered slots from
SlotTransport and performs relational and spatial reasoning to predict the
future appearance of each slot conditioned on robot actions. We demonstrate the
effectiveness of SlotTransport in learning object-centric features that
accurately encode both visual and positional information. Further, we highlight
the accuracy of SlotGNN in downstream robotic tasks, including challenging
multi-object rearrangement and long-horizon prediction. Finally, our
unsupervised approach proves effective in the real world. With only minimal
additional data, our framework robustly predicts slots and their corresponding
dynamics in real-world control tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Model Predictive Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Sacks, Rwik Rana, Kevin Huang, Alex Spitzer, Guanya Shi, Byron Boots
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major challenge in robotics is to design robust policies which enable
complex and agile behaviors in the real world. On one end of the spectrum, we
have model-free reinforcement learning (MFRL), which is incredibly flexible and
general but often results in brittle policies. In contrast, model predictive
control (MPC) continually re-plans at each time step to remain robust to
perturbations and model inaccuracies. However, despite its real-world
successes, MPC often under-performs the optimal strategy. This is due to model
quality, myopic behavior from short planning horizons, and approximations due
to computational constraints. And even with a perfect model and enough compute,
MPC can get stuck in bad local optima, depending heavily on the quality of the
optimization algorithm. To this end, we propose Deep Model Predictive
Optimization (DMPO), which learns the inner-loop of an MPC optimization
algorithm directly via experience, specifically tailored to the needs of the
control problem. We evaluate DMPO on a real quadrotor agile trajectory tracking
task, on which it improves performance over a baseline MPC algorithm for a
given computational budget. It can outperform the best MPC algorithm by up to
27% with fewer samples and an end-to-end policy trained with MFRL by 19%.
Moreover, because DMPO requires fewer samples, it can also achieve these
benefits with 4.3X less memory. When we subject the quadrotor to turbulent wind
fields with an attached drag plate, DMPO can adapt zero-shot while still
outperforming all baselines. Additional results can be found at
https://tinyurl.com/mr2ywmnw.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper is 6 pages with 4 figures and 1 table. Code available at:
  https://github.com/jisacks/dmpo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Humanoid Motion Representations for Physics-Based Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris Kitani, Weipeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a universal motion representation that encompasses a comprehensive
range of motor skills for physics-based humanoid control. Due to the
high-dimensionality of humanoid control as well as the inherent difficulties in
reinforcement learning, prior methods have focused on learning skill embeddings
for a narrow range of movement styles (e.g. locomotion, game characters) from
specialized motion datasets. This limited scope hampers its applicability in
complex tasks. Our work closes this gap, significantly increasing the coverage
of motion representation space. To achieve this, we first learn a motion
imitator that can imitate all of human motion from a large, unstructured motion
dataset. We then create our motion representation by distilling skills directly
from the imitator. This is achieved using an encoder-decoder structure with a
variational information bottleneck. Additionally, we jointly learn a prior
conditioned on proprioception (humanoid's own pose and velocities) to improve
model expressiveness and sampling efficiency for downstream tasks. Sampling
from the prior, we can generate long, stable, and diverse human motions. Using
this latent space for hierarchical RL, we show that our policies solve tasks
using natural and realistic human behavior. We demonstrate the effectiveness of
our motion representation by solving generative tasks (e.g. strike, terrain
traversal) and motion tracking using VR controllers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zhengyiluo.github.io/PULSE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIVE: Lidar Informed Visual Search for Multiple Objects with Multiple
  Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Gupta, Minkyu Kim, Juliana T Rodriguez, Kyle Morgenstein, Luis Sentis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LIVE: Lidar Informed Visual Search focused on the
problem of multi-robot (MR) planning and execution for robust visual detection
of multiple objects. We perform extensive real-world experiments with a
two-robot team in an indoor apartment setting. LIVE acts as a perception module
that detects unmapped obstacles, or Short Term Features (STFs), in Lidar
observations. STFs are filtered, resulting in regions to be visually inspected
by modifying plans online. Lidar Coverage Path Planning (CPP) is employed for
generating highly efficient global plans for heterogeneous robot teams.
Finally, we present a data model and a demonstration dataset, which can be
found by visiting our project website
https://sites.google.com/view/live-iros2023/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages + references; 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knolling bot: A Transformer-based Approach to Organizing a Messy Table 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Hu, Zhizhuo Zhang, Ruibo Liu, Philippe Wyder, Hod Lipson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose an approach to equip domestic robots with the
ability to perform simple household tidying tasks. We focus specifically on
'knolling,' an activity related to organizing scattered items into neat and
space-efficient arrangements. Unlike the uniformity of industrial environments,
household settings present unique challenges due to their diverse array of
items and the subjectivity of tidiness. Here, we draw inspiration from natural
language processing (NLP) and utilize a transformer-based approach that
predicts the next position of an item in a sequence of neatly positioned items.
We integrate the knolling model with a visual perception model and a physical
robot arm to demonstrate a machine that declutters and organizes a dozen
freeform items of various shapes and sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mCLARI: a shape-morphing insect-scale robot capable of omnidirectional
  terrain-adaptive locomotion in laterally confined spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heiko Kabutz, Alexander Hedrick, Parker McDonnell, Kaushik Jayaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft compliant microrobots have the potential to deliver significant societal
impact when deployed in applications such as search and rescue. In this
research we present mCLARI, a body compliant quadrupedal microrobot of 20mm
neutral body length and 0.97g, improving on its larger predecessor, CLARI. This
robot has four independently actuated leg modules with 2 degrees of freedom,
each driven by piezoelectric actuators. The legs are interconnected in a closed
kinematic chain via passive body joints, enabling passive body compliance for
shape adaptation to external constraints. Despite scaling its larger
predecessor down to 60 percent in length and 38 percent in mass, mCLARI
maintains 80 percent of the actuation power to achieve high agility.
Additionally, we demonstrate the new capability of passively shape-morphing
mCLARI - omnidirectional laterally confined locomotion - and experimentally
quantify its running performance achieving a new unconstrained top speed of 3
bodylengths/s (60 mms-1). Leveraging passive body compliance, mCLARI can
navigate through narrow spaces with a body compression ratio of up to 1.5x the
neutral body shape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Randomization for Sim2real Transfer of Automatically Generated
  Grasping Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johann Huber, François Hélénon, Hippolyte Watrelot, Faiz Ben Amar, Stéphane Doncieux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic grasping refers to making a robotic system pick an object by applying
forces and torques on its surface. Many recent studies use data-driven
approaches to address grasping, but the sparse reward nature of this task made
the learning process challenging to bootstrap. To avoid constraining the
operational space, an increasing number of works propose grasping datasets to
learn from. But most of them are limited to simulations. The present paper
investigates how automatically generated grasps can be exploited in the real
world. More than 7000 reach-and-grasp trajectories have been generated with
Quality-Diversity (QD) methods on 3 different arms and grippers, including
parallel fingers and a dexterous hand, and tested in the real world. Conducted
analysis on the collected measure shows correlations between several Domain
Randomization-based quality criteria and sim-to-real transferability. Key
challenges regarding the reality gap for grasping have been identified,
stressing matters on which researchers on grasping should focus in the future.
A QD approach has finally been proposed for making grasps more robust to domain
randomization, resulting in a transfer ratio of 84% on the Franka Research 3
arm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, draft version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction
  Execution for Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitriy Rivkin, Nikhil Kakodkar, Francois Hogan, Bobak H. Baghi, Gregory Dudek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work explores the capacity of large language models (LLMs) to address
problems at the intersection of spatial planning and natural language
interfaces for navigation.Our focus is on following relatively complex
instructions that are more akin to natural conversation than traditional
explicit procedural directives seen in robotics. Unlike most prior work, where
navigation directives are provided as imperative commands (e.g., go to the
fridge), we examine implicit directives within conversational interactions. We
leverage the 3D simulator AI2Thor to create complex and repeatable scenarios at
scale, and augment it by adding complex language queries for 40 object types.
We demonstrate that a robot can better parse descriptive language queries than
existing methods by using an LLM to interpret the user interaction in the
context of a list of the objects in the scene.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10888v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10888v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikai Wang, Zheyuan Jiang, Jianyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, reinforcement learning has become a promising and polular solution
for robot legged locomotion. Compared to model-based control, reinforcement
learning based controllers can achieve better robustness against uncertainties
of environments through sim-to-real learning. However, the corresponding
learned gaits are in general overly conservative and unatural. In this paper,
we propose a new framework for learning robust, agile and natural legged
locomotion skills over challenging terrain. We incorporate an adversarial
training branch based on real animal locomotion data upon a teacher-student
training pipeline for robust sim-to-real transfer. Empirical results on both
simulation and real world of a quadruped robot demonstrate that our proposed
algorithm enables robustly traversing challenging terrains such as stairs,
rocky ground and slippery floor with only proprioceptive perception. Meanwhile,
the gaits are more agile, natural, and energy efficient compared to the
baselines. Both qualitative and quantitative results are presented in this
paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page and videos:
  https://sites.google.com/view/adaptive-multiskill-locomotion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-based Safe Autonomous UAV Docking with Panoramic Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuoc Nguyen Thuan, Tomi Westerlund, Jorge Peña Queralta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable growth of unmanned aerial vehicles (UAVs) has also sparked
concerns about safety measures during their missions. To advance towards safer
autonomous aerial robots, this work presents a vision-based solution to
ensuring safe autonomous UAV landings with minimal infrastructure. During
docking maneuvers, UAVs pose a hazard to people in the vicinity. In this paper,
we propose the use of a single omnidirectional panoramic camera pointing
upwards from a landing pad to detect and estimate the position of people around
the landing area. The images are processed in real-time in an embedded
computer, which communicates with the onboard computer of approaching UAVs to
transition between landing, hovering or emergency landing states. While
landing, the ground camera also aids in finding an optimal position, which can
be required in case of low-battery or when hovering is no longer possible. We
use a YOLOv7-based object detection model and a XGBooxt model for localizing
nearby people, and the open-source ROS and PX4 frameworks for communication,
interfacing, and control of the UAV. We present both simulation and real-world
indoor experimental results to show the efficiency of our methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A robophysical model of spacetime dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04835v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04835v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkai Li, Hussain N. Gynai, Steven Tarr, Emily Alicea-Muñoz, Pablo Laguna, Gongjie Li, Daniel I. Goldman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systems consisting of spheres rolling on elastic membranes have been used to
introduce a core conceptual idea of General Relativity (GR): how curvature
guides the movement of matter. However, such schemes cannot accurately
represent relativistic dynamics in the laboratory because of the dominance of
dissipation and external gravitational fields. Here we demonstrate that an
``active" object (a wheeled robot), which moves in a straight line on level
ground and can alter its speed depending on the curvature of the deformable
terrain it moves on, can exactly capture dynamics in curved relativistic
spacetimes. Via the systematic study of the robot's dynamics in the radial and
orbital directions, we develop a mapping of the emergent trajectories of a
wheeled vehicle on a spandex membrane to the motion in a curved spacetime. Our
mapping demonstrates how the driven robot's dynamics mix space and time in a
metric, and shows how active particles do not necessarily follow geodesics in
the real space but instead follow geodesics in a fiducial spacetime. The
mapping further reveals how parameters such as the membrane elasticity and
instantaneous speed allow the programming of a desired spacetime, such as the
Schwarzschild metric near a non-rotating blackhole. Our mapping and framework
facilitate creation of a robophysical analog to a general relativistic system
in the laboratory at low cost that can provide insights into active matter in
deformable environments and robot exploration in complex landscapes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CVTNet: A Cross-View Transformer Network for Place Recognition Using
  LiDAR Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Ma, Guangming Xiong, Jingyi Xu, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-based place recognition (LPR) is one of the most crucial components of
autonomous vehicles to identify previously visited places in GPS-denied
environments. Most existing LPR methods use mundane representations of the
input point cloud without considering different views, which may not fully
exploit the information from LiDAR sensors. In this paper, we propose a
cross-view transformer-based network, dubbed CVTNet, to fuse the range image
views (RIVs) and bird's eye views (BEVs) generated from the LiDAR data. It
extracts correlations within the views themselves using intra-transformers and
between the two different views using inter-transformers. Based on that, our
proposed CVTNet generates a yaw-angle-invariant global descriptor for each
laser scan end-to-end online and retrieves previously seen places by descriptor
matching between the current query scan and the pre-built database. We evaluate
our approach on three datasets collected with different sensor setups and
environmental conditions. The experimental results show that our method
outperforms the state-of-the-art LPR methods with strong robustness to
viewpoint changes and long-time spans. Furthermore, our approach has a good
real-time performance that can run faster than the typical LiDAR frame rate.
The implementation of our method is released as open source at:
https://github.com/BIT-MJY/CVTNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IEEE Transactions on Industrial Informatics 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rotating Objects via In-Hand Pivoting using Vision, Force and Touch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10865v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10865v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Xu, Tianyuan Liu, Michael Wong, Dana Kulić, Akansel Cosgun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a robotic manipulation system that can pivot objects on a surface
using vision, wrist force and tactile sensing. We aim to control the rotation
of an object around the grip point of a parallel gripper by allowing rotational
slip, while maintaining a desired wrist force profile. Our approach runs an
end-effector position controller and a gripper width controller concurrently in
a closed loop. The position controller maintains a desired force using vision
and wrist force. The gripper controller uses tactile sensing to keep the grip
firm enough to prevent translational slip, but loose enough to induce
rotational slip. Our sensor-based control approach relies on matching a desired
force profile derived from object dimensions and weight and vision-based
monitoring of the object pose. The gripper controller uses tactile sensors to
detect and prevent translational slip by tightening the grip when needed.
Experimental results where the robot was tasked with rotating cuboid objects 90
degrees show that the multi-modal pivoting approach was able to rotate the
objects without causing lift or slip, and was more energy-efficient compared to
using a single sensor modality and to pick-and-place. While our work
demonstrated the benefit of multi-modal sensing for the pivoting task, further
work is needed to generalize our approach to any given object.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Lead Time in Fall Detection for a Planar Bipedal Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Eva Mungai, Jessy Grizzle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For legged robots to operate in complex terrains, they must be robust to the
disturbances and uncertainties they encounter. This paper contributes to
enhancing robustness through the design of fall detection/prediction algorithms
that will provide sufficient lead time for corrective motions to be taken.
Falls can be caused by abrupt (fast-acting), incipient (slow-acting), or
intermittent (non-continuous) faults. Early fall detection is a challenging
task due to the masking effects of controllers (through their disturbance
attenuation actions), the inverse relationship between lead time and false
positive rates, and the temporal behavior of the faults/underlying factors. In
this paper, we propose a fall detection algorithm that is capable of detecting
both incipient and abrupt faults while maximizing lead time and meeting desired
thresholds on the false positive and negative rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2023 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quadratic Programming-based Reference Spreading Control for Dual-Arm
  Robotic Manipulation with Planned Simultaneous Impacts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jari van Steen, Gijs van den Brandt, Nathan van de Wouw, Jens Kober, Alessandro Saccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the aim of further enabling the exploitation of intentional impacts in
robotic manipulation, a control framework is presented that directly tackles
the challenges posed by tracking control of robotic manipulators that are
tasked to perform nominally simultaneous impacts. This framework is an
extension of the reference spreading control framework, in which overlapping
ante- and post-impact references that are consistent with impact dynamics are
defined. In this work, such a reference is constructed starting from a
teleoperation-based approach. By using the corresponding ante- and post-impact
control modes in the scope of a quadratic programming control approach, peaking
of the velocity error and control inputs due to impacts is avoided while
maintaining high tracking performance. With the inclusion of a novel interim
mode, we aim to also avoid input peaks and steps when uncertainty in the
environment causes a series of unplanned single impacts to occur rather than
the planned simultaneous impact. This work in particular presents for the first
time an experimental evaluation of reference spreading control on a robotic
setup, showcasing its robustness against uncertainty in the environment
compared to three baseline control approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures. Submitted for publication to IEEE Transactions
  on Robotics (T-RO) in September, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Generalizable Tool-use Skills through Trajectory Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carl Qi, Sarthak Shetty, Xingyu Lin, David Held
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems that efficiently utilize tools can assist humans in
completing many common tasks such as cooking and cleaning. However, current
systems fall short of matching human-level of intelligence in terms of adapting
to novel tools. Prior works based on affordance often make strong assumptions
about the environments and cannot scale to more complex, contact-rich tasks. In
this work, we tackle this challenge and explore how agents can learn to use
previously unseen tools to manipulate deformable objects. We propose to learn a
generative model of the tool-use trajectories as a sequence of point clouds,
which generalizes to different tool shapes. Given any novel tool, we first
generate a tool-use trajectory and then optimize the sequence of tool poses to
align with the generated trajectory. We train a single model for four different
challenging deformable object manipulation tasks. Our model is trained with
demonstration data from just a single tool for each task and is able to
generalize to various novel tools, significantly outperforming baselines.
Additional materials can be found on our project website:
https://sites.google.com/view/toolgen.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">70</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alice Benchmarks: Connecting Real World Object Re-Identification with
  the Synthetic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiao Sun, Yue Yao, Shengjin Wang, Hongdong Li, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For object re-identification (re-ID), learning from synthetic data has become
a promising strategy to cheaply acquire large-scale annotated datasets and
effective models, with few privacy concerns. Many interesting research problems
arise from this strategy, e.g., how to reduce the domain gap between synthetic
source and real-world target. To facilitate developing more new approaches in
learning from synthetic data, we introduce the Alice benchmarks, large-scale
datasets providing benchmarks as well as evaluation protocols to the research
community. Within the Alice benchmarks, two object re-ID tasks are offered:
person and vehicle re-ID. We collected and annotated two challenging real-world
target datasets: AlicePerson and AliceVehicle, captured under various
illuminations, image resolutions, etc. As an important feature of our real
target, the clusterability of its training set is not manually guaranteed to
make it closer to a real domain adaptation test scenario. Correspondingly, we
reuse existing PersonX and VehicleX as synthetic source domains. The primary
goal is to train models from synthetic data that can work effectively in the
real world. In this paper, we detail the settings of Alice benchmarks, provide
an analysis of existing commonly-used domain adaptation methods, and discuss
some interesting future directions. An online server will be set up for the
community to evaluate methods conveniently and fairly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model
  Generalization Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiao Sun, Xingjian Leng, Zijian Wang, Yang Yang, Zi Huang, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing model performance in various unseen environments is a critical
research problem in the machine learning community. To study this problem, it
is important to construct a testbed with out-of-distribution test sets that
have broad coverage of environmental discrepancies. However, existing testbeds
typically either have a small number of domains or are synthesized by image
corruptions, hindering algorithm design that demonstrates real-world
effectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of
180 datasets collected by prompting image search engines and diffusion models
in various ways. Generally sized between 300 and 8,000 images, the datasets
contain natural images, cartoons, certain colors, or objects that do not
naturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen
the understanding of two generalization tasks: domain generalization and model
accuracy prediction in various out-of-distribution environments. We conduct
extensive benchmarking and comparison experiments and show that CIFAR-10-W
offers new and interesting insights inherent to these tasks. We also discuss
other fields that would benefit from CIFAR-10-W.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedConv: Enhancing Convolutional Neural Networks for Handling Data
  Heterogeneity in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiran Xu, Zeyu Wang, Jieru Mei, Liangqiong Qu, Alan Yuille, Cihang Xie, Yuyin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is an emerging paradigm in machine learning, where a
shared model is collaboratively learned using data from multiple devices to
mitigate the risk of data leakage. While recent studies posit that Vision
Transformer (ViT) outperforms Convolutional Neural Networks (CNNs) in
addressing data heterogeneity in FL, the specific architectural components that
underpin this advantage have yet to be elucidated. In this paper, we
systematically investigate the impact of different architectural elements, such
as activation functions and normalization layers, on the performance within
heterogeneous FL. Through rigorous empirical analyses, we are able to offer the
first-of-its-kind general guidance on micro-architecture design principles for
heterogeneous FL.
  Intriguingly, our findings indicate that with strategic architectural
modifications, pure CNNs can achieve a level of robustness that either matches
or even exceeds that of ViTs when handling heterogeneous data clients in FL.
Additionally, our approach is compatible with existing FL techniques and
delivers state-of-the-art solutions across a broad spectrum of FL benchmarks.
The code is publicly available at https://github.com/UCSC-VLAA/FedConv
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures. Equal contribution by P. Xu and Z. Wang</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Agent Tree Search Unifies Reasoning Acting and Planning in
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have demonstrated impressive performance
on a range of decision-making tasks, they rely on simple acting processes and
fall short of broad deployment as autonomous agents. We introduce LATS
(Language Agent Tree Search), a general framework that synergizes the
capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration
from Monte Carlo tree search in model-based reinforcement learning, LATS
employs LLMs as agents, value functions, and optimizers, repurposing their
latent strengths for enhanced decision-making. What is crucial in this method
is the use of an environment for external feedback, which offers a more
deliberate and adaptive problem-solving mechanism that moves beyond the
limitations of existing techniques. Our experimental evaluation across diverse
domains, such as programming, HotPotQA, and WebShop, illustrates the
applicability of LATS for both reasoning and acting. In particular, LATS
achieves 94.4\% for programming on HumanEval with GPT-4 and an average score of
75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness
and generality of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website and code can be found at
  https://andyz245.github.io/LanguageAgentTreeSearch</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Consistency Models: Synthesizing High-Resolution Images with
  Few-Step Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent Diffusion models (LDMs) have achieved remarkable results in
synthesizing high-resolution images. However, the iterative sampling process is
computationally intensive and leads to slow generation. Inspired by Consistency
Models (song et al.), we propose Latent Consistency Models (LCMs), enabling
swift inference with minimal steps on any pre-trained LDMs, including Stable
Diffusion (rombach et al). Viewing the guided reverse diffusion process as
solving an augmented probability flow ODE (PF-ODE), LCMs are designed to
directly predict the solution of such ODE in latent space, mitigating the need
for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently
distilled from pre-trained classifier-free guided diffusion models, a
high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training.
Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method
that is tailored for fine-tuning LCMs on customized image datasets. Evaluation
on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve
state-of-the-art text-to-image generation performance with few-step inference.
Project Page: https://latent-consistency-models.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwimXYZ: A large-scale dataset of synthetic swimming motions and videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fiche Guénolé, Sevestre Vincent, Gonzalez-Barral Camila, Leglaive Simon, Séguier Renaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technologies play an increasingly important role in sports and become a real
competitive advantage for the athletes who benefit from it. Among them, the use
of motion capture is developing in various sports to optimize sporting
gestures. Unfortunately, traditional motion capture systems are expensive and
constraining. Recently developed computer vision-based approaches also struggle
in certain sports, like swimming, due to the aquatic environment. One of the
reasons for the gap in performance is the lack of labeled datasets with
swimming videos. In an attempt to address this issue, we introduce SwimXYZ, a
synthetic dataset of swimming motions and videos. SwimXYZ contains 3.4 million
frames annotated with ground truth 2D and 3D joints, as well as 240 sequences
of swimming motions in the SMPL parameters format. In addition to making this
dataset publicly available, we present use cases for SwimXYZ in swimming stroke
clustering and 2D pose estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM MIG 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Deep Joint Source-Channel Coding with Decoder-Only Side
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selim F. Yilmaz, Ezgi Ozyilkan, Deniz Gunduz, Elza Erkip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider low-latency image transmission over a noisy wireless channel when
correlated side information is present only at the receiver side (the Wyner-Ziv
scenario). In particular, we are interested in developing practical schemes
using a data-driven joint source-channel coding (JSCC) approach, which has been
previously shown to outperform conventional separation-based approaches in the
practical finite blocklength regimes, and to provide graceful degradation with
channel quality. We propose a novel neural network architecture that
incorporates the decoder-only side information at multiple stages at the
receiver side. Our results demonstrate that the proposed method succeeds in
integrating the side information, yielding improved performance at all channel
noise levels in terms of the various distortion criteria considered here,
especially at low channel signal-to-noise ratios (SNRs) and small bandwidth
ratios (BRs). We also provide the source code of the proposed method to enable
further research and reproducibility of the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Zhu, Qirong Mao, Jialin Zhang, Xiaohua Huang, Wenming Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group-level emotion recognition (GER) is an inseparable part of human
behavior analysis, aiming to recognize an overall emotion in a multi-person
scene. However, the existing methods are devoted to combing diverse emotion
cues while ignoring the inherent uncertainties under unconstrained
environments, such as congestion and occlusion occurring within a group.
Additionally, since only group-level labels are available, inconsistent emotion
predictions among individuals in one group can confuse the network. In this
paper, we propose an uncertainty-aware learning (UAL) method to extract more
robust representations for GER. By explicitly modeling the uncertainty of each
individual, we utilize stochastic embedding drawn from a Gaussian distribution
instead of deterministic point embedding. This representation captures the
probabilities of different emotions and generates diverse predictions through
this stochasticity during the inference stage. Furthermore,
uncertainty-sensitive scores are adaptively assigned as the fusion weights of
individuals' face within each group. Moreover, we develop an image enhancement
module to enhance the model's robustness against severe noise. The overall
three-branch model, encompassing face, object, and scene component, is guided
by a proportional-weighted fusion strategy and integrates the proposed
uncertainty-aware method to produce the final group-level output. Experimental
results demonstrate the effectiveness and generalization ability of our method
across three widely used databases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages,3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergent ADMM Plug and Play PET Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florent Sureau, Mahdi Latreche, Marion Savanier, Claude Comtat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate hybrid PET reconstruction algorithms based on
coupling a model-based variational reconstruction and the application of a
separately learnt Deep Neural Network operator (DNN) in an ADMM Plug and Play
framework. Following recent results in optimization, fixed point convergence of
the scheme can be achieved by enforcing an additional constraint on network
parameters during learning. We propose such an ADMM algorithm and show in a
realistic [18F]-FDG synthetic brain exam that the proposed scheme indeed lead
experimentally to convergence to a meaningful fixed point. When the proposed
constraint is not enforced during learning of the DNN, the proposed ADMM
algorithm was observed experimentally not to converge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph learning in robotics: a survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Pistilli, Giuseppe Averta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks for graphs have emerged as a powerful tool for learning
on complex non-euclidean data, which is becoming increasingly common for a
variety of different applications. Yet, although their potential has been
widely recognised in the machine learning community, graph learning is largely
unexplored for downstream tasks such as robotics applications. To fully unlock
their potential, hence, we propose a review of graph neural architectures from
a robotics perspective. The paper covers the fundamentals of graph-based
models, including their architecture, training procedures, and applications. It
also discusses recent advancements and challenges that arise in applied
settings, related for example to the integration of perception,
decision-making, and control. Finally, the paper provides an extensive review
of various robotic applications that benefit from learning on graph structures,
such as bodies and contacts modelling, robotic manipulation, action
recognition, fleet motion planning, and many more. This survey aims to provide
readers with a thorough understanding of the capabilities and limitations of
graph neural architectures in robotics, and to highlight potential avenues for
future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing Robustness via Score-Based Adversarial Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Kollovieh, Lukas Gosch, Yan Scholten, Marten Lienen, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most adversarial attacks and defenses focus on perturbations within small
$\ell_p$-norm constraints. However, $\ell_p$ threat models cannot capture all
relevant semantic-preserving perturbations, and hence, the scope of robustness
evaluations is limited. In this work, we introduce Score-Based Adversarial
Generation (ScoreAG), a novel framework that leverages the advancements in
score-based generative models to generate adversarial examples beyond
$\ell_p$-norm constraints, so-called unrestricted adversarial examples,
overcoming their limitations. Unlike traditional methods, ScoreAG maintains the
core semantics of images while generating realistic adversarial examples,
either by transforming existing images or synthesizing new ones entirely from
scratch. We further exploit the generative capability of ScoreAG to purify
images, empirically enhancing the robustness of classifiers. Our extensive
empirical evaluation demonstrates that ScoreAG matches the performance of
state-of-the-art attacks and defenses across multiple benchmarks. This work
highlights the importance of investigating adversarial examples bounded by
semantics rather than $\ell_p$-norm constraints. ScoreAG represents an
important step towards more encompassing robustness assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Servoing by Recombining Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Argus, Abhijeet Nayak, Martin Büchner, Silvio Galesso, Abhinav Valada, Thomas Brox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based manipulation policies from image inputs often show weak task
transfer capabilities. In contrast, visual servoing methods allow efficient
task transfer in high-precision scenarios while requiring only a few
demonstrations. In this work, we present a framework that formulates the visual
servoing task as graph traversal. Our method not only extends the robustness of
visual servoing, but also enables multitask capability based on a few
task-specific demonstrations. We construct demonstration graphs by splitting
existing demonstrations and recombining them. In order to traverse the
demonstration graph in the inference case, we utilize a similarity function
that helps select the best demonstration for a specific task. This enables us
to compute the shortest path through the graph. Ultimately, we show that
recombining demonstrations leads to higher task-respective success. We present
extensive simulation and real-world experimental results that demonstrate the
efficacy of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>http://compservo.cs.uni-freiburg.de</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Camouflaged Object Detection: A Large-Scale Dataset and
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Zhang, Hongbo Bi, Tian-Zhu Xiang, Ranwan Wu, Jinghui Tong, Xiufang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we provide a comprehensive study on a new task called
collaborative camouflaged object detection (CoCOD), which aims to
simultaneously detect camouflaged objects with the same properties from a group
of relevant images. To this end, we meticulously construct the first
large-scale dataset, termed CoCOD8K, which consists of 8,528 high-quality and
elaborately selected images with object mask annotations, covering 5
superclasses and 70 subclasses. The dataset spans a wide range of natural and
artificial camouflage scenes with diverse object appearances and backgrounds,
making it a very challenging dataset for CoCOD. Besides, we propose the first
baseline model for CoCOD, named bilateral-branch network (BBNet), which
explores and aggregates co-camouflaged cues within a single image and between
images within a group, respectively, for accurate camouflaged object detection
in given images. This is implemented by an inter-image collaborative feature
exploration (CFE) module, an intra-image object feature search (OFS) module,
and a local-global refinement (LGR) module. We benchmark 18 state-of-the-art
models, including 12 COD algorithms and 6 CoSOD algorithms, on the proposed
CoCOD8K dataset under 5 widely used evaluation metrics. Extensive experiments
demonstrate the effectiveness of the proposed method and the significantly
superior performance compared to other competitors. We hope that our proposed
dataset and model will boost growth in the COD community. The dataset, model,
and results will be available at: https://github.com/zc199823/BBNet--CoCOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic segmentation of longitudinal thermal images for identification
  of hot and cool spots in urban areas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasantha Ramani, Pandarasamy Arjunan, Kameshwar Poolla, Clayton Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents the analysis of semantically segmented, longitudinally,
and spatially rich thermal images collected at the neighborhood scale to
identify hot and cool spots in urban areas. An infrared observatory was
operated over a few months to collect thermal images of different types of
buildings on the educational campus of the National University of Singapore. A
subset of the thermal image dataset was used to train state-of-the-art deep
learning models to segment various urban features such as buildings,
vegetation, sky, and roads. It was observed that the U-Net segmentation model
with `resnet34' CNN backbone has the highest mIoU score of 0.99 on the test
dataset, compared to other models such as DeepLabV3, DeeplabV3+, FPN, and
PSPnet. The masks generated using the segmentation models were then used to
extract the temperature from thermal images and correct for differences in the
emissivity of various urban features. Further, various statistical measure of
the temperature extracted using the predicted segmentation masks is shown to
closely match the temperature extracted using the ground truth masks. Finally,
the masks were used to identify hot and cool spots in the urban feature at
various instances of time. This forms one of the very few studies demonstrating
the automated analysis of thermal images, which can be of potential use to
urban planners for devising mitigation strategies for reducing the urban heat
island (UHI) effect, improving building energy efficiency, and maximizing
outdoor thermal comfort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing the Authenticity of Rendered Portraits with
  Identity-Consistent Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyuan Wang, Yiqian Wu, Yongliang Yang, Chen Liu, Xiaogang Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite rapid advances in computer graphics, creating high-quality
photo-realistic virtual portraits is prohibitively expensive. Furthermore, the
well-know ''uncanny valley'' effect in rendered portraits has a significant
impact on the user experience, especially when the depiction closely resembles
a human likeness, where any minor artifacts can evoke feelings of eeriness and
repulsiveness. In this paper, we present a novel photo-realistic portrait
generation framework that can effectively mitigate the ''uncanny valley''
effect and improve the overall authenticity of rendered portraits. Our key idea
is to employ transfer learning to learn an identity-consistent mapping from the
latent space of rendered portraits to that of real portraits. During the
inference stage, the input portrait of an avatar can be directly transferred to
a realistic portrait by changing its appearance style while maintaining the
facial identity. To this end, we collect a new dataset, Daz-Rendered-Faces-HQ
(DRFHQ), that is specifically designed for rendering-style portraits. We
leverage this dataset to fine-tune the StyleGAN2 generator, using our carefully
crafted framework, which helps to preserve the geometric and color features
relevant to facial identity. We evaluate our framework using portraits with
diverse gender, age, and race variations. Qualitative and quantitative
evaluations and ablation studies show the advantages of our method compared to
state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Gap between Human Motion and Action Semantics via Kinematic
  Phrases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpeng Liu, Yong-Lu Li, Ailing Zeng, Zizheng Zhou, Yang You, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of motion understanding is to establish a reliable mapping between
motion and action semantics, while it is a challenging many-to-many problem. An
abstract action semantic (i.e., walk forwards) could be conveyed by
perceptually diverse motions (walk with arms up or swinging), while a motion
could carry different semantics w.r.t. its context and intention. This makes an
elegant mapping between them difficult. Previous attempts adopted
direct-mapping paradigms with limited reliability. Also, current automatic
metrics fail to provide reliable assessments of the consistency between motions
and action semantics. We identify the source of these problems as the
significant gap between the two modalities. To alleviate this gap, we propose
Kinematic Phrases (KP) that take the objective kinematic facts of human motion
with proper abstraction, interpretability, and generality characteristics.
Based on KP as a mediator, we can unify a motion knowledge base and build a
motion understanding system. Meanwhile, KP can be automatically converted from
motions and to text descriptions with no subjective bias, inspiring Kinematic
Prompt Generation (KPG) as a novel automatic motion generation benchmark. In
extensive experiments, our approach shows superiority over other methods. Our
code and data would be made publicly available at https://foruck.github.io/KP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole Slide Multiple Instance Learning for Predicting Axillary Lymph
  Node Metastasis <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Glejdis Shkëmbi, Johanna P. Müller, Zhe Li, Katharina Breininger, Peter Schüffler, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer is a major concern for women's health globally, with axillary
lymph node (ALN) metastasis identification being critical for prognosis
evaluation and treatment guidance. This paper presents a deep learning (DL)
classification pipeline for quantifying clinical information from digital
core-needle biopsy (CNB) images, with one step less than existing methods. A
publicly available dataset of 1058 patients was used to evaluate the
performance of different baseline state-of-the-art (SOTA) DL models in
classifying ALN metastatic status based on CNB images. An extensive ablation
study of various data augmentation techniques was also conducted. Finally, the
manual tumor segmentation and annotation step performed by the pathologists was
assessed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for MICCAI DEMI Workshop 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffPrompter: Differentiable Implicit Visual Prompts for
  Semantic-Segmentation in Adverse Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanket Kalwar, Mihir Ungarala, Shruti Jain, Aaron Monis, Krishna Reddy Konda, Sourav Garg, K Madhava Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation in adverse weather scenarios is a critical task for
autonomous driving systems. While foundation models have shown promise, the
need for specialized adaptors becomes evident for handling more challenging
scenarios. We introduce DiffPrompter, a novel differentiable visual and latent
prompting mechanism aimed at expanding the learning capabilities of existing
adaptors in foundation models. Our proposed $\nabla$HFC image processing block
excels particularly in adverse weather conditions, where conventional methods
often fall short. Furthermore, we investigate the advantages of jointly
training visual and latent prompts, demonstrating that this combined approach
significantly enhances performance in out-of-distribution scenarios. Our
differentiable visual prompts leverage parallel and series architectures to
generate prompts, effectively improving object segmentation tasks in adverse
conditions. Through a comprehensive series of experiments and evaluations, we
provide empirical evidence to support the efficacy of our approach. Project
page at https://diffprompter.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Degradation-Aware Self-Attention Based Transformer for Blind Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingguo Liu, Pan Gao, Kang Han, Ningzhong Liu, Wei Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to CNN-based methods, Transformer-based methods achieve impressive
image restoration outcomes due to their abilities to model remote dependencies.
However, how to apply Transformer-based methods to the field of blind
super-resolution (SR) and further make an SR network adaptive to degradation
information is still an open problem. In this paper, we propose a new
degradation-aware self-attention-based Transformer model, where we incorporate
contrastive learning into the Transformer network for learning the degradation
representations of input images with unknown noise. In particular, we integrate
both CNN and Transformer components into the SR network, where we first use the
CNN modulated by the degradation information to extract local features, and
then employ the degradation-aware Transformer to extract global semantic
features. We apply our proposed model to several popular large-scale benchmark
datasets for testing, and achieve the state-of-the-art performance compared to
existing methods. In particular, our method yields a PSNR of 32.43 dB on the
Urban100 dataset at $\times$2 scale, 0.94 dB higher than DASR, and 26.62 dB on
the Urban100 dataset at $\times$4 scale, 0.26 dB improvement over KDSR, setting
a new benchmark in this area. Source code is available at:
https://github.com/I2-Multimedia-Lab/DSAT/tree/main.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropic Score metric: Decoupling Topology and Size in Training-free NAS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niccolò Cavagnero, Luca Robbiano, Francesca Pistilli, Barbara Caputo, Giuseppe Averta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Networks design is a complex and often daunting task, particularly for
resource-constrained scenarios typical of mobile-sized models. Neural
Architecture Search is a promising approach to automate this process, but
existing competitive methods require large training time and computational
resources to generate accurate models. To overcome these limits, this paper
contributes with: i) a novel training-free metric, named Entropic Score, to
estimate model expressivity through the aggregated element-wise entropy of its
activations; ii) a cyclic search algorithm to separately yet synergistically
search model size and topology. Entropic Score shows remarkable ability in
searching for the topology of the network, and a proper combination with
LogSynflow, to search for model size, yields superior capability to completely
design high-performance Hybrid Transformers for edge applications in less than
1 GPU hour, resulting in the fastest and most accurate NAS method for ImageNet
classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Neural Radiance Field using Near-Surface Sampling with Point
  Cloud Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hye Bin Yoo, Hyun Min Han, Sung Soo Hwang, Il Yong Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance field (NeRF) is an emerging view synthesis method that
samples points in a three-dimensional (3D) space and estimates their existence
and color probabilities. The disadvantage of NeRF is that it requires a long
training time since it samples many 3D points. In addition, if one samples
points from occluded regions or in the space where an object is unlikely to
exist, the rendering quality of NeRF can be degraded. These issues can be
solved by estimating the geometry of 3D scene. This paper proposes a
near-surface sampling framework to improve the rendering quality of NeRF. To
this end, the proposed method estimates the surface of a 3D object using depth
images of the training set and sampling is performed around there only. To
obtain depth information on a novel view, the paper proposes a 3D point cloud
generation method and a simple refining method for projected depth from a point
cloud. Experimental results show that the proposed near-surface sampling NeRF
framework can significantly improve the rendering quality, compared to the
original NeRF and a state-of-the-art depth-based NeRF method. In addition, one
can significantly accelerate the training time of a NeRF model with the
proposed near-surface sampling framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement
  Learning <span class="chip">IJCAI 23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinda Chen, Wei Huang, Shenglong Zhou, Qi Chen, Zhiwei Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of existing supervised neuron segmentation methods is highly
dependent on the number of accurate annotations, especially when applied to
large scale electron microscopy (EM) data. By extracting semantic information
from unlabeled data, self-supervised methods can improve the performance of
downstream tasks, among which the mask image model (MIM) has been widely used
due to its simplicity and effectiveness in recovering original information from
masked images. However, due to the high degree of structural locality in EM
images, as well as the existence of considerable noise, many voxels contain
little discriminative information, making MIM pretraining inefficient on the
neuron segmentation task. To overcome this challenge, we propose a
decision-based MIM that utilizes reinforcement learning (RL) to automatically
search for optimal image masking ratio and masking strategy. Due to the vast
exploration space, using single-agent RL for voxel prediction is impractical.
Therefore, we treat each input patch as an agent with a shared behavior policy,
allowing for multi-agent collaboration. Furthermore, this multi-agent model can
capture dependencies between voxels, which is beneficial for the downstream
segmentation task. Experiments conducted on representative EM datasets
demonstrate that our approach has a significant advantage over alternative
self-supervised methods on the task of neuron segmentation. Code is available
at \url{https://github.com/ydchen0806/dbMiM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 23 main track paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TiC: Exploring Vision Transformer in Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Zhang, Qingzhong Wang, Jiang Bian, Haoyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While models derived from Vision Transformers (ViTs) have been phonemically
surging, pre-trained models cannot seamlessly adapt to arbitrary resolution
images without altering the architecture and configuration, such as sampling
the positional encoding, limiting their flexibility for various vision tasks.
For instance, the Segment Anything Model (SAM) based on ViT-Huge requires all
input images to be resized to 1024$\times$1024. To overcome this limitation, we
propose the Multi-Head Self-Attention Convolution (MSA-Conv) that incorporates
Self-Attention within generalized convolutions, including standard, dilated,
and depthwise ones. Enabling transformers to handle images of varying sizes
without retraining or rescaling, the use of MSA-Conv further reduces
computational costs compared to global attention in ViT, which grows costly as
image size increases. Later, we present the Vision Transformer in Convolution
(TiC) as a proof of concept for image classification with MSA-Conv, where two
capacity enhancing strategies, namely Multi-Directional Cyclic Shifted
Mechanism and Inter-Pooling Mechanism, have been proposed, through establishing
long-distance connections between tokens and enlarging the effective receptive
field. Extensive experiments have been carried out to validate the overall
effectiveness of TiC. Additionally, ablation studies confirm the performance
improvement made by MSA-Conv and the two capacity enhancing strategies
separately. Note that our proposal aims at studying an alternative to the
global attention used in ViT, while MSA-Conv meets our goal by making TiC
comparable to state-of-the-art on ImageNet-1K. Code will be released at
https://github.com/zs670980918/MSA-Conv.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VI-Diff: Unpaired Visible-Infrared Translation Diffusion Model for
  Single Modality Labeled Visible-Infrared Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Huang, Yan Huang, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible-Infrared person re-identification (VI-ReID) in real-world scenarios
poses a significant challenge due to the high cost of cross-modality data
annotation. Different sensing cameras, such as RGB/IR cameras for good/poor
lighting conditions, make it costly and error-prone to identify the same person
across modalities. To overcome this, we explore the use of single-modality
labeled data for the VI-ReID task, which is more cost-effective and practical.
By labeling pedestrians in only one modality (e.g., visible images) and
retrieving in another modality (e.g., infrared images), we aim to create a
training set containing both originally labeled and modality-translated data
using unpaired image-to-image translation techniques. In this paper, we propose
VI-Diff, a diffusion model that effectively addresses the task of
Visible-Infrared person image translation. Through comprehensive experiments,
we demonstrate that VI-Diff outperforms existing diffusion and GAN models,
making it a promising solution for VI-ReID with single-modality labeled data.
Our approach can be a promising solution to the VI-ReID task with
single-modality labeled data and serves as a good starting point for future
study. Code will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aorta Segmentation from 3D CT in MICCAI SEG.A. 2023 Challenge <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andriy Myronenko, Dong Yang, Yufan He, Daguang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aorta provides the main blood supply of the body. Screening of aorta with
imaging helps for early aortic disease detection and monitoring. In this work,
we describe our solution to the Segmentation of the Aorta (SEG.A.231) from 3D
CT challenge. We use automated segmentation method Auto3DSeg available in
MONAI. Our solution achieves an average Dice score of 0.920 and 95th percentile
of the Hausdorff Distance (HD95) of 6.013, which ranks first and wins the
SEG.A. 2023 challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023, SEG.A. 2023 challenge 1st place</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Random Texture Detection using Beta Distribution Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soeren Molander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This note describes a method for detecting dense random texture using fully
connected points sampled on image edges. An edge image is randomly sampled with
points, the standard L2 distance is calculated between all connected points in
a neighbourhood. For each point, a check is made if the point intersects with
an image edge. If this is the case, a unity value is added to the distance,
otherwise zero. From this an edge excess index is calculated for the fully
connected edge graph in the range [1.0..2.0], where 1.0 indicate no edges. The
ratio can be interpreted as a sampled Bernoulli process with unknown
probability. The Bayesian posterior estimate of the probability can be
associated with its conjugate prior which is a Beta($\alpha$, $\beta$)
distribution, with hyper parameters $\alpha$ and $\beta$ related to the number
of edge crossings. Low values of $\beta$ indicate a texture rich area, higher
values less rich. The method has been applied to real-time SLAM-based moving
object detection, where points are confined to tracked boxes (rois).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated 3D Segmentation of Kidneys and Tumors in MICCAI KiTS 2023
  Challenge <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andriy Myronenko, Dong Yang, Yufan He, Daguang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kidney and Kidney Tumor Segmentation Challenge (KiTS) 2023 offers a platform
for researchers to compare their solutions to segmentation from 3D CT. In this
work, we describe our submission to the challenge using automated segmentation
of Auto3DSeg available in MONAI. Our solution achieves the average dice of
0.835 and surface dice of 0.723, which ranks first and wins the KiTS 2023
challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023, KITS 2023 challenge 1st place</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClusVPR: Efficient Visual Place Recognition with Clustering-based
  Weighted Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Xu, Pourya Shamsolmoali, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition (VPR) is a highly challenging task that has a wide
range of applications, including robot navigation and self-driving vehicles.
VPR is particularly difficult due to the presence of duplicate regions and the
lack of attention to small objects in complex scenes, resulting in recognition
deviations. In this paper, we present ClusVPR, a novel approach that tackles
the specific issues of redundant information in duplicate regions and
representations of small objects. Different from existing methods that rely on
Convolutional Neural Networks (CNNs) for feature map generation, ClusVPR
introduces a unique paradigm called Clustering-based Weighted Transformer
Network (CWTNet). CWTNet leverages the power of clustering-based weighted
feature maps and integrates global dependencies to effectively address visual
deviations encountered in large-scale VPR problems. We also introduce the
optimized-VLAD (OptLAD) layer that significantly reduces the number of
parameters and enhances model efficiency. This layer is specifically designed
to aggregate the information obtained from scale-wise image patches.
Additionally, our pyramid self-supervised strategy focuses on extracting
representative and diverse information from scale-wise image patches instead of
entire images, which is crucial for capturing representative and diverse
information in VPR. Extensive experiments on four VPR datasets show our model's
superior performance compared to existing models while being less complex.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Chess Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Athanasios Masouris, Jan van Gemert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chess recognition refers to the task of identifying the chess pieces
configuration from a chessboard image. Contrary to the predominant approach
that aims to solve this task through the pipeline of chessboard detection,
square localization, and piece classification, we rely on the power of deep
learning models and introduce two novel methodologies to circumvent this
pipeline and directly predict the chessboard configuration from the entire
image. In doing so, we avoid the inherent error accumulation of the sequential
approaches and the need for intermediate annotations. Furthermore, we introduce
a new dataset, Chess Recognition Dataset (ChessReD), specifically designed for
chess recognition that consists of 10,800 images and their corresponding
annotations. In contrast to existing synthetic datasets with limited angles,
this dataset comprises a diverse collection of real images of chess formations
captured from various angles using smartphone cameras; a sensor choice made to
ensure real-world applicability. We use this dataset to both train our model
and evaluate and compare its performance to that of the current
state-of-the-art. Our approach in chess recognition on this new benchmark
dataset outperforms related approaches, achieving a board recognition accuracy
of 15.26% ($\approx$7x better than the current state-of-the-art).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deeply Supervised Semantic Segmentation Method Based on GAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhao, Qiyu Wei, Zeng Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the field of intelligent transportation has witnessed rapid
advancements, driven by the increasing demand for automation and efficiency in
transportation systems. Traffic safety, one of the tasks integral to
intelligent transport systems, requires accurately identifying and locating
various road elements, such as road cracks, lanes, and traffic signs. Semantic
segmentation plays a pivotal role in achieving this task, as it enables the
partition of images into meaningful regions with accurate boundaries. In this
study, we propose an improved semantic segmentation model that combines the
strengths of adversarial learning with state-of-the-art semantic segmentation
techniques. The proposed model integrates a generative adversarial network
(GAN) framework into the traditional semantic segmentation model, enhancing the
model's performance in capturing complex and subtle features in transportation
images. The effectiveness of our approach is demonstrated by a significant
boost in performance on the road crack dataset compared to the existing
methods, \textit{i.e.,} SEGAN. This improvement can be attributed to the
synergistic effect of adversarial learning and semantic segmentation, which
leads to a more refined and accurate representation of road structures and
conditions. The enhanced model not only contributes to better detection of road
cracks but also to a wide range of applications in intelligent transportation,
such as traffic sign recognition, vehicle detection, and lane segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, ITSC conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In the Blink of an Eye: Event-based Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwei Zhang, Jiqing Zhang, Bo Dong, Pieter Peers, Wenwei Wu, Xiaopeng Wei, Felix Heide, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a wearable single-eye emotion recognition device and a real-time
approach to recognizing emotions from partial observations of an emotion that
is robust to changes in lighting conditions. At the heart of our method is a
bio-inspired event-based camera setup and a newly designed lightweight Spiking
Eye Emotion Network (SEEN). Compared to conventional cameras, event-based
cameras offer a higher dynamic range (up to 140 dB vs. 80 dB) and a higher
temporal resolution. Thus, the captured events can encode rich temporal cues
under challenging lighting conditions. However, these events lack texture
information, posing problems in decoding temporal information effectively. SEEN
tackles this issue from two different perspectives. First, we adopt
convolutional spiking layers to take advantage of the spiking neural network's
ability to decode pertinent temporal information. Second, SEEN learns to
extract essential spatial cues from corresponding intensity frames and
leverages a novel weight-copy scheme to convey spatial attention to the
convolutional spiking layers during training and inference. We extensively
validate and demonstrate the effectiveness of our approach on a specially
collected Single-eye Event-based Emotion (SEE) dataset. To the best of our
knowledge, our method is the first eye-based emotion recognition method that
leverages event-based cameras and spiking neural network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Excision and Recovery: Enhancing Surface Anomaly Detection with
  Attention-based Single Deterministic Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YeongHyeon Park, Sungho Kang, Myung Jin Kim, Yeonho Lee, Juneho Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection (AD) in surface inspection is an essential yet challenging
task in manufacturing due to the quantity imbalance problem of scarce abnormal
data. To overcome the above, a reconstruction encoder-decoder (ED) such as
autoencoder or U-Net which is trained with only anomaly-free samples is widely
adopted, in the hope that unseen abnormals should yield a larger reconstruction
error than normal. Over the past years, researches on self-supervised
reconstruction-by-inpainting have been reported. They mask out suspected
defective regions for inpainting in order to make them invisible to the
reconstruction ED to deliberately cause inaccurate reconstruction for
abnormals. However, their limitation is multiple random masking to cover the
whole input image due to defective regions not being known in advance. We
propose a novel reconstruction-by-inpainting method dubbed Excision and
Recovery (EAR) that features single deterministic masking. For this, we exploit
a pre-trained spatial attention model to predict potential suspected defective
regions that should be masked out. We also employ a variant of U-Net as our ED
to further limit the reconstruction ability of the U-Net model for abnormals,
in which skip connections of different layers can be selectively disabled. In
the training phase, all the skip connections are switched on to fully take the
benefits from the U-Net architecture. In contrast, for inferencing, we only
keep deeper skip connections with shallower connections off. We validate the
effectiveness of EAR using an MNIST pre-trained attention for a commonly used
surface AD dataset, KolektorSDD2. The experimental results show that EAR
achieves both better AD performance and higher throughput than state-of-the-art
methods. We expect that the proposed EAR model can be widely adopted as
training and inference strategies for AD purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Multimodal Learning with Missing Modalities via
  Parameter-Efficient Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning seeks to utilize data from multiple sources to improve
the overall performance of downstream tasks. It is desirable for redundancies
in the data to make multimodal systems robust to missing or corrupted
observations in some correlated modalities. However, we observe that the
performance of several existing multimodal networks significantly deteriorates
if one or multiple modalities are absent at test time. To enable robustness to
missing modalities, we propose simple and parameter-efficient adaptation
procedures for pretrained multimodal networks. In particular, we exploit
low-rank adaptation and modulation of intermediate features to compensate for
the missing modalities. We demonstrate that such adaptation can partially
bridge performance drop due to missing modalities and outperform independent,
dedicated networks trained for the available modality combinations in some
cases. The proposed adaptation requires extremely small number of parameters
(e.g., fewer than 0.7% of the total parameters in most experiments). We conduct
a series of experiments to highlight the robustness of our proposed method
using diverse datasets for RGB-thermal and RGB-Depth semantic segmentation,
multimodal material segmentation, and multimodal sentiment analysis tasks. Our
proposed method demonstrates versatility across various tasks and datasets, and
outperforms existing methods for robust multimodal learning with missing
modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weibin Liao, Xuhong Li, Qingzhong Wang, Yanwu Xu, Zhaozheng Yin, Haoyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While pre-training on object detection tasks, such as Common Objects in
Contexts (COCO) [1], could significantly boost the performance of cell
segmentation, it still consumes on massive fine-annotated cell images [2] with
bounding boxes, masks, and cell types for every cell in every image, to
fine-tune the pre-trained model. To lower the cost of annotation, this work
considers the problem of pre-training DNN models for few-shot cell
segmentation, where massive unlabeled cell images are available but only a
small proportion is annotated. Hereby, we propose Cross-domain Unsupervised
Pre-training, namely CUPre, transferring the capability of object detection and
instance segmentation for common visual objects (learned from COCO) to the
visual domain of cells using unlabeled images. Given a standard COCO
pre-trained network with backbone, neck, and head modules, CUPre adopts an
alternate multi-task pre-training (AMT2) procedure with two sub-tasks -- in
every iteration of pre-training, AMT2 first trains the backbone with cell
images from multiple cell datasets via unsupervised momentum contrastive
learning (MoCo) [3], and then trains the whole model with vanilla COCO datasets
via instance segmentation. After pre-training, CUPre fine-tunes the whole model
on the cell segmentation task using a few annotated images. We carry out
extensive experiments to evaluate CUPre using LIVECell [2] and BBBC038 [4]
datasets in few-shot instance segmentation settings. The experiment shows that
CUPre can outperform existing pre-training methods, achieving the highest
average precision (AP) for few-shot cell segmentation and detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sub-token ViT Embedding via Stochastic Resonance Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Lao, Yangchao Wu, Tian Yu Liu, Alex Wong, Stefano Soatto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discover the presence of quantization artifacts in Vision Transformers
(ViTs), which arise due to the image tokenization step inherent in these
architectures. These artifacts result in coarsely quantized features, which
negatively impact performance, especially on downstream dense prediction tasks.
We present a zero-shot method to improve how pre-trained ViTs handle spatial
quantization. In particular, we propose to ensemble the features obtained from
perturbing input images via sub-token spatial translations, inspired by
Stochastic Resonance, a method traditionally applied to climate dynamics and
signal processing. We term our method ``Stochastic Resonance Transformer"
(SRT), which we show can effectively super-resolve features of pre-trained
ViTs, capturing more of the local fine-grained structures that might otherwise
be neglected as a result of tokenization. SRT can be applied at any layer, on
any task, and does not require any fine-tuning. The advantage of the former is
evident when applied to monocular depth prediction, where we show that
ensembling model outputs are detrimental while applying SRT on intermediate ViT
features outperforms the baseline models by an average of 4.7% and 14.9% on the
RMSE and RMSE-log metrics across three different architectures. When applied to
semi-supervised video object segmentation, SRT also improves over the baseline
models uniformly across all metrics, and by an average of 2.4% in F&J score. We
further show that these quantization artifacts can be attenuated to some extent
via self-distillation. On the unsupervised salient region segmentation, SRT
improves upon the base model by an average of 2.1% on the maxF metric. Finally,
despite operating purely on pixel-level features, SRT generalizes to non-dense
prediction tasks such as image retrieval and object discovery, yielding
consistent improvements of up to 2.6% and 1.0% respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Increasing the Robustness of Predictive Steering-Control
  Autonomous Navigation Systems Against Dash Cam Image Angle Perturbations Due
  to Pothole Encounters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Aarya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle manufacturers are racing to create autonomous navigation and steering
control algorithms for their vehicles. These software are made to handle
various real-life scenarios such as obstacle avoidance and lane maneuvering.
There is some ongoing research to incorporate pothole avoidance into these
autonomous systems. However, there is very little research on the effect of
hitting a pothole on the autonomous navigation software that uses cameras to
make driving decisions. Perturbations in the camera angle when hitting a
pothole can cause errors in the predicted steering angle. In this paper, we
present a new model to compensate for such angle perturbations and reduce any
errors in steering control prediction algorithms. We evaluate our model on
perturbations of publicly available datasets and show our model can reduce the
errors in the estimated steering angle from perturbed images to 2.3%, making
autonomous steering control robust against the dash cam image angle
perturbations induced when one wheel of a car goes over a pothole.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding prompt engineering may not require rethinking
  generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Akinwande, Yiding Jiang, Dylan Sam, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot learning in prompted vision-language models, the practice of
crafting prompts to build classifiers without an explicit training process, has
achieved impressive performance in many settings. This success presents a
seemingly surprising observation: these methods suffer relatively little from
overfitting, i.e., when a prompt is manually engineered to achieve low error on
a given training set (thus rendering the method no longer actually zero-shot),
the approach still performs well on held-out test data. In this paper, we show
that we can explain such performance well via recourse to classical PAC-Bayes
bounds. Specifically, we show that the discrete nature of prompts, combined
with a PAC-Bayes prior given by a language model, results in generalization
bounds that are remarkably tight by the standards of the literature: for
instance, the generalization bound of an ImageNet classifier is often within a
few percentage points of the true test error. We demonstrate empirically that
this holds for existing handcrafted prompts and prompts generated through
simple greedy search. Furthermore, the resulting bound is well-suited for model
selection: the models with the best bound typically also have the best test
performance. This work thus provides a possible justification for the
widespread practice of prompt engineering, even if it seems that such methods
could potentially overfit the training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Descent Provably Solves Nonlinear Tomographic Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Fridovich-Keil, Fabrizio Valdivia, Gordon Wetzstein, Benjamin Recht, Mahdi Soltanolkotabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computed tomography (CT), the forward model consists of a linear Radon
transform followed by an exponential nonlinearity based on the attenuation of
light according to the Beer-Lambert Law. Conventional reconstruction often
involves inverting this nonlinearity as a preprocessing step and then solving a
convex inverse problem. However, this nonlinear measurement preprocessing
required to use the Radon transform is poorly conditioned in the vicinity of
high-density materials, such as metal. This preprocessing makes CT
reconstruction methods numerically sensitive and susceptible to artifacts near
high-density regions. In this paper, we study a technique where the signal is
directly reconstructed from raw measurements through the nonlinear forward
model. Though this optimization is nonconvex, we show that gradient descent
provably converges to the global optimum at a geometric rate, perfectly
reconstructing the underlying signal with a near minimal number of random
measurements. We also prove similar results in the under-determined setting
where the number of measurements is significantly smaller than the dimension of
the signal. This is achieved by enforcing prior structural information about
the signal through constraints on the optimization variables. We illustrate the
benefits of direct nonlinear CT reconstruction with cone-beam CT experiments on
synthetic and real 3D volumes. We show that this approach reduces metal
artifacts compared to a commercial reconstruction of a human skull with metal
dental crowns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ILSH: The Imperial Light-Stage Head Dataset for Human Head View
  Synthesis <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiali Zheng, Youngkyoon Jang, Athanasios Papaioannou, Christos Kampouris, Rolandos Alexandros Potamias, Foivos Paraperas Papantoniou, Efstathios Galanakis, Ales Leonardis, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Imperial Light-Stage Head (ILSH) dataset, a novel
light-stage-captured human head dataset designed to support view synthesis
academic challenges for human heads. The ILSH dataset is intended to facilitate
diverse approaches, such as scene-specific or generic neural rendering,
multiple-view geometry, 3D vision, and computer graphics, to further advance
the development of photo-realistic human avatars. This paper details the setup
of a light-stage specifically designed to capture high-resolution (4K) human
head images and describes the process of addressing challenges (preprocessing,
ethical issues) in collecting high-quality data. In addition to the data
collection, we address the split of the dataset into train, validation, and
test sets. Our goal is to design and support a fair view synthesis challenge
task for this novel dataset, such that a similar level of performance can be
maintained and expected when using the test set, as when using the validation
set. The ILSH dataset consists of 52 subjects captured using 24 cameras with
all 82 lighting sources turned on, resulting in a total of 1,248 close-up head
images, border masks, and camera pose pairs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023 Workshop, 9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Bao, Srinivasan Sivanandan, Theofanis Karaletsos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformer (ViT) has emerged as a powerful architecture in the realm
of modern computer vision. However, its application in certain imaging fields,
such as microscopy and satellite imaging, presents unique challenges. In these
domains, images often contain multiple channels, each carrying semantically
distinct and independent information. Furthermore, the model must demonstrate
robustness to sparsity in input channels, as they may not be densely available
during training or testing. In this paper, we propose a modification to the ViT
architecture that enhances reasoning across the input channels and introduce
Hierarchical Channel Sampling (HCS) as an additional regularization technique
to ensure robustness when only partial channels are presented during test time.
Our proposed model, ChannelViT, constructs patch tokens independently from each
input channel and utilizes a learnable channel embedding that is added to the
patch tokens, similar to positional embeddings. We evaluate the performance of
ChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat
(satellite imaging). Our results show that ChannelViT outperforms ViT on
classification tasks and generalizes well, even when a subset of input channels
is used during testing. Across our experiments, HCS proves to be a powerful
regularizer, independent of the architecture employed, suggesting itself as a
straightforward technique for robust ViT training. Lastly, we find that
ChannelViT generalizes effectively even when there is limited access to all
channels during training, highlighting its potential for multi-channel imaging
under real-world conditions with sparse sensors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Domain Long-Tailed Learning by Augmenting Disentangled
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14358v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14358v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Yang, Huaxiu Yao, Allan Zhou, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an inescapable long-tailed class-imbalance issue in many real-world
classification problems. Current methods for addressing this problem only
consider scenarios where all examples come from the same distribution. However,
in many cases, there are multiple domains with distinct class imbalance. We
study this multi-domain long-tailed learning problem and aim to produce a model
that generalizes well across all classes and domains. Towards that goal, we
introduce TALLY, a method that addresses this multi-domain long-tailed learning
problem. Built upon a proposed selective balanced sampling strategy, TALLY
achieves this by mixing the semantic representation of one example with the
domain-associated nuisances of another, producing a new representation for use
as data augmentation. To improve the disentanglement of semantic
representations, TALLY further utilizes a domain-invariant class prototype that
averages out domain-specific effects. We evaluate TALLY on several benchmarks
and real-world datasets and find that it consistently outperforms other
state-of-the-art methods in both subpopulation and domain shift. Our code and
data have been released at https://github.com/huaxiuyao/TALLY.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedFTN: Personalized Federated Learning with Deep Feature Transformation
  Network for Multi-institutional Low-count PET Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.00570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.00570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Zhou, Huidong Xie, Qiong Liu, Xiongchao Chen, Xueqi Guo, Zhicheng Feng, Jun Hou, S. Kevin Zhou, Biao Li, Axel Rominger, Kuangyu Shi, James S. Duncan, Chi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-count PET is an efficient way to reduce radiation exposure and
acquisition time, but the reconstructed images often suffer from low
signal-to-noise ratio (SNR), thus affecting diagnosis and other downstream
tasks. Recent advances in deep learning have shown great potential in improving
low-count PET image quality, but acquiring a large, centralized, and diverse
dataset from multiple institutions for training a robust model is difficult due
to privacy and security concerns of patient data. Moreover, low-count PET data
at different institutions may have different data distribution, thus requiring
personalized models. While previous federated learning (FL) algorithms enable
multi-institution collaborative training without the need of aggregating local
data, addressing the large domain shift in the application of
multi-institutional low-count PET denoising remains a challenge and is still
highly under-explored. In this work, we propose FedFTN, a personalized
federated learning strategy that addresses these challenges. FedFTN uses a
local deep feature transformation network (FTN) to modulate the feature outputs
of a globally shared denoising network, enabling personalized low-count PET
denoising for each institution. During the federated learning process, only the
denoising network's weights are communicated and aggregated, while the FTN
remains at the local institutions for feature transformation. We evaluated our
method using a large-scale dataset of multi-institutional low-count PET imaging
data from three medical centers located across three continents, and showed
that FedFTN provides high-quality low-count PET images, outperforming previous
baseline FL reconstruction methods across all low-count levels at all three
institutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures, Accepted at Medical Image Analysis Journal
  (MedIA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuroSURF: Neural Uncertainty-aware Robust Surface Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Sang, Abhishek Saroha, Maolin Gao, Daniel Cremers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit functions have become popular for representing surfaces
because they offer an adaptive resolution and support arbitrary topologies.
While previous works rely on ground truth point clouds, they often ignore the
effect of input quality and sampling methods on the reconstruction. In this
paper, we introduce NeuroSURF, which generates significantly improved
qualitative and quantitative reconstructions driven by a novel sampling and
interpolation technique. We show that employing a sampling technique that
considers the geometric characteristics of inputs can enhance the training
process. To this end, we introduce a strategy that efficiently computes
differentiable geometric features, namely, mean curvatures, to augment the
sampling phase during the training period. Moreover, we augment the neural
implicit surface representation with uncertainty, which offers insights into
the occupancy and reliability of the output signed distance value, thereby
expanding representation capabilities into open surfaces. Finally, we
demonstrate that NeuroSURF leads to state-of-the-art reconstructions on both
synthetic and real-world data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Survey of Dataset Refinement for Problems in Computer Vision Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11717v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11717v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijing Wan, Zhixiang Wang, CheukTing Chung, Zheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale datasets have played a crucial role in the advancement of
computer vision. However, they often suffer from problems such as class
imbalance, noisy labels, dataset bias, or high resource costs, which can
inhibit model performance and reduce trustworthiness. With the advocacy of
data-centric research, various data-centric solutions have been proposed to
solve the dataset problems mentioned above. They improve the quality of
datasets by re-organizing them, which we call dataset refinement. In this
survey, we provide a comprehensive and structured overview of recent advances
in dataset refinement for problematic computer vision datasets. Firstly, we
summarize and analyze the various problems encountered in large-scale computer
vision datasets. Then, we classify the dataset refinement algorithms into three
categories based on the refinement process: data sampling, data subset
selection, and active learning. In addition, we organize these dataset
refinement methods according to the addressed data problems and provide a
systematic comparative description. We point out that these three types of
dataset refinement have distinct advantages and disadvantages for dataset
problems, which informs the choice of the data-centric method appropriate to a
particular research objective. Finally, we summarize the current literature and
propose potential future research topics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 10 figures, to be published in ACM Computing Surveys</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kim Youwang, Lee Hyun, Kim Sung-Bin, Suekyeong Nam, Janghoon Ju, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose NeuFace, a 3D face mesh pseudo annotation method on videos via
neural re-parameterized optimization. Despite the huge progress in 3D face
reconstruction methods, generating reliable 3D face labels for in-the-wild
dynamic videos remains challenging. Using NeuFace optimization, we annotate the
per-view/-frame accurate and consistent face meshes on large-scale face videos,
called the NeuFace-dataset. We investigate how neural re-parameterization helps
to reconstruct image-aligned facial details on 3D meshes via gradient analysis.
By exploiting the naturalness and diversity of 3D faces in our dataset, we
demonstrate the usefulness of our dataset for 3D face-related tasks: improving
the reconstruction accuracy of an existing 3D face reconstruction model and
learning 3D facial motion prior. Code and datasets will be available at
https://neuface-dataset.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, and 3 tables for the main paper. 8 pages, 6
  figures and 3 tables for the appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CancerUniT: Towards a Single Unified Model for Effective Detection,
  Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection
  of CT Scans <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieneng Chen, Yingda Xia, Jiawen Yao, Ke Yan, Jianpeng Zhang, Le Lu, Fakai Wang, Bo Zhou, Mingyan Qiu, Qihang Yu, Mingze Yuan, Wei Fang, Yuxing Tang, Minfeng Xu, Jian Zhou, Yuqian Zhao, Qifeng Wang, Xianghua Ye, Xiaoli Yin, Yu Shi, Xin Chen, Jingren Zhou, Alan Yuille, Zaiyi Liu, Ling Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human readers or radiologists routinely perform full-body multi-organ
multi-disease detection and diagnosis in clinical practice, while most medical
AI systems are built to focus on single organs with a narrow list of a few
diseases. This might severely limit AI's clinical adoption. A certain number of
AI models need to be assembled non-trivially to match the diagnostic process of
a human reading a CT scan. In this paper, we construct a Unified Tumor
Transformer (CancerUniT) model to jointly detect tumor existence & location and
diagnose tumor characteristics for eight major cancers in CT scans. CancerUniT
is a query-based Mask Transformer model with the output of multi-tumor
prediction. We decouple the object queries into organ queries, tumor detection
queries and tumor diagnosis queries, and further establish hierarchical
relationships among the three groups. This clinically-inspired architecture
effectively assists inter- and intra-organ representation learning of tumors
and facilitates the resolution of these complex, anatomically related
multi-organ cancer image reading tasks. CancerUniT is trained end-to-end using
a curated large-scale CT images of 10,042 patients including eight major types
of cancers and occurring non-cancer tumors (all are pathology-confirmed with 3D
tumor masks annotated by radiologists). On the test set of 631 patients,
CancerUniT has demonstrated strong performance under a set of clinically
relevant evaluation metrics, substantially outperforming both multi-disease
methods and an assembly of eight single-organ expert models in tumor detection,
segmentation, and diagnosis. This moves one step closer towards a universal
high performance cancer screening tool.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023 Camera Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CellViT: Vision Transformers for Precise Cell Segmentation and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Hörst, Moritz Rempe, Lukas Heine, Constantin Seibold, Julius Keyl, Giulia Baldini, Selma Ugurel, Jens Siveke, Barbara Grünwald, Jan Egger, Jens Kleesiek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nuclei detection and segmentation in hematoxylin and eosin-stained (H&E)
tissue images are important clinical tasks and crucial for a wide range of
applications. However, it is a challenging task due to nuclei variances in
staining and size, overlapping boundaries, and nuclei clustering. While
convolutional neural networks have been extensively used for this task, we
explore the potential of Transformer-based networks in this domain. Therefore,
we introduce a new method for automated instance segmentation of cell nuclei in
digitized tissue samples using a deep learning architecture based on Vision
Transformer called CellViT. CellViT is trained and evaluated on the PanNuke
dataset, which is one of the most challenging nuclei instance segmentation
datasets, consisting of nearly 200,000 annotated Nuclei into 5 clinically
important classes in 19 tissue types. We demonstrate the superiority of
large-scale in-domain and out-of-domain pre-trained Vision Transformers by
leveraging the recently published Segment Anything Model and a ViT-encoder
pre-trained on 104 million histological image patches - achieving
state-of-the-art nuclei detection and instance segmentation performance on the
PanNuke dataset with a mean panoptic quality of 0.50 and an F1-detection score
of 0.83. The code is publicly available at https://github.com/TIO-IKIM/CellViT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures, appendix included</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for
  Low-Dose CT Denoising and Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Gao, Zilong Li, Junping Zhang, Yi Zhang, Hongming Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-dose computed tomography (CT) images suffer from noise and artifacts due
to photon starvation and electronic noise. Recently, some works have attempted
to use diffusion models to address the over-smoothness and training instability
encountered by previous deep-learning-based denoising models. However,
diffusion models suffer from long inference times due to the large number of
sampling steps involved. Very recently, cold diffusion model generalizes
classical diffusion models and has greater flexibility. Inspired by the cold
diffusion, this paper presents a novel COntextual eRror-modulated gEneralized
Diffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First,
CoreDiff utilizes LDCT images to displace the random Gaussian noise and employs
a novel mean-preserving degradation operator to mimic the physical process of
CT degradation, significantly reducing sampling steps thanks to the informative
LDCT images as the starting point of the sampling process. Second, to alleviate
the error accumulation problem caused by the imperfect restoration operator in
the sampling process, we propose a novel ContextuaL Error-modulAted Restoration
Network (CLEAR-Net), which can leverage contextual information to constrain the
sampling process from structural distortion and modulate time step embedding
features for better alignment with the input at the next time step. Third, to
rapidly generalize to a new, unseen dose level with as few resources as
possible, we devise a one-shot learning framework to make CoreDiff generalize
faster and better using only a single LDCT image (un)paired with NDCT.
Extensive experimental results on two datasets demonstrate that our CoreDiff
outperforms competing methods in denoising and generalization performance, with
a clinically acceptable inference time. Source code is made available at
https://github.com/qgao21/CoreDiff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Medical Imaging, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative AI for Rapid Diffusion MRI with Improved Image Quality,
  Reliability and Generalizability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Sadikov, Xinlei Pan, Hannah Choi, Lanya T. Cai, Pratik Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion MRI is a non-invasive, in-vivo biomedical imaging method for
mapping tissue microstructure. Applications include structural connectivity
imaging of the human brain and detecting microstructural neural changes.
However, acquiring high signal-to-noise ratio dMRI datasets with high angular
and spatial resolution requires prohibitively long scan times, limiting usage
in many important clinical settings, especially for children, the elderly, and
in acute neurological disorders that may require conscious sedation or general
anesthesia. We employ a Swin UNEt Transformers model, trained on augmented
Human Connectome Project data and conditioned on registered T1 scans, to
perform generalized denoising of dMRI. We also qualitatively demonstrate
super-resolution with artificially downsampled HCP data in normal adult
volunteers. Remarkably, Swin UNETR can be fine-tuned for an out-of-domain
dataset with a single example scan, as we demonstrate on dMRI of children with
neurodevelopmental disorders and of adults with acute evolving traumatic brain
injury, each cohort scanned on different models of scanners with different
imaging protocols at different sites. We exceed current state-of-the-art
denoising methods in accuracy and test-retest reliability of rapid diffusion
tensor imaging requiring only 90 seconds of scan time. Applied to tissue
microstructural modeling of dMRI, Swin UNETR denoising achieves dramatic
improvements over the state-of-the-art for test-retest reliability of
intracellular volume fraction and free water fraction measurements and can
remove heavy-tail noise, improving biophysical modeling fidelity. Swin UNeTR
enables rapid diffusion MRI with unprecedented accuracy and reliability,
especially for probing biological tissues for scientific and clinical
applications. The code and model are publicly available at
https://github.com/ucsfncl/dmri-swin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alloy Das, Sanket Biswas, Umapada Pal, Josep Lladós
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When used in a real-world noisy environment, the capacity to generalize to
multiple domains is essential for any autonomous scene text spotting system.
However, existing state-of-the-art methods employ pretraining and fine-tuning
strategies on natural scene datasets, which do not exploit the feature
interaction across other complex domains. In this work, we explore and
investigate the problem of domain-agnostic scene text spotting, i.e., training
a model on multi-domain source data such that it can directly generalize to
target domains rather than being specialized for a specific domain or scenario.
In this regard, we present the community a text spotting validation benchmark
called Under-Water Text (UWT) for noisy underwater scenes to establish an
important case study. Moreover, we also design an efficient super-resolution
based end-to-end transformer baseline called DA-TextSpotter which achieves
comparable or superior performance over existing text spotting architectures
for both regular and arbitrary-shaped scene text spotting benchmarks in terms
of both accuracy and model efficiency. The dataset, code and pre-trained models
will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Leakage of Fuzzy Matchers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.13717v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.13717v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Durbet, Kevin Thiry-Atighehchi, Dorine Chagnon, Paul-Marie Grollemund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a biometric recognition system, the matcher compares an old and a fresh
template to decide if it is a match or not. Beyond the binary output (`yes' or
`no'), more information is computed. This paper provides an in-depth analysis
of information leakage during distance evaluation, with an emphasis on
threshold-based obfuscated distance (\textit{i.e.}, Fuzzy Matcher). Leakage can
occur due to a malware infection or the use of a weakly privacy-preserving
matcher, exemplified by side channel attacks or partially obfuscated designs.
We provide an exhaustive catalog of information leakage scenarios as well as
their impacts on the security concerning data privacy. Each of the scenarios
leads to generic attacks whose impacts are expressed in terms of computational
costs, hence allowing the establishment of upper bounds on the security level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor corrections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing the Power of Multi-Lingual Datasets for Pre-training: Towards
  Enhancing Text Spotting Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alloy Das, Sanket Biswas, Ayan Banerjee, Saumik Bhattacharya, Josep Lladós, Umapada Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adaptation capability to a wide range of domains is crucial for scene
text spotting models when deployed to real-world conditions. However, existing
state-of-the-art (SOTA) approaches usually incorporate scene text detection and
recognition simply by pretraining on natural scene text datasets, which do not
directly exploit the intermediate feature representations between multiple
domains. Here, we investigate the problem of domain-adaptive scene text
spotting, i.e., training a model on multi-domain source data such that it can
directly adapt to target domains rather than being specialized for a specific
domain or scenario. Further, we investigate a transformer baseline called
Swin-TESTR to focus on solving scene-text spotting for both regular and
arbitrary-shaped scene text along with an exhaustive evaluation. The results
clearly demonstrate the potential of intermediate representations to achieve
significant performance on text spotting benchmarks across multiple domains
(e.g. language, synth-to-real, and documents). both in terms of accuracy and
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FusionFormer: A Multi-sensory Fusion in Bird's-Eye-View and Temporal
  Consistent Transformer for 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyong Hu, Hang Zheng, Kun Li, Jianyun Xu, Weibo Mao, Maochun Luo, Lingxuan Wang, Mingxia Chen, Qihao Peng, Kaixuan Liu, Yiru Zhao, Peihan Hao, Minzhe Liu, Kaicheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-sensor modal fusion has demonstrated strong advantages in 3D object
detection tasks. However, existing methods that fuse multi-modal features
require transforming features into the bird's eye view space and may lose
certain information on Z-axis, thus leading to inferior performance. To this
end, we propose a novel end-to-end multi-modal fusion transformer-based
framework, dubbed FusionFormer, that incorporates deformable attention and
residual structures within the fusion encoding module. Specifically, by
developing a uniform sampling strategy, our method can easily sample from 2D
image and 3D voxel features spontaneously, thus exploiting flexible
adaptability and avoiding explicit transformation to the bird's eye view space
during the feature concatenation process. We further implement a residual
structure in our feature encoder to ensure the model's robustness in case of
missing an input modality. Through extensive experiments on a popular
autonomous driving benchmark dataset, nuScenes, our method achieves
state-of-the-art single model performance of 72.6% mAP and 75.1% NDS in the 3D
object detection task without test time augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deformable Generator Networks: Unsupervised Disentanglement of
  Appearance and Geometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1806.06298v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1806.06298v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianglei Xing, Ruiqi Gao, Tian Han, Song-Chun Zhu, Ying Nian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deformable generator model to disentangle the appearance and
geometric information for both image and video data in a purely unsupervised
manner. The appearance generator network models the information related to
appearance, including color, illumination, identity or category, while the
geometric generator performs geometric warping, such as rotation and
stretching, through generating deformation field which is used to warp the
generated appearance to obtain the final image or video sequences. Two
generators take independent latent vectors as input to disentangle the
appearance and geometric information from image or video sequences. For video
data, a nonlinear transition model is introduced to both the appearance and
geometric generators to capture the dynamics over time. The proposed scheme is
general and can be easily integrated into different generative models. An
extensive set of qualitative and quantitative experiments shows that the
appearance and geometric information can be well disentangled, and the learned
geometric generator can be conveniently transferred to other image datasets to
facilitate knowledge transfer tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The version of IEEE Transactions on Pattern Analysis and Machine
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DragDiffusion: Harnessing Diffusion Models for Interactive Point-based
  Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14435v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14435v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and controllable image editing is a challenging task that has
attracted significant attention recently. Notably, DragGAN is an interactive
point-based image editing framework that achieves impressive editing results
with pixel-level precision. However, due to its reliance on generative
adversarial networks (GANs), its generality is limited by the capacity of
pretrained GAN models. In this work, we extend this editing framework to
diffusion models and propose a novel approach DragDiffusion. By harnessing
large-scale pretrained diffusion models, we greatly enhance the applicability
of interactive point-based editing on both real and diffusion-generated images.
Our approach involves optimizing the diffusion latents to achieve precise
spatial control. The supervision signal of this optimization process is from
the diffusion model's UNet features, which are known to contain rich semantic
and geometric information. Moreover, we introduce two additional techniques,
namely LoRA fine-tuning and latent-MasaCtrl, to further preserve the identity
of the original image. Lastly, we present a challenging benchmark dataset
called DragBench -- the first benchmark to evaluate the performance of
interactive point-based image editing methods. Experiments across a wide range
of challenging cases (e.g., images with multiple objects, diverse object
categories, various styles, etc.) demonstrate the versatility and generality of
DragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is released at https://github.com/Yujun-Shi/DragDiffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Diffusion Models Are Fast Distribution Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11363v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11363v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen Lei, Qinglong Wang, Peng Cheng, Zhongjie Ba, Zhan Qin, Zhibo Wang, Zhenguang Liu, Kui Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as the \emph{de-facto} generative model for
image synthesis, yet they entail significant training overhead, hindering the
technique's broader adoption in the research community. We observe that these
models are commonly trained to learn all fine-grained visual information from
scratch, thus motivating our investigation on its necessity. In this work, we
show that it suffices to set up pre-training stage to initialize a diffusion
model by encouraging it to learn some primer distribution of the unknown real
image distribution. Then the pre-trained model can be fine-tuned for specific
generation tasks efficiently. To approximate the primer distribution, our
approach centers on masking a high proportion (e.g., up to 90\%) of an input
image and employing masked denoising score matching to denoise visible areas.
Utilizing the learned primer distribution in subsequent fine-tuning, we
efficiently train a ViT-based diffusion model on CelebA-HQ $256 \times 256$ in
the raw pixel space, achieving superior training acceleration compared to
denoising diffusion probabilistic model (DDPM) counterpart and a new FID score
record of 6.73 for ViT-based diffusion models. Moreover, our masked
pre-training technique can be universally applied to various diffusion models
that directly generate images in the pixel space, aiding in the learning of
pre-trained models with superior generalizability. For instance, a diffusion
model pre-trained on VGGFace2 attains a 46\% quality improvement through
fine-tuning on only 10\% data from a different dataset. Our code is available
at \url{https://github.com/jiachenlei/maskdm}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Processing and Machine Learning for Hyperspectral Unmixing: An
  Overview and the HySUPP Python Package 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behnood Rasti, Alexandre Zouaoui, Julien Mairal, Jocelyn Chanussot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spectral pixels are often a mixture of the pure spectra of the materials,
called endmembers, due to the low spatial resolution of hyperspectral sensors,
double scattering, and intimate mixtures of materials in the scenes. Unmixing
estimates the fractional abundances of the endmembers within the pixel.
Depending on the prior knowledge of endmembers, linear unmixing can be divided
into three main groups: supervised, semi-supervised, and unsupervised (blind)
linear unmixing. Advances in Image processing and machine learning
substantially affected unmixing. This paper provides an overview of advanced
and conventional unmixing approaches. Additionally, we draw a critical
comparison between advanced and conventional techniques from the three
categories. We compare the performance of the unmixing techniques on three
simulated and two real datasets. The experimental results reveal the advantages
of different unmixing categories for different unmixing scenarios. Moreover, we
provide an open-source Python-based package available at
https://github.com/BehnoodRasti/HySUPP to reproduce the results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CVTNet: A Cross-View Transformer Network for Place Recognition Using
  LiDAR Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Ma, Guangming Xiong, Jingyi Xu, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-based place recognition (LPR) is one of the most crucial components of
autonomous vehicles to identify previously visited places in GPS-denied
environments. Most existing LPR methods use mundane representations of the
input point cloud without considering different views, which may not fully
exploit the information from LiDAR sensors. In this paper, we propose a
cross-view transformer-based network, dubbed CVTNet, to fuse the range image
views (RIVs) and bird's eye views (BEVs) generated from the LiDAR data. It
extracts correlations within the views themselves using intra-transformers and
between the two different views using inter-transformers. Based on that, our
proposed CVTNet generates a yaw-angle-invariant global descriptor for each
laser scan end-to-end online and retrieves previously seen places by descriptor
matching between the current query scan and the pre-built database. We evaluate
our approach on three datasets collected with different sensor setups and
environmental conditions. The experimental results show that our method
outperforms the state-of-the-art LPR methods with strong robustness to
viewpoint changes and long-time spans. Furthermore, our approach has a good
real-time performance that can run faster than the typical LiDAR frame rate.
The implementation of our method is released as open source at:
https://github.com/BIT-MJY/CVTNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IEEE Transactions on Industrial Informatics 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeSpace MFNet: Precise and Efficient MultiFeature Drone Detection
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Misha Urooj Khan, Mahnoor Dil, Muhammad Zeshan Alam, Farooq Alam Orakazi, Abdullah M. Almasoud, Zeeshan Kaleem, Chau Yuen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing prevalence of unmanned aerial vehicles (UAVs), commonly known
as drones, has generated a demand for reliable detection systems. The
inappropriate use of drones presents potential security and privacy hazards,
particularly concerning sensitive facilities. To overcome those obstacles, we
proposed the concept of MultiFeatureNet (MFNet), a solution that enhances
feature representation by capturing the most concentrated feature maps.
Additionally, we present MultiFeatureNet-Feature Attention (MFNet-FA), a
technique that adaptively weights different channels of the input feature maps.
To meet the requirements of multi-scale detection, we presented the versions of
MFNet and MFNet-FA, namely the small (S), medium (M), and large (L). The
outcomes reveal notable performance enhancements. For optimal bird detection,
MFNet-M (Ablation study 2) achieves an impressive precision of 99.8\%, while
for UAV detection, MFNet-L (Ablation study 2) achieves a precision score of
97.2\%. Among the options, MFNet-FA-S (Ablation study 3) emerges as the most
resource-efficient alternative, considering its small feature map size,
computational demands (GFLOPs), and operational efficiency (in frame per
second). This makes it particularly suitable for deployment on hardware with
limited capabilities. Additionally, MFNet-FA-S (Ablation study 3) stands out
for its swift real-time inference and multiple-object detection due to the
incorporation of the FA module. The proposed MFNet-L with the focus module
(Ablation study 2) demonstrates the most remarkable classification outcomes,
boasting an average precision of 98.4\%, average recall of 96.6\%, average mean
average precision (mAP) of 98.3\%, and average intersection over union (IoU) of
72.8\%. To encourage reproducible research, the dataset, and code for MFNet are
freely available as an open-source project:
github.com/ZeeshanKaleem/MultiFeatureNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted in IEEE TVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TUVF: Learning Generalizable Texture UV Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03040v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03040v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An-Chieh Cheng, Xueting Li, Sifei Liu, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textures are a vital aspect of creating visually appealing and realistic 3D
models. In this paper, we study the problem of generating high-fidelity texture
given shapes of 3D assets, which has been relatively less explored compared
with generic 3D shape modeling. Our goal is to facilitate a controllable
texture generation process, such that one texture code can correspond to a
particular appearance style independent of any input shapes from a category. We
introduce Texture UV Radiance Fields (TUVF) that generate textures in a
learnable UV sphere space rather than directly on the 3D shape. This allows the
texture to be disentangled from the underlying shape and transferable to other
shapes that share the same UV space, i.e., from the same category. We integrate
the UV sphere space with the radiance field, which provides a more efficient
and accurate representation of textures than traditional texture maps. We
perform our experiments on synthetic and real-world object datasets where we
achieve not only realistic synthesis but also substantial improvements over
state-of-the-arts on texture controlling and editing. Project Page:
https://www.anjiecheng.me/TUVF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://www.anjiecheng.me/TUVF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyper-pixel-wise Contrastive Learning Augmented Segmentation Network for
  Old Landslide Detection through Fusing High-Resolution Remote Sensing Images
  and Digital Elevation Model Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Zhou, Yuexing Peng, Wei Li, Junchuan Yu, Daqing Ge, Wei Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a natural disaster, landslide often brings tremendous losses to human
lives, so it urgently demands reliable detection of landslide risks. When
detecting old landslides that present important information for landslide risk
warning, problems such as visual blur and small-sized dataset cause great
challenges when using remote sensing data. To extract accurate semantic
features, a hyper-pixel-wise contrastive learning augmented segmentation
network (HPCL-Net) is proposed, which augments the local salient feature
extraction from boundaries of landslides through HPCL-Net and fuses
heterogeneous infromation in the semantic space from high-resolution remote
sensing images and digital elevation model data. For full utilization of
precious samples, a global hyper-pixel-wise sample pair queues-based
contrastive learning method is developed, which includes the construction of
global queues that store hyper-pixel-wise samples and the updating scheme of a
momentum encoder, reliably enhancing the extraction ability of semantic
features. The proposed HPCL-Net is evaluated on the Loess Plateau old landslide
dataset and experimental results verify that the proposed HPCL-Net greatly
outperforms existing models, where the mIoU is increased from 0.620 to 0.651,
the Landslide IoU is improved from 0.334 to 0.394 and the F1score is enhanced
from 0.501 to 0.565.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attributing Learned Concepts in Neural Networks to Training Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Konz, Charles Godfrey, Madelyn Shapiro, Jonathan Tu, Henry Kvinge, Davis Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model's original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NAS-<span class="highlight-title">NeRF</span>: Generative Neural Architecture Search for Neural Radiance
  Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeejith Nair, Yuhao Chen, Mohammad Javad Shafiee, Alexander Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but
their high computational complexity limits deployability. While existing
neural-based solutions strive for efficiency, they use one-size-fits-all
architectures regardless of scene complexity. The same architecture may be
unnecessarily large for simple scenes but insufficient for complex ones. Thus,
there is a need to dynamically optimize the neural network component of NeRFs
to achieve a balance between computational complexity and specific targets for
synthesis quality. We introduce NAS-NeRF, a generative neural architecture
search strategy that generates compact, scene-specialized NeRF architectures by
balancing architecture complexity and target synthesis quality metrics. Our
method incorporates constraints on target metrics and budgets to guide the
search towards architectures tailored for each scene. Experiments on the
Blender synthetic dataset show the proposed NAS-NeRF can generate architectures
up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$
faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.
Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to
23$\times$ smaller, with 22$\times$ fewer FLOPs, and 4.7$\times$ faster than
baseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made
publicly available at https://saeejithnair.github.io/NAS-NeRF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MV-Map: Offboard HD-Map Generation with Multi-view Consistency <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xie, Ziqi Pang, Yuxiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While bird's-eye-view (BEV) perception models can be useful for building
high-definition maps (HD-Maps) with less human labor, their results are often
unreliable and demonstrate noticeable inconsistencies in the predicted HD-Maps
from different viewpoints. This is because BEV perception is typically set up
in an 'onboard' manner, which restricts the computation and consequently
prevents algorithms from reasoning multiple views simultaneously. This paper
overcomes these limitations and advocates a more practical 'offboard' HD-Map
generation setup that removes the computation constraints, based on the fact
that HD-Maps are commonly reusable infrastructures built offline in data
centers. To this end, we propose a novel offboard pipeline called MV-Map that
capitalizes multi-view consistency and can handle an arbitrary number of frames
with the key design of a 'region-centric' framework. In MV-Map, the target
HD-Maps are created by aggregating all the frames of onboard predictions,
weighted by the confidence scores assigned by an 'uncertainty network'. To
further enhance multi-view consistency, we augment the uncertainty network with
the global 3D structure optimized by a voxelized neural radiance field
(Voxel-NeRF). Extensive experiments on nuScenes show that our MV-Map
significantly improves the quality of HD-Maps, further highlighting the
importance of offboard methods for HD-Map generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reducing Adversarial Training Cost with Gradient Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huihui Gong, Shuo Yang, Siqi Ma, Seyit Camtepe, Surya Nepal, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have achieved state-of-the-art performances in various
domains, while they are vulnerable to the inputs with well-crafted but small
perturbations, which are named after adversarial examples (AEs). Among many
strategies to improve the model robustness against AEs, Projected Gradient
Descent (PGD) based adversarial training is one of the most effective methods.
Unfortunately, the prohibitive computational overhead of generating strong
enough AEs, due to the maximization of the loss function, sometimes makes the
regular PGD adversarial training impractical when using larger and more
complicated models. In this paper, we propose that the adversarial loss can be
approximated by the partial sum of Taylor series. Furthermore, we approximate
the gradient of adversarial loss and propose a new and efficient adversarial
training method, adversarial training with gradient approximation (GAAT), to
reduce the cost of building up robust models. Additionally, extensive
experiments demonstrate that this efficiency improvement can be achieved
without any or with very little loss in accuracy on natural and adversarial
examples, which show that our proposed method saves up to 60\% of the training
time with comparable model test accuracy on MNIST, CIFAR-10 and CIFAR-100
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There are some issues of the experiments. Withraw this manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Input-image Normalization for Solving the Mode Collapse Problem
  in GAN-based X-ray Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O'Reilly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical image datasets can be imbalanced due to the rarity of targeted
diseases. Generative Adversarial Networks play a key role in addressing this
imbalance by enabling the generation of synthetic images to augment datasets.
It is important to generate synthetic images that incorporate a diverse range
of features to accurately represent the distribution of features present in the
training imagery. Furthermore, the absence of diverse features in synthetic
images can degrade the performance of machine learning classifiers. The mode
collapse problem impacts Generative Adversarial Networks' capacity to generate
diversified images. Mode collapse comes in two varieties: intra-class and
inter-class. In this paper, both varieties of the mode collapse problem are
investigated, and their subsequent impact on the diversity of synthetic X-ray
images is evaluated. This work contributes an empirical demonstration of the
benefits of integrating the adaptive input-image normalization with the Deep
Convolutional GAN and Auxiliary Classifier GAN to alleviate the mode collapse
problems. Synthetically generated images are utilized for data augmentation and
training a Vision Transformer model. The classification performance of the
model is evaluated using accuracy, recall, and precision scores. Results
demonstrate that the DCGAN and the ACGAN with adaptive input-image
normalization outperform the DCGAN and ACGAN with un-normalized X-ray images as
evidenced by the superior diversity scores and classification scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the Elsevier Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Efficient Continuous Manifold Learning for Time Series Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungwoo Jeong, Wonjun Ko, Ahmad Wisnu Mulyadi, Heung-Il Suk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling non-Euclidean data is drawing extensive attention along with the
unprecedented successes of deep neural networks in diverse fields.
Particularly, a symmetric positive definite matrix is being actively studied in
computer vision, signal processing, and medical image analysis, due to its
ability to learn beneficial statistical representations. However, owing to its
rigid constraints, it remains challenging to optimization problems and
inefficient computational costs, especially, when incorporating it with a deep
learning framework. In this paper, we propose a framework to exploit a
diffeomorphism mapping between Riemannian manifolds and a Cholesky space, by
which it becomes feasible not only to efficiently solve optimization problems
but also to greatly reduce computation costs. Further, for dynamic modeling of
time-series data, we devise a continuous manifold learning method by
systematically integrating a manifold ordinary differential equation and a
gated recurrent neural network. It is worth noting that due to the nice
parameterization of matrices in a Cholesky space, training our proposed network
equipped with Riemannian geometric metrics is straightforward. We demonstrate
through experiments over regular and irregular time-series datasets that our
proposed model can be efficiently and reliably trained and outperforms existing
manifold methods and state-of-the-art methods in various time-series tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03559v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03559v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwu Xu, Li Sun, Wei Peng, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative methodology for producing high-quality 3D
lung CT images guided by textual information. While diffusion-based generative
models are increasingly used in medical imaging, current state-of-the-art
approaches are limited to low-resolution outputs and underutilize radiology
reports' abundant information. The radiology reports can enhance the generation
process by providing additional guidance and offering fine-grained control over
the synthesis of images. Nevertheless, expanding text-guided generation to
high-resolution 3D images poses significant memory and anatomical
detail-preserving challenges. Addressing the memory issue, we introduce a
hierarchical scheme that uses a modified UNet architecture. We start by
synthesizing low-resolution images conditioned on the text, serving as a
foundation for subsequent generators for complete volumetric data. To ensure
the anatomical plausibility of the generated samples, we provide further
guidance by generating vascular, airway, and lobular segmentation masks in
conjunction with the CT images. The model demonstrates the capability to use
textual input and segmentation tasks to generate synthesized images. The
results of comparative assessments indicate that our approach exhibits superior
performance compared to the most advanced models based on GAN and diffusion
techniques, especially in accurately retaining crucial anatomical features such
as fissure lines, airways, and vascular structures. This innovation introduces
novel possibilities. This study focuses on two main objectives: (1) the
development of a method for creating images based on textual prompts and
anatomical components, and (2) the capability to generate new images
conditioning on anatomical elements. The advancements in image generation can
be applied to enhance numerous downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClusterFormer: Clustering As A Universal Visual Learner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13196v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13196v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James C. Liang, Yiming Cui, Qifan Wang, Tong Geng, Wenguan Wang, Dongfang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents CLUSTERFORMER, a universal vision model that is based on
the CLUSTERing paradigm with TransFORMER. It comprises two novel designs: 1.
recurrent cross-attention clustering, which reformulates the cross-attention
mechanism in Transformer and enables recursive updates of cluster centers to
facilitate strong representation learning; and 2. feature dispatching, which
uses the updated cluster centers to redistribute image features through
similarity-based metrics, resulting in a transparent pipeline. This elegant
design streamlines an explainable and transferable workflow, capable of
tackling heterogeneous vision tasks (i.e., image classification, object
detection, and image segmentation) with varying levels of clustering
granularity (i.e., image-, box-, and pixel-level). Empirical results
demonstrate that CLUSTERFORMER outperforms various well-known specialized
architectures, achieving 83.41% top-1 acc. over ImageNet-1K for image
classification, 54.2% and 47.0% mAP over MS COCO for object detection and
instance segmentation, 52.4% mIoU over ADE20K for semantic segmentation, and
55.8% PQ over COCO Panoptic for panoptic segmentation. For its efficacy, we
hope our work can catalyze a paradigm shift in universal models in computer
vision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Overcoming General Knowledge Loss with Selective Parameter Update 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Zhang, Paul Janson, Rahaf Aljundi, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models encompass an extensive knowledge base and offer remarkable
transferability. However, this knowledge becomes outdated or insufficient over
time. The challenge lies in continuously updating foundation models to
accommodate novel information while retaining their original capabilities.
Leveraging the fact that foundation models have initial knowledge on various
tasks and domains, we propose a novel approach that, instead of updating all
parameters equally, localizes the updates to a sparse set of parameters
relevant to the task being learned. We strike a balance between efficiency and
new tasks performance, while maintaining the transferability and
generalizability of foundation models. We extensively evaluate our method on
foundational vision-language models with a diverse spectrum of continual
learning tasks. Our method achieves improvements on the newly learned tasks
accuracy up to 7% while preserving the pretraining knowledge with a negligible
decrease of 0.9% on a representative control set accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-05T00:00:00Z">2023-10-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">41</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Un-Kidnappable Robot: Acoustic Localization of Sneaking People 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyu Yang, Patrick Grady, Samarth Brahmbhatt, Arun Balajee Vasudevan, Charles C. Kemp, James Hays
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How easy is it to sneak up on a robot? We examine whether we can detect
people using only the incidental sounds they produce as they move, even when
they try to be quiet. We collect a robotic dataset of high-quality 4-channel
audio paired with 360 degree RGB data of people moving in different indoor
settings. We train models that predict if there is a moving person nearby and
their location using only audio. We implement our method on a robot, allowing
it to track a single person moving quietly with only passive audio sensing. For
demonstration videos, see our project page:
https://sites.google.com/view/unkidnappable-robot
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://sites.google.com/view/unkidnappable-robot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContactGen: Generative Contact Modeling for Grasp Generation <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaowei Liu, Yang Zhou, Jimei Yang, Saurabh Gupta, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel object-centric contact representation ContactGen
for hand-object interaction. The ContactGen comprises three components: a
contact map indicates the contact location, a part map represents the contact
hand part, and a direction map tells the contact direction within each part.
Given an input object, we propose a conditional generative model to predict
ContactGen and adopt model-based optimization to predict diverse and
geometrically feasible grasps. Experimental results demonstrate our method can
generate high-fidelity and diverse human grasps for various objects. Project
page: https://stevenlsw.github.io/contactgen/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2023. Website:
  https://stevenlsw.github.io/contactgen/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Text-to-Image Diffusion Models with Reward Backpropagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have recently emerged at the forefront of
image generation, powered by very large-scale unsupervised or weakly supervised
text-to-image training datasets. Due to their unsupervised training,
controlling their behavior in downstream tasks, such as maximizing
human-perceived image quality, image-text alignment, or ethical image
generation, is difficult. Recent works finetune diffusion models to downstream
reward functions using vanilla reinforcement learning, notorious for the high
variance of the gradient estimators. In this paper, we propose AlignProp, a
method that aligns diffusion models to downstream reward functions using
end-to-end backpropagation of the reward gradient through the denoising
process. While naive implementation of such backpropagation would require
prohibitive memory resources for storing the partial derivatives of modern
text-to-image models, AlignProp finetunes low-rank adapter weight modules and
uses gradient checkpointing, to render its memory usage viable. We test
AlignProp in finetuning diffusion models to various objectives, such as
image-text semantic alignment, aesthetics, compressibility and controllability
of the number of objects present, as well as their combinations. We show
AlignProp achieves higher rewards in fewer training steps than alternatives,
while being conceptually simpler, making it a straightforward choice for
optimizing diffusion models for differentiable reward functions of interest.
Code and Visualization results are available at https://align-prop.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://align-prop.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Generative Modeling for Procedural Roundabout Generation
  for Developing Countries <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zarif Ikram, Ling Pan, Dianbo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to limited resources and fast economic growth, designing optimal
transportation road networks with traffic simulation and validation in a
cost-effective manner is vital for developing countries, where extensive manual
testing is expensive and often infeasible. Current rule-based road design
generators lack diversity, a key feature for design robustness. Generative Flow
Networks (GFlowNets) learn stochastic policies to sample from an unnormalized
reward distribution, thus generating high-quality solutions while preserving
their diversity. In this work, we formulate the problem of linking incident
roads to the circular junction of a roundabout by a Markov decision process,
and we leverage GFlowNets as the Junction-Art road generator. We compare our
method with related methods and our empirical results show that our method
achieves better diversity while preserving a high validity score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages. Submitted to ReALML@NeurIPS (2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PV-OSIMr: A Lowest Order Complexity Algorithm for Computing the Delassus
  Matrix 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay Suresha Sathya, Wilm Decre, Jan Swevers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PV-OSIMr, an efficient algorithm for computing the Delassus matrix
(also known as the inverse operational space inertia matrix) for a kinematic
tree, with the lowest order computational complexity known in literature.
PV-OSIMr is derived by optimizing the Popov-Vereshchagin (PV) solver
computations using the compositionality of the force and motion propagators. It
has a computational complexity of O(n + m^2 ) compared to O(n + m^2d) of the
original PV-OSIM algorithm and O(n+md+m^2 ) of the extended force propagator
algorithm (EFPA), where n is the number of joints, m is the number of
constraints and d is the depth of the kinematic tree. Since Delassus matrix
computation requires constructing an m x m sized matrix and must consider all
the n joints at least once, the asymptotic computational complexity of PV-OSIMr
is optimal. We further benchmark our algorithm and find it to be often more
efficient than the PV-OSIM and EFPA in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, submitted for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling
  and Motion Planning <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Schulze, Hod Lipson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A robot self-model is a task-agnostic representation of the robot's physical
morphology that can be used for motion planning tasks in absence of classical
geometric kinematic models. In particular, when the latter are hard to engineer
or the robot's kinematics change unexpectedly, human-free self-modeling is a
necessary feature of truly autonomous agents. In this work, we leverage neural
fields to allow a robot to self-model its kinematics as a neural-implicit query
model learned only from 2D images annotated with camera poses and
configurations. This enables significantly greater applicability than existing
approaches which have been dependent on depth images or geometry knowledge. To
this end, alongside a curricular data sampling strategy, we propose a new
encoder-based neural density field architecture for dynamic object-centric
scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF
robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2%
of the robot's workspace dimension. We demonstrate the capabilities of this
model on a motion planning task as an exemplary downstream application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023 Workshop on Neural Fields for Autonomous Driving and
  Robotics (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Suspended Aerial Manipulation Avatar for Physical Interaction in
  Unstructured Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanyi Kong, Grazia Zambella, Simone Monteleone, Giorgio Grioli, Manuel G. Catalano, Antonio Bicchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a floating robot capable of performing physically
interactive tasks in unstructured environments with human-like dexterity under
human supervision. The robot consists of a humanoid torso attached to a
hexacopter. A two-degree-of-freedom head and two five-degree-of-freedom arms
equipped with softhands provide the requisite dexterity to allow human
operators to carry out various tasks. A robust tendon-driven structure is
purposefully designed for the arms, considerably reducing the impact of arm
inertia on the floating base in motion. In addition, tendons provide
flexibility to the joints, which enhances the robustness of the arm preventing
damage in interaction with the environment. To increase the payload of the
aerial system and the battery life, we use the concept of Suspended Aerial
Manipulation, i.e., the flying humanoid can be connected with a tether to a
structure, e.g., a larger airborne carrier or a supporting crane. Importantly,
to maximize portability and applicability, we adopt a modular approach
exploiting commercial components for the drone hardware and autopilot, while
developing a whole-body outer control loop to stabilize the robot attitude,
compensating for the tether force and for the humanoid head and arm motions.
The humanoid can be controlled by a remote operator, thus effectively realizing
a Suspended Aerial Manipulation Avatar. The proposed system is validated
through experiments in indoor scenarios reproducing post-disaster tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resilient Legged Local Navigation: Learning to Traverse with Compromised
  Perception End-to-End 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Jin, Chong Zhang, Jonas Frey, Nikita Rudin, Matias Mattamala, Cesar Cadena, Marco Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robots must navigate reliably in unknown environments even under
compromised exteroceptive perception, or perception failures. Such failures
often occur when harsh environments lead to degraded sensing, or when the
perception algorithm misinterprets the scene due to limited generalization. In
this paper, we model perception failures as invisible obstacles and pits, and
train a reinforcement learning (RL) based local navigation policy to guide our
legged robot. Unlike previous works relying on heuristics and anomaly detection
to update navigational information, we train our navigation policy to
reconstruct the environment information in the latent space from corrupted
perception and react to perception failures end-to-end. To this end, we
incorporate both proprioception and exteroception into our policy inputs,
thereby enabling the policy to sense collisions on different body parts and
pits, prompting corresponding reactions. We validate our approach in simulation
and on the real quadruped robot ANYmal running in real-time (<10 ms CPU
inference). In a quantitative comparison with existing heuristic-based locally
reactive planners, our policy increases the success rate over 30% when facing
perception failures. Project Page: https://bit.ly/45NBTuh.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website and videos are available at our Project Page:
  https://bit.ly/45NBTuh</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BID-<span class="highlight-title">NeRF</span>: RGB-D image pose estimation with inverted Neural Radiance
  Fields <span class="chip">ICCV23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ágoston István Csehi, Csaba Máté Józsa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which
defines the image pose estimation problem as a NeRF based iterative linear
optimization. NeRFs are novel neural space representation models that can
synthesize photorealistic novel views of real-world scenes or objects. Our
contributions are as follows: we extend the localization optimization objective
with a depth-based loss function, we introduce a multi-image based loss
function where a sequence of images with known relative poses are used without
increasing the computational complexity, we omit hierarchical sampling during
volumetric rendering, meaning only the coarse model is used for pose
estimation, and we how that by extending the sampling interval convergence can
be achieved even or higher initial pose estimate errors. With the proposed
modifications the convergence speed is significantly improved, and the basin of
convergence is substantially extended.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Nerf4ADR workshop of ICCV23 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-Aware Hypothesis & Verification for Generalizable Relative Object
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhao, Tong Zhang, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior methods that tackle the problem of generalizable object pose estimation
highly rely on having dense views of the unseen object. By contrast, we address
the scenario where only a single reference view of the object is available. Our
goal then is to estimate the relative object pose between this reference view
and a query image that depicts the object in a different pose. In this
scenario, robust generalization is imperative due to the presence of unseen
objects during testing and the large-scale object pose variation between the
reference and the query. To this end, we present a new
hypothesis-and-verification framework, in which we generate and evaluate
multiple pose hypotheses, ultimately selecting the most reliable one as the
relative object pose. To measure reliability, we introduce a 3D-aware
verification that explicitly applies 3D transformations to the 3D object
representations learned from the two input images. Our comprehensive
experiments on the Objaverse, LINEMOD, and CO3D datasets evidence the superior
accuracy of our approach in relative pose estimation and its robustness in
large-scale pose variations, when dealing with unseen objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RadaRays: Real-time Simulation of Rotating FMCW Radar for Mobile
  Robotics via Hardware-accelerated Ray Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Mock, Martin Magnusson, Joachim Hertzberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RadaRays allows for the accurate modeling and simulation of rotating FMCW
radar sensors in complex environments, including the simulation of reflection,
refraction, and scattering of radar waves. Our software is able to handle large
numbers of objects and materials, making it suitable for use in a variety of
mobile robotics applications. We demonstrate the effectiveness of RadaRays
through a series of experiments and show that it can more accurately reproduce
the behavior of FMCW radar sensors in a variety of environments, compared to
the ray casting-based lidar-like simulations that are commonly used in
simulators for autonomous driving such as CARLA. Our experiments additionally
serve as valuable reference point for researchers to evaluate their own radar
simulations. By using RadaRays, developers can significantly reduce the time
and cost associated with prototyping and testing FMCW radar-based algorithms.
We also provide a Gazebo plugin that makes our work accessible to the mobile
robotics community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RGBManip: Monocular Image-based Robotic Manipulation through Active
  Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boshi An, Yiran Geng, Kai Chen, Xiaoqi Li, Qi Dou, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation requires accurate perception of the environment, which
poses a significant challenge due to its inherent complexity and constantly
changing nature. In this context, RGB image and point-cloud observations are
two commonly used modalities in visual-based robotic manipulation, but each of
these modalities have their own limitations. Commercial point-cloud
observations often suffer from issues like sparse sampling and noisy output due
to the limits of the emission-reception imaging principle. On the other hand,
RGB images, while rich in texture information, lack essential depth and 3D
information crucial for robotic manipulation. To mitigate these challenges, we
propose an image-only robotic manipulation framework that leverages an
eye-on-hand monocular camera installed on the robot's parallel gripper. By
moving with the robot gripper, this camera gains the ability to actively
perceive object from multiple perspectives during the manipulation process.
This enables the estimation of 6D object poses, which can be utilized for
manipulation. While, obtaining images from more and diverse viewpoints
typically improves pose estimation, it also increases the manipulation time. To
address this trade-off, we employ a reinforcement learning policy to
synchronize the manipulation strategy with active perception, achieving a
balance between 6D pose accuracy and manipulation efficiency. Our experimental
results in both simulated and real-world environments showcase the
state-of-the-art effectiveness of our approach. %, which, to the best of our
knowledge, is the first to achieve robust real-world robotic manipulation
through active pose estimation. We believe that our method will inspire further
research on real-world-oriented robotic manipulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cyber Physical System Information Collection: Robot Location and
  Navigation Method Based on QR Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwei Li, Tao Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a method to estimate the exact location of a camera
in a cyber-physical system using the exact geographic coordinates of four
feature points stored in QR codes(Quick response codes) and the pixel
coordinates of four feature points analyzed from the QR code images taken by
the camera. Firstly, the P4P(Perspective 4 Points) algorithm is designed to
uniquely determine the initial pose estimation value of the QR coordinate
system relative to the camera coordinate system by using the four feature
points of the selected QR code. In the second step, the manifold gradient
optimization algorithm is designed. The rotation matrix and displacement vector
are taken as the initial values of iteration, and the iterative optimization is
carried out to improve the positioning accuracy and obtain the rotation matrix
and displacement vector with higher accuracy. The third step is to convert the
pose of the QR coordinate system with respect to the camera coordinate system
to the pose of the AGV(Automated Guided Vehicle) with respect to the world
coordinate system. Finally, the performance of manifold gradient optimization
algorithm and P4P analytical algorithm are simulated and compared under the
same conditions.One can see that the performance of the manifold gradient
optimization algorithm proposed in this paper is much better than that of the
P4P analytic algorithm when the signal-to-noise ratio is small.With the
increase of the signal-to-noise ratio,the performance of the P4P analytic
algorithm approaches that of the manifold gradient optimization algorithm.when
the noise is same,the performance of manifold gradient optimization algorithm
is better when there are more feature points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RUSOpt: Robotic UltraSound Probe Normalization with Bayesian
  Optimization for In-plane and Out-plane Scanning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Raina, Abhishek Mathur, Richard M. Voyles, Juan Wachs, SH Chandrashekhara, Subir Kumar Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The one of the significant challenges faced by autonomous robotic ultrasound
systems is acquiring high-quality images across different patients. The proper
orientation of the robotized probe plays a crucial role in governing the
quality of ultrasound images. To address this challenge, we propose a
sample-efficient method to automatically adjust the orientation of the
ultrasound probe normal to the point of contact on the scanning surface,
thereby improving the acoustic coupling of the probe and resulting image
quality. Our method utilizes Bayesian Optimization (BO) based search on the
scanning surface to efficiently search for the normalized probe orientation. We
formulate a novel objective function for BO that leverages the contact force
measurements and underlying mechanics to identify the normal. We further
incorporate a regularization scheme in BO to handle the noisy objective
function. The performance of the proposed strategy has been assessed through
experiments on urinary bladder phantoms. These phantoms included planar,
tilted, and rough surfaces, and were examined using both linear and convex
probes with varying search space limits. Further, simulation-based studies have
been carried out using 3D human mesh models. The results demonstrate that the
mean ($\pm$SD) absolute angular error averaged over all phantoms and 3D models
is $\boldsymbol{2.4\pm0.7^\circ}$ and $\boldsymbol{2.1\pm1.3^\circ}$,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE International Conference on Automation Science and
  Engineering (CASE) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kinodynamic Motion Planning for a Team of Multirotors Transporting a
  Cable-Suspended Payload in Cluttered Environments <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled Wahba, Joaquim Ortiz-Haro, Marc Toussaint, Wolfgang Hönig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a motion planner for cable-driven payload transportation using
multiple unmanned aerial vehicles (UAVs) in an environment cluttered with
obstacles. Our planner is kinodynamic, i.e., it considers the full dynamics
model of the transporting system including actuation constraints. Due to the
high dimensionality of the planning problem, we use a hierarchical approach
where we first solve the geometric motion planning using a sampling-based
method with a novel sampler, followed by constrained trajectory optimization
that considers the full dynamics of the system. Both planning stages consider
inter-robot and robot/obstacle collisions. We demonstrate in a
software-in-the-loop simulation that there is a significant benefit in
kinodynamic motion planning for such payload transport systems with respect to
payload tracking error and energy consumption compared to the standard methods
of planning for the payload alone. Notably, we observe a significantly higher
success rate in scenarios where the team formation changes are needed to move
through tight spaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Adaptive Chance-Constrained Safeguards for Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaorun Chen, Binhao Chen, Tairan He, Liang Gong, Chengliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety assurance of Reinforcement Learning (RL) is critical for exploration
in real-world scenarios. In handling the Constrained Markov Decision Process,
current approaches experience intrinsic difficulties in trading-off between
optimality and feasibility. Direct optimization methods cannot strictly
guarantee state-wise in-training safety while projection-based methods are
usually inefficient and correct actions through lengthy iterations. To address
these two challenges, this paper proposes an adaptive surrogate chance
constraint for the safety cost, and a hierarchical architecture that corrects
actions produced by the upper policy layer via a fast Quasi-Newton method.
Theoretical analysis indicates that the relaxed probabilistic constraint can
sufficiently guarantee forward invariance to the safe set. We validate the
proposed method on 4 simulated and real-world safety-critical robotic tasks.
Results indicate that the proposed method can efficiently enforce safety
(nearly zero-violation), while preserving optimality (+23.8%), robustness and
generalizability to stochastic real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design Optimizer for Planar Soft-Growing Robot Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Stroppa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft-growing robots are innovative devices that feature plant-inspired growth
to navigate environments. Thanks to their embodied intelligence of adapting to
their surroundings and the latest innovation in actuation and manufacturing, it
is possible to employ them for specific manipulation tasks. The applications of
these devices include exploration of delicate/dangerous environments,
manipulation of items, or assistance in domestic environments.
  This work presents a novel approach for design optimization of soft-growing
robots, which will be used prior to manufacturing to suggest engineers -- or
robot designer enthusiasts -- the optimal dimension of the robot to be built
for solving a specific task. I modeled the design process as a multi-objective
optimization problem, in which I optimize the kinematic chain of a soft
manipulator to reach targets and avoid unnecessary overuse of material and
resources. The method exploits the advantages of population-based optimization
algorithms, in particular evolutionary algorithms, to transform the problem
from multi-objective into a single-objective thanks to an efficient
mathematical formulation, the novel rank-partitioning algorithm, and obstacle
avoidance integrated within the optimizer operators.
  I tested the proposed method on different tasks to access its optimality,
which showed significant performance in solving the problem. Finally,
comparative experiments showed that the proposed method works better than the
one existing in the literature in terms of precision, resource consumption, and
run time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time-Optimal Trajectory Planning in Highway Scenarios using Basis-Spline
  Parameterization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Dorpmüller, Thomas Schmitz, Naveen Bejagam, Torsten Bertram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Basis splines enable a time-continuous feasibility check with a finite number
of constraints. Constraints apply to the whole trajectory for motion planning
applications that require a collision-free and dynamically feasible trajectory.
Existing motion planners that rely on gradient-based optimization apply time
scaling to implement a shrinking planning horizon. They neither guarantee a
recursively feasible trajectory nor enable reaching two terminal manifold parts
at different time scales. This paper proposes a nonlinear optimization problem
that addresses the drawbacks of existing approaches. Therefore, the spline
breakpoints are included in the optimization variables. Transformations between
spline bases are implemented so a sparse problem formulation is achieved. A
strategy for breakpoint removal enables the convergence into a terminal
manifold. The evaluation in an overtaking scenario shows the influence of the
breakpoint number on the solution quality and the time required for
optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for 2023 IEEE 26th International Conference on Intelligent
  Transportation Systems (ITSC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Benders Decomposition with Continual Learning for Hybrid
  Model Predictive Control in <span class="highlight-title">Dynamic Environment</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Xuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid model predictive control (MPC) with both continuous and discrete
variables is widely applicable to robotic control tasks, especially those
involving contact with the environment. Due to the combinatorial complexity,
the solving speed of hybrid MPC can be insufficient for real-time applications.
In this paper, we proposed a hybrid MPC solver based on Generalized Benders
Decomposition (GBD) with continual learning. The algorithm accumulates cutting
planes from the invariant dual space of the subproblems. After a short
cold-start phase, the accumulated cuts provide warm-starts for the new problem
instances to increase the solving speed. Despite the randomly changing
environment that the control is unprepared for, the solving speed maintains. We
verified our solver on controlling a cart-pole system with randomly moving soft
contact walls and show that the solving speed is 2-3 times faster than the
off-the-shelf solver Gurobi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Human-Robot Collaboration using Constrained Probabilistic
  Human-Motion Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aadi Kothari, Tony Tohme, Xiaotong Zhang, Kamal Youcef-Toumi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion prediction is an essential step for efficient and safe
human-robot collaboration. Current methods either purely rely on representing
the human joints in some form of neural network-based architecture or use
regression models offline to fit hyper-parameters in the hope of capturing a
model encompassing human motion. While these methods provide good initial
results, they are missing out on leveraging well-studied human body kinematic
models as well as body and scene constraints which can help boost the efficacy
of these prediction frameworks while also explicitly avoiding implausible human
joint configurations. We propose a novel human motion prediction framework that
incorporates human joint constraints and scene constraints in a Gaussian
Process Regression (GPR) model to predict human motion over a set time horizon.
This formulation is combined with an online context-aware constraints model to
leverage task-dependent motions. It is tested on a human arm kinematic model
and implemented on a human-robot collaborative setup with a UR5 robot arm to
demonstrate the real-time capability of our approach. Simulations were also
performed on datasets like HA4M and ANDY. The simulation and experimental
results demonstrate considerable improvements in a Gaussian Process framework
when these constraints are explicitly considered.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures. Associated video demonstration can be found at
  https://www.youtube.com/@MITMechatronics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Two-stage Based Social Preference Recognition in Multi-Agent
  Autonomous Driving System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Xue, Dongkun Zhang, Rong Xiong, Yue Wang, Eryun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Reinforcement Learning (MARL) has become a promising solution for
constructing a multi-agent autonomous driving system (MADS) in complex and
dense scenarios. But most methods consider agents acting selfishly, which leads
to conflict behaviors. Some existing works incorporate the concept of social
value orientation (SVO) to promote coordination, but they lack the knowledge of
other agents' SVOs, resulting in conservative maneuvers. In this paper, we aim
to tackle the mentioned problem by enabling the agents to understand other
agents' SVOs. To accomplish this, we propose a two-stage system framework.
Firstly, we train a policy by allowing the agents to share their ground truth
SVOs to establish a coordinated traffic flow. Secondly, we develop a
recognition network that estimates agents' SVOs and integrates it with the
policy trained in the first stage. Experiments demonstrate that our developed
method significantly improves the performance of the driving policy in MADS
compared to two state-of-the-art MARL algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ${\tt MORALS}$: Analysis of High-Dimensional Robot Controllers via
  Topological Tools in a Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ewerton R. Vieira, Aravind Sivaramakrishnan, Sumanth Tangirala, Edgar Granados, Konstantin Mischaikow, Kostas E. Bekris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the region of attraction (${\tt RoA}$) for a robotic system's
controller is essential for safe application and controller composition. Many
existing methods require access to a closed-form expression that limit
applicability to data-driven controllers. Methods that operate only over
trajectory rollouts tend to be data-hungry. In prior work, we have demonstrated
that topological tools based on Morse Graphs offer data-efficient ${\tt RoA}$
estimation without needing an analytical model. They struggle, however, with
high-dimensional systems as they operate over a discretization of the state
space. This paper presents ${\it Mo}$rse Graph-aided discovery of ${\it
R}$egions of ${\it A}$ttraction in a learned ${\it L}$atent ${\it S}$pace
(${\tt MORALS}$). The approach combines autoencoding neural networks with Morse
Graphs. ${\tt MORALS}$ shows promising predictive capabilities in estimating
attractors and their ${\tt RoA}$s for data-driven controllers operating over
high-dimensional systems, including a 67-dim humanoid robot and a 96-dim
3-fingered manipulator. It first projects the dynamics of the controlled system
into a learned latent space. Then, it constructs a reduced form of Morse Graphs
representing the bistability of the underlying dynamics, i.e., detecting when
the controller results in a desired versus an undesired behavior. The
evaluation on high-dimensional robotic datasets indicates the data efficiency
of the approach in ${\tt RoA}$ estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Roadmaps with Gaps over Controllers: Achieving Efficiency in Planning
  under Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aravind Sivaramakrishnan, Noah R. Carver, Sumanth Tangirala, Kostas E. Bekris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to improve the computational efficiency of motion planning
for mobile robots with non-trivial dynamics by taking advantage of learned
controllers. It adopts a decoupled strategy, where a system-specific controller
is first trained offline in an empty environment to deal with the system's
dynamics. For an environment, the proposed approach constructs offline a data
structure, a "Roadmap with Gaps," to approximately learn how to solve planning
queries in this environment using the learned controller. Its nodes correspond
to local regions and edges correspond to applications of the learned control
policy that approximately connect these regions. Gaps arise due to the
controller not perfectly connecting pairs of individual states along edges.
Online, given a query, a tree sampling-based motion planner uses the roadmap so
that the tree's expansion is informed towards the goal region. The tree
expansion selects local subgoals given a wavefront on the roadmap that guides
towards the goal. When the controller cannot reach a subgoal region, the
planner resorts to random exploration to maintain probabilistic completeness
and asymptotic optimality. The experimental evaluation shows that the approach
significantly improves the computational efficiency of motion planning on
various benchmarks, including physics-based vehicular models on uneven and
varying friction terrains as well as a quadrotor under air pressure effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Exploration in Reinforcement Learning: A Generalized Formulation
  and Algorithms <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akifumi Wachi, Wataru Hashimoto, Xun Shen, Kazumune Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe exploration is essential for the practical use of reinforcement learning
(RL) in many real-world scenarios. In this paper, we present a generalized safe
exploration (GSE) problem as a unified formulation of common safe exploration
problems. We then propose a solution of the GSE problem in the form of a
meta-algorithm for safe exploration, MASE, which combines an unconstrained RL
algorithm with an uncertainty quantifier to guarantee safety in the current
episode while properly penalizing unsafe explorations before actual safety
violation to discourage them in future episodes. The advantage of MASE is that
we can optimize a policy while guaranteeing with a high probability that no
safety constraint will be violated under proper assumptions. Specifically, we
present two variants of MASE with different constructions of the uncertainty
quantifier: one based on generalized linear models with theoretical guarantees
of safety and near-optimality, and another that combines a Gaussian process to
ensure safety with a deep RL algorithm to maximize the reward. Finally, we
demonstrate that our proposed algorithm achieves better performance than
state-of-the-art algorithms on grid-world and Safety Gym benchmarks without
violating any safety constraints, even during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Low-level Geometry to High-level Concepts in Visual Servoing of
  Robot Manipulation Task Using Event Knowledge Graphs and Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Jiang, Martin Jagersand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a framework of building knowledgeable robot control
in the scope of smart human-robot interaction, by empowering a basic
uncalibrated visual servoing controller with contextual knowledge through the
joint usage of event knowledge graphs (EKGs) and large-scale pretrained
vision-language models (VLMs). The framework is expanded in twofold: first, we
interpret low-level image geometry as high-level concepts, allowing us to
prompt VLMs and to select geometric features of points and lines for motor
control skills; then, we create an event knowledge graph (EKG) to conceptualize
a robot manipulation task of interest, where the main body of the EKG is
characterized by an executable behavior tree, and the leaves by semantic
concepts relevant to the manipulation context. We demonstrate, in an
uncalibrated environment with real robot trials, that our method lowers the
reliance of human annotation during task interfacing, allows the robot to
perform activities of daily living more easily by treating low-level
geometric-based motor control skills as high-level concepts, and is beneficial
in building cognitive thinking for smart robot applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashu Yamazaki, Taisei Hanyu, Khoa Vo, Thang Pham, Minh Tran, Gianfranco Doretto, Anh Nguyen, Ngan Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise 3D environmental mapping is pivotal in robotics. Existing methods
often rely on predefined concepts during training or are time-intensive when
generating semantic maps. This paper presents Open-Fusion, a groundbreaking
approach for real-time open-vocabulary 3D mapping and queryable scene
representation using RGB-D data. Open-Fusion harnesses the power of a
pre-trained vision-language foundation model (VLFM) for open-set semantic
comprehension and employs the Truncated Signed Distance Function (TSDF) for
swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based
embeddings and their associated confidence maps. These are then integrated with
3D knowledge from TSDF using an enhanced Hungarian-based feature-matching
mechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D
segmentation for open-vocabulary without necessitating additional 3D training.
Benchmark tests on the ScanNet dataset against leading zero-shot methods
highlight Open-Fusion's superiority. Furthermore, it seamlessly combines the
strengths of region-based VLFM and TSDF, facilitating real-time 3D scene
comprehension that includes object concepts and open-world semantics. We
encourage the readers to view the demos on our project page:
https://uark-aicv.github.io/OpenFusion
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRAIL Team Description Paper for RoboCup@Home 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chikaha Tsuji, Dai Komukai, Mimo Shirasaka, Hikaru Wada, Tsunekazu Omija, Aoi Horo, Daiki Furuta, Saki Yamaguchi, So Ikoma, Soshi Tsunashima, Masato Kobayashi, Koki Ishimoto, Yuya Ikeda, Tatsuya Matsushima, Yusuke Iwasawa, Yutaka Matsuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our team, TRAIL, consists of AI/ML laboratory members from The University of
Tokyo. We leverage our extensive research experience in state-of-the-art
machine learning to build general-purpose in-home service robots. We previously
participated in two competitions using Human Support Robot (HSR): RoboCup@Home
Japan Open 2020 (DSPL) and World Robot Summit 2020, equivalent to RoboCup World
Tournament. Throughout the competitions, we showed that a data-driven approach
is effective for performing in-home tasks. Aiming for further development of
building a versatile and fast-adaptable system, in RoboCup @Home 2023, we unify
three technologies that have recently been evaluated as components in the
fields of deep learning and robot learning into a real household robot system.
In addition, to stimulate research all over the RoboCup@Home community, we
build a platform that manages data collected from each site belonging to the
community around the world, taking advantage of the characteristics of the
community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TWICE Dataset: Digital Twin of Test Scenarios in a Controlled
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Novicki Neto, Fabio Reway, Yuri Poledna, Maikol Funk Drechsler, Eduardo Parente Ribeiro, Werner Huber, Christian Icking
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the safe and reliable operation of autonomous vehicles under adverse
weather remains a significant challenge. To address this, we have developed a
comprehensive dataset composed of sensor data acquired in a real test track and
reproduced in the laboratory for the same test scenarios. The provided dataset
includes camera, radar, LiDAR, inertial measurement unit (IMU), and GPS data
recorded under adverse weather conditions (rainy, night-time, and snowy
conditions). We recorded test scenarios using objects of interest such as car,
cyclist, truck and pedestrian -- some of which are inspired by EURONCAP
(European New Car Assessment Programme). The sensor data generated in the
laboratory is acquired by the execution of simulation-based tests in
hardware-in-the-loop environment with the digital twin of each real test
scenario. The dataset contains more than 2 hours of recording, which totals
more than 280GB of data. Therefore, it is a valuable resource for researchers
in the field of autonomous vehicles to test and improve their algorithms in
adverse weather conditions, as well as explore the simulation-to-reality gap.
The dataset is available for download at: https://twicedataset.github.io/site/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 13 figures, submitted to IEEE Sensors Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency Domain Analysis of Nonlinear Series Elastic Actuator via
  Describing Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Motohiro Hirao, Burak Kurkcu, Alireza Ghanbarpour, Masayoshi Tomizuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonlinear stiffness SEAs (NSEAs) inspired by biological muscles offer promise
in achieving adaptable stiffness for assistive robots. While assistive robots
are often designed and compared based on torque capability and control
bandwidth, NSEAs have not been systematically designed in the frequency domain
due to their nonlinearity. The describing function, an analytical concept for
nonlinear systems, offers a means to understand their behavior in the frequency
domain. This paper introduces a frequency domain analysis of nonlinear series
elastic actuators using the describing function method. This framework aims to
equip researchers and engineers with tools for improved design and control in
assistive robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by 2023 IEEE ROBIO conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuromorphic Robust Framework for Concurrent Estimation and Control in
  Dynamical Systems using Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Ahmadvand, Sarah Safura Sharif, Yaser Mike Banad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concurrent estimation and control of robotic systems remains an ongoing
challenge, where controllers rely on data extracted from states/parameters
riddled with uncertainties and noises. Framework suitability hinges on task
complexity and computational constraints, demanding a balance between
computational efficiency and mission-critical accuracy. This study leverages
recent advancements in neuromorphic computing, particularly spiking neural
networks (SNNs), for estimation and control applications. Our presented
framework employs a recurrent network of leaky integrate-and-fire (LIF)
neurons, mimicking a linear quadratic regulator (LQR) through a robust
filtering strategy, a modified sliding innovation filter (MSIF). Benefiting
from both the robustness of MSIF and the computational efficiency of SNN, our
framework customizes SNN weight matrices to match the desired system model
without requiring training. Additionally, the network employs a biologically
plausible firing rule similar to predictive coding. In the presence of
uncertainties, we compare the SNN-LQR-MSIF with non-spiking LQR-MSIF and the
optimal linear quadratic Gaussian (LQG) strategy. Evaluation across a workbench
linear problem and a satellite rendezvous maneuver, implementing the
Clohessy-Wiltshire (CW) model in space robotics, demonstrates that the
SNN-LQR-MSIF achieves acceptable performance in computational efficiency,
robustness, and accuracy. This positions it as a promising solution for
addressing dynamic systems' concurrent estimation and control challenges in
dynamic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WLST: Weak Labels Guided Self-training for Weakly-supervised Domain
  Adaptation on 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsung-Lin Tsou, Tsung-Han Wu, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of domain adaptation (DA) on 3D object detection, most of the
work is dedicated to unsupervised domain adaptation (UDA). Yet, without any
target annotations, the performance gap between the UDA approaches and the
fully-supervised approach is still noticeable, which is impractical for
real-world applications. On the other hand, weakly-supervised domain adaptation
(WDA) is an underexplored yet practical task that only requires few labeling
effort on the target domain. To improve the DA performance in a cost-effective
way, we propose a general weak labels guided self-training framework, WLST,
designed for WDA on 3D object detection. By incorporating autolabeler, which
can generate 3D pseudo labels from 2D bounding boxes, into the existing
self-training pipeline, our method is able to generate more robust and
consistent pseudo labels that would benefit the training process on the target
domain. Extensive experiments demonstrate the effectiveness, robustness, and
detector-agnosticism of our WLST framework. Notably, it outperforms previous
state-of-the-art methods on all evaluation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HandMeThat: Human-Robot Communication in Physical and Social
  Environments <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanming Wan, Jiayuan Mao, Joshua B. Tenenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HandMeThat, a benchmark for a holistic evaluation of instruction
understanding and following in physical and social environments. While previous
datasets primarily focused on language grounding and planning, HandMeThat
considers the resolution of human instructions with ambiguities based on the
physical (object states and relations) and social (human actions and goals)
information. HandMeThat contains 10,000 episodes of human-robot interactions.
In each episode, the robot first observes a trajectory of human actions towards
her internal goal. Next, the robot receives a human instruction and should take
actions to accomplish the subgoal set through the instruction. In this paper,
we present a textual interface for our benchmark, where the robot interacts
with a virtual environment through textual commands. We evaluate several
baseline models on HandMeThat, and show that both offline and online
reinforcement learning algorithms perform poorly on HandMeThat, suggesting
significant room for future work on physical and social human-robot
communications and interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 (Dataset and Benchmark Track). First two authors
  contributed equally. Project page: http://handmethat.csail.mit.edu/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extended Kalman Filter State Estimation for Autonomous Competition
  Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Kou, Acshi Haggenmiller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous mobile robot competitions judge based on a robot's ability to
quickly and accurately navigate the game field. This means accurate
localization is crucial for creating an autonomous competition robot. Two
common localization methods are odometry and computer vision landmark
detection. Odometry provides frequent velocity measurements, while landmark
detection provides infrequent position measurements. The state can also be
predicted with a physics model. These three types of localization can be
"fused" to create a more accurate state estimate using an Extended Kalman
Filter (EKF). The EKF is a nonlinear full-state estimator that approximates the
state estimate with the lowest covariance error when given the sensor
measurements, the model prediction, and their variances. In this paper, we
demonstrate the effectiveness of the EKF by implementing it on a 4-wheel
mecanum-drive robot simulation. The position and velocity accuracy of fusing
together various combinations of these three data sources are compared. We also
discuss the assumptions and limitations of an EKF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Information-state based Approach to the Optimal Output Feedback
  Control of Nonlinear Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.08086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.08086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raman Goyal, Ran Wang, Mohamed Naveed Gul Mohamed, Aayushman Sharma, Suman Chakravorty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a data-based approach to the closed-loop output feedback
control of nonlinear dynamical systems with a partial nonlinear observation
model. We propose an information state based approach to rigorously transform
the partially observed problem into a fully observed problem where the
information state consists of the past several observations and control inputs.
We further show the equivalence of the transformed and the initial partially
observed optimal control problems and provide the conditions to solve for the
deterministic optimal solution. We develop a data based generalization of the
iterative Linear Quadratic Regulator (iLQR) to partially observed systems using
a local linear time varying model of the information state dynamics
approximated by an Autoregressive moving average (ARMA) model, that is
generated using only the input-output data. This open-loop trajectory
optimization solution is then used to design a local feedback control law, and
the composite law then provides an optimum solution to the partially observed
feedback design problem. The efficacy of the developed method is shown by
controlling complex high dimensional nonlinear dynamical systems in the
presence of model and sensing uncertainty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RedMotion: Motion Prediction via Redundancy Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Royden Wagner, Omer Sahin Tas, Marvin Klemp, Carlos Fernandez Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the future motion of traffic agents is vital for self-driving
vehicles to ensure their safe operation. We introduce RedMotion, a transformer
model for motion prediction that incorporates two types of redundancy
reduction. The first type of redundancy reduction is induced by an internal
transformer decoder and reduces a variable-sized set of road environment
tokens, such as road graphs with agent data, to a fixed-sized embedding. The
second type of redundancy reduction is a self-supervised learning objective and
applies the redundancy reduction principle to embeddings generated from
augmented views of road environments. Our experiments reveal that our
representation learning approach can outperform PreTraM, Traj-MAE, and
GraphDINO in a semi-supervised setting. Our RedMotion model achieves results
that are competitive with those of Scene Transformer or MTR++. We provide an
open source implementation that is accessible via GitHub
(https://github.com/kit-mrt/red-motion) and Colab
(https://colab.research.google.com/drive/1Q-Z9VdiqvfPfctNG8oqzPcgm0lP3y1il).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report, 13 pages, 8 figures; v2: focus on transformer model</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PlaceNav: Topological Navigation through Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17260v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17260v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauri Suomela, Jussi Kalliola, Harry Edelman, Joni-Kristian Kämäräinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent results suggest that splitting topological navigation into
robot-independent and robot-specific components improves navigation performance
by enabling the robot-independent part to be trained with data collected by
different robot types. However, the navigation methods are still limited by the
scarcity of suitable training data and suffer from poor computational scaling.
In this work, we present PlaceNav, subdividing the robot-independent part into
navigation-specific and generic computer vision components. We utilize visual
place recognition for the subgoal selection of the topological navigation
pipeline. This makes subgoal selection more efficient and enables leveraging
large-scale datasets from non-robotics sources, increasing training data
availability. Bayesian filtering, enabled by place recognition, further
improves navigation performance by increasing the temporal consistency of
subgoals. Our experimental results verify the design and the new model obtains
a 76% higher success rate in indoor and 23% higher in outdoor navigation tasks
with higher computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online On-Demand Multi-Robot Coverage Path Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.00047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.00047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ratijit Mitra, Indranil Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an online centralized path planning algorithm to cover a large,
complex, unknown workspace with multiple homogeneous mobile robots. Our
algorithm is horizon-based, synchronous, and on-demand. The recently proposed
horizon-based synchronous algorithms compute all the robots' paths in each
horizon, significantly increasing the computation burden in large workspaces
with many robots. As a remedy, we propose an algorithm that computes the paths
for a subset of robots that have traversed previously computed paths entirely
(thus on-demand) and reuses the remaining paths for the other robots. We
formally prove that the algorithm guarantees complete coverage of the unknown
workspace. Experimental results on several standard benchmark workspaces show
that our algorithm scales to hundreds of robots in large complex workspaces and
consistently beats a state-of-the-art online centralized multi-robot coverage
path planning algorithm in terms of the time needed to achieve complete
coverage. For its validation, we perform ROS+Gazebo simulations in five 2D grid
benchmark workspaces with 10 Quadcopters and 10 TurtleBots, respectively. Also,
to demonstrate its practical feasibility, we conduct one indoor experiment with
two real TurtleBot2 robots and one outdoor experiment with three real
Quadcopters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Occupancy Grid Mapping without Ray-Casting for High-resolution LiDAR
  Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixi Cai, Fanze Kong, Yunfan Ren, Fangcheng Zhu, Jiarong Lin, Fu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Occupancy mapping is a fundamental component of robotic systems to reason
about the unknown and known regions of the environment. This article presents
an efficient occupancy mapping framework for high-resolution LiDAR sensors,
termed D-Map. The framework introduces three main novelties to address the
computational efficiency challenges of occupancy mapping. Firstly, we use a
depth image to determine the occupancy state of regions instead of the
traditional ray-casting method. Secondly, we introduce an efficient on-tree
update strategy on a tree-based map structure. These two techniques avoid
redundant visits to small cells, significantly reducing the number of cells to
be updated. Thirdly, we remove known cells from the map at each update by
leveraging the low false alarm rate of LiDAR sensors. This approach not only
enhances our framework's update efficiency by reducing map size but also endows
it with an interesting decremental property, which we have named D-Map. To
support our design, we provide theoretical analyses of the accuracy of the
depth image projection and time complexity of occupancy updates. Furthermore,
we conduct extensive benchmark experiments on various LiDAR sensors in both
public and private datasets. Our framework demonstrates superior efficiency in
comparison with other state-of-the-art methods while maintaining comparable
mapping accuracy and high memory efficiency. We demonstrate two real-world
applications of D-Map for real-time occupancy mapping on a handle device and an
aerial platform carrying a high-resolution LiDAR. In addition, we open-source
the implementation of D-Map on GitHub to benefit society:
github.com/hku-mars/D-Map.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplementary material included. Accepted for publication in IEEE
  Transactions on Robotics (T-RO)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in
  Prosthetic Hand Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.03893v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.03893v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrshad Zandigohar, Mo Han, Mohammadreza Sharif, Sezen Yagmur Gunay, Mariusz P. Furmanek, Mathew Yarossi, Paolo Bonato, Cagdas Onal, Taskin Padir, Deniz Erdogmus, Gunar Schirner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: For lower arm amputees, robotic prosthetic hands promise to regain
the capability to perform daily living activities. Current control methods
based on physiological signals such as electromyography (EMG) are prone to
yielding poor inference outcomes due to motion artifacts, muscle fatigue, and
many more. Vision sensors are a major source of information about the
environment state and can play a vital role in inferring feasible and intended
gestures. However, visual evidence is also susceptible to its own artifacts,
most often due to object occlusion, lighting changes, etc. Multimodal evidence
fusion using physiological and vision sensor measurements is a natural approach
due to the complementary strengths of these modalities. Methods: In this paper,
we present a Bayesian evidence fusion framework for grasp intent inference
using eye-view video, eye-gaze, and EMG from the forearm processed by neural
network models. We analyze individual and fused performance as a function of
time as the hand approaches the object to grasp it. For this purpose, we have
also developed novel data processing and augmentation techniques to train
neural network components. Results: Our results indicate that, on average,
fusion improves the instantaneous upcoming grasp type classification accuracy
while in the reaching phase by 13.66% and 14.8%, relative to EMG and visual
evidence individually, resulting in an overall fusion accuracy of 95.3%.
Conclusion: Our experimental data analyses demonstrate that EMG and visual
evidence show complementary strengths, and as a consequence, fusion of
multimodal evidence can outperform each individual evidence modality at any
given time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to Frontiers for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task and Motion Planning with Large Language Models for Object
  Rearrangement <span class="chip">IROS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06247v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06247v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Ding, Xiaohan Zhang, Chris Paxton, Shiqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-object rearrangement is a crucial skill for service robots, and
commonsense reasoning is frequently needed in this process. However, achieving
commonsense arrangements requires knowledge about objects, which is hard to
transfer to robots. Large language models (LLMs) are one potential source of
this knowledge, but they do not naively capture information about plausible
physical arrangements of the world. We propose LLM-GROP, which uses prompting
to extract commonsense knowledge about semantically valid object configurations
from an LLM and instantiates them with a task and motion planner in order to
generalize to varying scene geometry. LLM-GROP allows us to go from
natural-language commands to human-aligned object rearrangement in varied
environments. Based on human evaluations, our approach achieves the highest
rating while outperforming competitive baselines in terms of success rate while
maintaining comparable cumulative action costs. Finally, we demonstrate a
practical implementation of LLM-GROP on a mobile manipulator in real-world
scenarios. Supplementary materials are available at:
https://sites.google.com/view/llm-grop
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpted by IEEE IROS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Action Knowledge and LLMs for Task Planning and Situation
  Handling in Open Worlds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Ding, Xiaohan Zhang, Saeid Amiri, Nieqing Cao, Hao Yang, Andy Kaminski, Chad Esselink, Shiqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task planning systems have been developed to help robots use human knowledge
(about actions) to complete long-horizon tasks. Most of them have been
developed for "closed worlds" while assuming the robot is provided with
complete world knowledge. However, the real world is generally open, and the
robots frequently encounter unforeseen situations that can potentially break
the planner's completeness. Could we leverage the recent advances on
pre-trained Large Language Models (LLMs) to enable classical planning systems
to deal with novel situations?
  This paper introduces a novel framework, called COWP, for open-world task
planning and situation handling. COWP dynamically augments the robot's action
knowledge, including the preconditions and effects of actions, with
task-oriented commonsense knowledge. COWP embraces the openness from LLMs, and
is grounded to specific domains via action knowledge. For systematic
evaluations, we collected a dataset that includes 1,085 execution-time
situations. Each situation corresponds to a state instance wherein a robot is
potentially unable to complete a task using a solution that normally works.
Experimental results show that our approach outperforms competitive baselines
from the literature in the success rate of service tasks. Additionally, we have
demonstrated COWP using a mobile manipulator. Supplementary materials are
available at: https://cowplanning.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2210.01287</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">125</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Baselines with Visual Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMM) have recently shown encouraging progress with
visual instruction tuning. In this note, we show that the fully-connected
vision-language cross-modal connector in LLaVA is surprisingly powerful and
data-efficient. With simple modifications to LLaVA, namely, using
CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA
data with simple response formatting prompts, we establish stronger baselines
that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint
uses merely 1.2M publicly available data, and finishes full training in ~1 day
on a single 8-A100 node. We hope this can make state-of-the-art LMM research
more accessible. Code and model will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report, 4 pages. LLaVA project page: https://llava-vl.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContactGen: Generative Contact Modeling for Grasp Generation <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaowei Liu, Yang Zhou, Jimei Yang, Saurabh Gupta, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel object-centric contact representation ContactGen
for hand-object interaction. The ContactGen comprises three components: a
contact map indicates the contact location, a part map represents the contact
hand part, and a direction map tells the contact direction within each part.
Given an input object, we propose a conditional generative model to predict
ContactGen and adopt model-based optimization to predict diverse and
geometrically feasible grasps. Experimental results demonstrate our method can
generate high-fidelity and diverse human grasps for various objects. Project
page: https://stevenlsw.github.io/contactgen/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2023. Website:
  https://stevenlsw.github.io/contactgen/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Text-to-Image Diffusion Models with Reward Backpropagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have recently emerged at the forefront of
image generation, powered by very large-scale unsupervised or weakly supervised
text-to-image training datasets. Due to their unsupervised training,
controlling their behavior in downstream tasks, such as maximizing
human-perceived image quality, image-text alignment, or ethical image
generation, is difficult. Recent works finetune diffusion models to downstream
reward functions using vanilla reinforcement learning, notorious for the high
variance of the gradient estimators. In this paper, we propose AlignProp, a
method that aligns diffusion models to downstream reward functions using
end-to-end backpropagation of the reward gradient through the denoising
process. While naive implementation of such backpropagation would require
prohibitive memory resources for storing the partial derivatives of modern
text-to-image models, AlignProp finetunes low-rank adapter weight modules and
uses gradient checkpointing, to render its memory usage viable. We test
AlignProp in finetuning diffusion models to various objectives, such as
image-text semantic alignment, aesthetics, compressibility and controllability
of the number of objects present, as well as their combinations. We show
AlignProp achieves higher rewards in fewer training steps than alternatives,
while being conceptually simpler, making it a straightforward choice for
optimizing diffusion models for differentiable reward functions of interest.
Code and Visualization results are available at https://align-prop.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://align-prop.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stylist: Style-Driven Feature Ranking for Robust Novelty Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Smeu, Elena Burceanu, Emanuela Haller, Andrei Liviu Nicolicioiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novelty detection aims at finding samples that differ in some form from the
distribution of seen samples. But not all changes are created equal. Data can
suffer a multitude of distribution shifts, and we might want to detect only
some types of relevant changes. Similar to works in out-of-distribution
generalization, we propose to use the formalization of separating into semantic
or content changes, that are relevant to our task, and style changes, that are
irrelevant. Within this formalization, we define the robust novelty detection
as the task of finding semantic changes while being robust to style
distributional shifts. Leveraging pretrained, large-scale model
representations, we introduce Stylist, a novel method that focuses on dropping
environment-biased features. First, we compute a per-feature score based on the
feature distribution distances between environments. Next, we show that our
selection manages to remove features responsible for spurious correlations and
improve novelty detection performance. For evaluation, we adapt domain
generalization datasets to our task and analyze the methods behaviors. We
additionally built a large synthetic dataset where we have control over the
spurious correlations degree. We prove that our selection mechanism improves
novelty detection algorithms across multiple datasets, containing both
stylistic and content shifts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Unpaired Data for Vision-Language Generative Models via Cycle
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhong Li, Sangnie Bhardwaj, Yonglong Tian, Han Zhang, Jarred Barber, Dina Katabi, Guillaume Lajoie, Huiwen Chang, Dilip Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current vision-language generative models rely on expansive corpora of paired
image-text data to attain optimal performance and generalization capabilities.
However, automatically collecting such data (e.g. via large-scale web scraping)
leads to low quality and poor image-text correlation, while human annotation is
more accurate but requires significant manual effort and expense. We introduce
$\textbf{ITIT}$ ($\textbf{I}$n$\textbf{T}$egrating $\textbf{I}$mage
$\textbf{T}$ext): an innovative training paradigm grounded in the concept of
cycle consistency which allows vision-language training on unpaired image and
text data. ITIT is comprised of a joint image-text encoder with disjoint image
and text decoders that enable bidirectional image-to-text and text-to-image
generation in a single framework. During training, ITIT leverages a small set
of paired image-text data to ensure its output matches the input reasonably
well in both directions. Simultaneously, the model is also trained on much
larger datasets containing only images or texts. This is achieved by enforcing
cycle consistency between the original unpaired samples and the cycle-generated
counterparts. For instance, it generates a caption for a given input image and
then uses the caption to create an output image, and enforces similarity
between the input and output images. Our experiments show that ITIT with
unpaired datasets exhibits similar scaling behavior as using high-quality
paired data. We demonstrate image generation and captioning performance on par
with state-of-the-art text-to-image and image-to-text models with orders of
magnitude fewer (only 3M) paired image-text data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently released GPT-4 Code Interpreter has demonstrated remarkable
proficiency in solving challenging math problems, primarily attributed to its
ability to seamlessly reason with natural language, generate code, execute
code, and continue reasoning based on the execution output. In this paper, we
present a method to fine-tune open-source language models, enabling them to use
code for modeling and deriving math equations and, consequently, enhancing
their mathematical reasoning abilities. We propose a method of generating novel
and high-quality datasets with math problems and their code-based solutions,
referred to as MathCodeInstruct. Each solution interleaves natural language,
code, and execution results. We also introduce a customized supervised
fine-tuning and inference approach. This approach yields the MathCoder models,
a family of models capable of generating code-based solutions for solving
challenging math problems. Impressively, the MathCoder models achieve
state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K
(83.9%) datasets, substantially outperforming other open-source alternatives.
Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K
and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The
dataset and models will be released at https://github.com/mathllm/MathCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The state-of-the-art open-source language models for mathematical
  reasoning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable
  Evasion Attacks <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ofir Bar Tal, Adi Haviv, Amit H. Bermano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evasion Attacks (EA) are used to test the robustness of trained neural
networks by distorting input data to misguide the model into incorrect
classifications. Creating these attacks is a challenging task, especially with
the ever-increasing complexity of models and datasets. In this work, we
introduce a self-supervised, computationally economical method for generating
adversarial examples, designed for the unseen black-box setting. Adapting
techniques from representation learning, our method generates on-manifold EAs
that are encouraged to resemble the data distribution. These attacks are
comparable in effectiveness compared to the state-of-the-art when attacking the
model trained on, but are significantly more effective when attacking unseen
models, as the attacks are more related to the data rather than the model
itself. Our experiments consistently demonstrate the method is effective across
various models, unseen data categories, and even defended models, suggesting a
significant role for on-manifold EAs when targeting unseen models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023, AROW Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drag View: Generalizable Novel View Synthesis with Unposed Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwen Fan, Panwang Pan, Peihao Wang, Yifan Jiang, Hanwen Jiang, Dejia Xu, Zehao Zhu, Dilin Wang, Zhangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DragView, a novel and interactive framework for generating novel
views of unseen scenes. DragView initializes the new view from a single source
image, and the rendering is supported by a sparse set of unposed multi-view
images, all seamlessly executed within a single feed-forward pass. Our approach
begins with users dragging a source view through a local relative coordinate
system. Pixel-aligned features are obtained by projecting the sampled 3D points
along the target ray onto the source view. We then incorporate a view-dependent
modulation layer to effectively handle occlusion during the projection.
Additionally, we broaden the epipolar attention mechanism to encompass all
source pixels, facilitating the aggregation of initialized coordinate-aligned
point features from other unposed views. Finally, we employ another transformer
to decode ray features into final pixel intensities. Crucially, our framework
does not rely on either 2D prior models or the explicit estimation of camera
poses. During testing, DragView showcases the capability to generalize to new
scenes unseen during training, also utilizing only unposed support images,
enabling the generation of photo-realistic new views characterized by flexible
camera trajectories. In our experiments, we conduct a comprehensive comparison
of the performance of DragView with recent scene representation networks
operating under pose-free conditions, as well as with generalizable NeRFs
subject to noisy test camera poses. DragView consistently demonstrates its
superior performance in view synthesis quality, while also being more
user-friendly. Project page: https://zhiwenfan.github.io/DragView/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LumiNet: The Bright Side of Perceptual Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Ismail Hossain, M M Lutfe Elahi, Sameera Ramasinghe, Ali Cheraghian, Fuad Rahman, Nabeel Mohammed, Shafin Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In knowledge distillation research, feature-based methods have dominated due
to their ability to effectively tap into extensive teacher models. In contrast,
logit-based approaches are considered to be less adept at extracting hidden
'dark knowledge' from teachers. To bridge this gap, we present LumiNet, a novel
knowledge-transfer algorithm designed to enhance logit-based distillation. We
introduce a perception matrix that aims to recalibrate logits through
adjustments based on the model's representation capability. By meticulously
analyzing intra-class dynamics, LumiNet reconstructs more granular inter-class
relationships, enabling the student model to learn a richer breadth of
knowledge. Both teacher and student models are mapped onto this refined matrix,
with the student's goal being to minimize representational discrepancies.
Rigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO)
attests to LumiNet's efficacy, revealing its competitive edge over leading
feature-based methods. Moreover, in exploring the realm of transfer learning,
we assess how effectively the student model, trained using our method, adapts
to downstream tasks. Notably, when applied to Tiny ImageNet, the transferred
features exhibit remarkable performance, further underscoring LumiNet's
versatility and robustness in diverse settings. With LumiNet, we hope to steer
the research discourse towards a renewed interest in the latent capabilities of
logit-based knowledge distillation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Certification of Deep Learning Models for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Othmane Laousy, Alexandre Araujo, Guillaume Chassagnon, Nikos Paragios, Marie-Pierre Revel, Maria Vakalopoulou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In medical imaging, segmentation models have known a significant improvement
in the past decade and are now used daily in clinical practice. However,
similar to classification models, segmentation models are affected by
adversarial attacks. In a safety-critical field like healthcare, certifying
model predictions is of the utmost importance. Randomized smoothing has been
introduced lately and provides a framework to certify models and obtain
theoretical guarantees. In this paper, we present for the first time a
certified segmentation baseline for medical imaging based on randomized
smoothing and diffusion models. Our results show that leveraging the power of
denoising diffusion probabilistic models helps us overcome the limits of
randomized smoothing. We conduct extensive experiments on five public datasets
of chest X-rays, skin lesions, and colonoscopies, and empirically show that we
are able to maintain high certified Dice scores even for highly perturbed
images. Our work represents the first attempt to certify medical image
segmentation models, and we aspire for it to set a foundation for future
benchmarks in this crucial and largely uncharted area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustness-Guided Image Synthesis for Data-Free Quantization <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhong Bai, Yuchen Yang, Huanpeng Chu, Hualiang Wang, Zuozhu Liu, Ruizhe Chen, Xiaoxuan He, Lianrui Mu, Chengfei Cai, Haoji Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization has emerged as a promising direction for model compression.
Recently, data-free quantization has been widely studied as a promising method
to avoid privacy concerns, which synthesizes images as an alternative to real
training data. Existing methods use classification loss to ensure the
reliability of the synthesized images. Unfortunately, even if these images are
well-classified by the pre-trained model, they still suffer from low semantics
and homogenization issues. Intuitively, these low-semantic images are sensitive
to perturbations, and the pre-trained model tends to have inconsistent output
when the generator synthesizes an image with poor semantics. To this end, we
propose Robustness-Guided Image Synthesis (RIS), a simple but effective method
to enrich the semantics of synthetic images and improve image diversity,
further boosting the performance of downstream data-free compression tasks.
Concretely, we first introduce perturbations on input and model weight, then
define the inconsistency metrics at feature and prediction levels before and
after perturbations. On the basis of inconsistency on two levels, we design a
robustness optimization objective to enhance the semantics of synthetic images.
Moreover, we also make our approach diversity-aware by forcing the generator to
synthesize images with small correlations in the label space. With RIS, we
achieve state-of-the-art performance for various settings on data-free
quantization and can be extended to other data-free compression tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual inspection for illicit items in X-ray images using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Mademlis, Georgios Batsis, Adamantia Anna Rebolledo Chrysochoou, Georgios Th. Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated detection of contraband items in X-ray images can significantly
increase public safety, by enhancing the productivity and alleviating the
mental load of security officers in airports, subways, customs/post offices,
etc. The large volume and high throughput of passengers, mailed parcels, etc.,
during rush hours practically make it a Big Data problem. Modern computer
vision algorithms relying on Deep Neural Networks (DNNs) have proven capable of
undertaking this task even under resource-constrained and embedded execution
scenarios, e.g., as is the case with fast, single-stage object detectors.
However, no comparative experimental assessment of the various relevant DNN
components/methods has been performed under a common evaluation protocol, which
means that reliable cross-method comparisons are missing. This paper presents
exactly such a comparative assessment, utilizing a public relevant dataset and
a well-defined methodology for selecting the specific DNN components/modules
that are being evaluated. The results indicate the superiority of Transformer
detectors, the obsolete nature of auxiliary neural modules that have been
developed in the past few years for security applications and the efficiency of
the CSP-DarkNet backbone CNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLEVRER-Humans: Describing Physical and Causal Events the Human Way <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayuan Mao, Xuelin Yang, Xikun Zhang, Noah D. Goodman, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building machines that can reason about physical events and their causal
relationships is crucial for flexible interaction with the physical world.
However, most existing physical and causal reasoning benchmarks are exclusively
based on synthetically generated events and synthetic natural language
descriptions of causal relationships. This design brings up two issues. First,
there is a lack of diversity in both event types and natural language
descriptions; second, causal relationships based on manually-defined heuristics
are different from human judgments. To address both shortcomings, we present
the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of
physical events with human labels. We employ two techniques to improve data
collection efficiency: first, a novel iterative event cloze task to elicit a
new representation of events in videos, which we term Causal Event Graphs
(CEGs); second, a data augmentation technique based on neural language
generative models. We convert the collected CEGs into questions and answers to
be consistent with prior work. Finally, we study a collection of baseline
approaches for CLEVRER-Humans question-answering, highlighting the great
challenges set forth by our benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2022 (Dataset and Benchmark Track). First two authors
  contributed equally. Project page:
  https://sites.google.com/stanford.edu/clevrer-humans/home</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wasserstein Distortion: Unifying Fidelity and Realism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Qiu, Aaron B. Wagner, Johannes Ballé, Lucas Theis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a distortion measure for images, Wasserstein distortion, that
simultaneously generalizes pixel-level fidelity on the one hand and realism on
the other. We show how Wasserstein distortion reduces mathematically to a pure
fidelity constraint or a pure realism constraint under different parameter
choices. Pairs of images that are close under Wasserstein distortion illustrate
its utility. In particular, we generate random textures that have high fidelity
to a reference texture in one location of the image and smoothly transition to
an independent realization of the texture as one moves away from this point.
Connections between Wasserstein distortion and models of the human visual
system are noted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling
  and Motion Planning <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Schulze, Hod Lipson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A robot self-model is a task-agnostic representation of the robot's physical
morphology that can be used for motion planning tasks in absence of classical
geometric kinematic models. In particular, when the latter are hard to engineer
or the robot's kinematics change unexpectedly, human-free self-modeling is a
necessary feature of truly autonomous agents. In this work, we leverage neural
fields to allow a robot to self-model its kinematics as a neural-implicit query
model learned only from 2D images annotated with camera poses and
configurations. This enables significantly greater applicability than existing
approaches which have been dependent on depth images or geometry knowledge. To
this end, alongside a curricular data sampling strategy, we propose a new
encoder-based neural density field architecture for dynamic object-centric
scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF
robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2%
of the robot's workspace dimension. We demonstrate the capabilities of this
model on a motion planning task as an exemplary downstream application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023 Workshop on Neural Fields for Autonomous Driving and
  Robotics (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Animatable Virtual Humans: Learning pose-dependent human representations
  in UV space for interactive performance synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wieland Morgenstern, Milena T. Bagdasarian, Anna Hilsmann, Peter Eisert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel representation of virtual humans for highly realistic
real-time animation and rendering in 3D applications. We learn pose dependent
appearance and geometry from highly accurate dynamic mesh sequences obtained
from state-of-the-art multiview-video reconstruction. Learning pose-dependent
appearance and geometry from mesh sequences poses significant challenges, as it
requires the network to learn the intricate shape and articulated motion of a
human body. However, statistical body models like SMPL provide valuable
a-priori knowledge which we leverage in order to constrain the dimension of the
search space enabling more efficient and targeted learning and define
pose-dependency. Instead of directly learning absolute pose-dependent geometry,
we learn the difference between the observed geometry and the fitted SMPL
model. This allows us to encode both pose-dependent appearance and geometry in
the consistent UV space of the SMPL model. This approach not only ensures a
high level of realism but also facilitates streamlined processing and rendering
of virtual humans in real-time scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Good Are Synthetic Medical Images? An Empirical Study with Lung
  Ultrasound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menghan Yu, Sourabh Kulhare, Courosh Mehanian, Charles B Delahunt, Daniel E Shea, Zohreh Laverriere, Ishan Shah, Matthew P Horning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquiring large quantities of data and annotations is known to be effective
for developing high-performing deep learning models, but is difficult and
expensive to do in the healthcare context. Adding synthetic training data using
generative models offers a low-cost method to deal effectively with the data
scarcity challenge, and can also address data imbalance and patient privacy
issues. In this study, we propose a comprehensive framework that fits
seamlessly into model development workflows for medical image analysis. We
demonstrate, with datasets of varying size, (i) the benefits of generative
models as a data augmentation method; (ii) how adversarial methods can protect
patient privacy via data substitution; (iii) novel performance metrics for
these use cases by testing models on real holdout data. We show that training
with both synthetic and real data outperforms training with real data alone,
and that models trained solely with synthetic data approach their real-only
counterparts. Code is available at
https://github.com/Global-Health-Labs/US-DCGAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in Simulation and Synthesis in Medical Imaging (SASHIMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuan Fang, Xiaotao Hu, Kunming Luo, Ping Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven 3D indoor scene generation could be useful for gaming, film
industry, and AR/VR applications. However, existing methods cannot faithfully
capture the room layout, nor do they allow flexible editing of individual
objects in the room. To address these problems, we present Ctrl-Room, which is
able to generate convincing 3D rooms with designer-style layouts and
high-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables
versatile interactive editing operations such as resizing or moving individual
furniture items. Our key insight is to separate the modeling of layouts and
appearance. %how to model the room that takes into account both scene texture
and geometry at the same time. To this end, Our proposed method consists of two
stages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The
`Layout Generation Stage' trains a text-conditional diffusion model to learn
the layout distribution with our holistic scene code parameterization. Next,
the `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a
vivid panoramic image of the room guided by the 3D scene layout and text
prompt. In this way, we achieve a high-quality 3D room with convincing layouts
and lively textures. Benefiting from the scene code parameterization, we can
easily edit the generated room model through our mask-guided editing module,
without expensive editing-specific training. Extensive experiments on the
Structured3D dataset demonstrate that our method outperforms existing methods
in producing more reasonable, view-consistent, and editable 3D rooms from
natural language prompts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BID-<span class="highlight-title">NeRF</span>: RGB-D image pose estimation with inverted Neural Radiance
  Fields <span class="chip">ICCV23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ágoston István Csehi, Csaba Máté Józsa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which
defines the image pose estimation problem as a NeRF based iterative linear
optimization. NeRFs are novel neural space representation models that can
synthesize photorealistic novel views of real-world scenes or objects. Our
contributions are as follows: we extend the localization optimization objective
with a depth-based loss function, we introduce a multi-image based loss
function where a sequence of images with known relative poses are used without
increasing the computational complexity, we omit hierarchical sampling during
volumetric rendering, meaning only the coarse model is used for pose
estimation, and we how that by extending the sampling interval convergence can
be achieved even or higher initial pose estimate errors. With the proposed
modifications the convergence speed is significantly improved, and the basin of
convergence is substantially extended.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Nerf4ADR workshop of ICCV23 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedSynV1: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwu Xu, Li Sun, Wei Peng, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative methodology for producing high-quality 3D
lung CT images guided by textual information. While diffusion-based generative
models are increasingly used in medical imaging, current state-of-the-art
approaches are limited to low-resolution outputs and underutilize radiology
reports' abundant information. The radiology reports can enhance the generation
process by providing additional guidance and offering fine-grained control over
the synthesis of images. Nevertheless, expanding text-guided generation to
high-resolution 3D images poses significant memory and anatomical
detail-preserving challenges. Addressing the memory issue, we introduce a
hierarchical scheme that uses a modified UNet architecture. We start by
synthesizing low-resolution images conditioned on the text, serving as a
foundation for subsequent generators for complete volumetric data. To ensure
the anatomical plausibility of the generated samples, we provide further
guidance by generating vascular, airway, and lobular segmentation masks in
conjunction with the CT images. The model demonstrates the capability to use
textual input and segmentation tasks to generate synthesized images. The
results of comparative assessments indicate that our approach exhibits superior
performance compared to the most advanced models based on GAN and diffusion
techniques, especially in accurately retaining crucial anatomical features such
as fissure lines, airways, and vascular structures. This innovation introduces
novel possibilities. This study focuses on two main objectives: (1) the
development of a method for creating images based on textual prompts and
anatomical components, and (2) the capability to generate new images
conditioning on anatomical elements. The advancements in image generation can
be applied to enhance numerous downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Unified Deep Image Deraining: A Survey and A New Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Chen, Jinshan Pan, Jiangxin Dong, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed significant advances in image deraining due to
the kinds of effective image priors and deep learning models. As each deraining
approach has individual settings (e.g., training and test datasets, evaluation
criteria), how to fairly evaluate existing approaches comprehensively is not a
trivial task. Although existing surveys aim to review of image deraining
approaches comprehensively, few of them focus on providing unify evaluation
settings to examine the deraining capability and practicality evaluation. In
this paper, we provide a comprehensive review of existing image deraining
method and provide a unify evaluation setting to evaluate the performance of
image deraining methods. We construct a new high-quality benchmark named
HQ-RAIN to further conduct extensive evaluation, consisting of 5,000 paired
high-resolution synthetic images with higher harmony and realism. We also
discuss the existing challenges and highlight several future research
opportunities worth exploring. To facilitate the reproduction and tracking of
the latest deraining technologies for general users, we build an online
platform to provide the off-the-shelf toolkit, involving the large-scale
performance evaluation. This online platform and the proposed new benchmark are
publicly available and will be regularly updated at http://www.deraining.tech/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: http://www.deraining.tech/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-Aware Hypothesis & Verification for Generalizable Relative Object
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhao, Tong Zhang, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior methods that tackle the problem of generalizable object pose estimation
highly rely on having dense views of the unseen object. By contrast, we address
the scenario where only a single reference view of the object is available. Our
goal then is to estimate the relative object pose between this reference view
and a query image that depicts the object in a different pose. In this
scenario, robust generalization is imperative due to the presence of unseen
objects during testing and the large-scale object pose variation between the
reference and the query. To this end, we present a new
hypothesis-and-verification framework, in which we generate and evaluate
multiple pose hypotheses, ultimately selecting the most reliable one as the
relative object pose. To measure reliability, we introduce a 3D-aware
verification that explicitly applies 3D transformations to the 3D object
representations learned from the two input images. Our comprehensive
experiments on the Objaverse, LINEMOD, and CO3D datasets evidence the superior
accuracy of our approach in relative pose estimation and its robustness in
large-scale pose variations, when dealing with unseen objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V2X Cooperative Perception for Autonomous Driving: Recent Advances and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Huang, Jianan Liu, Xi Zhou, Dinh C. Nguyen, Mostafa Rahimi Azghadi, Yuxuan Xia, Qing-Long Han, Sumei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate perception is essential for advancing autonomous driving and
addressing safety challenges in modern transportation systems. Despite
significant advancements in computer vision for object recognition, current
perception methods still face difficulties in complex real-world traffic
environments. Challenges such as physical occlusion and limited sensor field of
view persist for individual vehicle systems. Cooperative Perception (CP) with
Vehicle-to-Everything (V2X) technologies has emerged as a solution to overcome
these obstacles and enhance driving automation systems. While some research has
explored CP's fundamental architecture and critical components, there remains a
lack of comprehensive summaries of the latest innovations, particularly in the
context of V2X communication technologies. To address this gap, this paper
provides a comprehensive overview of the evolution of CP technologies, spanning
from early explorations to recent developments, including advancements in V2X
communication technologies. Additionally, a contemporary generic framework is
proposed to illustrate the V2X-based CP workflow, aiding in the structured
understanding of CP system components. Furthermore, this paper categorizes
prevailing V2X-based CP methodologies based on the critical issues they
address. An extensive literature review is conducted within this taxonomy,
evaluating existing datasets and simulators. Finally, open challenges and
future directions in CP for autonomous driving are discussed by considering
both perception and V2X communication advancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrototypeFormer: Learning to Explore Prototype Relationships for
  Few-shot Image Classification <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feihong He, Gang Li, Lingyu Si, Leilei Yan, Fanzhang Li, Fuchun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot image classification has received considerable attention for
addressing the challenge of poor classification performance with limited
samples in novel classes. However, numerous studies have employed sophisticated
learning strategies and diversified feature extraction methods to address this
issue. In this paper, we propose our method called PrototypeFormer, which aims
to significantly advance traditional few-shot image classification approaches
by exploring prototype relationships. Specifically, we utilize a transformer
architecture to build a prototype extraction module, aiming to extract class
representations that are more discriminative for few-shot classification.
Additionally, during the model training process, we propose a contrastive
learning-based optimization approach to optimize prototype features in few-shot
learning scenarios. Despite its simplicity, the method performs remarkably
well, with no bells and whistles. We have experimented with our approach on
several popular few-shot image classification benchmark datasets, which shows
that our method outperforms all current state-of-the-art methods. In
particular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way
1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with
accuracy of 7.27% and 8.72%, respectively. The code will be released later.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring DINO: Emergent Properties and Limitations for Synthetic
  Aperture Radar Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph A. Gallego-Mejia, Anna Jungbluth, Laura Martínez-Ferrer, Matt Allen, Francisco Dorr, Freddie Kalaitzis, Raúl Ramos-Pollán
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) models have recently demonstrated remarkable
performance across various tasks, including image segmentation. This study
delves into the emergent characteristics of the Self-Distillation with No
Labels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR)
imagery. We pre-train a vision transformer (ViT)-based DINO model using
unlabeled SAR data, and later fine-tune the model to predict high-resolution
land cover maps. We rigorously evaluate the utility of attention maps generated
by the ViT backbone, and compare them with the model's token embedding space.
We observe a small improvement in model performance with pre-training compared
to training from scratch, and discuss the limitations and opportunities of SSL
for remote sensing and land cover segmentation. Beyond small performance
increases, we show that ViT attention maps hold great intrinsic value for
remote sensing, and could provide useful inputs to other algorithms. With this,
our work lays the ground-work for bigger and better SSL models for Earth
Observation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RL-based Stateful Neural Adaptive Sampling and Denoising for Real-Time
  Path Tracing <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Scardigli, Lukas Cavigelli, Lorenz K. Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monte-Carlo path tracing is a powerful technique for realistic image
synthesis but suffers from high levels of noise at low sample counts, limiting
its use in real-time applications. To address this, we propose a framework with
end-to-end training of a sampling importance network, a latent space encoder
network, and a denoiser network. Our approach uses reinforcement learning to
optimize the sampling importance network, thus avoiding explicit numerically
approximated gradients. Our method does not aggregate the sampled values per
pixel by averaging but keeps all sampled values which are then fed into the
latent space encoder. The encoder replaces handcrafted spatiotemporal
heuristics by learned representations in a latent space. Finally, a neural
denoiser is trained to refine the output image. Our approach increases visual
quality on several challenging datasets and reduces rendering times for equal
quality by a factor of 1.6x compared to the previous state-of-the-art, making
it a promising solution for real-time applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NeurIPS. https://openreview.net/forum?id=xNyR7DXUzJ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and
  Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, Denis Dimitrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image generation is a significant domain in modern computer vision
and has achieved substantial improvements through the evolution of generative
architectures. Among these, there are diffusion-based models that have
demonstrated essential quality enhancements. These models are generally split
into two categories: pixel-level and latent-level approaches. We present
Kandinsky1, a novel exploration of latent diffusion architecture, combining the
principles of the image prior models with latent diffusion techniques. The
image prior model is trained separately to map text embeddings to image
embeddings of CLIP. Another distinct feature of the proposed model is the
modified MoVQ implementation, which serves as the image autoencoder component.
Overall, the designed model contains 3.3B parameters. We also deployed a
user-friendly demo system that supports diverse generative modes such as
text-to-image generation, image fusion, text and image fusion, image variations
generation, and text-guided inpainting/outpainting. Additionally, we released
the source code and checkpoints for the Kandinsky models. Experimental
evaluations demonstrate a FID score of 8.03 on the COCO-30K dataset, marking
our model as the top open-source performer in terms of measurable image
generation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IceCloudNet: Cirrus and mixed-phase cloud prediction from SEVIRI input
  learned from sparse supervision <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Jeggle, Mikolaj Czerkawski, Federico Serva, Bertrand Le Saux, David Neubauer, Ulrike Lohmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clouds containing ice particles play a crucial role in the climate system.
Yet they remain a source of great uncertainty in climate models and future
climate projections. In this work, we create a new observational constraint of
regime-dependent ice microphysical properties at the spatio-temporal coverage
of geostationary satellite instruments and the quality of active satellite
retrievals. We achieve this by training a convolutional neural network on three
years of SEVIRI and DARDAR data sets. This work will enable novel research to
improve ice cloud process understanding and hence, reduce uncertainties in a
changing climate and help assess geoengineering methods for cirrus clouds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A Preprint. Submitted to Tackling Climate Change with Machine
  Learning: workshop at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BTDNet: a Multi-Modal Approach for Brain Tumor Radiogenomic
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Kollias, Karanjot Vendal, Priyanka Gadhavi, Solomon Russom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumors pose significant health challenges worldwide, with glioblastoma
being one of the most aggressive forms. Accurate determination of the
O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation status is
crucial for personalized treatment strategies. However, traditional methods are
labor-intensive and time-consuming. This paper proposes a novel multi-modal
approach, BTDNet, leveraging multi-parametric MRI scans, including FLAIR, T1w,
T1wCE, and T2 3D volumes, to predict MGMT promoter methylation status. BTDNet
addresses two main challenges: the variable volume lengths (i.e., each volume
consists of a different number of slices) and the volume-level annotations
(i.e., the whole 3D volume is annotated and not the independent slices that it
consists of). BTDNet consists of four components: i) the data augmentation one
(that performs geometric transformations, convex combinations of data pairs and
test-time data augmentation); ii) the 3D analysis one (that performs global
analysis through a CNN-RNN); iii) the routing one (that contains a mask layer
that handles variable input feature lengths), and iv) the modality fusion one
(that effectively enhances data representation, reduces ambiguities and
mitigates data scarcity). The proposed method outperforms by large margins the
state-of-the-art methods in the RSNA-ASNR-MICCAI BraTS 2021 Challenge, offering
a promising avenue for enhancing brain tumor diagnosis and treatment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ammonia-Net: A Multi-task Joint Learning Model for Multi-class
  Segmentation and Classification in Tooth-marked Tongue Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunkai Shi, Yuqi Wang, Qihui Ye, Yanran Wang, Yiming Zhu, Muhammad Hassan, Aikaterini Melliou, Dongmei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Traditional Chinese Medicine, the tooth marks on the tongue, stemming from
prolonged dental pressure, serve as a crucial indicator for assessing qi (yang)
deficiency, which is intrinsically linked to visceral health. Manual diagnosis
of tooth-marked tongue solely relies on experience. Nonetheless, the diversity
in shape, color, and type of tooth marks poses a challenge to diagnostic
accuracy and consistency. To address these problems, herein we propose a
multi-task joint learning model named Ammonia-Net. This model employs a
convolutional neural network-based architecture, specifically designed for
multi-class segmentation and classification of tongue images. Ammonia-Net
performs semantic segmentation of tongue images to identify tongue and tooth
marks. With the assistance of segmentation output, it classifies the images
into the desired number of classes: healthy tongue, light tongue, moderate
tongue, and severe tongue. As far as we know, this is the first attempt to
apply the semantic segmentation results of tooth marks for tooth-marked tongue
classification. To train Ammonia-Net, we collect 856 tongue images from 856
subjects. After a number of extensive experiments, the experimental results
show that the proposed model achieves 99.06% accuracy in the two-class
classification task of tooth-marked tongue identification and 80.02%. As for
the segmentation task, mIoU for tongue and tooth marks amounts to 71.65%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Resolution Audio-Visual Feature Fusion for Temporal Action
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Fish, Jon Weinbren, Andrew Gilbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Action Localization (TAL) aims to identify actions' start, end, and
class labels in untrimmed videos. While recent advancements using transformer
networks and Feature Pyramid Networks (FPN) have enhanced visual feature
recognition in TAL tasks, less progress has been made in the integration of
audio features into such frameworks. This paper introduces the Multi-Resolution
Audio-Visual Feature Fusion (MRAV-FF), an innovative method to merge
audio-visual data across different temporal resolutions. Central to our
approach is a hierarchical gated cross-attention mechanism, which discerningly
weighs the importance of audio information at diverse temporal scales. Such a
technique not only refines the precision of regression boundaries but also
bolsters classification confidence. Importantly, MRAV-FF is versatile, making
it compatible with existing FPN TAL architectures and offering a significant
enhancement in performance when audio data is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating the Influence of Domain Shift in Skin Lesion Classification:
  A Benchmark Study of Unsupervised Domain Adaptation Methods on Dermoscopic
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sireesha Chamarthi, Katharina Fogelberg, Roman C. Maron, Titus J. Brinker, Julia Niebling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The potential of deep neural networks in skin lesion classification has
already been demonstrated to be on-par if not superior to the dermatologists
diagnosis. However, the performance of these models usually deteriorates when
the test data differs significantly from the training data (i.e. domain shift).
This concerning limitation for models intended to be used in real-world skin
lesion classification tasks poses a risk to patients. For example, different
image acquisition systems or previously unseen anatomical sites on the patient
can suffice to cause such domain shifts. Mitigating the negative effect of such
shifts is therefore crucial, but developing effective methods to address domain
shift has proven to be challenging. In this study, we carry out an in-depth
analysis of eight different unsupervised domain adaptation methods to analyze
their effectiveness in improving generalization for dermoscopic datasets. To
ensure robustness of our findings, we test each method on a total of ten
distinct datasets, thereby covering a variety of possible domain shifts. In
addition, we investigated which factors in the domain shifted datasets have an
impact on the effectiveness of domain adaptation methods. Our findings show
that all of the eight domain adaptation methods result in improved AUPRC for
the majority of analyzed datasets. Altogether, these results indicate that
unsupervised domain adaptations generally lead to performance improvements for
the binary melanoma-nevus classification task regardless of the nature of the
domain shift. However, small or heavily imbalanced datasets lead to a reduced
conformity of the results due to the influence of these factors on the methods
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Zero Level-Set Extraction from Unsigned Distance Fields Based on
  Double Covering <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Hou, Xuhui Chen, Wencheng Wang, Hong Qin, Ying He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new method, called DoubleCoverUDF, for extracting
the zero level-set from unsigned distance fields (UDFs). DoubleCoverUDF takes a
learned UDF and a user-specified parameter $r$ (a small positive real number)
as input and extracts an iso-surface with an iso-value $r$ using the
conventional marching cubes algorithm. We show that the computed iso-surface is
the boundary of the $r$-offset volume of the target zero level-set $S$, which
is an orientable manifold, regardless of the topology of $S$. Next, the
algorithm computes a covering map to project the boundary mesh onto $S$,
preserving the mesh's topology and avoiding folding. If $S$ is an orientable
manifold surface, our algorithm separates the double-layered mesh into a single
layer using a robust minimum-cut post-processing step. Otherwise, it keeps the
double-layered mesh as the output. We validate our algorithm by reconstructing
3D surfaces of open models and demonstrate its efficacy and effectiveness on
synthetic models and benchmark datasets. Our experimental results confirm that
our method is robust and produces meshes with better quality in terms of both
visual evaluation and quantitative measures than existing UDF-based methods.
The source code is available at https://github.com/jjjkkyz/DCUDF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ACM Transactions on Graphics (SIGGRAPH Asia 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained
  Diffusion Models and Monocular Depth Estimators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiping Wang, Yuan Liu, Bing Wang, Yujing Sun, Zhen Dong, Wenping Wang, Bisheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matching cross-modality features between images and point clouds is a
fundamental problem for image-to-point cloud registration. However, due to the
modality difference between images and points, it is difficult to learn robust
and discriminative cross-modality features by existing metric learning methods
for feature matching. Instead of applying metric learning on cross-modality
data, we propose to unify the modality between images and point clouds by
pretrained large-scale models first, and then establish robust correspondence
within the same modality. We show that the intermediate features, called
diffusion features, extracted by depth-to-image diffusion models are
semantically consistent between images and point clouds, which enables the
building of coarse but robust cross-modality correspondences. We further
extract geometric features on depth maps produced by the monocular depth
estimator. By matching such geometric features, we significantly improve the
accuracy of the coarse correspondences produced by diffusion features.
Extensive experiments demonstrate that without any task-specific training,
direct utilization of both features produces accurate image-to-point cloud
registration. On three public indoor and outdoor benchmarks, the proposed
method averagely achieves a 20.6 percent improvement in Inlier Ratio, a
three-fold higher Inlier Number, and a 48.6 percent improvement in Registration
Recall than existing state-of-the-arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://whu-usi3dv.github.io/FreeReg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Complementary Global and Local Knowledge Network for Ultrasound
  denoising with Fine-grained Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Bu, Kai-Ni Wang, Fuxing Zhao, Shengxiao Li, Guang-Quan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound imaging serves as an effective and non-invasive diagnostic tool
commonly employed in clinical examinations. However, the presence of speckle
noise in ultrasound images invariably degrades image quality, impeding the
performance of subsequent tasks, such as segmentation and classification.
Existing methods for speckle noise reduction frequently induce excessive image
smoothing or fail to preserve detailed information adequately. In this paper,
we propose a complementary global and local knowledge network for ultrasound
denoising with fine-grained refinement. Initially, the proposed architecture
employs the L-CSwinTransformer as encoder to capture global information,
incorporating CNN as decoder to fuse local features. We expand the resolution
of the feature at different stages to extract more global information compared
to the original CSwinTransformer. Subsequently, we integrate Fine-grained
Refinement Block (FRB) within the skip-connection stage to further augment
features. We validate our model on two public datasets, HC18 and BUSI.
Experimental results demonstrate that our model can achieve competitive
performance in both quantitative metrics and visual performance. Our code will
be available at https://github.com/AAlkaid/USDenoising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Simplify Spatial-Temporal Graphs in Gait Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Cosma, Emilian Radoi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gait analysis leverages unique walking patterns for person identification and
assessment across multiple domains. Among the methods used for gait analysis,
skeleton-based approaches have shown promise due to their robust and
interpretable features. However, these methods often rely on hand-crafted
spatial-temporal graphs that are based on human anatomy disregarding the
particularities of the dataset and task. This paper proposes a novel method to
simplify the spatial-temporal graph representation for gait-based gender
estimation, improving interpretability without losing performance. Our approach
employs two models, an upstream and a downstream model, that can adjust the
adjacency matrix for each walking instance, thereby removing the fixed nature
of the graph. By employing the Straight-Through Gumbel-Softmax trick, our model
is trainable end-to-end. We demonstrate the effectiveness of our approach on
the CASIA-B dataset for gait-based gender estimation. The resulting graphs are
interpretable and differ qualitatively from fixed graphs used in existing
models. Our research contributes to enhancing the explainability and
task-specific adaptability of gait recognition, promoting more efficient and
reliable gait-based biometrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 Figures, 1 Table. Short Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paolo Rabino, Antonio Alliegro, Francesco Cappio Borlino, Tatiana Tommasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Moving deep learning models from the laboratory setting to the open world
entails preparing them to handle unforeseen conditions. In several applications
the occurrence of novel classes during deployment poses a significant threat,
thus it is crucial to effectively detect them. Ideally, this skill should be
used when needed without requiring any further computational training effort at
every new task. Out-of-distribution detection has attracted significant
attention in the last years, however the majority of the studies deal with 2D
images ignoring the inherent 3D nature of the real-world and often confusing
between domain and semantic novelty. In this work, we focus on the latter,
considering the objects geometric structure captured by 3D point clouds
regardless of the specific domain. We advance the field by introducing
OpenPatch that builds on a large pre-trained model and simply extracts from its
intermediate features a set of patch representations that describe each known
class. For any new sample, we obtain a novelty score by evaluating whether it
can be recomposed mainly by patches of a single known class or rather via the
contribution of multiple classes. We present an extensive experimental
evaluation of our approach for the task of semantic novelty detection on
real-world point cloud samples when the reference known data are synthetic. We
demonstrate that OpenPatch excels in both the full and few-shot known sample
scenarios, showcasing its robustness across varying pre-training objectives and
network backbones. The inherent training-free nature of our method allows for
its immediate application to a wide array of real-world tasks, offering a
compelling advantage over approaches that need expensive retraining efforts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACT-Net: Anchor-context Action Detection in Surgery Videos <span class="chip">MICCAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luoying Hao, Yan Hu, Wenjun Lin, Qun Wang, Heng Li, Huazhu Fu, Jinming Duan, Jiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognition and localization of surgical detailed actions is an essential
component of developing a context-aware decision support system. However, most
existing detection algorithms fail to provide high-accuracy action classes even
having their locations, as they do not consider the surgery procedure's
regularity in the whole video. This limitation hinders their application.
Moreover, implementing the predictions in clinical applications seriously needs
to convey model confidence to earn entrustment, which is unexplored in surgical
action prediction. In this paper, to accurately detect fine-grained actions
that happen at every moment, we propose an anchor-context action detection
network (ACTNet), including an anchor-context detection (ACD) module and a
class conditional diffusion (CCD) module, to answer the following questions: 1)
where the actions happen; 2) what actions are; 3) how confidence predictions
are. Specifically, the proposed ACD module spatially and temporally highlights
the regions interacting with the extracted anchor in surgery video, which
outputs action location and its class distribution based on anchor-context
interactions. Considering the full distribution of action classes in videos,
the CCD module adopts a denoising diffusion-based generative model conditioned
on our ACD estimator to further reconstruct accurately the action predictions.
Moreover, we utilize the stochastic nature of the diffusion model outputs to
access model confidence for each prediction. Our method reports the
state-of-the-art performance, with improvements of 4.0% mAP against baseline on
the surgical video dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted early by MICCAI2023 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point-Based Radiance Fields for Controllable Human Motion Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Yu, Deheng Zhang, Peiyuan Xie, Tianyi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel controllable human motion synthesis method for
fine-level deformation based on static point-based radiance fields. Although
previous editable neural radiance field methods can generate impressive results
on novel-view synthesis and allow naive deformation, few algorithms can achieve
complex 3D human editing such as forward kinematics. Our method exploits the
explicit point cloud to train the static 3D scene and apply the deformation by
encoding the point cloud translation using a deformation MLP. To make sure the
rendering result is consistent with the canonical space training, we estimate
the local rotation using SVD and interpolate the per-point rotation to the
query view direction of the pre-trained radiance field. Extensive experiments
show that our approach can significantly outperform the state-of-the-art on
fine-level complex deformation which can be generalized to other 3D characters
besides humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video
  Sequences Using Swin Transformer-Enhanced UNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Jafari, Karim Faez, Hamidreza Amindavar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lung cancer is highly lethal, emphasizing the critical need for early
detection. However, identifying lung nodules poses significant challenges for
radiologists, who rely heavily on their expertise and experience for accurate
diagnosis. To address this issue, computer-aided diagnosis systems based on
machine learning techniques have emerged to assist doctors in identifying lung
nodules from computed tomography (CT) scans. Unfortunately, existing networks
in this domain often suffer from computational complexity, leading to high
rates of false negatives and false positives, limiting their effectiveness. To
address these challenges, we present an innovative model that harnesses the
strengths of both convolutional neural networks and vision transformers.
Inspired by object detection in videos, we treat each 3D CT image as a video,
individual slices as frames, and lung nodules as objects, enabling a
time-series application. The primary objective of our work is to overcome
hardware limitations during model training, allowing for efficient processing
of 2D data while utilizing inter-slice information for accurate identification
based on 3D image context. We validated the proposed network by applying a
10-fold cross-validation technique to the publicly available Lung Nodule
Analysis 2016 dataset. Our proposed architecture achieves an average
sensitivity criterion of 97.84% and a competition performance metrics (CPM) of
96.0% with few parameters. Comparative analysis with state-of-the-art
advancements in lung nodule identification demonstrates the significant
accuracy achieved by our proposed model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Realistic Speech-to-Face Generation with Speech-Conditioned Latent
  Diffusion Model with Face Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinting Wang, Li Liu, Jun Wang, Hei Victor Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-to-face generation is an intriguing area of research that focuses on
generating realistic facial images based on a speaker's audio speech. However,
state-of-the-art methods employing GAN-based architectures lack stability and
cannot generate realistic face images. To fill this gap, we propose a novel
speech-to-face generation framework, which leverages a Speech-Conditioned
Latent Diffusion Model, called SCLDM. To the best of our knowledge, this is the
first work to harness the exceptional modeling capabilities of diffusion models
for speech-to-face generation. Preserving the shared identity information
between speech and face is crucial in generating realistic results. Therefore,
we employ contrastive pre-training for both the speech encoder and the face
encoder. This pre-training strategy facilitates effective alignment between the
attributes of speech, such as age and gender, and the corresponding facial
characteristics in the face images. Furthermore, we tackle the challenge posed
by excessive diversity in the synthesis process caused by the diffusion model.
To overcome this challenge, we introduce the concept of residuals by
integrating a statistical face prior to the diffusion process. This addition
helps to eliminate the shared component across the faces and enhances the
subtle variations captured by the speech condition. Extensive quantitative,
qualitative, and user study experiments demonstrate that our method can produce
more realistic face images while preserving the identity of the speaker better
than state-of-the-art methods. Highlighting the notable enhancements, our
method demonstrates significant gains in all metrics on the AVSpeech dataset
and Voxceleb dataset, particularly noteworthy are the improvements of 32.17 and
32.72 on the cosine distance metric for the two datasets, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSI: Enhancing the Robustness of 3D Point Cloud Recognition against
  Corruption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wu, Jiachen Sun, Chaowei Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advancements in deep neural networks for point cloud
recognition, real-world safety-critical applications present challenges due to
unavoidable data corruption. Current models often fall short in generalizing to
unforeseen distribution shifts. In this study, we harness the inherent set
property of point cloud data to introduce a novel critical subset
identification (CSI) method, aiming to bolster recognition robustness in the
face of data corruption. Our CSI framework integrates two pivotal components:
density-aware sampling (DAS) and self-entropy minimization (SEM), which cater
to static and dynamic CSI, respectively. DAS ensures efficient robust anchor
point sampling by factoring in local density, while SEM is employed during
training to accentuate the most salient point-to-point attention. Evaluations
reveal that our CSI approach yields error rates of 18.4\% and 16.3\% on
ModelNet40-C and PointCloud-C, respectively, marking a notable improvement over
state-of-the-art methods by margins of 5.2\% and 4.2\% on the respective
benchmarks. Code is available at
\href{https://github.com/masterwu2115/CSI/tree/main}{https://github.com/masterwu2115/CSI/tree/main}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Representation Learning via Asymmetric Negative Contrast and
  Reverse Attention <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuoyan Zhou, Decheng Liu, Dawei Zhou, Xinbo Gao, Nannan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are vulnerable to adversarial noise. Adversarial
training (AT) has been demonstrated to be the most effective defense strategy
to protect neural networks from being fooled. However, we find AT omits to
learning robust features, resulting in poor performance of adversarial
robustness. To address this issue, we highlight two characteristics of robust
representation: (1) $\bf{exclusion}$: the feature of natural examples keeps
away from that of other classes; (2) $\bf{alignment}$: the feature of natural
and corresponding adversarial examples is close to each other. These motivate
us to propose a generic framework of AT to gain robust representation, by the
asymmetric negative contrast and reverse attention. Specifically, we design an
asymmetric negative contrast based on predicted probabilities, to push away
examples of different classes in the feature space. Moreover, we propose to
weight feature by parameters of the linear classifier as the reverse attention,
to obtain class-aware feature and pull close the feature of the same class.
Empirical evaluations on three benchmark datasets show our methods greatly
advance the robustness of AT and achieve state-of-the-art performance. Code is
available at <https://github.com/changzhang777/ANCRA>.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Datasets with Different Label Sets for Improved Nucleus
  Segmentation and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amruta Parulekar, Utkarsh Kanwat, Ravi Kant Gupta, Medha Chippa, Thomas Jacob, Tripti Bameta, Swapnil Rane, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation and classification of cell nuclei in histopathology images using
deep neural networks (DNNs) can save pathologists' time for diagnosing various
diseases, including cancers, by automating cell counting and morphometric
assessments. It is now well-known that the accuracy of DNNs increases with the
sizes of annotated datasets available for training. Although multiple datasets
of histopathology images with nuclear annotations and class labels have been
made publicly available, the set of class labels differ across these datasets.
We propose a method to train DNNs for instance segmentation and classification
on multiple datasets where the set of classes across the datasets are related
but not the same. Specifically, our method is designed to utilize a
coarse-to-fine class hierarchy, where the set of classes labeled and annotated
in a dataset can be at any level of the hierarchy, as long as the classes are
mutually exclusive. Within a dataset, the set of classes need not even be at
the same level of the class hierarchy tree. Our results demonstrate that
segmentation and classification metrics for the class set used by the test
split of a dataset can improve by pre-training on another dataset that may even
have a different set of classes due to the expansion of the training set
enabled by our method. Furthermore, generalization to previously unseen
datasets also improves by combining multiple other datasets with different sets
of classes for training. The improvement is both qualitative and quantitative.
The proposed method can be adapted for various loss functions, DNN
architectures, and application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Diffusion Step-aware Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, Yingcong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for
data generation across various domains. However, a significant bottleneck is
the necessity for whole-network computation during every step of the generative
process, leading to high computational overheads. This paper presents a novel
framework, Denoising Diffusion Step-aware Models (DDSM), to address this
challenge. Unlike conventional approaches, DDSM employs a spectrum of neural
networks whose sizes are adapted according to the importance of each generative
step, as determined through evolutionary search. This step-wise network
variation effectively circumvents redundant computational efforts, particularly
in less critical steps, thereby enhancing the efficiency of the diffusion
model. Furthermore, the step-aware design can be seamlessly integrated with
other efficiency-geared diffusion models such as DDIMs and latent diffusion,
thus broadening the scope of computational savings. Empirical evaluations
demonstrate that DDSM achieves computational savings of 49% for CIFAR-10, 61%
for CelebA-HQ, 59% for LSUN-bedroom, 71% for AFHQ, and 76% for ImageNet, all
without compromising the generation quality. Our code and models will be
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Test-time Domain Adaptation via Dynamic Sample Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanshuo Wang, Jie Hong, Ali Cheraghian, Shafin Rahman, David Ahmedt-Aristizabal, Lars Petersson, Mehrtash Harandi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of Continual Test-time Domain Adaptation (CTDA) is to gradually
adapt a pre-trained model to a sequence of target domains without accessing the
source data. This paper proposes a Dynamic Sample Selection (DSS) method for
CTDA. DSS consists of dynamic thresholding, positive learning, and negative
learning processes. Traditionally, models learn from unlabeled unknown
environment data and equally rely on all samples' pseudo-labels to update their
parameters through self-training. However, noisy predictions exist in these
pseudo-labels, so all samples are not equally trustworthy. Therefore, in our
method, a dynamic thresholding module is first designed to select suspected
low-quality from high-quality samples. The selected low-quality samples are
more likely to be wrongly predicted. Therefore, we apply joint positive and
negative learning on both high- and low-quality samples to reduce the risk of
using wrong information. We conduct extensive experiments that demonstrate the
effectiveness of our proposed method for CTDA in the image domain,
outperforming the state-of-the-art results. Furthermore, our approach is also
evaluated in the 3D point cloud domain, showcasing its versatility and
potential for broader applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Multi-modal Object Detection and Tracking on Edge for
  Regulatory Compliance Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Syuen Lim, Ziwei Wang, Jiajun Liu, Abdelwahed Khamis, Reza Arablouei, Robert Barlow, Ryan McAllister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regulatory compliance auditing across diverse industrial domains requires
heightened quality assurance and traceability. Present manual and intermittent
approaches to such auditing yield significant challenges, potentially leading
to oversights in the monitoring process. To address these issues, we introduce
a real-time, multi-modal sensing system employing 3D time-of-flight and RGB
cameras, coupled with unsupervised learning techniques on edge AI devices. This
enables continuous object tracking thereby enhancing efficiency in
record-keeping and minimizing manual interventions. While we validate the
system in a knife sanitization context within agrifood facilities, emphasizing
its prowess against occlusion and low-light issues with RGB cameras, its
potential spans various industrial monitoring settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Concept-Based Visual Causal Transition and Symbolic Reasoning
  for Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories and unseen object categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Limitation of CLIP Models: The Worst-Performing
  Categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie-Jing Shao, Jiang-Xin Shi, Xiao-Wen Yang, Lan-Zhe Guo, Yu-Feng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) provides a foundation model by
integrating natural language into visual concepts, enabling zero-shot
recognition on downstream tasks. It is usually expected that satisfactory
overall accuracy can be achieved across numerous domains through well-designed
textual prompts. However, we found that their performance in the worst
categories is significantly inferior to the overall performance. For example,
on ImageNet, there are a total of 10 categories with class-wise accuracy as low
as 0\%, even though the overall performance has achieved 64.1\%. This
phenomenon reveals the potential risks associated with using CLIP models,
particularly in risk-sensitive applications where specific categories hold
significant importance. To address this issue, we investigate the alignment
between the two modalities in the CLIP model and propose the Class-wise
Matching Margin (\cmm) to measure the inference confusion. \cmm\ can
effectively identify the worst-performing categories and estimate the potential
performance of the candidate prompts. We further query large language models to
enrich descriptions of worst-performing categories and build a weighted
ensemble to highlight the efficient prompts. Experimental results clearly
verify the effectiveness of our proposal, where the accuracy on the worst-10
categories on ImageNet is boosted to 5.2\%, without manual prompt engineering,
laborious optimization, or access to labeled validation data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Human-Robot Collaboration using Constrained Probabilistic
  Human-Motion Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aadi Kothari, Tony Tohme, Xiaotong Zhang, Kamal Youcef-Toumi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion prediction is an essential step for efficient and safe
human-robot collaboration. Current methods either purely rely on representing
the human joints in some form of neural network-based architecture or use
regression models offline to fit hyper-parameters in the hope of capturing a
model encompassing human motion. While these methods provide good initial
results, they are missing out on leveraging well-studied human body kinematic
models as well as body and scene constraints which can help boost the efficacy
of these prediction frameworks while also explicitly avoiding implausible human
joint configurations. We propose a novel human motion prediction framework that
incorporates human joint constraints and scene constraints in a Gaussian
Process Regression (GPR) model to predict human motion over a set time horizon.
This formulation is combined with an online context-aware constraints model to
leverage task-dependent motions. It is tested on a human arm kinematic model
and implemented on a human-robot collaborative setup with a UR5 robot arm to
demonstrate the real-time capability of our approach. Simulations were also
performed on datasets like HA4M and ANDY. The simulation and experimental
results demonstrate considerable improvements in a Gaussian Process framework
when these constraints are explicitly considered.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures. Associated video demonstration can be found at
  https://www.youtube.com/@MITMechatronics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can pre-trained models assist in dataset distillation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Lu, Xuguang Chen, Yuchen Zhang, Jianyang Gu, Tianle Zhang, Yifan Zhang, Xiaoniu Yang, Qi Xuan, Kai Wang, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset Distillation (DD) is a prominent technique that encapsulates
knowledge from a large-scale original dataset into a small synthetic dataset
for efficient training. Meanwhile, Pre-trained Models (PTMs) function as
knowledge repositories, containing extensive information from the original
dataset. This naturally raises a question: Can PTMs effectively transfer
knowledge to synthetic datasets, guiding DD accurately? To this end, we conduct
preliminary experiments, confirming the contribution of PTMs to DD. Afterwards,
we systematically study different options in PTMs, including initialization
parameters, model architecture, training epoch and domain knowledge, revealing
that: 1) Increasing model diversity enhances the performance of synthetic
datasets; 2) Sub-optimal models can also assist in DD and outperform
well-trained ones in certain cases; 3) Domain-specific PTMs are not mandatory
for DD, but a reasonable domain match is crucial. Finally, by selecting optimal
options, we significantly improve the cross-architecture generalization over
baseline DD methods. We hope our work will facilitate researchers to develop
better DD techniques. Our code is available at
https://github.com/yaolu-zjut/DDInterpreter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimVLG: Simple and Efficient Pretraining of Visual Language Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiren Jian, Tingkai Liu, Yunzhe Tao, Soroush Vosoughi, HX Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose ``SimVLG'', a streamlined framework for the
pre-training of computationally intensive vision-language generative models,
leveraging frozen pre-trained large language models (LLMs). The prevailing
paradigm in vision-language pre-training (VLP) typically involves a two-stage
optimization process: an initial resource-intensive phase dedicated to
general-purpose vision-language representation learning, aimed at extracting
and consolidating pertinent visual features, followed by a subsequent phase
focusing on end-to-end alignment between visual and linguistic modalities. Our
one-stage, single-loss framework circumvents the aforementioned computationally
demanding first stage of training by gradually merging similar visual tokens
during training. This gradual merging process effectively compacts the visual
information while preserving the richness of semantic content, leading to fast
convergence without sacrificing performance. Our experiments show that our
approach can speed up the training of vision-language models by a factor
$\times 5$ without noticeable impact on the overall performance. Additionally,
we show that our models can achieve comparable performance to current
vision-language models with only $1/10$ of the data. Finally, we demonstrate
how our image-text models can be easily adapted to video-language generative
tasks through a novel soft attentive temporal token merging modules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoseAction: Action Recognition for Patients in the Ward using Deep
  Learning Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zherui Li, Raye Chen-Hua Yeow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time intelligent detection and prediction of subjects' behavior
particularly their movements or actions is critical in the ward. This approach
offers the advantage of reducing in-hospital care costs and improving the
efficiency of healthcare workers, which is especially true for scenarios at
night or during peak admission periods. Therefore, in this work, we propose
using computer vision (CV) and deep learning (DL) methods for detecting
subjects and recognizing their actions. We utilize OpenPose as an accurate
subject detector for recognizing the positions of human subjects in the video
stream. Additionally, we employ AlphAction's Asynchronous Interaction
Aggregation (AIA) network to predict the actions of detected subjects. This
integrated model, referred to as PoseAction, is proposed. At the same time, the
proposed model is further trained to predict 12 common actions in ward areas,
such as staggering, chest pain, and falling down, using medical-related video
clips from the NTU RGB+D and NTU RGB+D 120 datasets. The results demonstrate
that PoseAction achieves the highest classification mAP of 98.72% (IoU@0.5).
Additionally, this study develops an online real-time mode for action
recognition, which strongly supports the clinical translation of PoseAction.
Furthermore, using OpenPose's function for recognizing face key points, we also
implement face blurring, which is a practical solution to address the privacy
protection concerns of patients and healthcare workers. Nevertheless, the
training data for PoseAction is currently limited, particularly in terms of
label diversity. Consequently, the subsequent step involves utilizing a more
diverse dataset (including general actions) to train the model's parameters for
improved generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifying Whole Slide Images: What Matters? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Nguyen, Aiden Nibali, Joshua Millward, Zhen He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there have been many algorithms proposed for the classification of
very high resolution whole slide images (WSIs). These new algorithms are mostly
focused on finding novel ways to combine the information from small local
patches extracted from the slide, with an emphasis on effectively aggregating
more global information for the final predictor. In this paper we thoroughly
explore different key design choices for WSI classification algorithms to
investigate what matters most for achieving high accuracy. Surprisingly, we
found that capturing global context information does not necessarily mean
better performance. A model that captures the most global information
consistently performs worse than a model that captures less global information.
In addition, a very simple multi-instance learning method that captures no
global information performs almost as well as models that capture a lot of
global information. These results suggest that the most important features for
effective WSI classification are captured at the local small patch level, where
cell and tissue micro-environment detail is most pronounced. Another surprising
finding was that unsupervised pre-training on a larger set of 33 cancers gives
significantly worse performance compared to pre-training on a smaller dataset
of 7 cancers (including the target cancer). We posit that pre-training on a
smaller, more focused dataset allows the feature extractor to make better use
of the limited feature space to better discriminate between subtle differences
in the input patch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ablation Study to Clarify the Mechanism of Object Segmentation in
  Multi-Object Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takayuki Komatsu, Yoshiyuki Ohmura, Yasuo Kuniyoshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-object representation learning aims to represent complex real-world
visual input using the composition of multiple objects. Representation learning
methods have often used unsupervised learning to segment an input image into
individual objects and encode these objects into each latent vector. However,
it is not clear how previous methods have achieved the appropriate segmentation
of individual objects. Additionally, most of the previous methods regularize
the latent vectors using a Variational Autoencoder (VAE). Therefore, it is not
clear whether VAE regularization contributes to appropriate object
segmentation. To elucidate the mechanism of object segmentation in multi-object
representation learning, we conducted an ablation study on MONet, which is a
typical method. MONet represents multiple objects using pairs that consist of
an attention mask and the latent vector corresponding to the attention mask.
Each latent vector is encoded from the input image and attention mask. Then,
the component image and attention mask are decoded from each latent vector. The
loss function of MONet consists of 1) the sum of reconstruction losses between
the input image and decoded component image, 2) the VAE regularization loss of
the latent vector, and 3) the reconstruction loss of the attention mask to
explicitly encode shape information. We conducted an ablation study on these
three loss functions to investigate the effect on segmentation performance. Our
results showed that the VAE regularization loss did not affect segmentation
performance and the others losses did affect it. Based on this result, we
hypothesize that it is important to maximize the attention mask of the image
region best represented by a single latent vector corresponding to the
attention mask. We confirmed this hypothesis by evaluating a new loss function
with the same mechanism as the hypothesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yefei He, Jing Liu, Weijia Wu, Hong Zhou, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable capabilities in image synthesis
and related generative tasks. Nevertheless, their practicality for low-latency
real-world applications is constrained by substantial computational costs and
latency issues. Quantization is a dominant way to compress and accelerate
diffusion models, where post-training quantization (PTQ) and quantization-aware
training (QAT) are two main approaches, each bearing its own properties. While
PTQ exhibits efficiency in terms of both time and data usage, it may lead to
diminished performance in low bit-width. On the other hand, QAT can alleviate
performance degradation but comes with substantial demands on computational and
data resources. To capitalize on the advantages while avoiding their respective
drawbacks, we introduce a data-free and parameter-efficient fine-tuning
framework for low-bit diffusion models, dubbed EfficientDM, to achieve
QAT-level performance with PTQ-like efficiency. Specifically, we propose a
quantization-aware variant of the low-rank adapter (QALoRA) that can be merged
with model weights and jointly quantized to low bit-width. The fine-tuning
process distills the denoising capabilities of the full-precision model into
its quantized counterpart, eliminating the requirement for training data. We
also introduce scale-aware optimization and employ temporal learned step-size
quantization to further enhance performance. Extensive experimental results
demonstrate that our method significantly outperforms previous PTQ-based
diffusion models while maintaining similar time and data efficiency.
Specifically, there is only a marginal 0.05 sFID increase when quantizing both
weights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to
QAT-based methods, our EfficientDM also boasts a 16.2x faster quantization
speed with comparable generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hard View Selection for Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Ferreira, Ivo Rapant, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many Contrastive Learning (CL) methods train their models to be invariant to
different "views" of an image input for which a good data augmentation pipeline
is crucial. While considerable efforts were directed towards improving pre-text
tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax
centering), the majority of these methods remain strongly reliant on the random
sampling of operations within the image augmentation pipeline, such as the
random resized crop or color distortion operation. In this paper, we argue that
the role of the view generation and its effect on performance has so far
received insufficient attention. To address this, we propose an easy,
learning-free, yet powerful Hard View Selection (HVS) strategy designed to
extend the random view generation to expose the pretrained model to harder
samples during CL training. It encompasses the following iterative steps: 1)
randomly sample multiple views and create pairs of two views, 2) run forward
passes for each view pair on the currently trained model, 3) adversarially
select the pair yielding the worst loss, and 4) run the backward pass with the
selected pair. In our empirical analysis we show that under the hood, HVS
increases task difficulty by controlling the Intersection over Union of views
during pretraining. With only 300-epoch pretraining, HVS is able to closely
rival the 800-epoch DINO baseline which remains very favorable even when
factoring in the slowdown induced by the additional forwards of HVS.
Additionally, HVS consistently achieves accuracy improvements on ImageNet
between 0.55% and 1.9% on linear evaluation and similar improvements on
transfer tasks across multiple CL methods, such as DINO, SimSiam, and SimCLR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models as Masked Audio-Video Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elvis Nunez, Yanzi Jin, Mohammad Rastegari, Sachin Mehta, Maxwell Horton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past several years, the synchronization between audio and visual
signals has been leveraged to learn richer audio-visual representations. Aided
by the large availability of unlabeled videos, many unsupervised training
frameworks have demonstrated impressive results in various downstream audio and
video tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a
state-of-the-art audio-video pre-training framework. MAViL couples contrastive
learning with masked autoencoding to jointly reconstruct audio spectrograms and
video frames by fusing information from both modalities. In this paper, we
study the potential synergy between diffusion models and MAViL, seeking to
derive mutual benefits from these two frameworks. The incorporation of
diffusion into MAViL, combined with various training efficiency methodologies
that include the utilization of a masking ratio curriculum and adaptive batch
sizing, results in a notable 32% reduction in pre-training Floating-Point
Operations (FLOPS) and an 18% decrease in pre-training wall clock time.
Crucially, this enhanced efficiency does not compromise the model's performance
in downstream audio-classification tasks when compared to MAViL's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashu Yamazaki, Taisei Hanyu, Khoa Vo, Thang Pham, Minh Tran, Gianfranco Doretto, Anh Nguyen, Ngan Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise 3D environmental mapping is pivotal in robotics. Existing methods
often rely on predefined concepts during training or are time-intensive when
generating semantic maps. This paper presents Open-Fusion, a groundbreaking
approach for real-time open-vocabulary 3D mapping and queryable scene
representation using RGB-D data. Open-Fusion harnesses the power of a
pre-trained vision-language foundation model (VLFM) for open-set semantic
comprehension and employs the Truncated Signed Distance Function (TSDF) for
swift 3D scene reconstruction. By leveraging the VLFM, we extract region-based
embeddings and their associated confidence maps. These are then integrated with
3D knowledge from TSDF using an enhanced Hungarian-based feature-matching
mechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D
segmentation for open-vocabulary without necessitating additional 3D training.
Benchmark tests on the ScanNet dataset against leading zero-shot methods
highlight Open-Fusion's superiority. Furthermore, it seamlessly combines the
strengths of region-based VLFM and TSDF, facilitating real-time 3D scene
comprehension that includes object concepts and open-world semantics. We
encourage the readers to view the demos on our project page:
https://uark-aicv.github.io/OpenFusion
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coloring Deep CNN Layers with Activation Hue Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis-François Bouchard, Mohsen Ben Lazreg, Matthew Toews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel hue-like angular parameter to model the structure
of deep convolutional neural network (CNN) activation space, referred to as the
{\em activation hue}, for the purpose of regularizing models for more effective
learning. The activation hue generalizes the notion of color hue angle in
standard 3-channel RGB intensity space to $N$-channel activation space. A
series of observations based on nearest neighbor indexing of activation vectors
with pre-trained networks indicate that class-informative activations are
concentrated about an angle $\theta$ in both the $(x,y)$ image plane and in
multi-channel activation space. A regularization term in the form of hue-like
angular $\theta$ labels is proposed to complement standard one-hot loss.
Training from scratch using combined one-hot + activation hue loss improves
classification performance modestly for a wide variety of classification tasks,
including ImageNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TWICE Dataset: Digital Twin of Test Scenarios in a Controlled
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Novicki Neto, Fabio Reway, Yuri Poledna, Maikol Funk Drechsler, Eduardo Parente Ribeiro, Werner Huber, Christian Icking
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the safe and reliable operation of autonomous vehicles under adverse
weather remains a significant challenge. To address this, we have developed a
comprehensive dataset composed of sensor data acquired in a real test track and
reproduced in the laboratory for the same test scenarios. The provided dataset
includes camera, radar, LiDAR, inertial measurement unit (IMU), and GPS data
recorded under adverse weather conditions (rainy, night-time, and snowy
conditions). We recorded test scenarios using objects of interest such as car,
cyclist, truck and pedestrian -- some of which are inspired by EURONCAP
(European New Car Assessment Programme). The sensor data generated in the
laboratory is acquired by the execution of simulation-based tests in
hardware-in-the-loop environment with the digital twin of each real test
scenario. The dataset contains more than 2 hours of recording, which totals
more than 280GB of data. Therefore, it is a valuable resource for researchers
in the field of autonomous vehicles to test and improve their algorithms in
adverse weather conditions, as well as explore the simulation-to-reality gap.
The dataset is available for download at: https://twicedataset.github.io/site/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 13 figures, submitted to IEEE Sensors Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterizing the Features of Mitotic Figures Using a Conditional
  Diffusion Probabilistic Model <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cagla Deniz Bahadir, Benjamin Liechty, David J. Pisapia, Mert R. Sabuncu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitotic figure detection in histology images is a hard-to-define, yet
clinically significant task, where labels are generated with pathologist
interpretations and where there is no ``gold-standard'' independent
ground-truth. However, it is well-established that these interpretation based
labels are often unreliable, in part, due to differences in expertise levels
and human subjectivity. In this paper, our goal is to shed light on the
inherent uncertainty of mitosis labels and characterize the mitotic figure
classification task in a human interpretable manner. We train a probabilistic
diffusion model to synthesize patches of cell nuclei for a given mitosis label
condition. Using this model, we can then generate a sequence of synthetic
images that correspond to the same nucleus transitioning into the mitotic
state. This allows us to identify different image features associated with
mitosis, such as cytoplasm granularity, nuclear density, nuclear irregularity
and high contrast between the nucleus and the cell body. Our approach offers a
new tool for pathologists to interpret and communicate the features driving the
decision to recognize a mitotic figure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Deep Generative Models Workshop at Medical Image
  Computing and Computer Assisted Intervention (MICCAI) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerated Neural Network Training with Rooted Logistic Objectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhu Wang, Praveen Raj Veluswami, Harsh Mishra, Sathya N. Ravi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many neural networks deployed in the real world scenarios are trained using
cross entropy based loss functions. From the optimization perspective, it is
known that the behavior of first order methods such as gradient descent
crucially depend on the separability of datasets. In fact, even in the most
simplest case of binary classification, the rate of convergence depends on two
factors: (1) condition number of data matrix, and (2) separability of the
dataset. With no further pre-processing techniques such as
over-parametrization, data augmentation etc., separability is an intrinsic
quantity of the data distribution under consideration. We focus on the
landscape design of the logistic function and derive a novel sequence of {\em
strictly} convex functions that are at least as strict as logistic loss. The
minimizers of these functions coincide with those of the minimum norm solution
wherever possible. The strict convexity of the derived function can be extended
to finetune state-of-the-art models and applications. In empirical experimental
analysis, we apply our proposed rooted logistic objective to multiple deep
models, e.g., fully-connected neural networks and transformers, on various of
classification benchmarks. Our results illustrate that training with rooted
loss function is converged faster and gains performance improvements.
Furthermore, we illustrate applications of our novel rooted loss function in
generative modeling based downstream applications, such as finetuning StyleGAN
model with the rooted loss. The code implementing our losses and models can be
found here for open source software development purposes:
https://anonymous.4open.science/r/rooted_loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FNOSeg3D: Resolution-Robust 3D Image Segmentation with Fourier Neural
  Operator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ken C. L. Wong, Hongzhi Wang, Tanveer Syeda-Mahmood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the computational complexity of 3D medical image segmentation,
training with downsampled images is a common remedy for out-of-memory errors in
deep learning. Nevertheless, as standard spatial convolution is sensitive to
variations in image resolution, the accuracy of a convolutional neural network
trained with downsampled images can be suboptimal when applied on the original
resolution. To address this limitation, we introduce FNOSeg3D, a 3D
segmentation model robust to training image resolution based on the Fourier
neural operator (FNO). The FNO is a deep learning framework for learning
mappings between functions in partial differential equations, which has the
appealing properties of zero-shot super-resolution and global receptive field.
We improve the FNO by reducing its parameter requirement and enhancing its
learning capability through residual connections and deep supervision, and
these result in our FNOSeg3D model which is parameter efficient and resolution
robust. When tested on the BraTS'19 dataset, it achieved superior robustness to
training image resolution than other tested models with less than 1% of their
model parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by the IEEE International Symposium on
  Biomedical Imaging (ISBI) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistency Regularization Improves Placenta Segmentation in Fetal EPI
  MRI Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingcheng Liu, Neerav Karani, Neel Dey, S. Mazdak Abulnaga, Junshen Xu, P. Ellen Grant, Esra Abaci Turk, Polina Golland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The placenta plays a crucial role in fetal development. Automated 3D placenta
segmentation from fetal EPI MRI holds promise for advancing prenatal care. This
paper proposes an effective semi-supervised learning method for improving
placenta segmentation in fetal EPI MRI time series. We employ consistency
regularization loss that promotes consistency under spatial transformation of
the same image and temporal consistency across nearby images in a time series.
The experimental results show that the method improves the overall segmentation
accuracy and provides better performance for outliers and hard samples. The
evaluation also indicates that our method improves the temporal coherency of
the prediction, which could lead to more accurate computation of temporal
placental biomarkers. This work contributes to the study of the placenta and
prenatal clinical decision-making. Code is available at
https://github.com/firstmover/cr-seg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenIncrement: A Unified Framework for Open Set Recognition and Deep
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawen Xu, Claas Grohnfeldt, Odej Kao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In most works on deep incremental learning research, it is assumed that novel
samples are pre-identified for neural network retraining. However, practical
deep classifiers often misidentify these samples, leading to erroneous
predictions. Such misclassifications can degrade model performance. Techniques
like open set recognition offer a means to detect these novel samples,
representing a significant area in the machine learning domain.
  In this paper, we introduce a deep class-incremental learning framework
integrated with open set recognition. Our approach refines class-incrementally
learned features to adapt them for distance-based open set recognition.
Experimental results validate that our method outperforms state-of-the-art
incremental learning techniques and exhibits superior performance in open set
recognition compared to baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is More: On the Feature Redundancy of Pretrained Models When
  Transferring to Few-shot Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Luo, Difan Zou, Lianli Gao, Zenglin Xu, Jingkuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transferring a pretrained model to a downstream task can be as easy as
conducting linear probing with target data, that is, training a linear
classifier upon frozen features extracted from the pretrained model. As there
may exist significant gaps between pretraining and downstream datasets, one may
ask whether all dimensions of the pretrained features are useful for a given
downstream task. We show that, for linear probing, the pretrained features can
be extremely redundant when the downstream data is scarce, or few-shot. For
some cases such as 5-way 1-shot tasks, using only 1\% of the most important
feature dimensions is able to recover the performance achieved by using the
full representation. Interestingly, most dimensions are redundant only under
few-shot settings and gradually become useful when the number of shots
increases, suggesting that feature redundancy may be the key to characterizing
the "few-shot" nature of few-shot transfer problems. We give a theoretical
understanding of this phenomenon and show how dimensions with high variance and
small distance between class centroids can serve as confounding factors that
severely disturb classification results under few-shot settings. As an attempt
at solving this problem, we find that the redundant features are difficult to
identify accurately with a small number of training samples, but we can instead
adjust feature magnitude with a soft mask based on estimated feature
importance. We show that this method can generally improve few-shot transfer
performance across various pretrained models and downstream datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Audio-Visual Features for Multimodal Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sneha Muppalla, Shan Jia, Siwei Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfakes are AI-generated media in which an image or video has been
digitally modified. The advancements made in deepfake technology have led to
privacy and security issues. Most deepfake detection techniques rely on the
detection of a single modality. Existing methods for audio-visual detection do
not always surpass that of the analysis based on single modalities. Therefore,
this paper proposes an audio-visual-based method for deepfake detection, which
integrates fine-grained deepfake identification with binary classification. We
categorize the samples into four types by combining labels specific to each
single modality. This method enhances the detection under intra-domain and
cross-domain testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WLST: Weak Labels Guided Self-training for Weakly-supervised Domain
  Adaptation on 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsung-Lin Tsou, Tsung-Han Wu, Winston H. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of domain adaptation (DA) on 3D object detection, most of the
work is dedicated to unsupervised domain adaptation (UDA). Yet, without any
target annotations, the performance gap between the UDA approaches and the
fully-supervised approach is still noticeable, which is impractical for
real-world applications. On the other hand, weakly-supervised domain adaptation
(WDA) is an underexplored yet practical task that only requires few labeling
effort on the target domain. To improve the DA performance in a cost-effective
way, we propose a general weak labels guided self-training framework, WLST,
designed for WDA on 3D object detection. By incorporating autolabeler, which
can generate 3D pseudo labels from 2D bounding boxes, into the existing
self-training pipeline, our method is able to generate more robust and
consistent pseudo labels that would benefit the training process on the target
domain. Extensive experiments demonstrate the effectiveness, robustness, and
detector-agnosticism of our WLST framework. Notably, it outperforms previous
state-of-the-art methods on all evaluation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Functional data learning using convolutional neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Galarza, Tamer Oraby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we show how convolutional neural networks (CNN) can be used in
regression and classification learning problems of noisy and non-noisy
functional data. The main idea is to transform the functional data into a 28 by
28 image. We use a specific but typical architecture of a convolutional neural
network to perform all the regression exercises of parameter estimation and
functional form classification. First, we use some functional case studies of
functional data with and without random noise to showcase the strength of the
new method. In particular, we use it to estimate exponential growth and decay
rates, the bandwidths of sine and cosine functions, and the magnitudes and
widths of curve peaks. We also use it to classify the monotonicity and
curvatures of functional data, algebraic versus exponential growth, and the
number of peaks of functional data. Second, we apply the same convolutional
neural networks to Lyapunov exponent estimation in noisy and non-noisy chaotic
data, in estimating rates of disease transmission from epidemic curves, and in
detecting the similarity of drug dissolution profiles. Finally, we apply the
method to real-life data to detect Parkinson's disease patients in a
classification problem. The method, although simple, shows high accuracy and is
promising for future use in engineering and medical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases <span class="chip">NeurIPS '23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mazda Moayeri, Wenxiao Wang, Sahil Singla, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple but effective method to measure and mitigate model biases
caused by reliance on spurious cues. Instead of requiring costly changes to
one's data or model training, our method better utilizes the data one already
has by sorting them. Specifically, we rank images within their classes based on
spuriosity (the degree to which common spurious cues are present), proxied via
deep neural features of an interpretable network. With spuriosity rankings, it
is easy to identify minority subpopulations (i.e. low spuriosity images) and
assess model bias as the gap in accuracy between high and low spuriosity
images. One can even efficiently remove a model's bias at little cost to
accuracy by finetuning its classification head on low spuriosity images,
resulting in fairer treatment of samples regardless of spuriosity. We
demonstrate our method on ImageNet, annotating $5000$ class-feature
dependencies ($630$ of which we find to be spurious) and generating a dataset
of $325k$ soft segmentations for these features along the way. Having computed
spuriosity rankings via the identified spurious neural features, we assess
biases for $89$ diverse models and find that class-wise biases are highly
correlated across models. Our results suggest that model bias due to spurious
feature reliance is influenced far more by what the model is trained on than
how it is trained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS '23 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Realistic Zero-Shot Classification via Self Structural Semantic
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Zhang, Muzammal Naseer, Guangyi Chen, Zhiqiang Shen, Salman Khan, Kun Zhang, Fahad Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained Vision Language Models (VLMs) have proven effective
for zero-shot classification. Despite the success, most traditional VLMs-based
methods are restricted by the assumption of partial source supervision or ideal
vocabularies, which rarely satisfy the open-world scenario. In this paper, we
aim at a more challenging setting, Realistic Zero-Shot Classification, which
assumes no annotation but instead a broad vocabulary. To address this
challenge, we propose the Self Structural Semantic Alignment (S^3A) framework,
which extracts the structural semantic information from unlabeled data while
simultaneously self-learning. Our S^3A framework adopts a unique
Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups
unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR
process includes iterative clustering on images, voting within each cluster to
identify initial class candidates from the vocabulary, generating
discriminative prompts with large language models to discern confusing
candidates, and realigning images and the vocabulary as structural semantic
alignment. Finally, we propose to self-learn the CLIP image encoder with both
individual and structural semantic alignment through a teacher-student learning
strategy. Our comprehensive experiments across various generic and fine-grained
benchmarks demonstrate that the S^3A method offers substantial improvements
over existing VLMs-based approaches, achieving a more than 15% accuracy
improvement over CLIP on average. Our codes, models, and prompts are publicly
released at https://github.com/sheng-eatamath/S3A.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Understanding the Effect of Pretraining Label Granularity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan Zhe Hong, Yin Cui, Ariel Fuxman, Stanley H. Chan, Enming Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study how the granularity of pretraining labels affects the
generalization of deep neural networks in image classification tasks. We focus
on the "fine-to-coarse" transfer learning setting, where the pretraining label
space is more fine-grained than that of the target problem. Empirically, we
show that pretraining on the leaf labels of ImageNet21k produces better
transfer results on ImageNet1k than pretraining on other coarser granularity
levels, which supports the common practice used in the community.
Theoretically, we explain the benefit of fine-grained pretraining by proving
that, for a data distribution satisfying certain hierarchy conditions, 1)
coarse-grained pretraining only allows a neural network to learn the "common"
or "easy-to-learn" features well, while 2) fine-grained pretraining helps the
network learn the "rarer" or "fine-grained" features in addition to the common
ones, thus improving its accuracy on hard downstream test samples in which
common features are missing or weak in strength. Furthermore, we perform
comprehensive experiments using the label hierarchies of iNaturalist 2021 and
observe that the following conditions, in addition to proper choice of label
granularity, enable the transfer to work well in practice: 1) the pretraining
dataset needs to have a meaningful label hierarchy, and 2) the pretraining and
target label functions need to align well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chatting Makes Perfect: Chat-based Image Retrieval <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.20062v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.20062v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matan Levy, Rami Ben-Ari, Nir Darshan, Dani Lischinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chats emerge as an effective user-friendly approach for information
retrieval, and are successfully employed in many domains, such as customer
service, healthcare, and finance. However, existing image retrieval approaches
typically address the case of a single query-to-image round, and the use of
chats for image retrieval has been mostly overlooked. In this work, we
introduce ChatIR: a chat-based image retrieval system that engages in a
conversation with the user to elicit information, in addition to an initial
query, in order to clarify the user's search intent. Motivated by the
capabilities of today's foundation models, we leverage Large Language Models to
generate follow-up questions to an initial image description. These questions
form a dialog with the user in order to retrieve the desired image from a large
corpus. In this study, we explore the capabilities of such a system tested on a
large dataset and reveal that engaging in a dialog yields significant gains in
image retrieval. We start by building an evaluation pipeline from an existing
manually generated dataset and explore different modules and training
strategies for ChatIR. Our comparison includes strong baselines derived from
related applications trained with Reinforcement Learning. Our system is capable
of retrieving the target image from a pool of 50K images with over 78% success
rate after 5 dialogue rounds, compared to 75% when questions are asked by
humans, and 64% for a single shot text-to-image retrieval. Extensive
evaluations reveal the strong capabilities and examine the limitations of
CharIR under different settings. Project repository is available at
https://github.com/levymsn/ChatIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera Ready version for NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PMSSC: Parallelizable multi-subset based self-expressive model for
  subspace clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.12232v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.12232v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katsuya Hotta, Takuya Akashi, Shogo Tokai, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subspace clustering methods which embrace a self-expressive model that
represents each data point as a linear combination of other data points in the
dataset provide powerful unsupervised learning techniques. However, when
dealing with large datasets, representation of each data point by referring to
all data points via a dictionary suffers from high computational complexity. To
alleviate this issue, we introduce a parallelizable multi-subset based
self-expressive model (PMS) which represents each data point by combining
multiple subsets, with each consisting of only a small proportion of the
samples. The adoption of PMS in subspace clustering (PMSSC) leads to
computational advantages because the optimization problems decomposed over each
subset are small, and can be solved efficiently in parallel. Furthermore, PMSSC
is able to combine multiple self-expressive coefficient vectors obtained from
subsets, which contributes to an improvement in self-expressiveness. Extensive
experiments on synthetic and real-world datasets show the efficiency and
effectiveness of our approach in comparison to other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boost Video Frame Interpolation via Motion Adaptation <span class="chip">BMVC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13933v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13933v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoning Wu, Xiaoyun Zhang, Weidi Xie, Ya Zhang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video frame interpolation (VFI) is a challenging task that aims to generate
intermediate frames between two consecutive frames in a video. Existing
learning-based VFI methods have achieved great success, but they still suffer
from limited generalization ability due to the limited motion distribution of
training datasets. In this paper, we propose a novel optimization-based VFI
method that can adapt to unseen motions at test time. Our method is based on a
cycle-consistency adaptation strategy that leverages the motion characteristics
among video frames. We also introduce a lightweight adapter that can be
inserted into the motion estimation module of existing pre-trained VFI models
to improve the efficiency of adaptation. Extensive experiments on various
benchmarks demonstrate that our method can boost the performance of two-frame
VFI models, outperforming the existing state-of-the-art methods, even those
that use extra input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by BMVC 2023 (Oral Presentation) Project Page:
  https://haoningwu3639.github.io/VFI_Adapter_Webpage/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RedMotion: Motion Prediction via Redundancy Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Royden Wagner, Omer Sahin Tas, Marvin Klemp, Carlos Fernandez Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the future motion of traffic agents is vital for self-driving
vehicles to ensure their safe operation. We introduce RedMotion, a transformer
model for motion prediction that incorporates two types of redundancy
reduction. The first type of redundancy reduction is induced by an internal
transformer decoder and reduces a variable-sized set of road environment
tokens, such as road graphs with agent data, to a fixed-sized embedding. The
second type of redundancy reduction is a self-supervised learning objective and
applies the redundancy reduction principle to embeddings generated from
augmented views of road environments. Our experiments reveal that our
representation learning approach can outperform PreTraM, Traj-MAE, and
GraphDINO in a semi-supervised setting. Our RedMotion model achieves results
that are competitive with those of Scene Transformer or MTR++. We provide an
open source implementation that is accessible via GitHub
(https://github.com/kit-mrt/red-motion) and Colab
(https://colab.research.google.com/drive/1Q-Z9VdiqvfPfctNG8oqzPcgm0lP3y1il).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report, 13 pages, 8 figures; v2: focus on transformer model</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00616v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00616v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao, Lei Zhu, Joan Lasenby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D open-vocabulary scene understanding methods mostly utilize
well-aligned 2D images as the bridge to learn 3D features with language.
However, applying these approaches becomes challenging in scenarios where 2D
images are absent. In this work, we introduce a new pipeline, namely,
OpenIns3D, which requires no 2D image inputs, for 3D open-vocabulary scene
understanding at the instance level. The OpenIns3D framework employs a
"Mask-Snap-Lookup" scheme. The "Mask" module learns class-agnostic mask
proposals in 3D point clouds. The "Snap" module generates synthetic scene-level
images at multiple scales and leverages 2D vision language models to extract
interesting objects. The "Lookup" module searches through the outcomes of
"Snap" with the help of Mask2Pixel maps, which contain the precise
correspondence between 3D masks and synthetic images, to assign category names
to the proposed masks. This 2D input-free and flexible approach achieves
state-of-the-art results on a wide range of indoor and outdoor datasets by a
large margin. Moreover, OpenIns3D allows for effortless switching of 2D
detectors without re-training. When integrated with powerful 2D open-world
models such as ODISE and GroundingDINO, excellent results were observed on
open-vocabulary instance segmentation. When integrated with LLM-powered 2D
models like LISA, it demonstrates a remarkable capacity to process highly
complex text queries which require intricate reasoning and world knowledge.
Project page: https://zheninghuang.github.io/OpenIns3D/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 17 figures, 13 tables. Project page:
  https://zheninghuang.github.io/OpenIns3D/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eliminating Contextual Prior Bias for Semantic Image Editing via
  Dual-Cycle Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02394v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02394v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuopeng Yang, Tianshu Chu, Xin Lin, Erdun Gao, Daqing Liu, Jie Yang, Chaoyue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent success of text-to-image generation diffusion models has also
revolutionized semantic image editing, enabling the manipulation of images
based on query/target texts. Despite these advancements, a significant
challenge lies in the potential introduction of contextual prior bias in
pre-trained models during image editing, e.g., making unexpected modifications
to inappropriate regions. To address this issue, we present a novel approach
called Dual-Cycle Diffusion, which generates an unbiased mask to guide image
editing. The proposed model incorporates a Bias Elimination Cycle that consists
of both a forward path and an inverted path, each featuring a Structural
Consistency Cycle to ensure the preservation of image content during the
editing process. The forward path utilizes the pre-trained model to produce the
edited image, while the inverted path converts the result back to the source
image. The unbiased mask is generated by comparing differences between the
processed source image and the edited image to ensure that both conform to the
same distribution. Our experiments demonstrate the effectiveness of the
proposed method, as it significantly improves the D-CLIP score from 0.272 to
0.283. The code will be available at
https://github.com/JohnDreamer/DualCycleDiffsion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by the IEEE Transactions on Circuits and
  Systems for Video Technology (TCSVT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Facade Parsing with Vision Transformers and Line Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15523v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15523v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Wang, Jiaxing Zhang, Ran Zhang, Yunqin Li, Liangzhi Li, Yuta Nakashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facade parsing stands as a pivotal computer vision task with far-reaching
applications in areas like architecture, urban planning, and energy efficiency.
Despite the recent success of deep learning-based methods in yielding
impressive results on certain open-source datasets, their viability for
real-world applications remains uncertain. Real-world scenarios are
considerably more intricate, demanding greater computational efficiency.
Existing datasets often fall short in representing these settings, and previous
methods frequently rely on extra models to enhance accuracy, which requires
much computation cost. In this paper, we introduce Comprehensive Facade Parsing
(CFP), a dataset meticulously designed to encompass the intricacies of
real-world facade parsing tasks. Comprising a total of 602 high-resolution
street-view images, this dataset captures a diverse array of challenging
scenarios, including sloping angles and densely clustered buildings, with
painstakingly curated annotations for each image. We introduce a new pipeline
known as Revision-based Transformer Facade Parsing (RTFP). This marks the
pioneering utilization of Vision Transformers (ViT) in facade parsing, and our
experimental results definitively substantiate its merit. We also design Line
Acquisition, Filtering, and Revision (LAFR), an efficient yet accurate revision
algorithm that can improve the segment result solely from simple line detection
using prior knowledge of the facade. In ECP 2011, RueMonge 2014, and our CFP,
we evaluate the superiority of our method. The dataset and code are available
at https://github.com/wbw520/RTFP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling the Link Between Image Statistics and Human Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09874v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09874v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Hepburn, Valero Laparra, Raúl Santos-Rodriguez, Jesús Malo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the 1950s, Barlow and Attneave hypothesised a link between biological
vision and information maximisation. Following Shannon, information was defined
using the probability of natural images. A number of physiological and
psychophysical phenomena have been derived ever since from principles like
info-max, efficient coding, or optimal denoising. However, it remains unclear
how this link is expressed in mathematical terms from image probability. First,
classical derivations were subjected to strong assumptions on the probability
models and on the behaviour of the sensors. Moreover, the direct evaluation of
the hypothesis was limited by the inability of the classical image models to
deliver accurate estimates of the probability. In this work we directly
evaluate image probabilities using an advanced generative model for natural
images, and we analyse how probability-related factors can be combined to
predict human perception via sensitivity of state-of-the-art subjective image
quality metrics. We use information theory and regression analysis to find a
combination of just two probability-related factors that achieves 0.8
correlation with subjective metrics. This probability-based sensitivity is
psychophysically validated by reproducing the basic trends of the Contrast
Sensitivity Function, its suprathreshold variation, and trends of the Weber-law
and masking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alzheimer's Disease Prediction via Brain Structural-Functional Deep
  Fusing Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16206v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16206v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiankun Zuo, Junren Pan, Shuqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fusing structural-functional images of the brain has shown great potential to
analyze the deterioration of Alzheimer's disease (AD). However, it is a big
challenge to effectively fuse the correlated and complementary information from
multimodal neuroimages. In this paper, a novel model termed cross-modal
transformer generative adversarial network (CT-GAN) is proposed to effectively
fuse the functional and structural information contained in functional magnetic
resonance imaging (fMRI) and diffusion tensor imaging (DTI). The CT-GAN can
learn topological features and generate multimodal connectivity from multimodal
imaging data in an efficient end-to-end manner. Moreover, the swapping
bi-attention mechanism is designed to gradually align common features and
effectively enhance the complementary features between modalities. By analyzing
the generated connectivity features, the proposed model can identify AD-related
brain connections. Evaluations on the public ADNI dataset show that the
proposed CT-GAN can dramatically improve prediction performance and detect
AD-related brain regions effectively. The proposed model also provides new
insights for detecting AD-related abnormal neural circuits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversity in deep generative models and generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.09573v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.09573v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Turinici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The decoder-based machine learning generative algorithms such as Generative
Adversarial Networks (GAN), Variational Auto-Encoders (VAE), Transformers show
impressive results when constructing objects similar to those in a training
ensemble. However, the generation of new objects builds mainly on the
understanding of the hidden structure of the training dataset followed by a
sampling from a multi-dimensional normal variable. In particular each sample is
independent from the others and can repeatedly propose same type of objects. To
cure this drawback we introduce a kernel-based measure quantization method that
can produce new objects from a given target measure by approximating it as a
whole and even staying away from elements already drawn from that distribution.
This ensures a better diversity of the produced objects. The method is tested
on classic machine learning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Detection of Backdoor Attacks via Density-based Clustering and
  Centroids Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Guo, Benedetta Tondi, Mauro Barni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a Universal Defence against backdoor attacks based on Clustering
and Centroids Analysis (CCA-UD). The goal of the defence is to reveal whether a
Deep Neural Network model is subject to a backdoor attack by inspecting the
training dataset. CCA-UD first clusters the samples of the training set by
means of density-based clustering. Then, it applies a novel strategy to detect
the presence of poisoned clusters. The proposed strategy is based on a general
misclassification behaviour observed when the features of a representative
example of the analysed cluster are added to benign samples. The capability of
inducing a misclassification error is a general characteristic of poisoned
samples, hence the proposed defence is attack-agnostic. This marks a
significant difference with respect to existing defences, that, either can
defend against only some types of backdoor attacks, or are effective only when
some conditions on the poisoning ratio or the kind of triggering signal used by
the attacker are satisfied.
  Experiments carried out on several classification tasks and network
architectures, considering different types of backdoor attacks (with either
clean or corrupted labels), and triggering signals, including both global and
local triggering signals, as well as sample-specific and source-specific
triggers, reveal that the proposed method is very effective to defend against
backdoor attacks in all the cases, always outperforming the state of the art
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Hidden Language of Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00966v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00966v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Michal Irani, Inbar Mosseri, Lior Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have demonstrated an unparalleled ability to
generate high-quality, diverse images from a textual prompt. However, the
internal representations learned by these models remain an enigma. In this
work, we present Conceptor, a novel method to interpret the internal
representation of a textual concept by a diffusion model. This interpretation
is obtained by decomposing the concept into a small set of human-interpretable
textual elements. Applied over the state-of-the-art Stable Diffusion model,
Conceptor reveals non-trivial structures in the representations of concepts.
For example, we find surprising visual connections between concepts, that
transcend their textual semantics. We additionally discover concepts that rely
on mixtures of exemplars, biases, renowned artistic styles, or a simultaneous
fusion of multiple meanings of the concept. Through a large battery of
experiments, we demonstrate Conceptor's ability to provide meaningful, robust,
and faithful decompositions for a wide variety of abstract, concrete, and
complex textual concepts, while allowing to naturally connect each
decomposition element to its corresponding visual impact on the generated
images. Our code will be available at: https://hila-chefer.github.io/Conceptor/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Anatomical Labeling of Pulmonary Tree Structures via Implicit
  Point-Graph Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangxian Xie, Jiancheng Yang, Donglai Wei, Ziqiao Weng, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulmonary diseases rank prominently among the principal causes of death
worldwide. Curing them will require, among other things, a better understanding
of the many complex 3D tree-shaped structures within the pulmonary system, such
as airways, arteries, and veins. In theory, they can be modeled using
high-resolution image stacks. Unfortunately, standard CNN approaches operating
on dense voxel grids are prohibitively expensive. To remedy this, we introduce
a point-based approach that preserves graph connectivity of tree skeleton and
incorporates an implicit surface representation. It delivers SOTA accuracy at a
low computational cost and the resulting models have usable surfaces. Due to
the scarcity of publicly accessible data, we have also curated an extensive
dataset to evaluate our approach and will make it public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modality Unifying Network for Visible-Infrared Person Re-Identification <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yu, Xu Cheng, Wei Peng, Weihao Liu, Guoying Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible-infrared person re-identification (VI-ReID) is a challenging task due
to large cross-modality discrepancies and intra-class variations. Existing
methods mainly focus on learning modality-shared representations by embedding
different modalities into the same feature space. As a result, the learned
feature emphasizes the common patterns across modalities while suppressing
modality-specific and identity-aware information that is valuable for Re-ID. To
address these issues, we propose a novel Modality Unifying Network (MUN) to
explore a robust auxiliary modality for VI-ReID. First, the auxiliary modality
is generated by combining the proposed cross-modality learner and
intra-modality learner, which can dynamically model the modality-specific and
modality-shared representations to alleviate both cross-modality and
intra-modality variations. Second, by aligning identity centres across the
three modalities, an identity alignment loss function is proposed to discover
the discriminative feature representations. Third, a modality alignment loss is
introduced to consistently reduce the distribution distance of visible and
infrared images by modality prototype modeling. Extensive experiments on
multiple public datasets demonstrate that the proposed method surpasses the
current state-of-the-art methods by a significant margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures. Accepted as the poster paper in ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ V3Det: Vast Vocabulary Visual Detection Dataset <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in detecting arbitrary objects in the real world are trained
and evaluated on object detection datasets with a relatively restricted
vocabulary. To facilitate the development of more general visual object
detection, we propose V3Det, a vast vocabulary visual detection dataset with
precisely annotated bounding boxes on massive images. V3Det has several
appealing properties: 1) Vast Vocabulary: It contains bounding boxes of objects
from 13,204 categories on real-world images, which is 10 times larger than the
existing large vocabulary object detection dataset, e.g., LVIS. 2) Hierarchical
Category Organization: The vast vocabulary of V3Det is organized by a
hierarchical category tree which annotates the inclusion relationship among
categories, encouraging the exploration of category relationships in vast and
open vocabulary object detection. 3) Rich Annotations: V3Det comprises
precisely annotated objects in 243k images and professional descriptions of
each category written by human experts and a powerful chatbot. By offering a
vast exploration space, V3Det enables extensive benchmarks on both vast and
open vocabulary object detection, leading to new observations, practices, and
insights for future research. It has the potential to serve as a cornerstone
dataset for developing more general visual perception systems. V3Det is
available at https://v3det.openxlab.org.cn/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023 Oral Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdvRain: Adversarial Raindrops to Attack Camera-based Smart Vision
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amira Guesmi, Muhammad Abdullah Hanif, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based perception modules are increasingly deployed in many
applications, especially autonomous vehicles and intelligent robots. These
modules are being used to acquire information about the surroundings and
identify obstacles. Hence, accurate detection and classification are essential
to reach appropriate decisions and take appropriate and safe actions at all
times. Current studies have demonstrated that "printed adversarial attacks",
known as physical adversarial attacks, can successfully mislead perception
models such as object detectors and image classifiers. However, most of these
physical attacks are based on noticeable and eye-catching patterns for
generated perturbations making them identifiable/detectable by human eye or in
test drives. In this paper, we propose a camera-based inconspicuous adversarial
attack (\textbf{AdvRain}) capable of fooling camera-based perception systems
over all objects of the same class. Unlike mask based fake-weather attacks that
require access to the underlying computing hardware or image memory, our attack
is based on emulating the effects of a natural weather condition (i.e.,
Raindrops) that can be printed on a translucent sticker, which is externally
placed over the lens of a camera. To accomplish this, we provide an iterative
process based on performing a random search aiming to identify critical
positions to make sure that the performed transformation is adversarial for a
target classifier. Our transformation is based on blurring predefined parts of
the captured image corresponding to the areas covered by the raindrop. We
achieve a drop in average model accuracy of more than $45\%$ and $40\%$ on
VGG19 for ImageNet and Resnet34 for Caltech-101, respectively, using only $20$
raindrops.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessment of the Reliablity of a Model's Decision by Generalizing
  Attribution to the Wavelet Domain <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14979v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14979v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Kasmi, Laurent Dubus, Yves-Marie Saint Drenan, Philippe Blanc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have shown remarkable performance in computer vision, but
their deployment in numerous scientific and technical fields is challenging due
to their black-box nature. Scientists and practitioners need to evaluate the
reliability of a decision, i.e., to know simultaneously if a model relies on
the relevant features and whether these features are robust to image
corruptions. Existing attribution methods aim to provide human-understandable
explanations by highlighting important regions in the image domain, but fail to
fully characterize a decision process's reliability. To bridge this gap, we
introduce the Wavelet sCale Attribution Method (WCAM), a generalization of
attribution from the pixel domain to the space-scale domain using wavelet
transforms. Attribution in the wavelet domain reveals where {\it and} on what
scales the model focuses, thus enabling us to assess whether a decision is
reliable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures, 2 tables. v1 of the manuscript rejected from
  NeurIPS 2023, mainly due to the lack of quantitative evidence of the
  relevance of the proposed methodology. In the v2, we propose steps to address
  this issue and also plan on expanding the insertion and deletion scores for
  our method</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Next-Active Objects for Context-Aware Anticipation in
  Egocentric Videos <span class="chip">WACV'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08303v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08303v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanket Thakur, Cigdem Beyan, Pietro Morerio, Vittorio Murino, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objects are crucial for understanding human-object interactions. By
identifying the relevant objects, one can also predict potential future
interactions or actions that may occur with these objects. In this paper, we
study the problem of Short-Term Object interaction anticipation (STA) and
propose NAOGAT (Next-Active-Object Guided Anticipation Transformer), a
multi-modal end-to-end transformer network, that attends to objects in observed
frames in order to anticipate the next-active-object (NAO) and, eventually, to
guide the model to predict context-aware future actions. The task is
challenging since it requires anticipating future action along with the object
with which the action occurs and the time after which the interaction will
begin, a.k.a. the time to contact (TTC). Compared to existing video modeling
architectures for action anticipation, NAOGAT captures the relationship between
objects and the global scene context in order to predict detections for the
next active object and anticipate relevant future actions given these
detections, leveraging the objects' dynamics to improve accuracy. One of the
key strengths of our approach, in fact, is its ability to exploit the motion
dynamics of objects within a given clip, which is often ignored by other
models, and separately decoding the object-centric and motion-centric
information. Through our experiments, we show that our model outperforms
existing methods on two separate datasets, Ego4D and EpicKitchens-100 ("Unseen
Set"), as measured by several additional metrics, such as time to contact, and
next-active-object localization. The code will be available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WACV'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Supervised Masked Convolutional Transformer Block for Anomaly
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12148v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12148v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neelu Madan, Nicolae-Catalin Ristea, Radu Tudor Ionescu, Kamal Nasrollahi, Fahad Shahbaz Khan, Thomas B. Moeslund, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection has recently gained increasing attention in the field of
computer vision, likely due to its broad set of applications ranging from
product fault detection on industrial production lines and impending event
detection in video surveillance to finding lesions in medical scans. Regardless
of the domain, anomaly detection is typically framed as a one-class
classification task, where the learning is conducted on normal examples only.
An entire family of successful anomaly detection methods is based on learning
to reconstruct masked normal inputs (e.g. patches, future frames, etc.) and
exerting the magnitude of the reconstruction error as an indicator for the
abnormality level. Unlike other reconstruction-based methods, we present a
novel self-supervised masked convolutional transformer block (SSMCTB) that
comprises the reconstruction-based functionality at a core architectural level.
The proposed self-supervised block is extremely flexible, enabling information
masking at any layer of a neural network and being compatible with a wide range
of neural architectures. In this work, we extend our previous self-supervised
predictive convolutional attentive block (SSPCAB) with a 3D masked
convolutional layer, a transformer for channel-wise attention, as well as a
novel self-supervised objective based on Huber loss. Furthermore, we show that
our block is applicable to a wider variety of tasks, adding anomaly detection
in medical images and thermal videos to the previously considered tasks based
on RGB images and surveillance videos. We exhibit the generality and
flexibility of SSMCTB by integrating it into multiple state-of-the-art neural
models for anomaly detection, bringing forth empirical results that confirm
considerable performance improvements on five benchmarks. We release our code
and data as open source at: https://github.com/ristea/ssmctb.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards domain-invariant Self-Supervised Learning with Batch Styles
  Standardization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06088v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06088v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marin Scalbert, Maria Vakalopoulou, Florent Couzinié-Devy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Self-Supervised Learning (SSL), models are typically pretrained,
fine-tuned, and evaluated on the same domains. However, they tend to perform
poorly when evaluated on unseen domains, a challenge that Unsupervised Domain
Generalization (UDG) seeks to address. Current UDG methods rely on domain
labels, which are often challenging to collect, and domain-specific
architectures that lack scalability when confronted with numerous domains,
making the current methodology impractical and rigid. Inspired by
contrastive-based UDG methods that mitigate spurious correlations by
restricting comparisons to examples from the same domain, we hypothesize that
eliminating style variability within a batch could provide a more convenient
and flexible way to reduce spurious correlations without requiring domain
labels. To verify this hypothesis, we introduce Batch Styles Standardization
(BSS), a relatively simple yet powerful Fourier-based method to standardize the
style of images in a batch specifically designed for integration with SSL
methods to tackle UDG. Combining BSS with existing SSL methods offers serious
advantages over prior UDG methods: (1) It eliminates the need for domain labels
or domain-specific network components to enhance domain-invariance in SSL
representations, and (2) offers flexibility as BSS can be seamlessly integrated
with diverse contrastive-based but also non-contrastive-based SSL methods.
Experiments on several UDG datasets demonstrate that it significantly improves
downstream task performances on unseen domains, often outperforming or rivaling
with UDG methods. Finally, this work clarifies the underlying mechanisms
contributing to BSS's effectiveness in improving domain-invariance in SSL
representations and performances on unseen domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-label Image Classification using Adaptive Graph Convolutional
  Networks: from a Single Domain to Multiple Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Indel Pal Singh, Enjie Ghorbel, Oyebade Oyedotun, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an adaptive graph-based approach for multi-label image
classification. Graph-based methods have been largely exploited in the field of
multi-label classification, given their ability to model label correlations.
Specifically, their effectiveness has been proven not only when considering a
single domain but also when taking into account multiple domains. However, the
topology of the used graph is not optimal as it is pre-defined heuristically.
In addition, consecutive Graph Convolutional Network (GCN) aggregations tend to
destroy the feature similarity. To overcome these issues, an architecture for
learning the graph connectivity in an end-to-end fashion is introduced. This is
done by integrating an attention-based mechanism and a similarity-preserving
strategy. The proposed framework is then extended to multiple domains using an
adversarial training scheme. Numerous experiments are reported on well-known
single-domain and multi-domain benchmarks. The results demonstrate that our
approach achieves competitive results in terms of mean Average Precision (mAP)
and model size as compared to the state-of-the-art. The code will be made
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NAISR: A 3D Neural Additive Model for Interpretable Shape Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09234v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09234v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Jiao, Carlton Zdanski, Julia Kimbell, Andrew Prince, Cameron Worden, Samuel Kirse, Christopher Rutter, Benjamin Shields, William Dunn, Jisan Mahmud, Marc Niethammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep implicit functions (DIFs) have emerged as a powerful paradigm for many
computer vision tasks such as 3D shape reconstruction, generation,
registration, completion, editing, and understanding. However, given a set of
3D shapes with associated covariates there is at present no shape
representation method which allows to precisely represent the shapes while
capturing the individual dependencies on each covariate. Such a method would be
of high utility to researchers to discover knowledge hidden in a population of
shapes. For scientific shape discovery, we propose a 3D Neural Additive Model
for Interpretable Shape Representation ($\texttt{NAISR}$) which describes
individual shapes by deforming a shape atlas in accordance to the effect of
disentangled covariates. Our approach captures shape population trends and
allows for patient-specific predictions through shape transfer.
$\texttt{NAISR}$ is the first approach to combine the benefits of deep implicit
shape representations with an atlas deforming according to specified
covariates. We evaluate $\texttt{NAISR}$ with respect to shape reconstruction,
shape disentanglement, shape evolution, and shape transfer on three datasets:
1) $\textit{Starman}$, a simulated 2D shape dataset; 2) the ADNI hippocampus 3D
shape dataset; and 3) a pediatric airway 3D shape dataset. Our experiments
demonstrate that $\textit{Starman}$ achieves excellent shape reconstruction
performance while retaining interpretability. Our code is available at
$\href{https://github.com/uncbiag/NAISR}{https://github.com/uncbiag/NAISR}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building Flyweight FLIM-based CNNs with Adaptive Decoding for Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo de Melo Joao, Azael de Melo e Sousa, Bianca Martins dos Santos, Silvio Jamil Ferzoli Guimaraes, Jancarlo Ferreira Gomes, Ewa Kijak, Alexandre Xavier Falcao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art (SOTA) object detection methods have succeeded in several
applications at the price of relying on heavyweight neural networks, which
makes them inefficient and inviable for many applications with computational
resource constraints. This work presents a method to build a Convolutional
Neural Network (CNN) layer by layer for object detection from user-drawn
markers on discriminative regions of representative images. We address the
detection of Schistosomiasis mansoni eggs in microscopy images of fecal
samples, and the detection of ships in satellite images as application
examples. We could create a flyweight CNN without backpropagation from very few
input images. Our method explores a recent methodology, Feature Learning from
Image Markers (FLIM), to build convolutional feature extractors (encoders) from
marker pixels. We extend FLIM to include a single-layer adaptive decoder, whose
weights vary with the input image -- a concept never explored in CNNs. Our CNN
weighs thousands of times less than SOTA object detectors, being suitable for
CPU execution and showing superior or equivalent performance to three methods
in five measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CANet: Channel Extending and Axial Attention Catching Network for
  Multi-structure Kidney Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.05241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.05241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Bu, Kai-Ni Wang, Guang-Quan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Renal cancer is one of the most prevalent cancers worldwide. Clinical signs
of kidney cancer include hematuria and low back discomfort, which are quite
distressing to the patient. Some surgery-based renal cancer treatments like
laparoscopic partial nephrectomy relys on the 3D kidney parsing on computed
tomography angiography (CTA) images. Many automatic segmentation techniques
have been put forward to make multi-structure segmentation of the kidneys more
accurate. The 3D visual model of kidney anatomy will help clinicians plan
operations accurately before surgery. However, due to the diversity of the
internal structure of the kidney and the low grey level of the edge. It is
still challenging to separate the different parts of the kidney in a clear and
accurate way. In this paper, we propose a channel extending and axial attention
catching Network(CANet) for multi-structure kidney segmentation. Our solution
is founded based on the thriving nn-UNet architecture. Firstly, by extending
the channel size, we propose a larger network, which can provide a broader
perspective, facilitating the extraction of complex structural information.
Secondly, we include an axial attention catching(AAC) module in the decoder,
which can obtain detailed information for refining the edges. We evaluate our
CANet on the KiPA2022 dataset, achieving the dice scores of 95.8%, 89.1%, 87.5%
and 84.9% for kidney, tumor, artery and vein, respectively, which helps us get
fourth place in the challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KiPA2022 Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSDA: Generative Adversarial Network-based Semi-Supervised Data
  Augmentation for Ultrasound Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06184v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06184v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoshan Liu, Qiujie Lv, Chau Hung Lee, Lei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical Ultrasound (US) is one of the most widely used imaging modalities in
clinical practice, but its usage presents unique challenges such as variable
imaging quality. Deep Learning (DL) models can serve as advanced medical US
image analysis tools, but their performance is greatly limited by the scarcity
of large datasets. To solve the common data shortage, we develop GSDA, a
Generative Adversarial Network (GAN)-based semi-supervised data augmentation
method. GSDA consists of the GAN and Convolutional Neural Network (CNN). The
GAN synthesizes and pseudo-labels high-resolution, high-quality US images, and
both real and synthesized images are then leveraged to train the CNN. To
address the training challenges of both GAN and CNN with limited data, we
employ transfer learning techniques during their training. We also introduce a
novel evaluation standard that balances classification accuracy with
computational time. We evaluate our method on the BUSI dataset and GSDA
outperforms existing state-of-the-art methods. With the high-resolution and
high-quality images synthesized, GSDA achieves a 97.9% accuracy using merely
780 images. Given these promising results, we believe that GSDA holds potential
as an auxiliary tool for medical US analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Heliyon Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SC-DepthV3: Robust Self-supervised Monocular Depth Estimation for
  Dynamic Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.03660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.03660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libo Sun, Jia-Wang Bian, Huangying Zhan, Wei Yin, Ian Reid, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised monocular depth estimation has shown impressive results in
static scenes. It relies on the multi-view consistency assumption for training
networks, however, that is violated in dynamic object regions and occlusions.
Consequently, existing methods show poor accuracy in dynamic scenes, and the
estimated depth map is blurred at object boundaries because they are usually
occluded in other training views. In this paper, we propose SC-DepthV3 for
addressing the challenges. Specifically, we introduce an external pretrained
monocular depth estimation model for generating single-image depth prior,
namely pseudo-depth, based on which we propose novel losses to boost
self-supervised training. As a result, our model can predict sharp and accurate
depth maps, even when training from monocular videos of highly-dynamic scenes.
We demonstrate the significantly superior performance of our method over
previous methods on six challenging datasets, and we provide detailed ablation
studies for the proposed terms. Source code and data will be released at
https://github.com/JiawangBian/sc_depth_pl
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in TPAMI; The code will be available at
  https://github.com/JiawangBian/sc_depth_pl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BiViT: Extremely Compressed Binary Vision Transformer <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yefei He, Zhenyu Lou, Luoming Zhang, Jing Liu, Weijia Wu, Hong Zhou, Bohan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model binarization can significantly compress model size, reduce energy
consumption, and accelerate inference through efficient bit-wise operations.
Although binarizing convolutional neural networks have been extensively
studied, there is little work on exploring binarization of vision Transformers
which underpin most recent breakthroughs in visual recognition. To this end, we
propose to solve two fundamental challenges to push the horizon of Binary
Vision Transformers (BiViT). First, the traditional binary method does not take
the long-tailed distribution of softmax attention into consideration, bringing
large binarization errors in the attention module. To solve this, we propose
Softmax-aware Binarization, which dynamically adapts to the data distribution
and reduces the error caused by binarization. Second, to better preserve the
information of the pretrained model and restore accuracy, we propose a
Cross-layer Binarization scheme that decouples the binarization of
self-attention and multi-layer perceptrons (MLPs), and Parameterized Weight
Scales which introduce learnable scaling factors for weight binarization.
Overall, our method performs favorably against state-of-the-arts by 19.8% on
the TinyImageNet dataset. On ImageNet, our BiViT achieves a competitive 75.6%
Top-1 accuracy over Swin-S model. Additionally, on COCO object detection, our
method achieves an mAP of 40.8 with a Swin-T backbone over Cascade Mask R-CNN
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMURF: Spatial Multi-Representation Fusion for 3D Object Detection with
  4D Imaging Radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10784v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10784v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianan Liu, Qiuchi Zhao, Weiyi Xiong, Tao Huang, Qing-Long Han, Bing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 4D Millimeter wave (mmWave) radar is a promising technology for vehicle
sensing due to its cost-effectiveness and operability in adverse weather
conditions. However, the adoption of this technology has been hindered by
sparsity and noise issues in radar point cloud data. This paper introduces
spatial multi-representation fusion (SMURF), a novel approach to 3D object
detection using a single 4D imaging radar. SMURF leverages multiple
representations of radar detection points, including pillarization and density
features of a multi-dimensional Gaussian mixture distribution through kernel
density estimation (KDE). KDE effectively mitigates measurement inaccuracy
caused by limited angular resolution and multi-path propagation of radar
signals. Additionally, KDE helps alleviate point cloud sparsity by capturing
density features. Experimental evaluations on View-of-Delft (VoD) and
TJ4DRadSet datasets demonstrate the effectiveness and generalization ability of
SMURF, outperforming recently proposed 4D imaging radar-based
single-representation models. Moreover, while using 4D imaging radar only,
SMURF still achieves comparable performance to the state-of-the-art 4D imaging
radar and camera fusion-based method, with an increase of 1.22% in the mean
average precision on bird's-eye view of TJ4DRadSet dataset and 1.32% in the 3D
mean average precision on the entire annotated area of VoD dataset. Our
proposed method demonstrates impressive inference time and addresses the
challenges of real-time detection, with the inference time no more than 0.05
seconds for most scans on both datasets. This research highlights the benefits
of 4D mmWave radar and is a strong benchmark for subsequent works regarding 3D
object detection with 4D imaging radar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Intelligent Vehicles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MagicDrive: Street View Generation with Diverse 3D Geometry Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, Qiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models have significantly enhanced the data
synthesis with 2D control. Yet, precise 3D control in street view generation,
crucial for 3D perception tasks, remains elusive. Specifically, utilizing
Bird's-Eye View (BEV) as the primary condition often leads to challenges in
geometry control (e.g., height), affecting the representation of object shapes,
occlusion patterns, and road surface elevations, all of which are essential to
perception data synthesis, especially for 3D object detection tasks. In this
paper, we introduce MagicDrive, a novel street view generation framework
offering diverse 3D geometry controls, including camera poses, road maps, and
3D bounding boxes, together with textual descriptions, achieved through
tailored encoding strategies. Besides, our design incorporates a cross-view
attention module, ensuring consistency across multiple camera views. With
MagicDrive, we achieve high-fidelity street-view synthesis that captures
nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV
segmentation and 3D object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://flymin.github.io/magicdrive</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Representations on the Unit Sphere: Investigating Angular
  Gaussian and von Mises-Fisher Distributions for Online Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Michel, Giovanni Chierchia, Romain Negrel, Jean-François Bercher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use the maximum a posteriori estimation principle for learning
representations distributed on the unit sphere. We propose to use the angular
Gaussian distribution, which corresponds to a Gaussian projected on the
unit-sphere and derive the associated loss function. We also consider the von
Mises-Fisher distribution, which is the conditional of a Gaussian in the
unit-sphere. The learned representations are pushed toward fixed directions,
which are the prior means of the Gaussians; allowing for a learning strategy
that is resilient to data drift. This makes it suitable for online continual
learning, which is the problem of training neural networks on a continuous data
stream, where multiple classification tasks are presented sequentially so that
data from past tasks are no longer accessible, and data from the current task
can be seen only once. To address this challenging scenario, we propose a
memory-based representation learning technique equipped with our new loss
functions. Our approach does not require negative data or knowledge of task
boundaries and performs well with smaller batch sizes while being
computationally efficient. We demonstrate with extensive experiments that the
proposed method outperforms the current state-of-the-art methods on both
standard evaluation scenarios and realistic scenarios with blurry task
boundaries. For reproducibility, we use the same training pipeline for every
compared method and share the code at https://t.ly/SQTj.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, under review, update title</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for
  Real-time Soccer Commentary Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Qi, Jifan Yu, Teng Tu, Kunyu Gao, Yifan Xu, Xinyu Guan, Xiaozhi Wang, Yuxiao Dong, Bin Xu, Lei Hou, Juanzi Li, Jie Tang, Weidong Guo, Hui Liu, Yu Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent emergence of video captioning models, how to generate
vivid, fine-grained video descriptions based on the background knowledge (i.e.,
long and informative commentary about the domain-specific scenes with
appropriate reasoning) is still far from being solved, which however has great
applications such as automatic sports narrative. In this paper, we present
GOAL, a benchmark of over 8.9k soccer video clips, 22k sentences, and 42k
knowledge triples for proposing a challenging new task setting as
Knowledge-grounded Video Captioning (KGVC). Moreover, we conduct experimental
adaption of existing methods to show the difficulty and potential directions
for solving this valuable and applicable task. Our data and code are available
at https://github.com/THU-KEG/goal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CIKM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust 3D Object Detection In Rainy Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aldi Piroli, Vinzenz Dallabetta, Johannes Kopp, Marc Walessa, Daniel Meissner, Klaus Dietmayer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR sensors are used in autonomous driving applications to accurately
perceive the environment. However, they are affected by adverse weather
conditions such as snow, fog, and rain. These everyday phenomena introduce
unwanted noise into the measurements, severely degrading the performance of
LiDAR-based perception systems. In this work, we propose a framework for
improving the robustness of LiDAR-based 3D object detectors against road spray.
Our approach uses a state-of-the-art adverse weather detection network to
filter out spray from the LiDAR point cloud, which is then used as input for
the object detector. In this way, the detected objects are less affected by the
adverse weather in the scene, resulting in a more accurate perception of the
environment. In addition to adverse weather filtering, we explore the use of
radar targets to further filter false positive detections. Tests on real-world
data show that our approach improves the robustness to road spray of several
popular 3D object detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IEEE International Conference on Intelligent
  Transportation Systems ITSC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COOLer: Class-Incremental Learning for Appearance-Based Multiple Object
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizheng Liu, Mattia Segu, Fisher Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning allows a model to learn multiple tasks sequentially while
retaining the old knowledge without the training data of the preceding tasks.
This paper extends the scope of continual learning research to
class-incremental learning for multiple object tracking (MOT), which is
desirable to accommodate the continuously evolving needs of autonomous systems.
Previous solutions for continual learning of object detectors do not address
the data association stage of appearance-based trackers, leading to
catastrophic forgetting of previous classes' re-identification features. We
introduce COOLer, a COntrastive- and cOntinual-Learning-based tracker, which
incrementally learns to track new categories while preserving past knowledge by
training on a combination of currently available ground truth labels and
pseudo-labels generated by the past tracker. To further exacerbate the
disentanglement of instance representations, we introduce a novel contrastive
class-incremental instance representation learning technique. Finally, we
propose a practical evaluation protocol for continual learning for MOT and
conduct experiments on the BDD100K and SHIFT datasets. Experimental results
demonstrate that COOLer continually learns while effectively addressing
catastrophic forgetting of both tracking and detection. The code is available
at https://github.com/BoSmallEar/COOLer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GCPR 2023 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15357v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15357v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Ma, Huan Yang, Wenhan Yang, Jianlong Fu, Jiaying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models, as a kind of powerful generative model, have given
impressive results on image super-resolution (SR) tasks. However, due to the
randomness introduced in the reverse process of diffusion models, the
performances of diffusion-based SR models are fluctuating at every time of
sampling, especially for samplers with few resampled steps. This inherent
randomness of diffusion models results in ineffectiveness and instability,
making it challenging for users to guarantee the quality of SR results.
However, our work takes this randomness as an opportunity: fully analyzing and
leveraging it leads to the construction of an effective plug-and-play sampling
method that owns the potential to benefit a series of diffusion-based SR
methods. More in detail, we propose to steadily sample high-quality SR images
from pre-trained diffusion-based SR models by solving diffusion ordinary
differential equations (diffusion ODEs) with optimal boundary conditions (BCs)
and analyze the characteristics between the choices of BCs and their
corresponding SR results. Our analysis shows the route to obtain an
approximately optimal BC via an efficient exploration in the whole space. The
quality of SR results sampled by the proposed method with fewer steps
outperforms the quality of results sampled by current methods with randomness
from the same pre-trained diffusion-based SR model, which means that our
sampling method "boosts" current diffusion-based SR models without any
additional training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unpaired Image-to-Image Translation via Neural Schrödinger Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beomsu Kim, Gihyun Kwon, Kwanyoung Kim, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are a powerful class of generative models which simulate
stochastic differential equations (SDEs) to generate data from noise. Although
diffusion models have achieved remarkable progress in recent years, they have
limitations in the unpaired image-to-image translation tasks due to the
Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to
translate between two arbitrary distributions, have risen as an attractive
solution to this problem. However, none of SB models so far have been
successful at unpaired translation between high-resolution images. In this
work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which
expresses SB problem as a sequence of adversarial learning problems. This
allows us to incorporate advanced discriminators and regularization to learn a
SB between unpaired data. We demonstrate that UNSB is scalable and successfully
solves various unpaired image-to-image translation tasks. Code:
\url{https://github.com/cyclomon/UNSB}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private GANs, Revisited 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Bie, Gautam Kamath, Guojun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that the canonical approach for training differentially private GANs
-- updating the discriminator with differentially private stochastic gradient
descent (DPSGD) -- can yield significantly improved results after modifications
to training. Specifically, we propose that existing instantiations of this
approach neglect to consider how adding noise only to discriminator updates
inhibits discriminator training, disrupting the balance between the generator
and discriminator necessary for successful GAN training. We show that a simple
fix -- taking more discriminator steps between generator steps -- restores
parity between the generator and discriminator and improves results.
  Additionally, with the goal of restoring parity, we experiment with other
modifications -- namely, large batch sizes and adaptive discriminator update
frequency -- to improve discriminator training and see further improvements in
generation quality. Our results demonstrate that on standard image synthesis
benchmarks, DPSGD outperforms all alternative GAN privatization schemes. Code:
https://github.com/alexbie98/dpgan-revisit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages; revisions and new experiments from TMLR camera-ready + code
  release at https://github.com/alexbie98/dpgan-revisit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIE: Simulating Disease Progression via Progressive Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizhao Liang, Xu Cao, Kuei-Da Liao, Tianren Gao, Wenqian Ye, Zhengyu Chen, Jianguo Cao, Tejas Nama, Jimeng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disease progression simulation is a crucial area of research that has
significant implications for clinical diagnosis, prognosis, and treatment. One
major challenge in this field is the lack of continuous medical imaging
monitoring of individual patients over time. To address this issue, we develop
a novel framework termed Progressive Image Editing (PIE) that enables
controlled manipulation of disease-related image features, facilitating precise
and realistic disease progression simulation. Specifically, we leverage recent
advancements in text-to-image generative models to simulate disease progression
accurately and personalize it for each patient. We theoretically analyze the
iterative refining process in our framework as a gradient descent with an
exponentially decayed learning rate. To validate our framework, we conduct
experiments in three medical imaging domains. Our results demonstrate the
superiority of PIE over existing methods such as Stable Diffusion Walk and
Style-Based Manifold Extrapolation based on CLIP score (Realism) and Disease
Classification Confidence (Alignment). Our user study collected feedback from
35 veteran physicians to assess the generated progressions. Remarkably, 76.2%
of the feedback agrees with the fidelity of the generated progressions. To our
best knowledge, PIE is the first of its kind to generate disease progression
images meeting real-world standards. It is a promising tool for medical
research and clinical practice, potentially allowing healthcare providers to
model disease trajectories over time, predict future treatment responses, and
improve patient outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and checkpoints for replicating our results can be found at
  https://github.com/IrohXu/PIE and
  https://huggingface.co/IrohXu/stable-diffusion-mimic-cxr-v0.1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Role of Language Priors in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan Zhang, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) are impactful in part because they can be
applied to a variety of visual understanding tasks in a zero-shot fashion,
without any fine-tuning. We study $\textit{generative VLMs}$ that are trained
for next-word generation given an image. We explore their zero-shot performance
on the illustrative task of image-text retrieval across 8 popular
vision-language benchmarks. Our first observation is that they can be
repurposed for discriminative tasks (such as image-text retrieval) by simply
computing the match score of generating a particular text string given an
image. We call this probabilistic score the $\textit{Visual Generative
Pre-Training Score}$ (VisualGPTScore). While the VisualGPTScore produces
near-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on
others. We analyze this behavior through a probabilistic lens, pointing out
that some benchmarks inadvertently capture unnatural language distributions by
creating adversarial but unlikely text captions. In fact, we demonstrate that
even a "blind" language model that ignores any image evidence can sometimes
outperform all prior art, reminiscent of similar challenges faced by the
visual-question answering (VQA) community many years ago. We derive a
probabilistic post-processing scheme that controls for the amount of linguistic
bias in generative VLMs at test time without having to retrain or fine-tune the
model. We show that the VisualGPTScore, when appropriately debiased, is a
strong zero-shot baseline for vision-language understanding, oftentimes
producing state-of-the-art accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://linzhiqiu.github.io/papers/visual_gpt_score/ Code:
  https://github.com/linzhiqiu/visual_gpt_score/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14883v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14883v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen Huang, Yuliang Liu, Boxiang Wang, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Transformer models has pushed the deep learning model scale to
billions of parameters. Due to the limited memory resource of a single GPU,
However, the best practice for choosing the optimal parallel strategy is still
lacking, since it requires domain expertise in both deep learning and parallel
computing.
  The Colossal-AI system addressed the above challenge by introducing a unified
interface to scale your sequential code of model training to distributed
environments. It supports parallel training methods such as data, pipeline,
tensor, and sequence parallelism, as well as heterogeneous training methods
integrated with zero redundancy optimizer. Compared to the baseline system,
Colossal-AI can achieve up to 2.76 times training speedup on large-scale
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tinghao Xie, Xiangyu Qi, Ping He, Yiming Li, Jiachen T. Wang, Prateek Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel defense, against backdoor attacks on Deep Neural Networks
(DNNs), wherein adversaries covertly implant malicious behaviors (backdoors)
into DNNs. Our defense falls within the category of post-development defenses
that operate independently of how the model was generated. The proposed defense
is built upon a novel reverse engineering approach that can directly extract
backdoor functionality of a given backdoored model to a backdoor expert model.
The approach is straightforward -- finetuning the backdoored model over a small
set of intentionally mislabeled clean samples, such that it unlearns the normal
functionality while still preserving the backdoor functionality, and thus
resulting in a model (dubbed a backdoor expert model) that can only recognize
backdoor inputs. Based on the extracted backdoor expert model, we show the
feasibility of devising highly accurate backdoor input detectors that filter
out the backdoor inputs during model inference. Further augmented by an
ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert
(Backdoor Input Detection with Backdoor Expert), effectively mitigates 17 SOTA
backdoor attacks while minimally impacting clean utility. The effectiveness of
BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet)
across various model architectures (ResNet, VGG, MobileNetV2 and Vision
Transformer).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompt-MIL: Boosting Multi-Instance Learning Schemes via Task-specific
  Prompt Tuning <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Zhang, Saarthak Kapse, Ke Ma, Prateek Prasanna, Joel Saltz, Maria Vakalopoulou, Dimitris Samaras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide image (WSI) classification is a critical task in computational
pathology, requiring the processing of gigapixel-sized images, which is
challenging for current deep-learning methods. Current state of the art methods
are based on multi-instance learning schemes (MIL), which usually rely on
pretrained features to represent the instances. Due to the lack of
task-specific annotated data, these features are either obtained from
well-established backbones on natural images, or, more recently from
self-supervised models pretrained on histopathology. However, both approaches
yield task-agnostic features, resulting in performance loss compared to the
appropriate task-related supervision, if available. In this paper, we show that
when task-specific annotations are limited, we can inject such supervision into
downstream task training, to reduce the gap between fully task-tuned and task
agnostic features. We propose Prompt-MIL, an MIL framework that integrates
prompts into WSI classification. Prompt-MIL adopts a prompt tuning mechanism,
where only a small fraction of parameters calibrates the pretrained features to
encode task-specific information, rather than the conventional full fine-tuning
approaches. Extensive experiments on three WSI datasets, TCGA-BRCA, TCGA-CRC,
and BRIGHT, demonstrate the superiority of Prompt-MIL over conventional MIL
methods, achieving a relative improvement of 1.49%-4.03% in accuracy and
0.25%-8.97% in AUROC while using fewer than 0.3% additional parameters.
Compared to conventional full fine-tuning approaches, we fine-tune less than
1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in
accuracy and 3.22%-27.18% in AUROC and reduce GPU memory consumption by 38%-45%
while training 21%-27% faster. Our code is available at
https://github.com/cvlab-stonybrook/PromptMIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MICCAI 2023 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PostRainBench: A comprehensive benchmark and a new model for
  precipitation forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Tang, Jiaming Zhou, Xiang Pan, Zeying Gong, Junwei Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate precipitation forecasting is a vital challenge of both scientific
and societal importance. Data-driven approaches have emerged as a widely used
solution for addressing this challenge. However, solely relying on data-driven
approaches has limitations in modeling the underlying physics, making accurate
predictions difficult. Coupling AI-based post-processing techniques with
traditional Numerical Weather Prediction (NWP) methods offers a more effective
solution for improving forecasting accuracy. Despite previous post-processing
efforts, accurately predicting heavy rainfall remains challenging due to the
imbalanced precipitation data across locations and complex relationships
between multiple meteorological variables. To address these limitations, we
introduce the PostRainBench, a comprehensive multi-variable NWP post-processing
benchmark consisting of three datasets for NWP post-processing-based
precipitation forecasting. We propose CAMT, a simple yet effective Channel
Attention Enhanced Multi-task Learning framework with a specially designed
weighted loss function. Its flexible design allows for easy plug-and-play
integration with various backbones. Extensive experimental results on the
proposed benchmark show that our method outperforms state-of-the-art methods by
6.3%, 4.7%, and 26.8% in rain CSI on the three datasets respectively. Most
notably, our model is the first deep learning-based method to outperform
traditional Numerical Weather Prediction (NWP) approaches in extreme
precipitation conditions. It shows improvements of 15.6%, 17.4%, and 31.8% over
NWP predictions in heavy rain CSI on respective datasets. These results
highlight the potential impact of our model in reducing the severe consequences
of extreme weather events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SFUSNet: A Spatial-Frequency domain-based Multi-branch Network for
  diagnosis of Cervical Lymph Node Lesions in Ultrasound Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubiao Yue, Jun Xue, Haihua Liang, Bingchun Luo, Zhenzhang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Booming deep learning has substantially improved the diagnosis for diverse
lesions in ultrasound images, but a conspicuous research gap concerning
cervical lymph node lesions still remains. The objective of this work is to
diagnose cervical lymph node lesions in ultrasound images by leveraging a deep
learning model. To this end, we first collected 3392 cervical ultrasound images
containing normal lymph nodes, benign lymph node lesions, malignant primary
lymph node lesions, and malignant metastatic lymph node lesions. Given that
ultrasound images are generated by the reflection and scattering of sound waves
across varied bodily tissues, we proposed the Conv-FFT Block. It integrates
convolutional operations with the fast Fourier transform to more astutely model
the images. Building upon this foundation, we designed a novel architecture,
named SFUSNet. SFUSNet not only discerns variances in ultrasound images from
the spatial domain but also adeptly captures micro-structural alterations
across various lesions in the frequency domain. To ascertain the potential of
SFUSNet, we benchmarked it against 12 popular architectures through five-fold
cross-validation. The results show that SFUSNet is the state-of-the-art model
and can achieve 92.89% accuracy. Moreover, its average precision, average
sensitivity and average specificity for four types of lesions achieve 90.46%,
89.95% and 97.49%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reconstructing Existing Levels through Level Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09472v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09472v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johor Jara Gonzalez, Matthew Guzdial
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural Content Generation (PCG) and Procedural Content Generation via
Machine Learning (PCGML) have been used in prior work for generating levels in
various games. This paper introduces Content Augmentation and focuses on the
subproblem of level inpainting, which involves reconstructing and extending
video game levels. Drawing inspiration from image inpainting, we adapt two
techniques from this domain to address our specific use case. We present two
approaches for level inpainting: an Autoencoder and a U-net. Through a
comprehensive case study, we demonstrate their superior performance compared to
a baseline method and discuss their relative merits. Furthermore, we provide a
practical demonstration of both approaches for the level inpainting task and
offer insights into potential directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, Artificial Intelligence and Interactive Digital
  Entertainment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-supervised Deep Unrolled Reconstruction Using Regularization by
  Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.03519v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.03519v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhou Huang, Chaoyi Zhang, Xiaoliang Zhang, Xiaojuan Li, Liang Dong, Leslie Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning methods have been successfully used in various computer vision
tasks. Inspired by that success, deep learning has been explored in magnetic
resonance imaging (MRI) reconstruction. In particular, integrating deep
learning and model-based optimization methods has shown considerable
advantages. However, a large amount of labeled training data is typically
needed for high reconstruction quality, which is challenging for some MRI
applications. In this paper, we propose a novel reconstruction method, named
DURED-Net, that enables interpretable self-supervised learning for MR image
reconstruction by combining a self-supervised denoising network and a
plug-and-play method. We aim to boost the reconstruction performance of
Noise2Noise in MR reconstruction by adding an explicit prior that utilizes
imaging physics. Specifically, the leverage of a denoising network for MRI
reconstruction is achieved using Regularization by Denoising (RED). Experiment
results demonstrate that the proposed method requires a reduced amount of
training data to achieve high reconstruction quality among the state-of-art of
MR reconstruction utilizing the Noise2Noise method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Prototypical Part Networks with Reward Reweighing,
  Reselection, and Retraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Netzorg, Jiaxun Li, Bin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, work has gone into developing deep interpretable methods for
image classification that clearly attributes a model's output to specific
features of the data. One such of these methods is the Prototypical Part
Network (ProtoPNet), which attempts to classify images based on meaningful
parts of the input. While this method results in interpretable classifications,
it often learns to classify from spurious or inconsistent parts of the image.
Hoping to remedy this, we take inspiration from the recent developments in
Reinforcement Learning with Human Feedback (RLHF) to fine-tune these
prototypes. By collecting human annotations of prototypes quality via a 1-5
scale on the CUB-200-2011 dataset, we construct a reward model that learns
human preferences and identify non-spurious prototypes. In place of a full RL
update, we propose the Reweighed, Reselected, and Retrained Prototypical Part
Network (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet
training loop. The first two steps are reward-based reweighting and
reselection, which align prototypes with human feedback. The final step is
retraining to realign the model's features with the updated prototypes. We find
that R3-ProtoPNet improves the overall meaningfulness of the prototypes, and
maintains or improves individual model performance. When multiple trained
R3-ProtoPNets are incorporated into an ensemble, we find increases in both
interpretability and predictive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Search-Space Generation Neural Architecture Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18030v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18030v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Chen, Luming Liang, Tianyu Ding, Ilya Zharkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To search an optimal sub-network within a general deep neural network (DNN),
existing neural architecture search (NAS) methods typically rely on
handcrafting a search space beforehand. Such requirements make it challenging
to extend them onto general scenarios without significant human expertise and
manual intervention. To overcome the limitations, we propose Automated
Search-Space Generation Neural Architecture Search (ASGNAS), perhaps the first
automated system to train general DNNs that cover all candidate connections and
operations and produce high-performing sub-networks in the one shot manner.
Technologically, ASGNAS delivers three noticeable contributions to minimize
human efforts: (i) automated search space generation for general DNNs; (ii) a
Hierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy
and dependency within generated search space to ensure the network validity
during optimization, and reliably produces a solution with both high
performance and hierarchical group sparsity; and (iii) automated sub-network
construction upon the H2SPG solution. Numerically, we demonstrate the
effectiveness of ASGNAS on a variety of general DNNs, including RegNet,
StackedUnets, SuperResNet, and DARTS, over benchmark datasets such as CIFAR10,
Fashion-MNIST, ImageNet, STL-10 , and SVNH. The sub-networks computed by ASGNAS
achieve competitive even superior performance compared to the starting full
DNNs and other state-of-the-arts. The library will be released at
https://github.com/tianyic/only_train_once.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Graph visualization for DARTS, SuperResNet are omitted for arXiv
  version due to exceeding page dimension limit. Please refer to the
  open-review version for taking the visualizations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ USB-<span class="highlight-title">NeRF</span>: Unrolling Shutter Bundle Adjusted Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moyang Li, Peng Wang, Lingzhe Zhao, Bangyan Liao, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) has received much attention recently due to its
impressive capability to represent 3D scene and synthesize novel view images.
Existing works usually assume that the input images are captured by a global
shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied
to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter
effect would also affect the accuracy of the camera pose estimation (e.g. via
COLMAP), which further prevents the success of NeRF algorithm with RS images.
In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance
Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and
recover accurate camera motion trajectory simultaneously under the framework of
NeRF, by modeling the physical image formation process of a RS camera.
Experimental results demonstrate that USB-NeRF achieves better performance
compared to prior works, in terms of RS effect removal, novel view image
synthesis as well as camera motion estimation. Furthermore, our algorithm can
also be used to recover high-fidelity high frame-rate global shutter video from
a sequence of RS images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in
  Prosthetic Hand Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.03893v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.03893v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrshad Zandigohar, Mo Han, Mohammadreza Sharif, Sezen Yagmur Gunay, Mariusz P. Furmanek, Mathew Yarossi, Paolo Bonato, Cagdas Onal, Taskin Padir, Deniz Erdogmus, Gunar Schirner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: For lower arm amputees, robotic prosthetic hands promise to regain
the capability to perform daily living activities. Current control methods
based on physiological signals such as electromyography (EMG) are prone to
yielding poor inference outcomes due to motion artifacts, muscle fatigue, and
many more. Vision sensors are a major source of information about the
environment state and can play a vital role in inferring feasible and intended
gestures. However, visual evidence is also susceptible to its own artifacts,
most often due to object occlusion, lighting changes, etc. Multimodal evidence
fusion using physiological and vision sensor measurements is a natural approach
due to the complementary strengths of these modalities. Methods: In this paper,
we present a Bayesian evidence fusion framework for grasp intent inference
using eye-view video, eye-gaze, and EMG from the forearm processed by neural
network models. We analyze individual and fused performance as a function of
time as the hand approaches the object to grasp it. For this purpose, we have
also developed novel data processing and augmentation techniques to train
neural network components. Results: Our results indicate that, on average,
fusion improves the instantaneous upcoming grasp type classification accuracy
while in the reaching phase by 13.66% and 14.8%, relative to EMG and visual
evidence individually, resulting in an overall fusion accuracy of 95.3%.
Conclusion: Our experimental data analyses demonstrate that EMG and visual
evidence show complementary strengths, and as a consequence, fusion of
multimodal evidence can outperform each individual evidence modality at any
given time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to Frontiers for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Convolutional Kernels for Steerable CNNs <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Zhdanov, Nico Hoffmann, Gabriele Cesa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Steerable convolutional neural networks (CNNs) provide a general framework
for building neural networks equivariant to translations and other
transformations belonging to an origin-preserving group $G$, such as
reflections and rotations. They rely on standard convolutions with
$G$-steerable kernels obtained by analytically solving the group-specific
equivariance constraint imposed onto the kernel space. As the solution is
tailored to a particular group $G$, the implementation of a kernel basis does
not generalize to other symmetry transformations, which complicates the
development of general group equivariant models. We propose using implicit
neural representation via multi-layer perceptrons (MLPs) to parameterize
$G$-steerable kernels. The resulting framework offers a simple and flexible way
to implement Steerable CNNs and generalizes to any group $G$ for which a
$G$-equivariant MLP can be built. We prove the effectiveness of our method on
multiple tasks, including N-body simulations, point cloud classification and
molecular property prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiniGPT-5: Interleaved Vision-and-Language Generation via Generative
  Vokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizhi Zheng, Xuehai He, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have garnered significant attention for their
advancements in natural language processing, demonstrating unparalleled prowess
in text comprehension and generation. Yet, the simultaneous generation of
images with coherent textual narratives remains an evolving frontier. In
response, we introduce an innovative interleaved vision-and-language generation
technique anchored by the concept of "generative vokens," acting as the bridge
for harmonized image-text outputs. Our approach is characterized by a
distinctive two-staged training strategy focusing on description-free
multimodal generation, where the training requires no comprehensive
descriptions of images. To bolster model integrity, classifier-free guidance is
incorporated, enhancing the effectiveness of vokens on image generation. Our
model, MiniGPT-5, exhibits substantial improvement over the baseline Divter
model on the MMDialog dataset and consistently delivers superior or comparable
multimodal outputs in human evaluations on the VIST dataset, highlighting its
efficacy across diverse benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harvard Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye
  Diseases Screening and Fair Identity Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Luo, Yu Tian, Min Shi, Tobias Elze, Mengyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness or equity in machine learning is profoundly important for societal
well-being, but limited public datasets hinder its progress, especially in the
area of medicine. It is undeniable that fairness in medicine is one of the most
important areas for fairness learning's applications. Currently, no large-scale
public medical datasets with 3D imaging data for fairness learning are
available, while 3D imaging data in modern clinics are standard tests for
disease diagnosis. In addition, existing medical fairness datasets are actually
repurposed datasets, and therefore they typically have limited demographic
identity attributes with at most three identity attributes of age, gender, and
race for fairness modeling. To address this gap, we introduce our Eye Fairness
dataset with 30,000 subjects (Harvard-EF) covering three major eye diseases
including age-related macular degeneration, diabetic retinopathy, and glaucoma
affecting 380 million patients globally. Our Harvard-EF dataset includes both
2D fundus photos and 3D optical coherence tomography scans with six demographic
identity attributes including age, gender, race, ethnicity, preferred language,
and marital status. We also propose a fair identity scaling (FIS) approach
combining group and individual scaling together to improve model fairness. Our
FIS approach is compared with various state-of-the-art fairness learning
methods with superior performance in the racial, gender, and ethnicity fairness
tasks with 2D and 3D imaging data, which demonstrate the utilities of our
Harvard-EF dataset for fairness learning. To facilitate fairness comparisons
between different models, we propose performance-scaled disparity measures,
which can be used to compare model fairness accounting for overall performance
levels. The dataset and code are publicly accessible via
https://ophai.hms.harvard.edu/datasets/harvard-ef30k.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-10-04T00:00:00Z">2023-10-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">43</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LanguageMPC: Large Language Models as Decision Makers for Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, Mingyu Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing learning-based autonomous driving (AD) systems face challenges in
comprehending high-level information, generalizing to rare events, and
providing interpretability. To address these problems, this work employs Large
Language Models (LLMs) as a decision-making component for complex AD scenarios
that require human commonsense understanding. We devise cognitive pathways to
enable comprehensive reasoning with LLMs, and develop algorithms for
translating LLM decisions into actionable driving commands. Through this
approach, LLM decisions are seamlessly integrated with low-level controllers by
guided parameter matrix adaptation. Extensive experiments demonstrate that our
proposed method not only consistently surpasses baseline approaches in
single-vehicle tasks, but also helps handle complex driving behaviors even
multi-vehicle coordination, thanks to the commonsense reasoning capabilities of
LLMs. This paper presents an initial step toward leveraging LLMs as effective
decision-makers for intricate AD scenarios in terms of safety, efficiency,
generalizability, and interoperability. We aspire for it to serve as
inspiration for future research in this field. Project page:
https://sites.google.com/view/llm-mpc
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-oriented Representation Learning for Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxiao Huo, Mingyu Ding, Chenfeng Xu, Thomas Tian, Xinghao Zhu, Yao Mu, Lingfeng Sun, Masayoshi Tomizuka, Wei Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans inherently possess generalizable visual representations that empower
them to efficiently explore and interact with the environments in manipulation
tasks. We advocate that such a representation automatically arises from
simultaneously learning about multiple simple perceptual skills that are
critical for everyday scenarios (e.g., hand detection, state estimate, etc.)
and is better suited for learning robot manipulation policies compared to
current state-of-the-art visual representations purely based on self-supervised
objectives. We formalize this idea through the lens of human-oriented
multi-task fine-tuning on top of pre-trained visual encoders, where each task
is a perceptual skill tied to human-environment interactions. We introduce Task
Fusion Decoder as a plug-and-play embedding translator that utilizes the
underlying relationships among these perceptual skills to guide the
representation learning towards encoding meaningful structure for what's
important for all perceptual skills, ultimately empowering learning of
downstream robotic manipulation tasks. Extensive experiments across a range of
robotic tasks and embodiments, in both simulations and real-world environments,
show that our Task Fusion Decoder consistently improves the representation of
three state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for
downstream manipulation policy-learning. Project page:
https://sites.google.com/view/human-oriented-robot-learning
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Landmark Color for AUV Docking in Visually <span class="highlight-title">Dynamic Environment</span>s <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Corey Knutson, Zhipeng Cao, Junaed Sattar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Underwater Vehicles (AUVs) conduct missions underwater without the
need for human intervention. A docking station (DS) can extend mission times of
an AUV by providing a location for the AUV to recharge its batteries and
receive updated mission information. Various methods for locating and tracking
a DS exist, but most rely on expensive acoustic sensors, or are vision-based,
which is significantly affected by water quality. In this \doctype, we present
a vision-based method that utilizes adaptive color LED markers and dynamic
color filtering to maximize landmark visibility in varying water conditions.
Both AUV and DS utilize cameras to determine the water background color in
order to calculate the desired marker color. No communication between AUV and
DS is needed to determine marker color. Experiments conducted in a pool and
lake show our method performs 10 times better than static color thresholding
methods as background color varies. DS detection is possible at a range of 5
meters in clear water with minimal false positives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA 2024 for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole-body MPC for highly redundant legged manipulators: experimental
  evaluation with a 37 DoF dual-arm quadruped 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Dadiotis, Arturo Laurenzi, Nikos Tsagarakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in legged locomotion has rendered quadruped manipulators a
promising solution for performing tasks that require both mobility and
manipulation (\emph{loco-manipulation}). In the real world, task specifications
and/or environment constraints may require the quadruped manipulator to be
equipped with \emph{high redundancy} as well as \emph{whole-body} motion
coordination capabilities. This work presents an experimental evaluation of a
whole-body Model Predictive Control (MPC) framework achieving real-time
performance on a dual-arm quadruped platform consisting of 37 actuated joints.
To the best of our knowledge this is the legged manipulator with the highest
number of joints to be controlled with real-time whole-body MPC so far. The
computational efficiency of the MPC while considering the full robot kinematics
and the centroidal dynamics model builds upon an open-source DDP-variant solver
and a state-of-the-art optimal control problem formulation. Differently from
previous works on quadruped manipulators, the MPC is directly interfaced with
the low-level joint impedance controllers without the need of designing an
instantaneous whole-body controller. The feasibility on the real hardware is
showcased using the CENTAURO platform for the challenging task of picking a
heavy object from the ground. Dynamic stepping (trotting) is also showcased for
first time with this robot. The results highlight the potential of replanning
with whole-body information in a predictive control loop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2023 IEEE-RAS International Conference on Humanoid
  Robots (Humanoids 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximating Robot Configuration Spaces with few Convex Sets using
  Clique Covers of Visibility Graphs <span class="chip">ICRA
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Werner, Alexandre Amice, Tobia Marcucci, Daniela Rus, Russ Tedrake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many computations in robotics can be dramatically accelerated if the robot
configuration space is described as a collection of simple sets. For example,
recently developed motion planners rely on a convex decomposition of the free
space to design collision-free trajectories using fast convex optimization. In
this work, we present an efficient method for approximately covering complex
configuration spaces with a small number of polytopes. The approach constructs
a visibility graph using sampling and generates a clique cover of this graph to
find clusters of samples that have mutual line of sight. These clusters are
then inflated into large, full-dimensional, polytopes. We evaluate our method
on a variety of robotic systems and show that it consistently covers larger
portions of free configuration space, with fewer polytopes, and in a fraction
of the time compared to previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, under review for possible publication at ICRA
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Target Vehicle Trajectories Predicted by Deep Learning
  Into Model Predictive Controlled Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ni Dang, Zengjie Zhang, Jizheng Liu, Marion Leibold, Martin Buss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Predictive Control (MPC) has been widely applied to the motion planning
of autonomous vehicles. An MPC-controlled vehicle is required to predict its
own trajectories in a finite prediction horizon according to its model. Beyond
this, the vehicle should also incorporate the prediction of the trajectory of
its nearby vehicles, or target vehicles (TVs) into its decision-making. The
conventional trajectory prediction methods, such as the constant-speed-based
ones, are too trivial to accurately capture the potential collision risks. In
this report, we propose a novel MPC-based motion planning method for an
autonomous vehicle with a set of risk-aware constraints. These constraints
incorporate the predicted trajectory of a TV learned using a
deep-learning-based method. A recurrent neural network (RNN) is used to predict
the TV's future trajectory based on its historical data. Then, the predicted TV
trajectory is incorporated into the optimization of the MPC of the ego vehicle
to generate collision-free motion. Simulation studies are conducted to showcase
the prediction accuracy of the RNN model and the collision-free trajectories
generated by the MPC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBEV: Elevating Roadside 3D Object Detection with Depth and Height
  Complementarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Shi, Chengshan Pang, Jiaming Zhang, Kailun Yang, Yuhao Wu, Huajian Ni, Yining Lin, Rainer Stiefelhagen, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Roadside camera-driven 3D object detection is a crucial task in intelligent
transportation systems, which extends the perception range beyond the
limitations of vision-centric vehicles and enhances road safety. While previous
studies have limitations in using only depth or height information, we find
both depth and height matter and they are in fact complementary. The depth
feature encompasses precise geometric cues, whereas the height feature is
primarily focused on distinguishing between various categories of height
intervals, essentially providing semantic context. This insight motivates the
development of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D
object detection framework that integrates depth and height to construct robust
BEV representations. In essence, CoBEV estimates each pixel's depth and height
distribution and lifts the camera features into 3D space for lateral fusion
using the newly proposed two-stage complementary feature selection (CFS)
module. A BEV feature distillation framework is also seamlessly integrated to
further enhance the detection accuracy from the prior knowledge of the
fusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D
detection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as
the private Supremind-Road dataset, demonstrating that CoBEV not only achieves
the accuracy of the new state-of-the-art, but also significantly advances the
robustness of previous methods in challenging long-distance scenarios and noisy
camera disturbance, and enhances generalization by a large margin in
heterologous settings with drastic changes in scene and camera parameters. For
the first time, the vehicle AP score of a camera model reaches 80% on
DAIR-V2X-I in terms of easy mode. The source code will be made publicly
available at https://github.com/MasterHow/CoBEV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code will be made publicly available at
  https://github.com/MasterHow/CoBEV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ R-LGP: A Reachability-guided Logic-geometric Programming Framework for
  Optimal Task and Motion Planning on Mobile Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kim Tien Ly, Valeriy Semenov, Mattia Risiglione, Wolfgang Merkt, Ioannis Havoutis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an optimization-based solution to task and motion
planning (TAMP) on mobile manipulators. Logic-geometric programming (LGP) has
shown promising capabilities for optimally dealing with hybrid TAMP problems
that involve abstract and geometric constraints. However, LGP does not scale
well to high-dimensional systems (e.g. mobile manipulators) and can suffer from
obstacle avoidance issues. In this work, we extend LGP with a sampling-based
reachability graph to enable solving optimal TAMP on high-DoF mobile
manipulators. The proposed reachability graph can incorporate environmental
information (obstacles) to provide the planner with sufficient geometric
constraints. This reachability-aware heuristic efficiently prunes infeasible
sequences of actions in the continuous domain, hence, it reduces replanning by
securing feasibility at the final full trajectory optimization. Our framework
proves to be time-efficient in computing optimal and collision-free solutions,
while outperforming the current state of the art on metrics of success rate,
planning time, path length and number of steps. We validate our framework on
the physical Toyota HSR robot and report comparisons on a series of mobile
manipulation tasks of increasing difficulty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Collaborative Transportation for Under-Capacitated Vehicle
  Routing Problems using Aerial Drone Swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Kopparam Sreedhara, Deepesh Padala, Shashank Mahesh, Kai Cui, Mengguang Li, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Swarms of aerial drones have recently been considered for last-mile
deliveries in urban logistics or automated construction. At the same time,
collaborative transportation of payloads by multiple drones is another
important area of recent research. However, efficient coordination algorithms
for collaborative transportation of many payloads by many drones remain to be
considered. In this work, we formulate the collaborative transportation of
payloads by a swarm of drones as a novel, under-capacitated generalization of
vehicle routing problems (VRP), which may also be of separate interest. In
contrast to standard VRP and capacitated VRP, we must additionally consider
waiting times for payloads lifted cooperatively by multiple drones, and the
corresponding coordination. Algorithmically, we provide a solution encoding
that avoids deadlocks and formulate an appropriate alternating minimization
scheme to solve the problem. On the hardware side, we integrate our algorithms
with collision avoidance and drone controllers. The approach and the impact of
the system integration are successfully verified empirically, both on a swarm
of real nano-quadcopters and for large swarms in simulation. Overall, we
provide a framework for collaborative transportation with aerial drone swarms,
that uses only as many drones as necessary for the transportation of any single
payload.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curve Trajectory Model for Human Preferred Path Planning of Automated
  Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gergo Igneczi, Erno Horvath, Roland Toth, Krisztian Nyilas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated driving systems are often used for lane keeping tasks. By these
systems, a local path is planned ahead of the vehicle. However, these paths are
often found unnatural by human drivers. We propose a linear driver model, which
can calculate node points that reflect the preferences of human drivers and
based on these node points a human driver preferred motion path can be designed
for autonomous driving. The model input is the road curvature. We apply this
model to a self-developed Euler-curve-based curve fitting algorithm. Through a
case study, we show that the model based planned path can reproduce the average
behavior of human curve path selection. We analyze the performance of the
proposed model through statistical analysis that shows the validity of the
captured relations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Gimbal: A 3 Degrees of Freedom Open Source Sensing and Testing
  Platform for Nano and Micro UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suryansh Sharma, Tristan Dijkstra, R. Venkatesha Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Testing the aerodynamics of micro- and nano-UAVs without actually flying is
highly challenging. To address this issue, we introduce Open Gimbal, a
specially designed 3 Degrees of Freedom platform that caters to the unique
requirements of micro- and nano-UAVs. This platform allows for unrestricted and
free rotational motion, enabling comprehensive experimentation and evaluation
of these UAVs. Our approach focuses on simplicity and accessibility. We
developed an open-source, 3D printable electro-mechanical design that has
minimal size and low complexity. This design facilitates easy replication and
customization, making it widely accessible to researchers and developers.
Addressing the challenges of sensing flight dynamics at a small scale, we have
devised an integrated wireless batteryless sensor subsystem. Our innovative
solution eliminates the need for complex wiring and instead uses wireless power
transfer for sensor data reception. To validate the effectiveness of open
gimbal, we thoroughly evaluate and test its communication link and sensing
performance using a typical nano-quadrotor. Through comprehensive testing, we
verify the reliability and accuracy of open gimbal in real-world scenarios.
These advancements provide valuable tools and insights for researchers and
developers working with mUAVs and nUAVs, contributing to the progress of this
rapidly evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Link to open source repository:
  https://doi.org/10.5281/zenodo.8052218</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Visual Localization for Multi-Agent Collaboration: A Data-Driven
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Hanlon, Boyang Sun, Marc Pollefeys, Hermann Blum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rather than having each newly deployed robot create its own map of its
surroundings, the growing availability of SLAM-enabled devices provides the
option of simply localizing in a map of another robot or device. In cases such
as multi-robot or human-robot collaboration, localizing all agents in the same
map is even necessary. However, localizing e.g. a ground robot in the map of a
drone or head-mounted MR headset presents unique challenges due to viewpoint
changes. This work investigates how active visual localization can be used to
overcome such challenges of viewpoint changes. Specifically, we focus on the
problem of selecting the optimal viewpoint at a given location. We compare
existing approaches in the literature with additional proposed baselines and
propose a novel data-driven approach. The result demonstrates the superior
performance of the data-driven approach when compared to existing methods, both
in controlled simulation experiments and real-world deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Term Dynamic Window Approach for Kinodynamic Local Planning in
  Static and Crowd Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Jian, Songyi Zhang, Lingfeng Sun, Wei Zhan, Nanning Zheng, Masayoshi Tomizuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local planning for a differential wheeled robot is designed to generate
kinodynamic feasible actions that guide the robot to a goal position along the
navigation path while avoiding obstacles. Reactive, predictive, and
learning-based methods are widely used in local planning. However, few of them
can fit static and crowd environments while satisfying kinodynamic constraints
simultaneously. To solve this problem, we propose a novel local planning
method. The method applies a long-term dynamic window approach to generate an
initial trajectory and then optimizes it with graph optimization. The method
can plan actions under the robot's kinodynamic constraints in real time while
allowing the generated actions to be safer and more jitterless. Experimental
results show that the proposed method adapts well to crowd and static
environments and outperforms most SOTA approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foundation Reinforcement Learning: towards Embodied Generalist Agents
  with Foundation Prior Assistance <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weirui Ye, Yunsheng Zhang, Mengchen Wang, Shengjie Wang, Xianfan Gu, Pieter Abbeel, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, people have shown that large-scale pre-training from internet-scale
data is the key to building generalist models, as witnessed in NLP. To build
embodied generalist agents, we and many other researchers hypothesize that such
foundation prior is also an indispensable component. However, it is unclear
what is the proper concrete form to represent those embodied foundation priors
and how they should be used in the downstream task. In this paper, we propose
an intuitive and effective set of embodied priors that consist of foundation
policy, value, and success reward. The proposed priors are based on the
goal-conditioned MDP. To verify their effectiveness, we instantiate an
actor-critic method assisted by the priors, called Foundation Actor-Critic
(FAC). We name our framework as Foundation Reinforcement Learning (FRL), since
it completely relies on embodied foundation priors to explore, learn and
reinforce. The benefits of FRL are threefold. (1) Sample efficient. With
foundation priors, FAC learns significantly faster than traditional RL. Our
evaluation on the Meta-World has proved that FAC can achieve 100% success rates
for 7/8 tasks under less than 200k frames, which outperforms the baseline
method with careful manual-designed rewards under 1M frames. (2) Robust to
noisy priors. Our method tolerates the unavoidable noise in embodied foundation
models. We show that FAC works well even under heavy noise or quantization
errors. (3) Minimal human intervention: FAC completely learns from the
foundation priors, without the need of human-specified dense reward, or
providing teleoperated demos. Thus, FAC can be easily scaled up. We believe our
FRL framework could enable the future robot to autonomously explore and learn
without human intervention in the physical world. In summary, our proposed FRL
is a novel and powerful learning paradigm, towards achieving embodied
generalist agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Spatio-Temporal Voxels Based Trajectory Planning for Autonomous
  Driving in Highway Traffic Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Jian, Songyi Zhang, Lingfeng Sun, Wei Zhan, Masayoshi Tomizuka, Nanning Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory planning is crucial for the safe driving of autonomous vehicles in
highway traffic flow. Currently, some advanced trajectory planning methods
utilize spatio-temporal voxels to construct feasible regions and then convert
trajectory planning into optimization problem solving based on the feasible
regions. However, these feasible region construction methods cannot adapt to
the changes in dynamic environments, making them difficult to apply in complex
traffic flow. In this paper, we propose a trajectory planning method based on
adaptive spatio-temporal voxels which improves the construction of feasible
regions and trajectory optimization while maintaining the quadratic programming
form. The method can adjust feasible regions and trajectory planning according
to real-time traffic flow and environmental changes, realizing vehicles to
drive safely in complex traffic flow. The proposed method has been tested in
both open-loop and closed-loop environments, and the test results show that our
method outperforms the current planning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning-Enabled Precision Position Control and Thermal
  Regulation in Advanced Thermal Actuators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Mo Mirvakili, Ehsan Haghighat, Douglas Sim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With their unique combination of characteristics - an energy density almost
100 times that of human muscle, and a power density of 5.3 kW/kg, similar to a
jet engine's output - Nylon artificial muscles stand out as particularly apt
for robotics applications. However, the necessity of integrating sensors and
controllers poses a limitation to their practical usage. Here we report a
constant power open-loop controller based on machine learning. We show that we
can control the position of a nylon artificial muscle without external sensors.
To this end, we construct a mapping from a desired displacement trajectory to a
required power using an ensemble encoder-style feed-forward neural network. The
neural controller is carefully trained on a physics-based denoised dataset and
can be fine-tuned to accommodate various types of thermal artificial muscles,
irrespective of the presence or absence of hysteresis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Collision Detection for Robots with Variable Stiffness Actuation
  by Using MAD-CNN: Modularized-Attention-Dilated Convolutional Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenwei Niu, Lyes Saad Saoud, Irfan Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring safety is paramount in the field of collaborative robotics to
mitigate the risks of human injury and environmental damage. Apart from
collision avoidance, it is crucial for robots to rapidly detect and respond to
unexpected collisions. While several learning-based collision detection methods
have been introduced as alternatives to purely model-based detection
techniques, there is currently a lack of such methods designed for
collaborative robots equipped with variable stiffness actuators. Moreover,
there is potential for further enhancing the network's robustness and improving
the efficiency of data training. In this paper, we propose a new network, the
Modularized Attention-Dilated Convolutional Neural Network (MAD-CNN), for
collision detection in robots equipped with variable stiffness actuators. Our
model incorporates a dual inductive bias mechanism and an attention module to
enhance data efficiency and improve robustness. In particular, MAD-CNN is
trained using only a four-minute collision dataset focusing on the highest
level of joint stiffness. Despite limited training data, MAD-CNN robustly
detects all collisions with minimal detection delay across various stiffness
conditions. Moreover, it exhibits a higher level of collision sensitivity,
which is beneficial for effectively handling false positives, which is a common
issue in learning-based methods. Experimental results demonstrate that the
proposed MAD-CNN model outperforms existing state-of-the-art models in terms of
collision sensitivity and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Drumming Robot Via Attention Transformer Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yi, Zonghan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic technology has been widely used in nowadays society, which has made
great progress in various fields such as agriculture, manufacturing and
entertainment. In this paper, we focus on the topic of drumming robots in
entertainment. To this end, we introduce an improving drumming robot that can
automatically complete music transcription based on the popular vision
transformer network based on the attention mechanism. Equipped with the
attention transformer network, our method can efficiently handle the sequential
audio embedding input and model their global long-range dependencies. Massive
experimental results demonstrate that the improving algorithm can help the
drumming robot promote drum classification performance, which can also help the
robot to enjoy a variety of smart applications and services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tightly Joining Positioning and Control for Trustworthy Unmanned Aerial
  Vehicles Based on Factor Graph Optimization in Urban Transportation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiwen Yang, Weisong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicles (UAV) showed great potential in improving the
efficiency of parcel delivery applications in the coming smart cities era.
Unfortunately, the trustworthy positioning and control algorithms of the UAV
are significantly challenged in complex urban areas. For example, the
ubiquitous global navigation satellite system (GNSS) positioning can be
degraded by the signal reflections from surrounding high-rising buildings,
leading to significantly increased positioning uncertainty. An additional
challenge is introduced to the control algorithm due to the complex wind
disturbances in urban canyons. Given the fact that the system positioning and
control are highly correlated with each other, for example, the system dynamics
of the control can largely help with the positioning, this paper proposed a
joint positioning and control method (JPCM) based on factor graph optimization
(FGO), which combines sensors' measurements and control intention. In
particular, the positioning measurements are formulated as the factors in the
factor graph model, such as the positioning from the GNSS. The model predictive
control (MPC) is also formulated as the additional factors in the factor graph
model. By solving the factor graph contributed by both the positioning factor
and the MPC-based factors, the complementariness of positioning and control can
be fully explored. To guarantee reliable system dynamic parameters, we validate
the effectiveness of the proposed method using a simulated quadrotor system
which showed significantly improved trajectory following performance. To
benefit the research community, we open-source our code and make it available
at https://github.com/RoboticsPolyu/IPN_MPC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proactive Human-Robot Interaction using Visuo-Lingual Transformers <span class="chip">IROS'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranay Mathur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans possess the innate ability to extract latent visuo-lingual cues to
infer context through human interaction. During collaboration, this enables
proactive prediction of the underlying intention of a series of tasks. In
contrast, robotic agents collaborating with humans naively follow elementary
instructions to complete tasks or use specific hand-crafted triggers to
initiate proactive collaboration when working towards the completion of a goal.
Endowing such robots with the ability to reason about the end goal and
proactively suggest intermediate tasks will engender a much more intuitive
method for human-robot collaboration. To this end, we propose a learning-based
method that uses visual cues from the scene, lingual commands from a user and
knowledge of prior object-object interaction to identify and proactively
predict the underlying goal the user intends to achieve. Specifically, we
propose ViLing-MMT, a vision-language multimodal transformer-based architecture
that captures inter and intra-modal dependencies to provide accurate scene
descriptions and proactively suggest tasks where applicable. We evaluate our
proposed model in simulation and real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS'23 Workshop: Geriatronics: AI and Robotics for
  Health & Well-Being in Older Age and Workshop: Assistive Robotics for
  Citizens</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sim-to-Real Learning for Humanoid Box Loco-Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy Dao, Helei Duan, Alan Fern
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose a learning-based approach to box loco-manipulation
for a humanoid robot. This is a particularly challenging problem due to the
need for whole-body coordination in order to lift boxes of varying weight,
position, and orientation while maintaining balance. To address this challenge,
we present a sim-to-real reinforcement learning approach for training general
box pickup and carrying skills for the bipedal robot Digit. Our reward
functions are designed to produce the desired interactions with the box while
also valuing balance and gait quality. We combine the learned skills into a
full system for box loco-manipulation to achieve the task of moving boxes from
one table to another with a variety of sizes, weights, and initial
configurations. In addition to quantitative simulation results, we demonstrate
successful sim-to-real transfer on the humanoid r
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Domain Walking with Reduced-Order Models of Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Dai, Jaemin Lee, Aaron D. Ames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drawing inspiration from human multi-domain walking, this work presents a
novel reduced-order model based framework for realizing multi-domain robotic
walking. At the core of our approach is the viewpoint that human walking can be
represented by a hybrid dynamical system, with continuous phases that are
fully-actuated, under-actuated, and over-actuated and discrete changes in
actuation type occurring with changes in contact. Leveraging this perspective,
we synthesize a multi-domain linear inverted pendulum (MLIP) model of
locomotion. Utilizing the step-to-step dynamics of the MLIP model, we
successfully demonstrate multi-domain walking behaviors on the bipedal robot
Cassie -- a high degree of freedom 3D bipedal robot. Thus, we show the ability
to bridge the gap between multi-domain reduced order models and full-order
multi-contact locomotion. Additionally, our results showcase the ability of the
proposed method to achieve versatile speed-tracking performance and robust push
recovery behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to ACC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization and Evaluation of Multi Robot Surface Inspection Through
  Particle Swarm Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darren Chiu, Radhika Nagpal, Bahar Haghighat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot swarms can be tasked with a variety of automated sensing and inspection
applications in aerial, aquatic, and surface environments. In this paper, we
study a simplified two-outcome surface inspection task. We task a group of
robots to inspect and collectively classify a 2D surface section based on a
binary pattern projected on the surface. We use a decentralized Bayesian
decision-making algorithm and deploy a swarm of miniature 3-cm sized wheeled
robots to inspect randomized black and white tiles of $1m\times 1m$. We first
describe the model parameters that characterize our simulated environment, the
robot swarm, and the inspection algorithm. We then employ a noise-resistant
heuristic optimization scheme based on the Particle Swarm Optimization (PSO)
using a fitness evaluation that combines decision accuracy and decision time.
We use our fitness measure definition to asses the optimized parameters through
100 randomized simulations that vary surface pattern and initial robot poses.
The optimized algorithm parameters show up to a 55% improvement in median of
fitness evaluations against an empirically chosen parameter set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech-Based Human-Exoskeleton Interaction for Lower Limb Motion
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eddie Guo, Christopher Perlette, Mojtaba Sharifi, Lukas Grasse, Matthew Tata, Vivian K. Mushahwar, Mahdi Tavakoli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a speech-based motion planning strategy (SBMP) developed
for lower limb exoskeletons to facilitate safe and compliant human-robot
interaction. A speech processing system, finite state machine, and central
pattern generator are the building blocks of the proposed strategy for online
planning of the exoskeleton's trajectory. According to experimental
evaluations, this speech-processing system achieved low levels of word and
intent errors. Regarding locomotion, the completion time for users with voice
commands was 54% faster than that using a mobile app interface. With the
proposed SBMP, users are able to maintain their postural stability with both
hands-free. This supports its use as an effective motion planning method for
the assistance and rehabilitation of individuals with lower-limb impairments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application-Oriented Co-Design of Motors and Motions for a 6DOF Robot
  Manipulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Stein, Yebin Wang, Yusuke Sakamoto, Bingnan Wang, Huazhen Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates an application-driven co-design problem where the
motion and motors of a six degrees of freedom robotic manipulator are optimized
simultaneously, and the application is characterized by a set of tasks. Unlike
the state-of-the-art which selects motors from a product catalogue and performs
co-design for a single task, this work designs the motor geometry as well as
motion for a specific application. Contributions are made towards solving the
proposed co-design problem in a computationally-efficient manner. First, a
two-step process is proposed, where multiple motor designs are identified by
optimizing motions and motors for multiple tasks one by one, and then are
reconciled to determine the final motor design. Second, magnetic equivalent
circuit modeling is exploited to establish the analytic mapping from motor
design parameters to dynamic models and objective functions to facilitate the
subsequent differentiable simulation. Third, a direct-collocation-based
differentiable simulator of motor and robotic arm dynamics is developed to
balance the computational complexity and numerical stability. Simulation
verifies that higher performance for a specific application can be achieved
with the multi-task method, compared to several benchmark co-design methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole-body MPC for highly redundant legged manipulators: experimental
  evaluation with a 37 DoF dual-arm quadruped 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Dadiotis, Arturo Laurenzi, Nikos Tsagarakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in legged locomotion has rendered quadruped manipulators a
promising solution for performing tasks that require both mobility and
manipulation (loco-manipulation). In the real world, task specifications and/or
environment constraints may require the quadruped manipulator to be equipped
with high redundancy as well as whole-body motion coordination capabilities.
This work presents an experimental evaluation of a whole-body Model Predictive
Control (MPC) framework achieving real-time performance on a dual-arm quadruped
platform consisting of 37 actuated joints. To the best of our knowledge this is
the legged manipulator with the highest number of joints to be controlled with
real-time whole-body MPC so far. The computational efficiency of the MPC while
considering the full robot kinematics and the centroidal dynamics model builds
upon an open-source DDP-variant solver and a state-of-the-art optimal control
problem formulation. Differently from previous works on quadruped manipulators,
the MPC is directly interfaced with the low-level joint impedance controllers
without the need of designing an instantaneous whole-body controller. The
feasibility on the real hardware is showcased using the CENTAURO platform for
the challenging task of picking a heavy object from the ground. Dynamic
stepping (trotting) is also showcased for first time with this robot. The
results highlight the potential of replanning with whole-body information in a
predictive control loop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2023 IEEE-RAS International Conference on Humanoid
  Robots (Humanoids 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Adaptive Safety for Multi-Agent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Berducci, Shuo Yang, Rahul Mangharam, Radu Grosu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring safety in dynamic multi-agent systems is challenging due to limited
information about the other agents. Control Barrier Functions (CBFs) are
showing promise for safety assurance but current methods make strong
assumptions about other agents and often rely on manual tuning to balance
safety, feasibility, and performance. In this work, we delve into the problem
of adaptive safe learning for multi-agent systems with CBF. We show how
emergent behavior can be profoundly influenced by the CBF configuration,
highlighting the necessity for a responsive and dynamic approach to CBF design.
We present ASRL, a novel adaptive safe RL framework, to fully automate the
optimization of policy and CBF coefficients, to enhance safety and long-term
performance through reinforcement learning. By directly interacting with the
other agents, ASRL learns to cope with diverse agent behaviours and maintains
the cost violations below a desired limit. We evaluate ASRL in a multi-robot
system and a competitive multi-agent racing scenario, against learning-based
and control-theoretic approaches. We empirically demonstrate the efficacy and
flexibility of ASRL, and assess generalization and scalability to
out-of-distribution scenarios. Code and supplementary material are public
online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update with appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image-based Navigation in Real-World Environments via Multiple Mid-level
  Representations: Fusion Models, Benchmark and Efficient Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.01069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.01069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Rosano, Antonino Furnari, Luigi Gulino, Corrado Santoro, Giovanni Maria Farinella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating complex indoor environments requires a deep understanding of the
space the robotic agent is acting into to correctly inform the navigation
process of the agent towards the goal location. In recent learning-based
navigation approaches, the scene understanding and navigation abilities of the
agent are achieved simultaneously by collecting the required experience in
simulation. Unfortunately, even if simulators represent an efficient tool to
train navigation policies, the resulting models often fail when transferred
into the real world. One possible solution is to provide the navigation model
with mid-level visual representations containing important domain-invariant
properties of the scene. But, what are the best representations that facilitate
the transfer of a model to the real-world? How can they be combined? In this
work we address these issues by proposing a benchmark of Deep Learning
architectures to combine a range of mid-level visual representations, to
perform a PointGoal navigation task following a Reinforcement Learning setup.
All the proposed navigation models have been trained with the Habitat simulator
on a synthetic office environment and have been tested on the same real-world
environment using a real robotic platform. To efficiently assess their
performance in a real context, a validation tool has been proposed to generate
realistic navigation episodes inside the simulator. Our experiments showed that
navigation models can benefit from the multi-modal input and that our
validation tool can provide good estimation of the expected navigation
performance in the real world, while saving time and resources. The acquired
synthetic and real 3D models of the environment, together with the code of our
validation tool built on top of Habitat, are publicly available at the
following link: https://iplab.dmi.unict.it/EmbodiedVN/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for submission in Autonomous Robots</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safety Index Synthesis via Sum-of-Squares Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.09134v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.09134v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiye Zhao, Tairan He, Tianhao Wei, Simin Liu, Changliu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control systems often need to satisfy strict safety requirements. Safety
index provides a handy way to evaluate the safety level of the system and
derive the resulting safe control policies. However, designing safety index
functions under control limits is difficult and requires a great amount of
expert knowledge. This paper proposes a framework for synthesizing the safety
index for general control systems using sum-of-squares programming. Our
approach is to show that ensuring the non-emptiness of safe control on the safe
set boundary is equivalent to a local manifold positiveness problem. We then
prove that this problem is equivalent to sum-of-squares programming via the
Positivstellensatz of algebraic geometry. We validate the proposed method on
robot arms with different degrees of freedom and ground vehicles. The results
show that the synthesized safety index guarantees safety and our method is
effective even in high-dimensional robot systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and
  Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongyang Zhu, Junqiao Zhao, Kai Huang, Xuebo Tian, Jiaye Lin, Chen Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous localization and mapping (SLAM) is critical to the
implementation of autonomous driving. Most LiDAR-inertial SLAM algorithms
assume a static environment, leading to unreliable localization in dynamic
environments. Moreover, the accurate tracking of moving objects is of great
significance for the control and planning of autonomous vehicles. This study
proposes LIMOT, a tightly-coupled multi-object tracking and LiDAR-inertial
odometry system that is capable of accurately estimating the poses of both
ego-vehicle and objects. We propose a trajectory-based dynamic feature
filtering method, which filters out features belonging to moving objects by
leveraging tracking results before scan-matching. Factor graph-based
optimization is then conducted to optimize the bias of the IMU and the poses of
both the ego-vehicle and surrounding objects in a sliding window. Experiments
conducted on the KITTI tracking dataset and self-collected dataset show that
our method achieves better pose and tracking accuracy than our previous work
DL-SLOT and other baseline methods. Our open-source implementation is available
at https://github.com/tiev-tongji/LIMOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures. This updated version mainly refines the
  experiments. This work has been submitted to the IEEE for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparison of Motion Encoding Frameworks on Human Manipulation Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Jahn, Florentin Wörgötter, Tomas Kulvicius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Movement generation, and especially generalisation to unseen situations,
plays an important role in robotics. Different types of movement generation
methods exist such as spline based methods, dynamical system based methods, and
methods based on Gaussian mixture models (GMMs). Using a large, new dataset on
human manipulations, in this paper we provide a highly detailed comparison of
three most widely used movement encoding and generation frameworks: dynamic
movement primitives (DMPs), time based Gaussian mixture regression (tbGMR) and
stable estimator of dynamical systems (SEDS). We compare these frameworks with
respect to their movement encoding efficiency, reconstruction accuracy, and
movement generalisation capabilities. The new dataset consists of nine object
manipulation actions performed by 12 humans: pick and place, put on top/take
down, put inside/take out, hide/uncover, and push/pull with a total of 7,652
movement examples. Our analysis shows that for movement encoding and
reconstruction DMPs are the most efficient framework with respect to the number
of parameters and reconstruction accuracy if a sufficient number of kernels is
used. In case of movement generalisation to new start- and end-point
situations, DMPs and task parameterized GMM (TP-GMM, movement generalisation
framework based on tbGMR) lead to similar performance and outperform SEDS.
Furthermore we observe that TP-GMM and SEDS suffer from inaccurate convergence
to the end-point as compared to DMPs. These different quantitative results will
help designing trajectory representations in an improved task-dependent way in
future robotic applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAMMA: Generalizable Articulation Modeling and Manipulation for
  Articulated Objects <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulated objects like cabinets and doors are widespread in daily life.
However, directly manipulating 3D articulated objects is challenging because
they have diverse geometrical shapes, semantic categories, and kinetic
constraints. Prior works mostly focused on recognizing and manipulating
articulated objects with specific joint types. They can either estimate the
joint parameters or distinguish suitable grasp poses to facilitate trajectory
planning. Although these approaches have succeeded in certain types of
articulated objects, they lack generalizability to unseen objects, which
significantly impedes their application in broader scenarios. In this paper, we
propose a novel framework of Generalizable Articulation Modeling and
Manipulating for Articulated Objects (GAMMA), which learns both articulation
modeling and grasp pose affordance from diverse articulated objects with
different categories. In addition, GAMMA adopts adaptive manipulation to
iteratively reduce the modeling errors and enhance manipulation performance. We
train GAMMA with the PartNet-Mobility dataset and evaluate with comprehensive
experiments in SAPIEN simulation and real-world Franka robot. Results show that
GAMMA significantly outperforms SOTA articulation modeling and manipulation
algorithms in unseen and cross-category articulated objects. We will
open-source all codes and datasets in both simulation and real robots for
reproduction in the final version. Images and videos are published on the
project website at: http://sites.google.com/view/gamma-articulation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, submitted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-based DRL Autonomous Driving Agent with Sim2Real Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dianzhao Li, Ostap Okhrin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve fully autonomous driving, vehicles must be capable of continuously
performing various driving tasks, including lane keeping and car following,
both of which are fundamental and well-studied driving ones. However, previous
studies have mainly focused on individual tasks, and car following tasks have
typically relied on complete leader-follower information to attain optimal
performance. To address this limitation, we propose a vision-based deep
reinforcement learning (DRL) agent that can simultaneously perform lane keeping
and car following maneuvers. To evaluate the performance of our DRL agent, we
compare it with a baseline controller and use various performance metrics for
quantitative analysis. Furthermore, we conduct a real-world evaluation to
demonstrate the Sim2Real transfer capability of the trained DRL agent. To the
best of our knowledge, our vision-based car following and lane keeping agent
with Sim2Real transfer capability is the first of its kind.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven Predictive Tracking Control based on Koopman Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12000v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12000v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Wang, Yujia Yang, Ye Pu, Chris Manzie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constraint handling during tracking operations is at the core of many
real-world control implementations and is well understood when dynamic models
of the underlying system exist, yet becomes more challenging when data-driven
models are used to describe the nonlinear system at hand. We seek to combine
the nonlinear modeling capabilities of a wide class of neural networks with the
constraint-handling guarantees of model predictive control (MPC) in a rigorous
and online computationally tractable framework. The class of networks
considered can be captured using Koopman operators, and are integrated into a
Koopman-based tracking MPC (KTMPC) for nonlinear systems to track piecewise
constant references. The effect of model mismatch between original nonlinear
dynamics and its trained Koopman linear model is handled by using a constraint
tightening approach in the proposed tracking MPC strategy. By choosing two
Lyapunov functions, we prove that solution is recursively feasible and
input-to-state stable to a neighborhood of both online and offline optimal
reachable steady outputs in the presence of bounded modeling errors under mild
assumptions. Finally, we demonstrate the results on a numerical example, before
applying the proposed approach to the problem of reference tracking by an
autonomous ground vehicle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regression and Classification Methods for Learning Sound Wave Amplitude
  Modulation in Soft Tactile Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnu Rajendran S, Willow Mandil, Simon Parsons, Amir Ghalamzan E
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel soft tactile skin (STS) technology operating with
sound waves. In this innovative approach, the sound waves generated by a
speaker travel in channels embedded in a soft membrane and get modulated due to
a deformation of the channel when pressed by an external force and received by
a microphone at the end of the channel. The sensor leverages regression and
classification methods for estimating the normal force and its contact
location. Our sensor can be affixed to any robot part, e.g., end effectors or
arm. We tested several regression and classifier methods to learn the relation
between sound wave modulation, the applied force, and its location,
respectively and picked the best-performing models for force and location
predictions. Our novel tactile sensor yields 93% of the force estimation within
1.5 N tolerances for a range of 0-30+1 N and estimates contact locations with
over 96% accuracy. We also demonstrated the performance of STS technology for a
real-time gripping force control application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ElasticROS: An Elastically Collaborative Robot Operation System for Fog
  and Cloud Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.01774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.01774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots are integrating more huge-size models to enrich functions and improve
accuracy, which leads to out-of-control computing pressure. And thus robots are
encountering bottlenecks in computing power and battery capacity. Fog or cloud
robotics is one of the most anticipated theories to address these issues.
Approaches of cloud robotics have developed from system-level to node-level.
However, the present node-level systems are not flexible enough to dynamically
adapt to changing conditions. To address this, we present ElasticROS, which
evolves the present node-level systems into an algorithm-level one. ElasticROS
is based on ROS and ROS2. For fog and cloud robotics, it is the first robot
operating system with algorithm-level collaborative computing. ElasticROS
develops elastic collaborative computing to achieve adaptability to dynamic
conditions. The collaborative computing algorithm is the core and challenge of
ElasticROS. We abstract the problem and then propose an algorithm named
ElasAction to address. It is a dynamic action decision algorithm based on
online learning, which determines how robots and servers cooperate. The
algorithm dynamically updates parameters to adapt to changes of conditions
where the robot is currently in. It achieves elastically distributing of
computing tasks to robots and servers according to configurations. In addition,
we prove that the regret upper bound of the ElasAction is sublinear, which
guarantees its convergence and thus enables ElasticROS to be stable in its
elasticity. Finally, we conducted experiments with ElasticROS on common tasks
of robotics, including SLAM, grasping and human-robot dialogue, and then
measured its performances in latency, CPU usage and power consumption. The
algorithm-level ElasticROS performs significantly better than the present
node-level system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Route Design in Sheepdog System--Traveling Salesman Problem Formulation
  and Evolutionary Computation Solution-- 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wataru Imahayashi, Yusuke Tsunoda, Masaki Ogura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we consider the guidance control problem of the sheepdog
system, which involves the guidance of the flock using the characteristics of
the sheepdog and sheep. Sheepdog systems require a strategy to guide sheep
agents to a target value using a small number of sheepdog agents, and various
methods have been proposed. Previous studies have proposed a guidance control
law to guide a herd of sheep reliably, but the movement distance of a sheepdog
required for guidance has not been considered. Therefore, in this study, we
propose a novel guidance algorithm in which a supposedly efficient route for
guiding a flock of sheep is designed via Traveling Salesman Problem and
evolutionary computation. Numerical simulations were performed to confirm
whether sheep flocks could be guided and controlled using the obtained guidance
routes. We specifically revealed that the proposed method reduces both the
guidance failure rate and the guidance distance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OceanGPT: A Large Language Model for Ocean Science Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ocean science, which delves into the oceans that are reservoirs of life and
biodiversity, is of great significance given that oceans cover over 70% of our
planet's surface. Recently, advances in Large Language Models (LLMs) have
transformed the paradigm in science. Despite the success in other domains,
current LLMs often fall short in catering to the needs of domain experts like
oceanographers, and the potential of LLMs for ocean science is under-explored.
The intrinsic reason may be the immense and intricate nature of ocean data as
well as the necessity for higher granularity and richness in knowledge. To
alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean
domain, which is expert in various ocean science tasks. We propose DoInstruct,
a novel framework to automatically obtain a large volume of ocean domain
instruction data, which generates instructions based on multi-agent
collaboration. Additionally, we construct the first oceanography benchmark,
OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though
comprehensive experiments, OceanGPT not only shows a higher level of knowledge
expertise for oceans science tasks but also gains preliminary embodied
intelligence capabilities in ocean technology. Codes, data and checkpoints will
soon be available at https://github.com/zjunlp/KnowLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. Project Website:
  https://zjunlp.github.io/project/OceanGPT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Differentiable Filters Enable Ubiquitous Robot Control
  with Smartwatches <span class="chip">IROS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian C Weigend, Xiao Liu, Heni Ben Amor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ubiquitous robot control and human-robot collaboration using smart devices
poses a challenging problem primarily due to strict accuracy requirements and
sparse information. This paper presents a novel approach that incorporates a
probabilistic differentiable filter, specifically the Differentiable Ensemble
Kalman Filter (DEnKF), to facilitate robot control solely using Inertial
Measurement Units (IMUs) from a smartwatch and a smartphone. The implemented
system is cost-effective and achieves accurate estimation of the human pose
state. Experiment results from human-robot handover tasks underscore that smart
devices allow versatile and ubiquitous robot control. The code for this paper
is available at https://github.com/ir-lab/DEnKF and
https://github.com/wearable-motion-capture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DiffPropRob Workshop IROS 2023 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Stochastic Compound Failure Model for Testing Resilience of Autonomous
  Fixed-Wing Aircraft I: Formulation and Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thelonious Cooper, Sai Ravela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a Markov chain model to dynamically emulate the effects
of adverse (failure) flight conditions on fixed-wing, autonomous aircraft
system actuators. It implements a PX4 Autopilot flight stack module that
perturbs the attitude control inputs to the plane's actuator mixer. We apply
this approach in simulation on a fixed-wing autonomous aircraft to test the
controller response to stochastic compound failures on a range of turning
radii. Statistical measures of the differences between target and simulated
flight paths demonstrate that a well-tuned PID controller remains competitive
with adaptive control in a cascading, compound, transient failure regime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Marginalized Importance Sampling for Off-Environment Policy Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pulkit Katdare, Nan Jiang, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) methods are typically sample-inefficient, making
it challenging to train and deploy RL-policies in real world robots. Even a
robust policy trained in simulation requires a real-world deployment to assess
their performance. This paper proposes a new approach to evaluate the
real-world performance of agent policies prior to deploying them in the real
world. Our approach incorporates a simulator along with real-world offline data
to evaluate the performance of any policy using the framework of Marginalized
Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large
density ratios that deviate from a reasonable range and (2) indirect
supervision, where the ratio needs to be inferred indirectly, thus exacerbating
estimation error. Our approach addresses these challenges by introducing the
target policy's occupancy in the simulator as an intermediate variable and
learning the density ratio as the product of two terms that can be learned
separately. The first term is learned with direct supervision and the second
term has a small magnitude, thus making it computationally efficient. We
analyze the sample complexity as well as error propagation of our two
step-procedure. Furthermore, we empirically evaluate our approach on Sim2Sim
environments such as Cartpole, Reacher, and Half-Cheetah. Our results show that
our method generalizes well across a variety of Sim2Sim gap, target policies
and offline data collection policies. We also demonstrate the performance of
our algorithm on a Sim2Real task of validating the performance of a 7 DoF
robotic arm using offline data along with the Gazebo simulator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online POMDP Planning with Anytime Deterministic Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moran Barenboim, Vadim Indelman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents operating in real-world scenarios frequently encounter
uncertainty and make decisions based on incomplete information. Planning under
uncertainty can be mathematically formalized using partially observable Markov
decision processes (POMDPs). However, finding an optimal plan for POMDPs can be
computationally expensive and is feasible only for small tasks. In recent
years, approximate algorithms, such as tree search and sample-based
methodologies, have emerged as state-of-the-art POMDP solvers for larger
problems. Despite their effectiveness, these algorithms offer only
probabilistic and often asymptotic guarantees toward the optimal solution due
to their dependence on sampling. To address these limitations, we derive a
deterministic relationship between a simplified solution that is easier to
obtain and the theoretically optimal one. First, we derive bounds for selecting
a subset of the observations to branch from while computing a complete belief
at each posterior node. Then, since a complete belief update may be
computationally demanding, we extend the bounds to support reduction of both
the state and the observation spaces. We demonstrate how our guarantees can be
integrated with existing state-of-the-art solvers that sample a subset of
states and observations. As a result, the returned solution holds deterministic
bounds relative to the optimal policy. Lastly, we substantiate our findings
with supporting experimental results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Hierarchical Interactive Multi-Object Search for Mobile
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Schmalstieg, Daniel Honerkamp, Tim Welschehold, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing object-search approaches enable robots to search through free
pathways, however, robots operating in unstructured human-centered environments
frequently also have to manipulate the environment to their needs. In this
work, we introduce a novel interactive multi-object search task in which a
robot has to open doors to navigate rooms and search inside cabinets and
drawers to find target objects. These new challenges require combining
manipulation and navigation skills in unexplored environments. We present
HIMOS, a hierarchical reinforcement learning approach that learns to compose
exploration, navigation, and manipulation skills. To achieve this, we design an
abstract high-level action space around a semantic map memory and leverage the
explored environment as instance navigation points. We perform extensive
experiments in simulation and the real world that demonstrate that, with
accurate perception, the decision making of HIMOS effectively transfers to new
environments in a zero-shot manner. It shows robustness to unseen subpolicies,
failures in their execution, and different robot kinematics. These capabilities
open the door to a wide range of downstream tasks across embodied AI and
real-world use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">120</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LanguageMPC: Large Language Models as Decision Makers for Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, Mingyu Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing learning-based autonomous driving (AD) systems face challenges in
comprehending high-level information, generalizing to rare events, and
providing interpretability. To address these problems, this work employs Large
Language Models (LLMs) as a decision-making component for complex AD scenarios
that require human commonsense understanding. We devise cognitive pathways to
enable comprehensive reasoning with LLMs, and develop algorithms for
translating LLM decisions into actionable driving commands. Through this
approach, LLM decisions are seamlessly integrated with low-level controllers by
guided parameter matrix adaptation. Extensive experiments demonstrate that our
proposed method not only consistently surpasses baseline approaches in
single-vehicle tasks, but also helps handle complex driving behaviors even
multi-vehicle coordination, thanks to the commonsense reasoning capabilities of
LLMs. This paper presents an initial step toward leveraging LLMs as effective
decision-makers for intricate AD scenarios in terms of safety, efficiency,
generalizability, and interoperability. We aspire for it to serve as
inspiration for future research in this field. Project page:
https://sites.google.com/view/llm-mpc
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-oriented Representation Learning for Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxiao Huo, Mingyu Ding, Chenfeng Xu, Thomas Tian, Xinghao Zhu, Yao Mu, Lingfeng Sun, Masayoshi Tomizuka, Wei Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans inherently possess generalizable visual representations that empower
them to efficiently explore and interact with the environments in manipulation
tasks. We advocate that such a representation automatically arises from
simultaneously learning about multiple simple perceptual skills that are
critical for everyday scenarios (e.g., hand detection, state estimate, etc.)
and is better suited for learning robot manipulation policies compared to
current state-of-the-art visual representations purely based on self-supervised
objectives. We formalize this idea through the lens of human-oriented
multi-task fine-tuning on top of pre-trained visual encoders, where each task
is a perceptual skill tied to human-environment interactions. We introduce Task
Fusion Decoder as a plug-and-play embedding translator that utilizes the
underlying relationships among these perceptual skills to guide the
representation learning towards encoding meaningful structure for what's
important for all perceptual skills, ultimately empowering learning of
downstream robotic manipulation tasks. Extensive experiments across a range of
robotic tasks and embodiments, in both simulations and real-world environments,
show that our Task Fusion Decoder consistently improves the representation of
three state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for
downstream manipulation policy-learning. Project page:
https://sites.google.com/view/human-oriented-robot-learning
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistent-1-to-3: Consistent Image to 3D View Synthesis via
  Geometry-aware Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, Heng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot novel view synthesis (NVS) from a single image is an essential
problem in 3D object understanding. While recent approaches that leverage
pre-trained generative models can synthesize high-quality novel views from
in-the-wild inputs, they still struggle to maintain 3D consistency across
different views. In this paper, we present Consistent-1-to-3, which is a
generative framework that significantly mitigate this issue. Specifically, we
decompose the NVS task into two stages: (i) transforming observed regions to a
novel view, and (ii) hallucinating unseen regions. We design a scene
representation transformer and view-conditioned diffusion model for performing
these two stages respectively. Inside the models, to enforce 3D consistency, we
propose to employ epipolor-guided attention to incorporate geometry
constraints, and multi-view attention to better aggregate multi-view
information. Finally, we design a hierarchy generation paradigm to generate
long sequences of consistent views, allowing a full 360 observation of the
provided object image. Qualitative and quantitative evaluation over multiple
datasets demonstrate the effectiveness of the proposed mechanisms against
state-of-the-art approaches. Our project page is at
https://jianglongye.com/consistent123/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jianglongye.com/consistent123/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient-3DiM: Learning a Generalizable Single-image Novel-view
  Synthesizer in One Day 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Jiang, Hao Tang, Jen-Hao Rick Chang, Liangchen Song, Zhangyang Wang, Liangliang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of novel view synthesis aims to generate unseen perspectives of an
object or scene from a limited set of input images. Nevertheless, synthesizing
novel views from a single image still remains a significant challenge in the
realm of computer vision. Previous approaches tackle this problem by adopting
mesh prediction, multi-plain image construction, or more advanced techniques
such as neural radiance fields. Recently, a pre-trained diffusion model that is
specifically designed for 2D image synthesis has demonstrated its capability in
producing photorealistic novel views, if sufficiently optimized on a 3D
finetuning task. Although the fidelity and generalizability are greatly
improved, training such a powerful diffusion model requires a vast volume of
training data and model parameters, resulting in a notoriously long time and
high computational costs. To tackle this issue, we propose Efficient-3DiM, a
simple but effective framework to learn a single-image novel-view synthesizer.
Motivated by our in-depth analysis of the inference process of diffusion
models, we propose several pragmatic strategies to reduce the training overhead
to a manageable scale, including a crafted timestep sampling strategy, a
superior 3D feature extractor, and an enhanced training scheme. When combined,
our framework is able to reduce the total training time from 10 days to less
than 1 day, significantly accelerating the training process under the same
computational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive
experiments are conducted to demonstrate the efficiency and generalizability of
our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Domain-Specific Features Disentanglement for Domain
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Qi Zhang, Zenan Huang, Haobo Wang, Junbo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributional shift between domains poses great challenges to modern machine
learning algorithms. The domain generalization (DG) signifies a popular line
targeting this issue, where these methods intend to uncover universal patterns
across disparate distributions. Noted, the crucial challenge behind DG is the
existence of irrelevant domain features, and most prior works overlook this
information. Motivated by this, we propose a novel contrastive-based
disentanglement method CDDG, to effectively utilize the disentangled features
to exploit the over-looked domain-specific features, and thus facilitating the
extraction of the desired cross-domain category features for DG tasks.
Specifically, CDDG learns to decouple inherent mutually exclusive features by
leveraging them in the latent space, thus making the learning discriminative.
Extensive experiments conducted on various benchmark datasets demonstrate the
superiority of our method compared to other state-of-the-art approaches.
Furthermore, visualization evaluations confirm the potential of our method in
achieving effective feature disentanglement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COOLer: Class-Incremental Learning for Appearance-Based Multiple Object
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizheng Liu, Mattia Segu, Fisher Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning allows a model to learn multiple tasks sequentially while
retaining the old knowledge without the training data of the preceding tasks.
This paper extends the scope of continual learning research to
class-incremental learning for \ac{mot}, which is desirable to accommodate the
continuously evolving needs of autonomous systems. Previous solutions for
continual learning of object detectors do not address the data association
stage of appearance-based trackers, leading to catastrophic forgetting of
previous classes' re-identification features. We introduce COOLer, a
COntrastive- and cOntinual-Learning-based tracker, which incrementally learns
to track new categories while preserving past knowledge by training on a
combination of currently available ground truth labels and pseudo-labels
generated by the past tracker. To further exacerbate the disentanglement of
instance representations, we introduce a novel contrastive class-incremental
instance representation learning technique. Finally, we propose a practical
evaluation protocol for continual learning for MOT and conduct experiments on
the \bdd and \shift datasets. Experimental results demonstrate that COOLer
continually learns while effectively addressing catastrophic forgetting of both
tracking and detection. The code is available at
\url{https://github.com/BoSmallEar/COOLer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GCPR 2023 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reversing Deep Face Embeddings with Probable Privacy Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daile Osorio-Roig, Paul A. Gerlitz, Christian Rathgeb, Christoph Busch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generally, privacy-enhancing face recognition systems are designed to offer
permanent protection of face embeddings. Recently, so-called soft-biometric
privacy-enhancement approaches have been introduced with the aim of canceling
soft-biometric attributes. These methods limit the amount of soft-biometric
information (gender or skin-colour) that can be inferred from face embeddings.
Previous work has underlined the need for research into rigorous evaluations
and standardised evaluation protocols when assessing privacy protection
capabilities. Motivated by this fact, this paper explores to what extent the
non-invertibility requirement can be met by methods that claim to provide
soft-biometric privacy protection. Additionally, a detailed vulnerability
assessment of state-of-the-art face embedding extractors is analysed in terms
of the transformation complexity used for privacy protection. In this context,
a well-known state-of-the-art face image reconstruction approach has been
evaluated on protected face embeddings to break soft biometric privacy
protection. Experimental results show that biometric privacy-enhanced face
embeddings can be reconstructed with an accuracy of up to approximately 98%,
depending on the complexity of the protection algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Convex Quantization: Revisiting Vector Quantization with Convex
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanmay Gautam, Reid Pryzant, Ziyi Yang, Chenguang Zhu, Somayeh Sojoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector Quantization (VQ) is a well-known technique in deep learning for
extracting informative discrete latent representations. VQ-embedded models have
shown impressive results in a range of applications including image and speech
generation. VQ operates as a parametric K-means algorithm that quantizes inputs
using a single codebook vector in the forward pass. While powerful, this
technique faces practical challenges including codebook collapse,
non-differentiability and lossy compression. To mitigate the aforementioned
issues, we propose Soft Convex Quantization (SCQ) as a direct substitute for
VQ. SCQ works like a differentiable convex optimization (DCO) layer: in the
forward pass, we solve for the optimal convex combination of codebook vectors
that quantize the inputs. In the backward pass, we leverage differentiability
through the optimality conditions of the forward solution. We then introduce a
scalable relaxation of the SCQ optimization and demonstrate its efficacy on the
CIFAR-10, GTSRB and LSUN datasets. We train powerful SCQ autoencoder models
that significantly outperform matched VQ-based architectures, observing an
order of magnitude better image reconstruction and codebook usage with
comparable quantization runtime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Lin Sung, Jaehong Yoon, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) can understand the world comprehensively
by integrating rich information from different modalities, achieving remarkable
performance improvements on various multimodal downstream tasks. However,
deploying LVLMs is often problematic due to their massive computational/energy
costs and carbon consumption. Such issues make it infeasible to adopt
conventional iterative global pruning, which is costly due to computing the
Hessian matrix of the entire large model for sparsification. Alternatively,
several studies have recently proposed layer-wise pruning approaches to avoid
the expensive computation of global pruning and efficiently compress model
weights according to their importance within a layer. However, these methods
often suffer from suboptimal model compression due to their lack of a global
perspective. To address this limitation in recent efficient pruning methods for
large models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP),
a two-stage coarse-to-fine weight pruning approach for LVLMs. We first
determine the sparsity ratios of different layers or blocks by leveraging the
global importance score, which is efficiently computed based on the
zeroth-order approximation of the global model gradients. Then, the multimodal
model performs local layer-wise unstructured weight pruning based on
globally-informed sparsity ratios. We validate our proposed method across
various multimodal and unimodal models and datasets, demonstrating significant
performance improvements over prevalent pruning techniques in the high-sparsity
regime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ecoflap.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Key-Selection for Face-based One-Time Biometrics via Morphing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daile Osorio-Roig, Mahdi Ghafourian, Christian Rathgeb, Ruben Vera-Rodriguez, Christoph Busch, Julian Fierrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, facial recognition systems are still vulnerable to adversarial
attacks. These attacks vary from simple perturbations of the input image to
modifying the parameters of the recognition model to impersonate an authorised
subject. So-called privacy-enhancing facial recognition systems have been
mostly developed to provide protection of stored biometric reference data, i.e.
templates. In the literature, privacy-enhancing facial recognition approaches
have focused solely on conventional security threats at the template level,
ignoring the growing concern related to adversarial attacks. Up to now, few
works have provided mechanisms to protect face recognition against adversarial
attacks while maintaining high security at the template level. In this paper,
we propose different key selection strategies to improve the security of a
competitive cancelable scheme operating at the signal level. Experimental
results show that certain strategies based on signal-level key selection can
lead to complete blocking of the adversarial attack based on an iterative
optimization for the most secure threshold, while for the most practical
threshold, the attack success chance can be decreased to approximately 5.0%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kosmos-G: Generating Images in Context with Multimodal Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-image (T2I) and vision-language-to-image
(VL2I) generation have made significant strides. However, the generation from
generalized vision-language inputs, especially involving multiple images,
remains under-explored. This paper presents Kosmos-G, a model that leverages
the advanced perception capabilities of Multimodal Large Language Models
(MLLMs) to tackle the aforementioned challenge. Our approach aligns the output
space of MLLM with CLIP using the textual modality as an anchor and performs
compositional instruction tuning on curated data. Kosmos-G demonstrates a
unique capability of zero-shot multi-entity subject-driven generation. Notably,
the score distillation instruction tuning requires no modifications to the
image decoder. This allows for a seamless substitution of CLIP and effortless
integration with a myriad of U-Net techniques ranging from fine-grained
controls to personalized image decoder variants. We posit Kosmos-G as an
initial attempt towards the goal of "image as a foreign language in image
generation."
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://aka.ms/Kosmos-G Project Page:
  https://xichenpan.github.io/kosmosg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing Intersectional Biases in Vision-Language Models with
  Counterfactual Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Vasudev Lal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While vision-language models (VLMs) have achieved remarkable performance
improvements recently, there is growing evidence that these models also posses
harmful biases with respect to social attributes such as gender and race. Prior
studies have primarily focused on probing such bias attributes individually
while ignoring biases associated with intersections between social attributes.
This could be due to the difficulty of collecting an exhaustive set of
image-text pairs for various combinations of social attributes from existing
datasets. To address this challenge, we employ text-to-image diffusion models
to produce counterfactual examples for probing intserctional social biases at
scale. Our approach utilizes Stable Diffusion with cross attention control to
produce sets of counterfactual image-text pairs that are highly similar in
their depiction of a subject (e.g., a given occupation) while differing only in
their depiction of intersectional social attributes (e.g., race & gender). We
conduct extensive experiments using our generated dataset which reveal the
intersectional social biases present in state-of-the-art VLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, Yong-Jin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent methods in text-to-3D leverage powerful pretrained diffusion models to
optimize NeRF. Notably, these methods are able to produce high-quality 3D
scenes without training on 3D data. Due to the open-ended nature of the task,
most studies evaluate their results with subjective case studies and user
experiments, thereby presenting a challenge in quantitatively addressing the
question: How has current progress in Text-to-3D gone so far? In this paper, we
introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing
diverse text prompts of three increasing complexity levels that are specially
designed for 3D generation. To assess both the subjective quality and the text
alignment, we propose two automatic metrics based on multi-view images produced
by the 3D contents. The quality metric combines multi-view text-image scores
and regional convolution to detect quality and view inconsistency. The
alignment metric uses multi-view captioning and Large Language Model (LLM)
evaluation to measure text-3D consistency. Both metrics closely correlate with
different dimensions of human judgments, providing a paradigm for efficiently
evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal
performance differences among six prevalent text-to-3D methods. Our analysis
further highlights the common struggles for current methods on generating
surroundings and multi-object scenes, as well as the bottleneck of leveraging
2D guidance for 3D generation. Our project page is available at:
https://t3bench.com.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully Automatic Segmentation of Gross Target Volume and Organs-at-Risk
  for Radiotherapy Planning of Nasopharyngeal Carcinoma <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Astaraki, Simone Bendazzoli, Iuliana Toma-Dasu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Target segmentation in CT images of Head&Neck (H&N) region is challenging due
to low contrast between adjacent soft tissue. The SegRap 2023 challenge has
been focused on benchmarking the segmentation algorithms of Nasopharyngeal
Carcinoma (NPC) which would be employed as auto-contouring tools for radiation
treatment planning purposes. We propose a fully-automatic framework and develop
two models for a) segmentation of 45 Organs at Risk (OARs) and b) two Gross
Tumor Volumes (GTVs). To this end, we preprocess the image volumes by
harmonizing the intensity distributions and then automatically cropping the
volumes around the target regions. The preprocessed volumes were employed to
train a standard 3D U-Net model for each task, separately. Our method took
second place for each of the tasks in the validation phase of the challenge.
The proposed framework is available at https://github.com/Astarakee/segrap2023
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 3 tables, MICCAI SegRap challenge contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for
  Open-vocabulary 3D Object Detection <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Cao, Yihan Zeng, Hang Xu, Dan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary 3D Object Detection (OV-3DDet) aims to detect objects from an
arbitrary list of categories within a 3D scene, which remains seldom explored
in the literature. There are primarily two fundamental problems in OV-3DDet,
i.e., localizing and classifying novel objects. This paper aims at addressing
the two problems simultaneously via a unified framework, under the condition of
limited base categories. To localize novel 3D objects, we propose an effective
3D Novel Object Discovery strategy, which utilizes both the 3D box geometry
priors and 2D semantic open-vocabulary priors to generate pseudo box labels of
the novel objects. To classify novel object boxes, we further develop a
cross-modal alignment module based on discovered novel boxes, to align feature
spaces between 3D point cloud and image/text modalities. Specifically, the
alignment process contains a class-agnostic and a class-discriminative
alignment, incorporating not only the base objects with annotations but also
the increasingly discovered novel objects, resulting in an iteratively enhanced
alignment. The novel box discovery and crossmodal alignment are jointly learned
to collaboratively benefit each other. The novel object discovery can directly
impact the cross-modal alignment, while a better feature alignment can, in
turn, boost the localization capability, leading to a unified OV-3DDet
framework, named CoDA, for simultaneous novel object localization and
classification. Extensive experiments on two challenging datasets (i.e.,
SUN-RGBD and ScanNet) demonstrate the effectiveness of our method and also show
a significant mAP improvement upon the best-performing alternative method by
80%. Codes and pre-trained models are released on the project page.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023. Project Page:
  https://yangcaoai.github.io/publications/CoDA.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Landmark Color for AUV Docking in Visually <span class="highlight-title">Dynamic Environment</span>s <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Corey Knutson, Zhipeng Cao, Junaed Sattar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Underwater Vehicles (AUVs) conduct missions underwater without the
need for human intervention. A docking station (DS) can extend mission times of
an AUV by providing a location for the AUV to recharge its batteries and
receive updated mission information. Various methods for locating and tracking
a DS exist, but most rely on expensive acoustic sensors, or are vision-based,
which is significantly affected by water quality. In this \doctype, we present
a vision-based method that utilizes adaptive color LED markers and dynamic
color filtering to maximize landmark visibility in varying water conditions.
Both AUV and DS utilize cameras to determine the water background color in
order to calculate the desired marker color. No communication between AUV and
DS is needed to determine marker color. Experiments conducted in a pool and
lake show our method performs 10 times better than static color thresholding
methods as background color varies. DS detection is possible at a range of 5
meters in clear water with minimal false positives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA 2024 for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph data modelling for outcome prediction in oropharyngeal cancer
  patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nithya Bhasker, Stefan Leger, Alexander Zwanenburg, Chethan Babu Reddy, Sebastian Bodenstedt, Steffen Löck, Stefanie Speidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are becoming increasingly popular in the medical
domain for the tasks of disease classification and outcome prediction. Since
patient data is not readily available as a graph, most existing methods either
manually define a patient graph, or learn a latent graph based on pairwise
similarities between the patients. There are also hypergraph neural network
(HGNN)-based methods that were introduced recently to exploit potential higher
order associations between the patients by representing them as a hypergraph.
In this work, we propose a patient hypergraph network (PHGN), which has been
investigated in an inductive learning setup for binary outcome prediction in
oropharyngeal cancer (OPC) patients using computed tomography (CT)-based
radiomic features for the first time. Additionally, the proposed model was
extended to perform time-to-event analyses, and compared with GNN and baseline
linear models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with
  Visual and Textual Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyi Du, Xiaosong Wang, Yongyi Lu, Yuyin Zhou, Shaoting Zhang, Alan Yuille, Kang Li, Zongwei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image synthesis approaches, e.g., generative adversarial networks, have been
popular as a form of data augmentation in medical image analysis tasks. It is
primarily beneficial to overcome the shortage of publicly accessible data and
associated quality annotations. However, the current techniques often lack
control over the detailed contents in generated images, e.g., the type of
disease patterns, the location of lesions, and attributes of the diagnosis. In
this work, we adapt the latest advance in the generative model, i.e., the
diffusion model, with the added control flow using lesion-specific visual and
textual prompts for generating dermatoscopic images. We further demonstrate the
advantage of our diffusion model-based framework over the classical generation
models in both the image quality and boosting the segmentation performance on
skin lesions. It can achieve a 9% increase in the SSIM image quality measure
and an over 5% increase in Dice coefficients over the prior arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computationally Efficient Quadratic Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathew Mithra Noel, Venkataraman Muthiah-Nakarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Higher order artificial neurons whose outputs are computed by applying an
activation function to a higher order multinomial function of the inputs have
been considered in the past, but did not gain acceptance due to the extra
parameters and computational cost. However, higher order neurons have
significantly greater learning capabilities since the decision boundaries of
higher order neurons can be complex surfaces instead of just hyperplanes. The
boundary of a single quadratic neuron can be a general hyper-quadric surface
allowing it to learn many nonlinearly separable datasets. Since quadratic forms
can be represented by symmetric matrices, only $\frac{n(n+1)}{2}$ additional
parameters are needed instead of $n^2$. A quadratic Logistic regression model
is first presented. Solutions to the XOR problem with a single quadratic neuron
are considered. The complete vectorized equations for both forward and backward
propagation in feedforward networks composed of quadratic neurons are derived.
A reduced parameter quadratic neural network model with just $ n $ additional
parameters per neuron that provides a compromise between learning ability and
computational cost is presented. Comparison on benchmark classification
datasets are used to demonstrate that a final layer of quadratic neurons
enables networks to achieve higher accuracy with significantly fewer hidden
layer neurons. In particular this paper shows that any dataset composed of $C$
bounded clusters can be separated with only a single layer of $C$ quadratic
neurons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-centric Behavior Description in Videos: New Benchmark and Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingru Zhou, Yiqi Gao, Manqing Zhang, Peng Wu, Peng Wang, Yanning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of video surveillance, describing the behavior of each
individual within the video is becoming increasingly essential, especially in
complex scenarios with multiple individuals present. This is because describing
each individual's behavior provides more detailed situational analysis,
enabling accurate assessment and response to potential risks, ensuring the
safety and harmony of public places. Currently, video-level captioning datasets
cannot provide fine-grained descriptions for each individual's specific
behavior. However, mere descriptions at the video-level fail to provide an
in-depth interpretation of individual behaviors, making it challenging to
accurately determine the specific identity of each individual. To address this
challenge, we construct a human-centric video surveillance captioning dataset,
which provides detailed descriptions of the dynamic behaviors of 7,820
individuals. Specifically, we have labeled several aspects of each person, such
as location, clothing, and interactions with other elements in the scene, and
these people are distributed across 1,012 videos. Based on this dataset, we can
link individuals to their respective behaviors, allowing for further analysis
of each person's behavior in surveillance videos. Besides the dataset, we
propose a novel video captioning approach that can describe individual behavior
in detail on a person-level basis, achieving state-of-the-art results. To
facilitate further research in this field, we intend to release our dataset and
code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Grammatical Compositional Model for Video Action Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijun Zhang, Xu Zou, Jiahuan Zhou, Sheng Zhong, Ying Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analysis of human actions in videos demands understanding complex human
dynamics, as well as the interaction between actors and context. However, these
interaction relationships usually exhibit large intra-class variations from
diverse human poses or object manipulations, and fine-grained inter-class
differences between similar actions. Thus the performance of existing methods
is severely limited. Motivated by the observation that interactive actions can
be decomposed into actor dynamics and participating objects or humans, we
propose to investigate the composite property of them. In this paper, we
present a novel Grammatical Compositional Model (GCM) for action detection
based on typical And-Or graphs. Our model exploits the intrinsic structures and
latent relationships of actions in a hierarchical manner to harness both the
compositionality of grammar models and the capability of expressing rich
features of DNNs. The proposed model can be readily embodied into a neural
network module for efficient optimization in an end-to-end manner. Extensive
experiments are conducted on the AVA dataset and the Something-Else task to
demonstrate the superiority of our model, meanwhile the interpretability is
enhanced through an inference parsing procedure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Resolution Fusion for Fully Automatic Cephalometric Landmark
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqian Guo, Wencheng Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cephalometric landmark detection on lateral skull X-ray images plays a
crucial role in the diagnosis of certain dental diseases. Accurate and
effective identification of these landmarks presents a significant challenge.
Based on extensive data observations and quantitative analyses, we discovered
that visual features from different receptive fields affect the detection
accuracy of various landmarks differently. As a result, we employed an image
pyramid structure, integrating multiple resolutions as input to train a series
of models with different receptive fields, aiming to achieve the optimal
feature combination for each landmark. Moreover, we applied several data
augmentation techniques during training to enhance the model's robustness
across various devices and measurement alternatives. We implemented this method
in the Cephalometric Landmark Detection in Lateral X-ray Images 2023 Challenge
and achieved a Mean Radial Error (MRE) of 1.62 mm and a Success Detection Rate
(SDR) 2.0mm of 74.18% in the final testing phase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Magicremover: Tuning-free Text-guided Image inpainting with Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Yang, Lu Zhang, Liqian Ma, Yu Liu, JingJing Fu, You He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image inpainting aims to fill in the missing pixels with visually coherent
and semantically plausible content. Despite the great progress brought from
deep generative models, this task still suffers from i. the difficulties in
large-scale realistic data collection and costly model training; and ii. the
intrinsic limitations in the traditionally user-defined binary masks on objects
with unclear boundaries or transparent texture. In this paper, we propose
MagicRemover, a tuning-free method that leverages the powerful diffusion models
for text-guided image inpainting. We introduce an attention guidance strategy
to constrain the sampling process of diffusion models, enabling the erasing of
instructed areas and the restoration of occluded content. We further propose a
classifier optimization algorithm to facilitate the denoising stability within
less sampling steps. Extensive comparisons are conducted among our MagicRemover
and state-of-the-art methods including quantitative evaluation and user study,
demonstrating the significant improvement of MagicRemover on high-quality image
inpainting. We will release our code at https://github.com/exisas/Magicremover.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delving into CLIP latent space for Video Anomaly Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Zanella, Benedetta Liberatori, Willi Menapace, Fabio Poiesi, Yiming Wang, Elisa Ricci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the complex problem of detecting and recognising anomalies in
surveillance videos at the frame level, utilising only video-level supervision.
We introduce the novel method AnomalyCLIP, the first to combine Large Language
and Vision (LLV) models, such as CLIP, with multiple instance learning for
joint video anomaly detection and classification. Our approach specifically
involves manipulating the latent CLIP feature space to identify the normal
event subspace, which in turn allows us to effectively learn text-driven
directions for abnormal events. When anomalous frames are projected onto these
directions, they exhibit a large feature magnitude if they belong to a
particular class. We also introduce a computationally efficient Transformer
architecture to model short- and long-term temporal dependencies between
frames, ultimately producing the final anomaly score and class prediction
probabilities. We compare AnomalyCLIP against state-of-the-art methods
considering three major anomaly detection benchmarks, i.e. ShanghaiTech,
UCF-Crime, and XD-Violence, and empirically show that it outperforms baselines
in recognising video anomalies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Computer Vision and Image Understanding, project website
  and code are available at https://luca-zanella-dvl.github.io/AnomalyCLIP/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All Sizes Matter: Improving Volumetric Brain Segmentation on Small
  Lesions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayhan Can Erdur, Daniel Scholz, Josef A. Buchner, Stephanie E. Combs, Daniel Rueckert, Jan C. Peeken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain metastases (BMs) are the most frequently occurring brain tumors. The
treatment of patients having multiple BMs with stereo tactic radiosurgery
necessitates accurate localization of the metastases. Neural networks can
assist in this time-consuming and costly task that is typically performed by
human experts. Particularly challenging is the detection of small lesions since
they are often underrepresented in exist ing approaches. Yet, lesion detection
is equally important for all sizes. In this work, we develop an ensemble of
neural networks explicitly fo cused on detecting and segmenting small BMs. To
accomplish this task, we trained several neural networks focusing on individual
aspects of the BM segmentation problem: We use blob loss that specifically
addresses the imbalance of lesion instances in terms of size and texture and
is, therefore, not biased towards larger lesions. In addition, a model using a
subtraction sequence between the T1 and T1 contrast-enhanced sequence focuses
on low-contrast lesions. Furthermore, we train additional models only on small
lesions. Our experiments demonstrate the utility of the ad ditional blob loss
and the subtraction sequence. However, including the specialized small lesion
models in the ensemble deteriorates segmentation results. We also find
domain-knowledge-inspired postprocessing steps to drastically increase our
performance in most experiments. Our approach enables us to submit a
competitive challenge entry to the ASNR-MICCAI BraTS Brain Metastasis Challenge
2023.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Vision Anomaly Detection with the Guidance of Language
  Modality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Chen, Kaihang Pan, Guoming Wang, Yueting Zhuang, Siliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen a surge of interest in anomaly detection for tackling
industrial defect detection, event detection, etc. However, existing
unsupervised anomaly detectors, particularly those for the vision modality,
face significant challenges due to redundant information and sparse latent
space. Conversely, the language modality performs well due to its relatively
single data. This paper tackles the aforementioned challenges for vision
modality from a multimodal point of view. Specifically, we propose Cross-modal
Guidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) and
Cross-modal Linear Embedding (CMLE), to tackle the redundant information issue
and sparse space issue, respectively. CMER masks parts of the raw image and
computes the matching score with the text. Then, CMER discards irrelevant
pixels to make the detector focus on critical contents. To learn a more compact
latent space for the vision anomaly detector, CMLE learns a correlation
structure matrix from the language modality, and then the latent space of
vision modality will be learned with the guidance of the matrix. Thereafter,
the vision latent space will get semantically similar images closer. Extensive
experiments demonstrate the effectiveness of the proposed methods.
Particularly, CMG outperforms the baseline that only uses images by 16.81%.
Ablation experiments further confirm the synergy among the proposed methods, as
each component depends on the other to achieve optimal performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBEV: Elevating Roadside 3D Object Detection with Depth and Height
  Complementarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Shi, Chengshan Pang, Jiaming Zhang, Kailun Yang, Yuhao Wu, Huajian Ni, Yining Lin, Rainer Stiefelhagen, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Roadside camera-driven 3D object detection is a crucial task in intelligent
transportation systems, which extends the perception range beyond the
limitations of vision-centric vehicles and enhances road safety. While previous
studies have limitations in using only depth or height information, we find
both depth and height matter and they are in fact complementary. The depth
feature encompasses precise geometric cues, whereas the height feature is
primarily focused on distinguishing between various categories of height
intervals, essentially providing semantic context. This insight motivates the
development of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D
object detection framework that integrates depth and height to construct robust
BEV representations. In essence, CoBEV estimates each pixel's depth and height
distribution and lifts the camera features into 3D space for lateral fusion
using the newly proposed two-stage complementary feature selection (CFS)
module. A BEV feature distillation framework is also seamlessly integrated to
further enhance the detection accuracy from the prior knowledge of the
fusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D
detection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as
the private Supremind-Road dataset, demonstrating that CoBEV not only achieves
the accuracy of the new state-of-the-art, but also significantly advances the
robustness of previous methods in challenging long-distance scenarios and noisy
camera disturbance, and enhances generalization by a large margin in
heterologous settings with drastic changes in scene and camera parameters. For
the first time, the vehicle AP score of a camera model reaches 80% on
DAIR-V2X-I in terms of easy mode. The source code will be made publicly
available at https://github.com/MasterHow/CoBEV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code will be made publicly available at
  https://github.com/MasterHow/CoBEV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DOMINO: A Dual-System for Multi-step Visual Language Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peifang Wang, Olga Golovneva, Armen Aghajanyan, Xiang Ren, Muhao Chen, Asli Celikyilmaz, Maryam Fazel-Zarandi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual language reasoning requires a system to extract text or numbers from
information-dense images like charts or plots and perform logical or arithmetic
reasoning to arrive at an answer. To tackle this task, existing work relies on
either (1) an end-to-end vision-language model trained on a large amount of
data, or (2) a two-stage pipeline where a captioning model converts the image
into text that is further read by another large language model to deduce the
answer. However, the former approach forces the model to answer a complex
question with one single step, and the latter approach is prone to inaccurate
or distracting information in the converted text that can confuse the language
model. In this work, we propose a dual-system for multi-step multimodal
reasoning, which consists of a "System-1" step for visual information
extraction and a "System-2" step for deliberate reasoning. Given an input,
System-2 breaks down the question into atomic sub-steps, each guiding System-1
to extract the information required for reasoning from the image. Experiments
on chart and plot datasets show that our method with a pre-trained System-2
module performs competitively compared to prior work on in- and
out-of-distribution data. By fine-tuning the System-2 module (LLaMA-2 70B) on
only a small amount of data on multi-step reasoning, the accuracy of our method
is further improved and surpasses the best fully-supervised end-to-end approach
by 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on a challenging
dataset with human-authored questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking Anything in Heart All at Once 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkang Shen, Hao Zhu, You Zhou, Yu Liu, Si Yi, Lili Dong, Weipeng Zhao, David J. Brady, Xun Cao, Zhan Ma, Yi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Myocardial motion tracking stands as an essential clinical tool in the
prevention and detection of Cardiovascular Diseases (CVDs), the foremost cause
of death globally. However, current techniques suffer incomplete and inaccurate
motion estimation of the myocardium both in spatial and temporal dimensions,
hindering the early identification of myocardial dysfunction. In addressing
these challenges, this paper introduces the Neural Cardiac Motion Field
(NeuralCMF). NeuralCMF leverages the implicit neural representation (INR) to
model the 3D structure and the comprehensive 6D forward/backward motion of the
heart. This approach offers memory-efficient storage and continuous capability
to query the precise shape and motion of the myocardium throughout the cardiac
cycle at any specific point. Notably, NeuralCMF operates without the need for
paired datasets, and its optimization is self-supervised through the physics
knowledge priors both in space and time dimensions, ensuring compatibility with
both 2D and 3D echocardiogram video inputs. Experimental validations across
three representative datasets support the robustness and innovative nature of
the NeuralCMF, marking significant advantages over existing state-of-the-arts
in cardiac imaging and motion tracking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LROC-PANGU-GAN: Closing the Simulation Gap in Learning Crater
  Segmentation with Planetary Simulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewon La, Jaime Phadke, Matt Hutton, Marius Schwinning, Gabriele De Canio, Florian Renk, Lars Kunze, Matthew Gadd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is critical for probes landing on foreign planetary bodies to be able to
robustly identify and avoid hazards - as, for example, steep cliffs or deep
craters can pose significant risks to a probe's landing and operational
success. Recent applications of deep learning to this problem show promising
results. These models are, however, often learned with explicit supervision
over annotated datasets. These human-labelled crater databases, such as from
the Lunar Reconnaissance Orbiter Camera (LROC), may lack in consistency and
quality, undermining model performance - as incomplete and/or inaccurate labels
introduce noise into the supervisory signal, which encourages the model to
learn incorrect associations and results in the model making unreliable
predictions. Physics-based simulators, such as the Planet and Asteroid Natural
Scene Generation Utility, have, in contrast, perfect ground truth, as the
internal state that they use to render scenes is known with exactness. However,
they introduce a serious simulation-to-real domain gap - because of fundamental
differences between the simulated environment and the real-world arising from
modelling assumptions, unaccounted for physical interactions, environmental
variability, etc. Therefore, models trained on their outputs suffer when
deployed in the face of realism they have not encountered in their training
data distributions. In this paper, we therefore introduce a system to close
this "realism" gap while retaining label fidelity. We train a CycleGAN model to
synthesise LROC from Planet and Asteroid Natural Scene Generation Utility
(PANGU) images. We show that these improve the training of a downstream crater
segmentation network, with segmentation performance on a test set of real LROC
images improved as compared to using only simulated PANGU images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17th Symposium on Advanced Space Technologies in Robotics and
  Automation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Shuffle: An Efficient Channel Mixture Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijun Gong, Zhuowen Yin, Yushu Li, Kailing Guo, Xiangmin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The redundancy of Convolutional neural networks not only depends on weights
but also depends on inputs. Shuffling is an efficient operation for mixing
channel information but the shuffle order is usually pre-defined. To reduce the
data-dependent redundancy, we devise a dynamic shuffle module to generate
data-dependent permutation matrices for shuffling. Since the dimension of
permutation matrix is proportional to the square of the number of input
channels, to make the generation process efficiently, we divide the channels
into groups and generate two shared small permutation matrices for each group,
and utilize Kronecker product and cross group shuffle to obtain the final
permutation matrices. To make the generation process learnable, based on
theoretical analysis, softmax, orthogonal regularization, and binarization are
employed to asymptotically approximate the permutation matrix. Dynamic shuffle
adaptively mixes channel information with negligible extra computation and
memory occupancy. Experiment results on image classification benchmark datasets
CIFAR-10, CIFAR-100, Tiny ImageNet and ImageNet have shown that our method
significantly increases ShuffleNets' performance. Adding dynamic generated
matrix with learnable static matrix, we further propose static-dynamic-shuffle
and show that it can serve as a lightweight replacement of ordinary pointwise
convolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUNCH: Modelling Unique 'N Controllable Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debayan Deb, Suvidha Tripathi, Pranit Puri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated generation of 3D human heads has been an intriguing and
challenging task for computer vision researchers. Prevailing methods synthesize
realistic avatars but with limited control over the diversity and quality of
rendered outputs and suffer from limited correlation between shape and texture
of the character. We propose a method that offers quality, diversity, control,
and realism along with explainable network design, all desirable features to
game-design artists in the domain. First, our proposed Geometry Generator
identifies disentangled latent directions and generate novel and diverse
samples. A Render Map Generator then learns to synthesize multiply high-fidelty
physically-based render maps including Albedo, Glossiness, Specular, and
Normals. For artists preferring fine-grained control over the output, we
introduce a novel Color Transformer Model that allows semantic color control
over generated maps. We also introduce quantifiable metrics called Uniqueness
and Novelty and a combined metric to test the overall performance of our model.
Demo for both shapes and textures can be found:
https://munch-seven.vercel.app/. We will release our model along with the
synthetic dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHOT: Suppressing the Hessian along the Optimization Trajectory for
  Gradient-Based Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JunHoo Lee, Jayeon Yoo, Nojun Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we hypothesize that gradient-based meta-learning (GBML)
implicitly suppresses the Hessian along the optimization trajectory in the
inner loop. Based on this hypothesis, we introduce an algorithm called SHOT
(Suppressing the Hessian along the Optimization Trajectory) that minimizes the
distance between the parameters of the target and reference models to suppress
the Hessian in the inner loop. Despite dealing with high-order terms, SHOT does
not increase the computational complexity of the baseline model much. It is
agnostic to both the algorithm and architecture used in GBML, making it highly
versatile and applicable to any GBML baseline. To validate the effectiveness of
SHOT, we conduct empirical tests on standard few-shot learning tasks and
qualitatively analyze its dynamics. We confirm our hypothesis empirically and
demonstrate that SHOT outperforms the corresponding baseline. Code is available
at: https://github.com/JunHoo-Lee/SHOT
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Condition numbers in multiview geometry, instability in relative pose
  estimation, and RANSAC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyi Fan, Joe Kileel, Benjamin Kimia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce a general framework for analyzing the numerical
conditioning of minimal problems in multiple view geometry, using tools from
computational algebra and Riemannian geometry. Special motivation comes from
the fact that relative pose estimation, based on standard 5-point or 7-point
Random Sample Consensus (RANSAC) algorithms, can fail even when no outliers are
present and there is enough data to support a hypothesis. We argue that these
cases arise due to the intrinsic instability of the 5- and 7-point minimal
problems. We apply our framework to characterize the instabilities, both in
terms of the world scenes that lead to infinite condition number, and directly
in terms of ill-conditioned image data. The approach produces computational
tests for assessing the condition number before solving the minimal problem.
Lastly synthetic and real data experiments suggest that RANSAC serves not only
to remove outliers, but also to select for well-conditioned image data, as
predicted by our theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Pan-Sharpening via Generalized Inverse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqi Liu, Yutong Bai, Xinyang Han, Alan Yuille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pan-sharpening algorithm utilizes panchromatic image and multispectral image
to obtain a high spatial and high spectral image. However, the optimizations of
the algorithms are designed with different standards. We adopt the simple
matrix equation to describe the Pan-sharpening problem. The solution existence
condition and the acquirement of spectral and spatial resolution are discussed.
A down-sampling enhancement method was introduced for better acquiring the
spatial and spectral down-sample matrices. By the generalized inverse theory,
we derived two forms of general inverse matrix formulations that can correspond
to the two prominent classes of Pan-sharpening methods, that is, component
substitution and multi-resolution analysis methods. Specifically, the Gram
Schmidt Adaptive(GSA) was proved to follow the general inverse matrix
formulation of component substitution. A model prior to the general inverse
matrix of the spectral function was rendered. The theoretical errors are
analyzed. Synthetic experiments and real data experiments are implemented. The
proposed methods are better and sharper than other methods qualitatively in
both synthetic and real experiments. The down-sample enhancement effect is
shown of better results both quantitatively and qualitatively in real
experiments. The generalized inverse matrix theory help us better understand
the Pan-sharpening.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GETAvatar: Generative Textured Meshes for Animatable Human Avatars <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanmeng Zhang, Jianfeng Zhang, Rohan Chacko, Hongyi Xu, Guoxian Song, Yi Yang, Jiashi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of 3D-aware full-body human generation, aiming at
creating animatable human avatars with high-quality textures and geometries.
Generally, two challenges remain in this field: i) existing methods struggle to
generate geometries with rich realistic details such as the wrinkles of
garments; ii) they typically utilize volumetric radiance fields and neural
renderers in the synthesis process, making high-resolution rendering
non-trivial. To overcome these problems, we propose GETAvatar, a Generative
model that directly generates Explicit Textured 3D meshes for animatable human
Avatar, with photo-realistic appearance and fine geometric details.
Specifically, we first design an articulated 3D human representation with
explicit surface modeling, and enrich the generated humans with realistic
surface details by learning from the 2D normal maps of 3D scan data. Second,
with the explicit mesh representation, we can use a rasterization-based
renderer to perform surface rendering, allowing us to achieve high-resolution
image generation efficiently. Extensive experiments demonstrate that GETAvatar
achieves state-of-the-art performance on 3D-aware human generation both in
appearance and geometry quality. Notably, GETAvatar can generate images at
512x512 resolution with 17FPS and 1024x1024 resolution with 14FPS, improving
upon previous methods by 2x. Our code and models will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2023. Project Page: https://getavatar.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ED-<span class="highlight-title">NeRF</span>: Efficient Text-Guided Editing of 3D Scene using Latent Space
  <span class="highlight-title">NeRF</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jangho Park, Gihyun Kwon, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a significant advancement in text-to-image diffusion
models, leading to groundbreaking performance in 2D image generation. These
advancements have been extended to 3D models, enabling the generation of novel
3D objects from textual descriptions. This has evolved into NeRF editing
methods, which allow the manipulation of existing 3D objects through textual
conditioning. However, existing NeRF editing techniques have faced limitations
in their performance due to slow training speeds and the use of loss functions
that do not adequately consider editing. To address this, here we present a
novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding
real-world scenes into the latent space of the latent diffusion model (LDM)
through a unique refinement layer. This approach enables us to obtain a NeRF
backbone that is not only faster but also more amenable to editing compared to
traditional image space NeRF editing. Furthermore, we propose an improved loss
function tailored for editing by migrating the delta denoising score (DDS)
distillation loss, originally used in 2D image editing to the three-dimensional
domain. This novel loss function surpasses the well-known score distillation
sampling (SDS) loss in terms of suitability for editing purposes. Our
experimental results demonstrate that ED-NeRF achieves faster editing speed
while producing improved output quality compared to state-of-the-art 3D editing
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Domain Gap by Clustering-based Image-Text Graph Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nokyung Park, Daewon Chae, Jeongyong Shim, Sangpil Kim, Eun-Sol Kim, Jinkyu Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning domain-invariant representations is important to train a model that
can generalize well to unseen target task domains. Text descriptions inherently
contain semantic structures of concepts and such auxiliary semantic cues can be
used as effective pivot embedding for domain generalization problems. Here, we
use multimodal graph representations, fusing images and text, to get
domain-invariant pivot embeddings by considering the inherent semantic
structure between local images and text descriptors. Specifically, we aim to
learn domain-invariant features by (i) representing the image and text
descriptions with graphs, and by (ii) clustering and matching the graph-based
image node features into textual graphs simultaneously. We experiment with
large-scale public datasets, such as CUB-DG and DomainBed, and our model
achieves matched or better state-of-the-art performance on these datasets. Our
code will be publicly available upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Dimension-Embedding-Aware Modality Fusion Transformer for
  Psychiatric Disorder Clasification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxin Wang, Xuyang Cao, Shan An, Fengmei Fan, Chao Zhang, Jinsong Wang, Feng Yu, Zhiren Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning approaches, together with neuroimaging techniques, play an
important role in psychiatric disorders classification. Previous studies on
psychiatric disorders diagnosis mainly focus on using functional connectivity
matrices of resting-state functional magnetic resonance imaging (rs-fMRI) as
input, which still needs to fully utilize the rich temporal information of the
time series of rs-fMRI data. In this work, we proposed a
multi-dimension-embedding-aware modality fusion transformer (MFFormer) for
schizophrenia and bipolar disorder classification using rs-fMRI and T1 weighted
structural MRI (T1w sMRI). Concretely, to fully utilize the temporal
information of rs-fMRI and spatial information of sMRI, we constructed a deep
learning architecture that takes as input 2D time series of rs-fMRI and 3D
volumes T1w. Furthermore, to promote intra-modality attention and information
fusion across different modalities, a fusion transformer module (FTM) is
designed through extensive self-attention of hybrid feature maps of
multi-modality. In addition, a dimension-up and dimension-down strategy is
suggested to properly align feature maps of multi-dimensional from different
modalities. Experimental results on our private and public OpenfMRI datasets
show that our proposed MFFormer performs better than that using a single
modality or multi-modality MRI on schizophrenia and bipolar disorder diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ USB-<span class="highlight-title">NeRF</span>: Unrolling Shutter Bundle Adjusted Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moyang Li, Peng Wang, Lingzhe Zhao, Bangyan Liao, Peidong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) has received much attention recently due to its
impressive capability to represent 3D scene and synthesize novel view images.
Existing works usually assume that the input images are captured by a global
shutter camera. Thus, rolling shutter (RS) images cannot be trivially applied
to an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter
effect would also affect the accuracy of the camera pose estimation (e.g. via
COLMAP), which further prevents the success of NeRF algorithm with RS images.
In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance
Fields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and
recover accurate camera motion trajectory simultaneously under the framework of
NeRF, by modeling the physical image formation process of a RS camera.
Experimental results demonstrate that USB-NeRF achieves better performance
compared to prior works, in terms of RS effect removal, novel view image
synthesis as well as camera motion estimation. Furthermore, our algorithm can
also be used to recover high-fidelity high frame-rate global shutter video from
a sequence of RS images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PostRainBench: A comprehensive benchmark and a new model for
  precipitation forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Tang, Jiaming Zhou, Xiang Pan, Zeying Gong, Junwei Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate precipitation forecasting is a vital challenge of both scientific
and societal importance. Data-driven approaches have emerged as a widely used
solution for addressing this challenge. However, solely relying on data-driven
approaches has limitations in modeling the underlying physics, making accurate
predictions difficult. Coupling AI-based post-processing techniques with
traditional Numerical Weather Prediction (NWP) methods offers a more effective
solution for improving forecasting accuracy. Despite previous post-processing
efforts, accurately predicting heavy rainfall remains challenging due to the
imbalanced precipitation data across locations and complex relationships
between multiple meteorological variables. To address these limitations, we
introduce the PostRainBench, a comprehensive multi-variable NWP post-processing
benchmark consisting of three datasets for NWP post-processing-based
precipitation forecasting. We propose CAMT, a simple yet effective Channel
Attention Enhanced Multi-task Learning framework with a specially designed
weighted loss function. Its flexible design allows for easy plug-and-play
integration with various backbones. Extensive experimental results on the
proposed benchmark show that our method outperforms state-of-the-art methods by
6.3%, 4.7%, and 26.8% in rain CSI on the three datasets respectively. Most
notably, our model is the first deep learning-based method to outperform
traditional Numerical Weather Prediction (NWP) approaches in extreme
precipitation conditions. It shows improvements of 15.6%, 17.4%, and 31.8% over
NWP predictions in heavy rain CSI on respective datasets. These results
highlight the potential impact of our model in reducing the severe consequences
of extreme weather events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:2105.05537, arXiv:2206.15241 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Land-cover change detection using paired OpenStreetMap data and optical
  high-resolution imagery via object-guided Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongruixuan Chen, Cuiling Lan, Jian Song, Clifford Broni-Bediako, Junshi Xia, Naoto Yokoya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical high-resolution imagery and OpenStreetMap (OSM) data are two
important data sources for land-cover change detection. Previous studies in
these two data sources focus on utilizing the information in OSM data to aid
the change detection on multi-temporal optical high-resolution images. This
paper pioneers the direct detection of land-cover changes utilizing paired OSM
data and optical imagery, thereby broadening the horizons of change detection
tasks to encompass more dynamic earth observations. To this end, we propose an
object-guided Transformer (ObjFormer) architecture by naturally combining the
prevalent object-based image analysis (OBIA) technique with the advanced vision
Transformer architecture. The introduction of OBIA can significantly reduce the
computational overhead and memory burden in the self-attention module.
Specifically, the proposed ObjFormer has a hierarchical pseudo-siamese encoder
consisting of object-guided self-attention modules that extract representative
features of different levels from OSM data and optical images; a decoder
consisting of object-guided cross-attention modules can progressively recover
the land-cover changes from the extracted heterogeneous features. In addition
to the basic supervised binary change detection task, this paper raises a new
semi-supervised semantic change detection task that does not require any
manually annotated land-cover labels of optical images to train semantic change
detectors. Two lightweight semantic decoders are added to ObjFormer to
accomplish this task efficiently. A converse cross-entropy loss is designed to
fully utilize the negative samples, thereby contributing to the great
performance improvement in this task. The first large-scale benchmark dataset
containing 1,287 map-image pairs (1024$\times$ 1024 pixels for each sample)
covering 40 regions on six continents ...(see the manuscript for the full
abstract)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Memorization in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, Ye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their capacity to generate novel and high-quality samples, diffusion
models have attracted significant research interest in recent years. Notably,
the typical training objective of diffusion models, i.e., denoising score
matching, has a closed-form optimal solution that can only generate training
data replicating samples. This indicates that a memorization behavior is
theoretically expected, which contradicts the common generalization ability of
state-of-the-art diffusion models, and thus calls for a deeper understanding.
Looking into this, we first observe that memorization behaviors tend to occur
on smaller-sized datasets, which motivates our definition of effective model
memorization (EMM), a metric measuring the maximum size of training data at
which a learned diffusion model approximates its theoretical optimum. Then, we
quantify the impact of the influential factors on these memorization behaviors
in terms of EMM, focusing primarily on data distribution, model configuration,
and training procedure. Besides comprehensive empirical results identifying the
influential factors, we surprisingly find that conditioning training data on
uninformative random labels can significantly trigger the memorization in
diffusion models. Our study holds practical significance for diffusion model
users and offers clues to theoretical research in deep generative models. Code
is available at https://github.com/sail-sg/DiffMemorize.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedPrompt: Cross-Modal Prompting for Multi-Task Medical Image
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhang Chen, Chi-Man Pun, Shuqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal medical image translation is an essential task for synthesizing
missing modality data for clinical diagnosis. However, current learning-based
techniques have limitations in capturing cross-modal and global features,
restricting their suitability to specific pairs of modalities. This lack of
versatility undermines their practical usefulness, particularly considering
that the missing modality may vary for different cases. In this study, we
present MedPrompt, a multi-task framework that efficiently translates different
modalities. Specifically, we propose the Self-adaptive Prompt Block, which
dynamically guides the translation network towards distinct modalities. Within
this framework, we introduce the Prompt Extraction Block and the Prompt Fusion
Block to efficiently encode the cross-modal prompt. To enhance the extraction
of global features across diverse modalities, we incorporate the Transformer
model. Extensive experimental results involving five datasets and four pairs of
modalities demonstrate that our proposed model achieves state-of-the-art visual
quality and exhibits excellent generalization capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Visual Localization for Multi-Agent Collaboration: A Data-Driven
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Hanlon, Boyang Sun, Marc Pollefeys, Hermann Blum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rather than having each newly deployed robot create its own map of its
surroundings, the growing availability of SLAM-enabled devices provides the
option of simply localizing in a map of another robot or device. In cases such
as multi-robot or human-robot collaboration, localizing all agents in the same
map is even necessary. However, localizing e.g. a ground robot in the map of a
drone or head-mounted MR headset presents unique challenges due to viewpoint
changes. This work investigates how active visual localization can be used to
overcome such challenges of viewpoint changes. Specifically, we focus on the
problem of selecting the optimal viewpoint at a given location. We compare
existing approaches in the literature with additional proposed baselines and
propose a novel data-driven approach. The result demonstrates the superior
performance of the data-driven approach when compared to existing methods, both
in controlled simulation experiments and real-world deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GET: Group Event Transformer for Event-Based Vision <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yansong Peng, Yueyi Zhang, Zhiwei Xiong, Xiaoyan Sun, Feng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are a type of novel neuromorphic sen-sor that has been gaining
increasing attention. Existing event-based backbones mainly rely on image-based
designs to extract spatial information within the image transformed from
events, overlooking important event properties like time and polarity. To
address this issue, we propose a novel Group-based vision Transformer backbone
for Event-based vision, called Group Event Transformer (GET), which de-couples
temporal-polarity information from spatial infor-mation throughout the feature
extraction process. Specifi-cally, we first propose a new event representation
for GET, named Group Token, which groups asynchronous events based on their
timestamps and polarities. Then, GET ap-plies the Event Dual Self-Attention
block, and Group Token Aggregation module to facilitate effective feature
commu-nication and integration in both the spatial and temporal-polarity
domains. After that, GET can be integrated with different downstream tasks by
connecting it with vari-ous heads. We evaluate our method on four event-based
classification datasets (Cifar10-DVS, N-MNIST, N-CARS, and DVS128Gesture) and
two event-based object detection datasets (1Mpx and Gen1), and the results
demonstrate that GET outperforms other state-of-the-art methods. The code is
available at https://github.com/Peterande/GET-Group-Event-Transformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deformation-Invariant Neural Network and Its Applications in Distorted
  Image Restoration and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Qiguang Chen, Lok Ming Lui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Images degraded by geometric distortions pose a significant challenge to
imaging and computer vision tasks such as object recognition. Deep
learning-based imaging models usually fail to give accurate performance for
geometrically distorted images. In this paper, we propose the
deformation-invariant neural network (DINN), a framework to address the problem
of imaging tasks for geometrically distorted images. The DINN outputs
consistent latent features for images that are geometrically distorted but
represent the same underlying object or scene. The idea of DINN is to
incorporate a simple component, called the quasiconformal transformer network
(QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep
neural network that outputs a quasiconformal map, which can be used to
transform a geometrically distorted image into an improved version that is
closer to the distribution of natural or good images. It first outputs a
Beltrami coefficient, which measures the quasiconformality of the output
deformation map. By controlling the Beltrami coefficient, the local geometric
distortion under the quasiconformal mapping can be controlled. The QCTN is
lightweight and simple, which can be readily integrated into other existing
deep neural networks to enhance their performance. Leveraging our framework, we
have developed an image classification network that achieves accurate
classification of distorted images. Our proposed framework has been applied to
restore geometrically distorted images by atmospheric turbulence and water
turbulence. DINN outperforms existing GAN-based restoration methods under these
scenarios, demonstrating the effectiveness of the proposed framework.
Additionally, we apply our proposed framework to the 1-1 verification of human
face images under atmospheric turbulence and achieve satisfactory performance,
further demonstrating the efficacy of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P2CADNet: An End-to-End Reconstruction Network for Parametric 3D CAD
  Model from Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Zong, Fazhi He, Rubin Fan, Yuxin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer Aided Design (CAD), especially the feature-based parametric CAD,
plays an important role in modern industry and society. However, the
reconstruction of featured CAD model is more challenging than the
reconstruction of other CAD models. To this end, this paper proposes an
end-to-end network to reconstruct featured CAD model from point cloud
(P2CADNet). Initially, the proposed P2CADNet architecture combines a point
cloud feature extractor, a CAD sequence reconstructor and a parameter
optimizer. Subsequently, in order to reconstruct the featured CAD model in an
autoregressive way, the CAD sequence reconstructor applies two transformer
decoders, one with target mask and the other without mask. Finally, for
predicting parameters more precisely, we design a parameter optimizer with
cross-attention mechanism to further refine the CAD feature parameters. We
evaluate P2CADNet on the public dataset, and the experimental results show that
P2CADNet has excellent reconstruction quality and accuracy. To our best
knowledge, P2CADNet is the first end-to-end network to reconstruct featured CAD
model from point cloud, and can be regarded as baseline for future works.
Therefore, we open the source code at https://github.com/Blice0415/P2CADNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing and Improving OT-based Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemoo Choi, Jaewoong Choi, Myungjoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal Transport (OT) problem aims to find a transport plan that bridges two
distributions while minimizing a given cost function. OT theory has been widely
utilized in generative modeling. In the beginning, OT distance has been used as
a measure for assessing the distance between data and generated distributions.
Recently, OT transport map between data and prior distributions has been
utilized as a generative model. These OT-based generative models share a
similar adversarial training objective. In this paper, we begin by unifying
these OT-based adversarial methods within a single framework. Then, we
elucidate the role of each component in training dynamics through a
comprehensive analysis of this unified framework. Moreover, we suggest a simple
but novel method that improves the previously best-performing OT-based model.
Intuitively, our approach conducts a gradual refinement of the generated
distribution, progressively aligning it with the data distribution. Our
approach achieves a FID score of 2.51 on CIFAR-10, outperforming unified
OT-based adversarial approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MagicDrive: Street View Generation with Diverse 3D Geometry Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, Qiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models have significantly enhanced the data
synthesis with 2D control. Yet, precise 3D control in street view generation,
crucial for 3D perception tasks, remains elusive. Specifically, utilizing
Bird's-Eye View (BEV) as the primary condition often leads to challenges in
geometry control (e.g., height), affecting the representation of object shapes,
occlusion patterns, and road surface elevations, all of which are essential to
perception data synthesis, especially for 3D object detection tasks. In this
paper, we introduce MagicDrive, a novel street view generation framework
offering diverse 3D geometry controls, including camera poses, road maps, and
3D bounding boxes, together with textual descriptions, achieved through
tailored encoding strategies. Besides, our design incorporates a cross-view
attention module, ensuring consistency across multiple camera views. With
MagicDrive, we achieve high-fidelity street-view synthesis that captures
nuanced 3D geometry and various scene descriptions, enhancing tasks like BEV
segmentation and 3D object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://flymin.github.io/magicdrive</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent
  Text-to-3D 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyu Li, Rui Chen, Xuelin Chen, Ping Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is inherently ambiguous to lift 2D results from pre-trained diffusion
models to a 3D world for text-to-3D generation. 2D diffusion models solely
learn view-agnostic priors and thus lack 3D knowledge during the lifting,
leading to the multi-view inconsistency problem. We find that this problem
primarily stems from geometric inconsistency, and avoiding misplaced geometric
structures substantially mitigates the problem in the final outputs. Therefore,
we improve the consistency by aligning the 2D geometric priors in diffusion
models with well-defined 3D shapes during the lifting, addressing the vast
majority of the problem. This is achieved by fine-tuning the 2D diffusion model
to be viewpoint-aware and to produce view-specific coordinate maps of
canonically oriented 3D objects. In our process, only coarse 3D information is
used for aligning. This "coarse" alignment not only resolves the multi-view
inconsistency in geometries but also retains the ability in 2D diffusion models
to generate detailed and diversified high-quality objects unseen in the 3D
datasets. Furthermore, our aligned geometric priors (AGP) are generic and can
be seamlessly integrated into various state-of-the-art pipelines, obtaining
high generalizability in terms of unseen shapes and visual appearance while
greatly alleviating the multi-view inconsistency problem. Our method represents
a new state-of-the-art performance with an 85+% consistency rate by human
evaluation, while many previous methods are around 30%. Our project page is
https://sweetdreamer3d.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://sweetdreamer3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for
  Vision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seok-Yong Byun, Wonju Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to address the challenges of
understanding the prediction process and debugging prediction errors in Vision
Transformers (ViT), which have demonstrated superior performance in various
computer vision tasks such as image classification and object detection. While
several visual explainability techniques, such as CAM, Grad-CAM, Score-CAM, and
Recipro-CAM, have been extensively researched for Convolutional Neural Networks
(CNNs), limited research has been conducted on ViT. Current state-of-the-art
solutions for ViT rely on class agnostic Attention-Rollout and Relevance
techniques. In this work, we propose a new gradient-free visual explanation
method for ViT, called ViT-ReciproCAM, which does not require attention matrix
and gradient information. ViT-ReciproCAM utilizes token masking and generated
new layer outputs from the target layer's input to exploit the correlation
between activated tokens and network predictions for target classes. Our
proposed method outperforms the state-of-the-art Relevance method in the
Average Drop-Coherence-Complexity (ADCC) metric by $4.58\%$ to $5.80\%$ and
generates more localized saliency maps. Our experiments demonstrate the
effectiveness of ViT-ReciproCAM and showcase its potential for understanding
and debugging ViT models. Our proposed method provides an efficient and
easy-to-implement alternative for generating visual explanations, without
requiring attention and gradient information, which can be beneficial for
various applications in the field of computer vision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Prototype-Based Neural Network for Image Anomaly Detection and
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Huang, Zhao Kang, Hong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image anomaly detection and localization perform not only image-level anomaly
classification but also locate pixel-level anomaly regions. Recently, it has
received much research attention due to its wide application in various fields.
This paper proposes ProtoAD, a prototype-based neural network for image anomaly
detection and localization. First, the patch features of normal images are
extracted by a deep network pre-trained on nature images. Then, the prototypes
of the normal patch features are learned by non-parametric clustering. Finally,
we construct an image anomaly localization network (ProtoAD) by appending the
feature extraction network with $L2$ feature normalization, a $1\times1$
convolutional layer, a channel max-pooling, and a subtraction operation. We use
the prototypes as the kernels of the $1\times1$ convolutional layer; therefore,
our neural network does not need a training phase and can conduct anomaly
detection and localization in an end-to-end manner. Extensive experiments on
two challenging industrial anomaly detection datasets, MVTec AD and BTAD,
demonstrate that ProtoAD achieves competitive performance compared to the
state-of-the-art methods with a higher inference speed. The source code is
available at: https://github.com/98chao/ProtoAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaMerging: Adaptive Model Merging for Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) aims to empower a model to tackle multiple tasks
simultaneously. A recent development known as task arithmetic has revealed that
several models, each fine-tuned for distinct tasks, can be directly merged into
a single model to execute MTL without necessitating a retraining process using
the initial training data. Nevertheless, this direct addition of models often
leads to a significant deterioration in the overall performance of the merged
model. This decline occurs due to potential conflicts and intricate
correlations among the multiple tasks. Consequently, the challenge emerges of
how to merge pre-trained models more effectively without using their original
training data. This paper introduces an innovative technique called Adaptive
Model Merging (AdaMerging). This approach aims to autonomously learn the
coefficients for model merging, either in a task-wise or layer-wise manner,
without relying on the original training data. Specifically, our AdaMerging
method operates as an automatic, unsupervised task arithmetic scheme. It
leverages entropy minimization on unlabeled test samples from the multi-task
setup as a surrogate objective function to iteratively refine the merging
coefficients of the multiple models. Our experimental findings across eight
tasks demonstrate the efficacy of the AdaMerging scheme we put forth. Compared
to the current state-of-the-art task arithmetic merging scheme, AdaMerging
showcases a remarkable 11\% improvement in performance. Notably, AdaMerging
also exhibits superior generalization capabilities when applied to unseen
downstream tasks. Furthermore, it displays a significantly enhanced robustness
to data distribution shifts that may occur during the testing phase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReForm-Eval: Evaluating Large Vision Language Models via Unified
  Re-Formulation of Task-Oriented Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zejun Li, Ye Wang, Mengfei Du, Qingwen Liu, Binhao Wu, Jiwen Zhang, Chengxing Zhou, Zhihao Fan, Jie Fu, Jingjing Chen, Xuanjing Huang, Zhongyu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed remarkable progress in the development of large
vision-language models (LVLMs). Benefiting from the strong language backbones
and efficient cross-modal alignment strategies, LVLMs exhibit surprising
capabilities to perceive visual signals and perform visually grounded
reasoning. However, the capabilities of LVLMs have not been comprehensively and
quantitatively evaluate. Most existing multi-modal benchmarks require
task-oriented input-output formats, posing great challenges to automatically
assess the free-form text output of LVLMs. To effectively leverage the
annotations available in existing benchmarks and reduce the manual effort
required for constructing new benchmarks, we propose to re-formulate existing
benchmarks into unified LVLM-compatible formats. Through systematic data
collection and reformulation, we present the ReForm-Eval benchmark, offering
substantial data for evaluating various capabilities of LVLMs. Based on
ReForm-Eval, we conduct extensive experiments, thoroughly analyze the strengths
and weaknesses of existing LVLMs, and identify the underlying factors. Our
benchmark and evaluation framework will be open-sourced as a cornerstone for
advancing the development of LVLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 11 figures, 24 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Automatic VQA Evaluation Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Mañas, Benno Krojer, Aishwarya Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  8 years after the visual question answering (VQA) task was proposed, accuracy
remains the primary metric for automatic evaluation. VQA Accuracy has been
effective so far in the IID evaluation setting. However, our community is
undergoing a shift towards open-ended generative models and OOD evaluation. In
this new paradigm, the existing VQA Accuracy metric is overly stringent and
underestimates the performance of VQA systems. Thus, there is a need to develop
more robust automatic VQA metrics that serve as a proxy for human judgment. In
this work, we propose to leverage the in-context learning capabilities of
instruction-tuned large language models (LLMs) to build a better VQA metric. We
formulate VQA evaluation as an answer-rating task where the LLM is instructed
to score the accuracy of a candidate answer given a set of reference answers.
We demonstrate the proposed metric better correlates with human judgment
compared to existing metrics across several VQA models and benchmarks. We hope
wide adoption of our metric will contribute to better estimating the research
progress on the VQA task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization in diffusion models arises from geometry-adaptive
  harmonic representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Kadkhodaie, Florentin Guth, Eero P. Simoncelli, Stéphane Mallat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality samples generated with score-based reverse diffusion algorithms
provide evidence that deep neural networks (DNN) trained for denoising can
learn high-dimensional densities, despite the curse of dimensionality. However,
recent reports of memorization of the training set raise the question of
whether these networks are learning the "true" continuous density of the data.
Here, we show that two denoising DNNs trained on non-overlapping subsets of a
dataset learn nearly the same score function, and thus the same density, with a
surprisingly small number of training images. This strong generalization
demonstrates an alignment of powerful inductive biases in the DNN architecture
and/or training algorithm with properties of the data distribution. We analyze
these, demonstrating that the denoiser performs a shrinkage operation in a
basis adapted to the underlying image. Examination of these bases reveals
oscillating harmonic structures along contours and in homogeneous image
regions. We show that trained denoisers are inductively biased towards these
geometry-adaptive harmonic representations by demonstrating that they arise
even when the network is trained on image classes such as low-dimensional
manifolds, for which the harmonic basis is suboptimal. Additionally, we show
that the denoising performance of the networks is near-optimal when trained on
regular image classes for which the optimal basis is known to be
geometry-adaptive and harmonic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NOLA: Networks as Linear Combination of Low Rank Random Basis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Abbasi Koohpayegani, KL Navaneet, Parsa Nooralinejad, Soheil Kolouri, Hamed Pirsiavash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently gained popularity due to their
impressive few-shot performance across various downstream tasks. However,
fine-tuning all parameters and storing a unique model for each downstream task
or domain becomes impractical because of the massive size of checkpoints (e.g.,
350GB in GPT-3). Current literature, such as LoRA, showcases the potential of
low-rank modifications to the original weights of an LLM, enabling efficient
adaptation and storage for task-specific models. These methods can reduce the
number of parameters needed to fine-tune an LLM by several orders of magnitude.
Yet, these methods face two primary limitations: 1) the parameter reduction is
lower-bounded by the rank one decomposition, and 2) the extent of reduction is
heavily influenced by both the model architecture and the chosen rank. For
instance, in larger models, even a rank one decomposition might exceed the
number of parameters truly needed for adaptation. In this paper, we introduce
NOLA, which overcomes the rank one lower bound present in LoRA. It achieves
this by re-parameterizing the low-rank matrices in LoRA using linear
combinations of randomly generated matrices (basis) and optimizing the linear
mixture coefficients only. This approach allows us to decouple the number of
trainable parameters from both the choice of rank and the network architecture.
We present adaptation results using GPT-2 and ViT in natural language and
computer vision tasks. NOLA performs as well as, or better than models with
equivalent parameter counts. Furthermore, we demonstrate that we can halve the
parameters in larger models compared to LoRA with rank one, without sacrificing
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available here: https://github.com/UCDvision/NOLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy
  Efficiency of Inference Efficient Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KL Navaneet, Soroush Abbasi Koohpayegani, Essam Sleiman, Hamed Pirsiavash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a lot of progress in reducing the computation of
deep models at inference time. These methods can reduce both the computational
needs and power usage of deep models. Some of these approaches adaptively scale
the compute based on the input instance. We show that such models can be
vulnerable to a universal adversarial patch attack, where the attacker
optimizes for a patch that when pasted on any image, can increase the compute
and power consumption of the model. We run experiments with three different
efficient vision transformer methods showing that in some cases, the attacker
can increase the computation to the maximum possible level by simply pasting a
patch that occupies only 8\% of the image area. We also show that a standard
adversarial training defense method can reduce some of the attack's success. We
believe adaptive efficient methods will be necessary for the future to lower
the power usage of deep models, so we hope our paper encourages the community
to study the robustness of these methods and develop better defense methods for
the proposed attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/UCDvision/SlowFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShaSTA-Fuse: Camera-LiDAR Sensor Fusion to Model Shape and
  Spatio-Temporal Affinities for 3D Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tara Sadjadpour, Rares Ambrus, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D multi-object tracking (MOT) is essential for an autonomous mobile agent to
safely navigate a scene. In order to maximize the perception capabilities of
the autonomous agent, we aim to develop a 3D MOT framework that fuses camera
and LiDAR sensor information. Building on our prior LiDAR-only work, ShaSTA,
which models shape and spatio-temporal affinities for 3D MOT, we propose a
novel camera-LiDAR fusion approach for learning affinities. At its core, this
work proposes a fusion technique that generates a rich sensory signal
incorporating information about depth and distant objects to enhance affinity
estimation for improved data association, track lifecycle management,
false-positive elimination, false-negative propagation, and track confidence
score refinement. Our main contributions include a novel fusion approach for
combining camera and LiDAR sensory signals to learn affinities, and a
first-of-its-kind multimodal sequential track confidence refinement technique
that fuses 2D and 3D detections. Additionally, we perform an ablative analysis
on each fusion step to demonstrate the added benefits of incorporating the
camera sensor, particular for small, distant objects that tend to suffer from
the depth-sensing limits and sparsity of LiDAR sensors. In sum, our technique
achieves state-of-the-art performance on the nuScenes benchmark amongst
multimodal 3D MOT algorithms using CenterPoint detections.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Cognition of Visual Question Answering Models and Human
  Intelligence: A Comparative Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liben Chen, Long Chen, Tian Ellison-Chen, Zhuoyuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) is a challenging task that requires
cross-modal understanding and reasoning of visual image and natural language
question. To inspect the association of VQA models to human cognition, we
designed a survey to record human thinking process and analyzed VQA models by
comparing the outputs and attention maps with those of humans. We found that
although the VQA models resemble human cognition in architecture and performs
similarly with human on the recognition-level, they still struggle with
cognitive inferences. The analysis of human thinking procedure serves to direct
future research and introduce more cognitive capacity into modeling features
and architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Spatio-Temporal Attention-Based Method for Detecting Student Classroom
  Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately detecting student behavior from classroom videos is beneficial for
analyzing their classroom status and improving teaching efficiency. However,
low accuracy in student classroom behavior detection is a prevalent issue. To
address this issue, we propose a Spatio-Temporal Attention-Based Method for
Detecting Student Classroom Behaviors (BDSTA). Firstly, the SlowFast network is
used to generate motion and environmental information feature maps from the
video. Then, the spatio-temporal attention module is applied to the feature
maps, including information aggregation, compression and stimulation processes.
Subsequently, attention maps in the time, channel and space dimensions are
obtained, and multi-label behavior classification is performed based on these
attention maps. To solve the long-tail data problem that exists in student
classroom behavior datasets, we use an improved focal loss function to assign
more weight to the tail class data during training. Experimental results are
conducted on a self-made student classroom behavior dataset named STSCB.
Compared with the SlowFast model, the average accuracy of student behavior
classification detection improves by 8.94\% using BDSTA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCB-Dataset3: A Benchmark for Detecting Student Classroom Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang, Tao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of deep learning methods to automatically detect students' classroom
behavior is a promising approach for analyzing their class performance and
improving teaching effectiveness. However, the lack of publicly available
datasets on student behavior poses a challenge for researchers in this field.
To address this issue, we propose the Student Classroom Behavior dataset
(SCB-dataset3), which represents real-life scenarios. Our dataset comprises
5686 images with 45578 labels, focusing on six behaviors: hand-raising,
reading, writing, using a phone, bowing the head, and leaning over the table.
We evaluated the dataset using the YOLOv5, YOLOv7, and YOLOv8 algorithms,
achieving a mean average precision (map) of up to 80.3$\%$. We believe that our
dataset can serve as a robust foundation for future research in student
behavior detection and contribute to advancements in this field. Our
SCB-dataset3 is available for download at:
https://github.com/Whiffe/SCB-dataset
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2304.02488,
  arXiv:2306.03318</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Performance of Multimodal Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utsav Garg, Erhan Bas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kim Youwang, Lee Hyun, Kim Sung-Bin, Suekyeong Nam, Janghoon Ju, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose NeuFace, a 3D face mesh pseudo annotation method on videos via
neural re-parameterized optimization. Despite the huge progress in 3D face
reconstruction methods, generating reliable 3D face labels for in-the-wild
dynamic videos remains challenging. Using NeuFace optimization, we annotate the
per-view/-frame accurate and consistent face meshes on large-scale face videos,
called the NeuFace-dataset. We investigate how neural re-parameterization helps
to reconstruct image-aligned facial details on 3D meshes via gradient analysis.
By exploiting the naturalness and diversity of 3D faces in our dataset, we
demonstrate the usefulness of our dataset for 3D face-related tasks: improving
the reconstruction accuracy of an existing 3D face reconstruction model and
learning 3D facial motion prior. Code and datasets will be available at
https://neuface-dataset.github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, and 3 tables for the main paper. 8 pages, 6
  figures and 3 tables for the appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Interpretable Medical Image Classifiers via Concept
  Bottleneck Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An Yan, Yu Wang, Yiwu Zhong, Zexue He, Petros Karypis, Zihan Wang, Chengyu Dong, Amilcare Gentili, Chun-Nan Hsu, Jingbo Shang, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image classification is a critical problem for healthcare, with the
potential to alleviate the workload of doctors and facilitate diagnoses of
patients. However, two challenges arise when deploying deep learning models to
real-world healthcare applications. First, neural models tend to learn spurious
correlations instead of desired features, which could fall short when
generalizing to new domains (e.g., patients with different ages). Second, these
black-box models lack interpretability. When making diagnostic predictions, it
is important to understand why a model makes a decision for trustworthy and
safety considerations. In this paper, to address these two limitations, we
propose a new paradigm to build robust and interpretable medical image
classifiers with natural language concepts. Specifically, we first query
clinical concepts from GPT-4, then transform latent image features into
explicit concepts with a vision-language model. We systematically evaluate our
method on eight medical image classification datasets to verify its
effectiveness. On challenging datasets with strong confounding factors, our
method can mitigate spurious correlations thus substantially outperform
standard visual encoders and other baselines. Finally, we show how
classification with a small number of concepts brings a level of
interpretability for understanding model decisions through case studies in real
medical data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attributing Learned Concepts in Neural Networks to Training Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Konz, Charles Godfrey, Madelyn Shapiro, Jonathan Tu, Henry Kvinge, Davis Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model's original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViFiT: Reconstructing Vision Trajectories from IMU and Wi-Fi Fine Time
  Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan Bo Cao, Abrar Alali, Hansi Liu, Nicholas Meegan, Marco Gruteser, Kristin Dana, Ashwin Ashok, Shubham Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking subjects in videos is one of the most widely used functions in
camera-based IoT applications such as security surveillance, smart city traffic
safety enhancement, vehicle to pedestrian communication and so on. In the
computer vision domain, tracking is usually achieved by first detecting
subjects with bounding boxes, then associating detected bounding boxes across
video frames. For many IoT systems, images captured by cameras are usually sent
over the network to be processed at a different site that has more powerful
computing resources than edge devices. However, sending entire frames through
the network causes significant bandwidth consumption that may exceed the system
bandwidth constraints. To tackle this problem, we propose ViFiT, a
transformer-based model that reconstructs vision bounding box trajectories from
phone data (IMU and Fine Time Measurements). It leverages a transformer ability
of better modeling long-term time series data. ViFiT is evaluated on Vi-Fi
Dataset, a large-scale multimodal dataset in 5 diverse real world scenes,
including indoor and outdoor environments. To fill the gap of proper metrics of
jointly capturing the system characteristics of both tracking quality and video
bandwidth reduction, we propose a novel evaluation framework dubbed Minimum
Required Frames (MRF) and Minimum Required Frames Ratio (MRFR). ViFiT achieves
an MRFR of 0.65 that outperforms the state-of-the-art approach for cross-modal
reconstruction in LSTM Encoder-Decoder architecture X-Translator of 0.98,
resulting in a high frame reduction rate as 97.76%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 12 figures, 9 tables. MobiCom 2023 ISACom</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shielding the Unseen: Privacy Protection through Poisoning <span class="highlight-title">NeRF</span> with
  Spatial Deformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Wu, Brandon Y. Feng, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce an innovative method of safeguarding user privacy
against the generative capabilities of Neural Radiance Fields (NeRF) models.
Our novel poisoning attack method induces changes to observed views that are
imperceptible to the human eye, yet potent enough to disrupt NeRF's ability to
accurately reconstruct a 3D scene. To achieve this, we devise a bi-level
optimization algorithm incorporating a Projected Gradient Descent (PGD)-based
spatial deformation. We extensively test our approach on two common NeRF
benchmark datasets consisting of 29 real-world scenes with high-quality images.
Our results compellingly demonstrate that our privacy-preserving method
significantly impairs NeRF's performance across these benchmark datasets.
Additionally, we show that our method is adaptable and versatile, functioning
across various perturbation strengths and NeRF architectures. This work offers
valuable insights into NeRF's vulnerabilities and emphasizes the need to
account for such potential privacy risks when developing robust 3D scene
reconstruction algorithms. Our study contributes to the larger conversation
surrounding responsible AI and generative machine learning, aiming to protect
user privacy and respect creative ownership in the digital age.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blind CT Image Quality Assessment Using DDPM-derived Content and
  Transformer-based Evaluator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyi Shi, Wenjun Xia, Ge Wang, Xuanqin Mou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lowering radiation dose per view and utilizing sparse views per scan are two
common CT scan modes, albeit often leading to distorted images characterized by
noise and streak artifacts. Blind image quality assessment (BIQA) strives to
evaluate perceptual quality in alignment with what radiologists perceive, which
plays an important role in advancing low-dose CT reconstruction techniques. An
intriguing direction involves developing BIQA methods that mimic the
operational characteristic of the human visual system (HVS). The internal
generative mechanism (IGM) theory reveals that the HVS actively deduces primary
content to enhance comprehension. In this study, we introduce an innovative
BIQA metric that emulates the active inference process of IGM. Initially, an
active inference module, implemented as a denoising diffusion probabilistic
model (DDPM), is constructed to anticipate the primary content. Then, the
dissimilarity map is derived by assessing the interrelation between the
distorted image and its primary content. Subsequently, the distorted image and
dissimilarity map are combined into a multi-channel image, which is inputted
into a transformer-based image quality evaluator. Remarkably, by exclusively
utilizing this transformer-based quality evaluator, we won the second place in
the MICCAI 2023 low-dose computed tomography perceptual image quality
assessment grand challenge. Leveraging the DDPM-derived primary content, our
approach further improves the performance on the challenge dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning-based Mixture of Vision Transformers for Video
  Violence Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamid Mohammadi, Ehsan Nazerfard, Tahereh Firoozi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video violence recognition based on deep learning concerns accurate yet
scalable human violence recognition. Currently, most state-of-the-art video
violence recognition studies use CNN-based models to represent and categorize
videos. However, recent studies suggest that pre-trained transformers are more
accurate than CNN-based models on various video analysis benchmarks. Yet these
models are not thoroughly evaluated for video violence recognition. This paper
introduces a novel transformer-based Mixture of Experts (MoE) video violence
recognition system. Through an intelligent combination of large vision
transformers and efficient transformer architectures, the proposed system not
only takes advantage of the vision transformer architecture but also reduces
the cost of utilizing large vision transformers. The proposed architecture
maximizes violence recognition system accuracy while actively reducing
computational costs through a reinforcement learning-based router. The
empirical results show the proposed MoE architecture's superiority over
CNN-based models by achieving 92.4% accuracy on the RWF dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creating an Atlas of Normal Tissue for Pruning WSI Patching Through
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peyman Nejat, Areej Alsaafin, Ghazal Alabtah, Nneka Comfere, Aaron Mangold, Dennis Murphree, Patricija Zot, Saba Yasir, Joaquin J. Garcia, H. R. Tizhoosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patching gigapixel whole slide images (WSIs) is an important task in
computational pathology. Some methods have been proposed to select a subset of
patches as WSI representation for downstream tasks. While most of the
computational pathology tasks are designed to classify or detect the presence
of pathological lesions in each WSI, the confounding role and redundant nature
of normal histology in tissue samples are generally overlooked in WSI
representations. In this paper, we propose and validate the concept of an
"atlas of normal tissue" solely using samples of WSIs obtained from normal
tissue biopsies. Such atlases can be employed to eliminate normal fragments of
tissue samples and hence increase the representativeness collection of patches.
We tested our proposed method by establishing a normal atlas using 107 normal
skin WSIs and demonstrated how established indexes and search engines like
Yottixel can be improved. We used 553 WSIs of cutaneous squamous cell carcinoma
(cSCC) to show the advantage. We also validated our method applied to an
external dataset of 451 breast WSIs. The number of selected WSI patches was
reduced by 30% to 50% after utilizing the proposed normal atlas while
maintaining the same indexing and search performance in leave-one-patinet-out
validation for both datasets. We show that the proposed normal atlas shows
promise for unsupervised selection of the most representative patches of the
abnormal/malignant WSI lesions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-preserving Multi-biometric Indexing based on Frequent Binary
  Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daile Osorio-Roig, Lazaro J. Gonzalez-Soler, Christian Rathgeb, Christoph Busch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large-scale identification systems that ensure the privacy
protection of enrolled subjects represents a major challenge. Biometric
deployments that provide interoperability and usability by including efficient
multi-biometric solutions are a recent requirement. In the context of privacy
protection, several template protection schemes have been proposed in the past.
However, these schemes seem inadequate for indexing (workload reduction) in
biometric identification systems. More specifically, they have been used in
identification systems that perform exhaustive searches, leading to a
degradation of computational efficiency. To overcome these limitations, we
propose an efficient privacy-preserving multi-biometric identification system
that retrieves protected deep cancelable templates and is agnostic with respect
to biometric characteristics and biometric template protection schemes. To this
end, a multi-biometric binning scheme is designed to exploit the low
intra-class variation properties contained in the frequent binary patterns
extracted from different types of biometric characteristics. Experimental
results reported on publicly available databases using state-of-the-art Deep
Neural Network (DNN)-based embedding extractors show that the protected
multi-biometric identification system can reduce the computational workload to
approximately 57\% (indexing up to three types of biometric characteristics)
and 53% (indexing up to two types of biometric characteristics), while
simultaneously improving the biometric performance of the baseline biometric
system at the high-security thresholds. The source code of the proposed
multi-biometric indexing approach together with the composed multi-biometric
dataset, will be made available to the research community once the article is
accepted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Tang, Eric Zhang, Ray Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/EvenJoker/Point-PEFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. The specialized PEFT framework for 3D pre-trained models,
  which achieves competitive performance to full fine-tuning, and significantly
  reduces the computational resources. Project page:
  https://github.com/EvenJoker/Point-PEFT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structural Adversarial Objectives for Self-Supervised Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Zhang, Michael Maire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the framework of generative adversarial networks (GANs), we propose
objectives that task the discriminator for self-supervised representation
learning via additional structural modeling responsibilities. In combination
with an efficient smoothness regularizer imposed on the network, these
objectives guide the discriminator to learn to extract informative
representations, while maintaining a generator capable of sampling from the
domain. Specifically, our objectives encourage the discriminator to structure
features at two levels of granularity: aligning distribution characteristics,
such as mean and variance, at coarse scales, and grouping features into local
clusters at finer scales. Operating as a feature learner within the GAN
framework frees our self-supervised system from the reliance on hand-crafted
data augmentation schemes that are prevalent across contrastive representation
learning methods. Across CIFAR-10/100 and an ImageNet subset, experiments
demonstrate that equipping GANs with our self-supervised objectives suffices to
produce discriminators which, evaluated in terms of representation learning,
compete with networks trained by contrastive learning approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image-based Navigation in Real-World Environments via Multiple Mid-level
  Representations: Fusion Models, Benchmark and Efficient Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.01069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.01069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Rosano, Antonino Furnari, Luigi Gulino, Corrado Santoro, Giovanni Maria Farinella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating complex indoor environments requires a deep understanding of the
space the robotic agent is acting into to correctly inform the navigation
process of the agent towards the goal location. In recent learning-based
navigation approaches, the scene understanding and navigation abilities of the
agent are achieved simultaneously by collecting the required experience in
simulation. Unfortunately, even if simulators represent an efficient tool to
train navigation policies, the resulting models often fail when transferred
into the real world. One possible solution is to provide the navigation model
with mid-level visual representations containing important domain-invariant
properties of the scene. But, what are the best representations that facilitate
the transfer of a model to the real-world? How can they be combined? In this
work we address these issues by proposing a benchmark of Deep Learning
architectures to combine a range of mid-level visual representations, to
perform a PointGoal navigation task following a Reinforcement Learning setup.
All the proposed navigation models have been trained with the Habitat simulator
on a synthetic office environment and have been tested on the same real-world
environment using a real robotic platform. To efficiently assess their
performance in a real context, a validation tool has been proposed to generate
realistic navigation episodes inside the simulator. Our experiments showed that
navigation models can benefit from the multi-modal input and that our
validation tool can provide good estimation of the expected navigation
performance in the real world, while saving time and resources. The acquired
synthetic and real 3D models of the environment, together with the code of our
validation tool built on top of Habitat, are publicly available at the
following link: https://iplab.dmi.unict.it/EmbodiedVN/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for submission in Autonomous Robots</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Probabilistic Image-Text Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyuk Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-Text Matching (ITM) task, a fundamental vision-language (VL) task,
suffers from the inherent ambiguity arising from multiplicity and imperfect
annotations. Deterministic functions are not sufficiently powerful to capture
ambiguity, prompting the exploration of probabilistic embeddings to tackle the
challenge. However, the existing probabilistic ITM approach encounters two key
shortcomings; the burden of heavy computations due to the Monte Carlo
approximation, and the loss saturation issue in the face of abundant false
negatives. To overcome the issues, this paper presents an improved
Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new
probabilistic distance with a closed-form solution. In addition, two
optimization techniques are proposed to enhance PCME++ further; first, the
incorporation of pseudo-positives to prevent the loss saturation problem under
massive false negatives; second, mixed sample data augmentation for
probabilistic matching. Experimental results on MS-COCO Caption and two
extended benchmarks, CxC and ECCV Caption, demonstrate the effectiveness of
PCME++ compared to state-of-the-art ITM methods. The robustness of PCME++ is
also evaluated under noisy image-text correspondences. In addition, the
potential applicability of PCME++ in automatic prompt tuning for zero-shot
classification is shown. The code is available at
https://naver-ai.github.io/pcmepp/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/naver-ai/pcmepp. Project page:
  https://naver-ai.github.io/pcmepp/. 26 pages, 1.2 MB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel diffusion-based model, CompoDiff, for solving
Composed Image Retrieval (CIR) with latent diffusion and presents a newly
created dataset, named SynthTriplets18M, of 18 million reference images,
conditions, and corresponding target image triplets to train the model.
CompoDiff and SynthTriplets18M tackle the shortages of the previous CIR
approaches, such as poor generalizability due to the small dataset scale and
the limited types of conditions. CompoDiff not only achieves a new zero-shot
state-of-the-art on four CIR benchmarks, including FashionIQ, CIRR, CIRCO, and
GeneCIS, but also enables a more versatile and controllable CIR by accepting
various conditions, such as negative text and image mask conditions, and the
controllability to the importance between multiple queries or the trade-off
between inference speed and the performance which are unavailable with existing
CIR methods. The code and dataset are available at
https://github.com/navervision/CompoDiff
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally; 26 pages, 4.1MB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustified ANNs Reveal Wormholes Between Human Category Percepts <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Gaziv, Michael J. Lee, James J. DiCarlo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual object category reports of artificial neural networks (ANNs) are
notoriously sensitive to tiny, adversarial image perturbations. Because human
category reports (aka human percepts) are thought to be insensitive to those
same small-norm perturbations -- and locally stable in general -- this argues
that ANNs are incomplete scientific models of human visual perception.
Consistent with this, we show that when small-norm image perturbations are
generated by standard ANN models, human object category percepts are indeed
highly stable. However, in this very same "human-presumed-stable" regime, we
find that robustified ANNs reliably discover low-norm image perturbations that
strongly disrupt human percepts. These previously undetectable human perceptual
disruptions are massive in amplitude, approaching the same level of sensitivity
seen in robustified ANNs. Further, we show that robustified ANNs support
precise perceptual state interventions: they guide the construction of low-norm
image perturbations that strongly alter human category percepts toward specific
prescribed percepts. These observations suggest that for arbitrary starting
points in image space, there exists a set of nearby "wormholes", each leading
the subject from their current category perceptual state into a semantically
very different state. Moreover, contemporary ANN models of biological visual
processing are now accurate enough to consistently guide us to those portals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In NeurIPS 2023. Code: https://github.com/ggaziv/Wormholes Project
  Webpage: https://himjl.github.io/pwormholes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FG-<span class="highlight-title">NeRF</span>: Flow-GAN based Probabilistic Neural Radiance Field for
  Independence-Assumption-Free Uncertainty Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songlin Wei, Jiazhao Zhang, Yang Wang, Fanbo Xiang, Hao Su, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields with stochasticity have garnered significant interest
by enabling the sampling of plausible radiance fields and quantifying
uncertainty for downstream tasks. Existing works rely on the independence
assumption of points in the radiance field or the pixels in input views to
obtain tractable forms of the probability density function. However, this
assumption inadvertently impacts performance when dealing with intricate
geometry and texture. In this work, we propose an independence-assumption-free
probabilistic neural radiance field based on Flow-GAN. By combining the
generative capability of adversarial learning and the powerful expressivity of
normalizing flow, our method explicitly models the density-radiance
distribution of the whole scene. We represent our probabilistic NeRF as a
mean-shifted probabilistic residual neural model. Our model is trained without
an explicit likelihood function, thereby avoiding the independence assumption.
Specifically, We downsample the training images with different strides and
centers to form fixed-size patches which are used to train the generator with
patch-based adversarial learning. Through extensive experiments, our method
demonstrates state-of-the-art performance by predicting lower rendering errors
and more reliable uncertainty on both synthetic and real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MUSTANG: Multi-Stain Self-Attention Graph Multiple Instance Learning
  Pipeline for Histopathology Whole Slide Images <span class="chip">BMVC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amaya Gallagher-Syed, Luca Rossi, Felice Rivellese, Costantino Pitzalis, Myles Lewis, Michael Barnes, Gregory Slabaugh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole Slide Images (WSIs) present a challenging computer vision task due to
their gigapixel size and presence of numerous artefacts. Yet they are a
valuable resource for patient diagnosis and stratification, often representing
the gold standard for diagnostic tasks. Real-world clinical datasets tend to
come as sets of heterogeneous WSIs with labels present at the patient-level,
with poor to no annotations. Weakly supervised attention-based multiple
instance learning approaches have been developed in recent years to address
these challenges, but can fail to resolve both long and short-range
dependencies. Here we propose an end-to-end multi-stain self-attention graph
(MUSTANG) multiple instance learning pipeline, which is designed to solve a
weakly-supervised gigapixel multi-image classification task, where the label is
assigned at the patient-level, but no slide-level labels or region annotations
are available. The pipeline uses a self-attention based approach by restricting
the operations to a highly sparse k-Nearest Neighbour Graph of embedded WSI
patches based on the Euclidean distance. We show this approach achieves a
state-of-the-art F1-score/AUC of 0.89/0.92, outperforming the widely used CLAM
model. Our approach is highly modular and can easily be modified to suit
different clinical datasets, as it only requires a patient-level label without
annotations and accepts WSI sets of different sizes, as the graphs can be of
varying sizes and structures. The source code can be found at
https://github.com/AmayaGS/MUSTANG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at BMVC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expanding Small-Scale Datasets with Guided Imagination <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13976v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13976v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Daquan Zhou, Bryan Hooi, Kai Wang, Jiashi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The power of DNNs relies heavily on the quantity and quality of training
data. However, collecting and annotating data on a large scale is often
expensive and time-consuming. To address this issue, we explore a new task,
termed dataset expansion, aimed at expanding a ready-to-use small dataset by
automatically creating new labeled samples. To this end, we present a Guided
Imagination Framework (GIF) that leverages cutting-edge generative models like
DALL-E2 and Stable Diffusion (SD) to "imagine" and create informative new data
from the input seed data. Specifically, GIF conducts data imagination by
optimizing the latent features of the seed data in the semantically meaningful
space of the prior model, resulting in the creation of photo-realistic images
with new content. To guide the imagination towards creating informative samples
for model training, we introduce two key criteria, i.e., class-maintained
information boosting and sample diversity promotion. These criteria are
verified to be essential for effective dataset expansion: GIF-SD obtains 13.5%
higher model accuracy on natural image datasets than unguided expansion with
SD. With these essential criteria, GIF successfully expands small datasets in
various scenarios, boosting model accuracy by 36.9% on average over six natural
image datasets and by 13.5% on average over three medical datasets. The source
code is available at https://github.com/Vanint/DatasetExpansion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023. Source code: https://github.com/Vanint/DatasetExpansion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Anisotropic Gaussian Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Keilmann, Michael Godehardt, Ali Moghiseh, Claudia Redenbach, Katja Schladitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Elongated anisotropic Gaussian filters are used for the orientation
estimation of fibers. In cases where computed tomography images are noisy,
roughly resolved, and of low contrast, they are the method of choice even if
being efficient only in virtual 2D slices. However, minor inaccuracies in the
anisotropic Gaussian filters can carry over to the orientation estimation.
Therefore, this paper proposes a modified algorithm for 2D anisotropic Gaussian
filters and shows that this improves their precision. Applied to synthetic
images of fiber bundles, it is more accurate and robust to noise. Finally, the
effectiveness of the approach is shown by applying it to real-world images of
sheet molding compounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trimap-guided Feature Mining and Fusion Network for Natural Image
  Matting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.00510v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.00510v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Jiang, Dongdong Yu, Zhaozhi Xie, Yaoyi Li, Zehuan Yuan, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing trimap guidance and fusing multi-level features are two important
issues for trimap-based matting with pixel-level prediction. To utilize trimap
guidance, most existing approaches simply concatenate trimaps and images
together to feed a deep network or apply an extra network to extract more
trimap guidance, which meets the conflict between efficiency and effectiveness.
For emerging content-based feature fusion, most existing matting methods only
focus on local features which lack the guidance of a global feature with strong
semantic information related to the interesting object. In this paper, we
propose a trimap-guided feature mining and fusion network consisting of our
trimap-guided non-background multi-scale pooling (TMP) module and global-local
context-aware fusion (GLF) modules. Considering that trimap provides strong
semantic guidance, our TMP module focuses effective feature mining on
interesting objects under the guidance of trimap without extra parameters.
Furthermore, our GLF modules use global semantic information of interesting
objects mined by our TMP module to guide an effective global-local
context-aware multi-level feature fusion. In addition, we build a common
interesting object matting (CIOM) dataset to advance high-quality image
matting. Particularly, results on the Composition-1k and our CIOM show that our
TMFNet achieves 13% and 25% relative improvement on SAD, respectively, against
a strong baseline with fewer parameters and 14% fewer FLOPs. Experimental
results on the Composition-1k test set, Alphamatting benchmark, and our CIOM
test set demonstrate that our method outperforms state-of-the-art approaches.
Our code and models are available at
https://github.com/Serge-weihao/TMF-Matting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Computer Vision and Image Understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAMMA: Generalizable Articulation Modeling and Manipulation for
  Articulated Objects <span class="chip">ICRA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulated objects like cabinets and doors are widespread in daily life.
However, directly manipulating 3D articulated objects is challenging because
they have diverse geometrical shapes, semantic categories, and kinetic
constraints. Prior works mostly focused on recognizing and manipulating
articulated objects with specific joint types. They can either estimate the
joint parameters or distinguish suitable grasp poses to facilitate trajectory
planning. Although these approaches have succeeded in certain types of
articulated objects, they lack generalizability to unseen objects, which
significantly impedes their application in broader scenarios. In this paper, we
propose a novel framework of Generalizable Articulation Modeling and
Manipulating for Articulated Objects (GAMMA), which learns both articulation
modeling and grasp pose affordance from diverse articulated objects with
different categories. In addition, GAMMA adopts adaptive manipulation to
iteratively reduce the modeling errors and enhance manipulation performance. We
train GAMMA with the PartNet-Mobility dataset and evaluate with comprehensive
experiments in SAPIEN simulation and real-world Franka robot. Results show that
GAMMA significantly outperforms SOTA articulation modeling and manipulation
algorithms in unseen and cross-category articulated objects. We will
open-source all codes and datasets in both simulation and real robots for
reproduction in the final version. Images and videos are published on the
project website at: http://sites.google.com/view/gamma-articulation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, submitted to ICRA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion
  Data and Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Messina, Jan Sedmidubsky, Fabrizio Falchi, Tomáš Rebok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to recent advances in pose-estimation methods, human motion can be
extracted from a common video in the form of 3D skeleton sequences. Despite
wonderful application opportunities, effective and efficient content-based
access to large volumes of such spatio-temporal skeleton data still remains a
challenging problem. In this paper, we propose a novel content-based
text-to-motion retrieval task, which aims at retrieving relevant motions based
on a specified natural-language textual description. To define baselines for
this uncharted task, we employ the BERT and CLIP language representations to
encode the text modality and successful spatio-temporal models to encode the
motion modality. We additionally introduce our transformer-based approach,
called Motion Transformer (MoT), which employs divided space-time attention to
effectively aggregate the different skeleton joints in space and time. Inspired
by the recent progress in text-to-image/video matching, we experiment with two
widely-adopted metric-learning loss functions. Finally, we set up a common
evaluation protocol by defining qualitative metrics for assessing the quality
of the retrieved motions, targeting the two recently-introduced KIT
Motion-Language and HumanML3D datasets. The code for reproducing our results is
available at https://github.com/mesnico/text-to-motion-retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2023 (best short paper honorable mention)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking superpixel segmentation from biologically inspired mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        TingYu Zhao, Bo Peng, Yuan Sun, DaiPeng Yang, ZhenGuang Zhange, Xi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, advancements in deep learning-based superpixel segmentation methods
have brought about improvements in both the efficiency and the performance of
segmentation. However, a significant challenge remains in generating
superpixels that strictly adhere to object boundaries while conveying rich
visual significance, especially when cross-surface color correlations may
interfere with objects. Drawing inspiration from neural structure and visual
mechanisms, we propose a biological network architecture comprising an Enhanced
Screening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel
segmentation. The ESM enhances semantic information by simulating the
interactive projection mechanisms of the visual cortex. Additionally, the BAL
emulates the spatial frequency characteristics of visual cortical cells to
facilitate the generation of superpixels with strong boundary adherence. We
demonstrate the effectiveness of our approach through evaluations on both the
BSDS500 dataset and the NYUv2 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLOps for Scarce Image Data: A Use Case in Microscopic Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Yamachui Sitcheu, Nils Friederich, Simon Baeuerle, Oliver Neumann, Markus Reischl, Ralf Mikut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, Machine Learning (ML) is experiencing tremendous popularity that
has never been seen before. The operationalization of ML models is governed by
a set of concepts and methods referred to as Machine Learning Operations
(MLOps). Nevertheless, researchers, as well as professionals, often focus more
on the automation aspect and neglect the continuous deployment and monitoring
aspects of MLOps. As a result, there is a lack of continuous learning through
the flow of feedback from production to development, causing unexpected model
deterioration over time due to concept drifts, particularly when dealing with
scarce data. This work explores the complete application of MLOps in the
context of scarce data analysis. The paper proposes a new holistic approach to
enhance biomedical image analysis. Our method includes: a fingerprinting
process that enables selecting the best models, datasets, and model development
strategy relative to the image analysis task at hand; an automated model
development stage; and a continuous deployment and monitoring process to ensure
continuous learning. For preliminary results, we perform a proof of concept for
fingerprinting in microscopic image datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures , 33. Workshop on Computational Intelligence
  Berlin Germany</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoPET Challenge 2023: Sliding Window-based Optimization of U-Net <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Hadlich, Zdravko Marinov, Rainer Stiefelhagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tumor segmentation in medical imaging is crucial and relies on precise
delineation. Fluorodeoxyglucose Positron-Emission Tomography (FDG-PET) is
widely used in clinical practice to detect metabolically active tumors.
However, FDG-PET scans may misinterpret irregular glucose consumption in
healthy or benign tissues as cancer. Combining PET with Computed Tomography
(CT) can enhance tumor segmentation by integrating metabolic and anatomic
information. FDG-PET/CT scans are pivotal for cancer staging and reassessment,
utilizing radiolabeled fluorodeoxyglucose to highlight metabolically active
regions. Accurately distinguishing tumor-specific uptake from physiological
uptake in normal tissues is a challenging aspect of precise tumor segmentation.
The AutoPET challenge addresses this by providing a dataset of 1014 FDG-PET/CT
studies, encouraging advancements in accurate tumor segmentation and analysis
within the FDG-PET/CT domain. Code:
https://github.com/matt3o/AutoPET2-Submission/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figure, MICCAI 2023 - AutoPET Challenge Submission Version
  2: Added all results on the preliminary test set</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Concept-Based System for Local, Global, and Misclassification
  Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Aghaeipoor, Dorsa Asgarian, Mohammad Sabokrou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability of Deep Neural Networks (DNNs) has been garnering increasing
attention in recent years. Of the various explainability approaches,
concept-based techniques stand out for their ability to utilize
human-meaningful concepts instead of focusing solely on individual pixels.
However, there is a scarcity of methods that consistently provide both local
and global explanations. Moreover, most of the methods have no offer to explain
misclassification cases. Considering these challenges, we present a unified
concept-based system for unsupervised learning of both local and global
concepts. Our primary objective is to uncover the intrinsic concepts underlying
each data category by training surrogate explainer networks to estimate the
importance of the concepts. Our experimental results substantiated the efficacy
of the discovered concepts through diverse quantitative and qualitative
assessments, encompassing faithfulness, completeness, and generality.
Furthermore, our approach facilitates the explanation of both accurate and
erroneous predictions, rendering it a valuable tool for comprehending the
characteristics of the target objects and classes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-Visual Prompting for Efficient 2D Temporal Video Grounding <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04995v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04995v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of temporal video grounding (TVG), which
aims to predict the starting/ending time points of moments described by a text
sentence within a long untrimmed video. Benefiting from fine-grained 3D visual
features, the TVG techniques have achieved remarkable progress in recent years.
However, the high complexity of 3D convolutional neural networks (CNNs) makes
extracting dense 3D visual features time-consuming, which calls for intensive
memory and computing resources. Towards efficient TVG, we propose a novel
text-visual prompting (TVP) framework, which incorporates optimized
perturbation patterns (that we call 'prompts') into both visual inputs and
textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP
allows us to effectively co-train vision encoder and language encoder in a 2D
TVG model and improves the performance of crossmodal feature fusion using only
low-complexity sparse 2D visual features. Further, we propose a
Temporal-Distance IoU (TDIoU) loss for efficient learning of TVG. Experiments
on two benchmark datasets, Charades-STA and ActivityNet Captions datasets,
empirically show that the proposed TVP significantly boosts the performance of
2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on
ActivityNet Captions) and achieves 5x inference acceleration over TVG using 3D
visual features. Codes are available at Open.Intel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the CVPR 2023 and code released
  (https://github.com/intel/TVP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoGraph: Predicting Lane Graphs from Traffic Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannik Zürn, Ingmar Posner, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane graph estimation is a long-standing problem in the context of autonomous
driving. Previous works aimed at solving this problem by relying on
large-scale, hand-annotated lane graphs, introducing a data bottleneck for
training models to solve this task. To overcome this limitation, we propose to
use the motion patterns of traffic participants as lane graph annotations. In
our AutoGraph approach, we employ a pre-trained object tracker to collect the
tracklets of traffic participants such as vehicles and trucks. Based on the
location of these tracklets, we predict the successor lane graph from an
initial position using overhead RGB images only, not requiring any human
supervision. In a subsequent stage, we show how the individual successor
predictions can be aggregated into a consistent lane graph. We demonstrate the
efficacy of our approach on the UrbanLaneGraph dataset and perform extensive
quantitative and qualitative evaluations, indicating that AutoGraph is on par
with models trained on hand-annotated graph data. Model and dataset will be
made available at redacted-for-review.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning the Geodesic Embedding with Graph Neural Networks <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Pang, Zhongtian Zheng, Guoping Wang, Peng-Shuai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GeGnn, a learning-based method for computing the approximate
geodesic distance between two arbitrary points on discrete polyhedra surfaces
with constant time complexity after fast precomputation. Previous relevant
methods either focus on computing the geodesic distance between a single source
and all destinations, which has linear complexity at least or require a long
precomputation time. Our key idea is to train a graph neural network to embed
an input mesh into a high-dimensional embedding space and compute the geodesic
distance between a pair of points using the corresponding embedding vectors and
a lightweight decoding function. To facilitate the learning of the embedding,
we propose novel graph convolution and graph pooling modules that incorporate
local geodesic information and are verified to be much more effective than
previous designs. After training, our method requires only one forward pass of
the network per mesh as precomputation. Then, we can compute the geodesic
distance between a pair of points using our decoding function, which requires
only several matrix multiplications and can be massively parallelized on GPUs.
We verify the efficiency and effectiveness of our method on ShapeNet and
demonstrate that our method is faster than existing methods by orders of
magnitude while achieving comparable or better accuracy. Additionally, our
method exhibits robustness on noisy and incomplete meshes and strong
generalization ability on out-of-distribution meshes. The code and pretrained
model can be found on https://github.com/IntelligentGeometry/GeGnn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2023, Journal Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MS-PS: A Multi-Scale Network for Photometric Stereo With a New
  Comprehensive Training Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clément Hardy, Yvain Quéau, David Tschumperlé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The photometric stereo (PS) problem consists in reconstructing the 3D-surface
of an object, thanks to a set of photographs taken under different lighting
directions. In this paper, we propose a multi-scale architecture for PS which,
combined with a new dataset, yields state-of-the-art results. Our proposed
architecture is flexible: it permits to consider a variable number of images as
well as variable image size without loss of performance. In addition, we define
a set of constraints to allow the generation of a relevant synthetic dataset to
train convolutional neural networks for the PS problem. Our proposed dataset is
much larger than pre-existing ones, and contains many objects with challenging
materials having anisotropic reflectance (e.g. metals, glass). We show on
publicly available benchmarks that the combination of both these contributions
drastically improves the accuracy of the estimated normal field, in comparison
with previous state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenz Linhardt, Klaus-Robert Müller, Grégoire Montavon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable AI has become a popular tool for validating machine learning
models. Mismatches between the explained model's decision strategy and the
user's domain knowledge (e.g. Clever Hans effects) have also been recognized as
a starting point for improving faulty models. However, it is less clear what to
do when the user and the explanation agree. In this paper, we demonstrate that
acceptance of explanations by the user is not a guarantee for a machine
learning model to function well, in particular, some Clever Hans effects may
remain undetected. Such hidden flaws of the model can nevertheless be
mitigated, and we demonstrate this by contributing a new method,
Explanation-Guided Exposure Minimization (EGEM), that preemptively prunes
variations in the ML model that have not been the subject of positive
explanation feedback. Experiments on natural image data demonstrate that our
approach leads to models that strongly reduce their reliance on hidden Clever
Hans strategies, and consequently achieve higher accuracy on new data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages + supplement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06991v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06991v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zike Wu, Pan Zhou, Kenji Kawaguchi, Hanwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have been adopted across diverse fields with its
remarkable abilities in capturing intricate data distributions. In this paper,
we propose a Fast Diffusion Model (FDM) to significantly speed up DMs from a
stochastic optimization perspective for both faster training and sampling. We
first find that the diffusion process of DMs accords with the stochastic
optimization process of stochastic gradient descent (SGD) on a stochastic
time-variant problem. Then, inspired by momentum SGD that uses both gradient
and an extra momentum to achieve faster and more stable convergence than SGD,
we integrate momentum into the diffusion process of DMs. This comes with a
unique challenge of deriving the noise perturbation kernel from the
momentum-based diffusion process. To this end, we frame the process as a Damped
Oscillation system whose critically damped state -- the kernel solution --
avoids oscillation and yields a faster convergence speed of the diffusion
process. Empirical results show that our FDM can be applied to several popular
DM frameworks, e.g., VP, VE, and EDM, and reduces their training cost by about
50% with comparable image synthesis performance on CIFAR-10, FFHQ, and AFHQv2
datasets. Moreover, FDM decreases their sampling steps by about 3x to achieve
similar performance under the same samplers. The code is available at
https://github.com/sail-sg/FDM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Attention for Next Active Object @ EGO4D STA Challenge <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16066v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16066v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanket Thakur, Cigdem Beyan, Pietro Morerio, Vittorio Murino, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical report, we describe the Guided-Attention mechanism based
solution for the short-term anticipation (STA) challenge for the EGO4D
challenge. It combines the object detections, and the spatiotemporal features
extracted from video clips, enhancing the motion and contextual information,
and further decoding the object-centric and motion-centric information to
address the problem of STA in egocentric videos. For the challenge, we build
our model on top of StillFast with Guided Attention applied on fast network.
Our model obtains better performance on the validation set and also achieves
state-of-the-art (SOTA) results on the challenge test set for EGO4D Short-Term
Object Interaction Anticipation Challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Winner of CVPR@2023 Ego4D STA challenge. arXiv admin note:
  substantial text overlap with arXiv:2305.12953</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-supervised Learning of Contextualized Local Visual Embeddings <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00527v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00527v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thalles Santos Silva, Helio Pedrini, Adín Ramírez Rivera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Contextualized Local Visual Embeddings (CLoVE), a self-supervised
convolutional-based method that learns representations suited for dense
prediction tasks. CLoVE deviates from current methods and optimizes a single
loss function that operates at the level of contextualized local embeddings
learned from output feature maps of convolution neural network (CNN) encoders.
To learn contextualized embeddings, CLoVE proposes a normalized mult-head
self-attention layer that combines local features from different parts of an
image based on similarity. We extensively benchmark CLoVE's pre-trained
representations on multiple datasets. CLoVE reaches state-of-the-art
performance for CNN-based architectures in 4 dense prediction downstream tasks,
including object detection, instance segmentation, keypoint detection, and
dense pose estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print. 4th Visual Inductive Priors for Data-Efficient Deep
  Learning Workshop ICCV 2023. Code at https://github.com/sthalles/CLoVE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Next-Active Objects for Context-Aware Anticipation in
  Egocentric Videos <span class="chip">WACV'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanket Thakur, Cigdem Beyan, Pietro Morerio, Vittorio Murino, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objects are crucial for understanding human-object interactions. By
identifying the relevant objects, one can also predict potential future
interactions or actions that may occur with these objects. In this paper, we
study the problem of Short-Term Object interaction anticipation (STA) and
propose NAOGAT (Next-Active-Object Guided Anticipation Transformer), a
multi-modal end-to-end transformer network, that attends to objects in observed
frames in order to anticipate the next-active-object (NAO) and, eventually, to
guide the model to predict context-aware future actions. The task is
challenging since it requires anticipating future action along with the object
with which the action occurs and the time after which the interaction will
begin, a.k.a. the time to contact (TTC). Compared to existing video modeling
architectures for action anticipation, NAOGAT captures the relationship between
objects and the global scene context in order to predict detections for the
next active object and anticipate relevant future actions given these
detections, leveraging the objects' dynamics to improve accuracy. One of the
key strengths of our approach, in fact, is its ability to exploit the motion
dynamics of objects within a given clip, which is often ignored by other
models, and separately decoding the object-centric and motion-centric
information. Through our experiments, we show that our model outperforms
existing methods on two separate datasets, Ego4D and EpicKitchens-100 ("Unseen
Set"), as measured by several additional metrics, such as time to contact, and
next-active-object localization. The code will be available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WACV'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Co-Located Human-Human Interaction Analysis using Nonverbal Cues: A
  Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.10574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.10574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cigdem Beyan, Alessandro Vinciarelli, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated co-located human-human interaction analysis has been addressed by
the use of nonverbal communication as measurable evidence of social and
psychological phenomena. We survey the computing studies (since 2010) detecting
phenomena related to social traits (e.g., leadership, dominance, personality
traits), social roles/relations, and interaction dynamics (e.g., group
cohesion, engagement, rapport). Our target is to identify the nonverbal cues
and computational methodologies resulting in effective performance. This survey
differs from its counterparts by involving the widest spectrum of social
phenomena and interaction settings (free-standing conversations, meetings,
dyads, and crowds). We also present a comprehensive summary of the related
datasets and outline future research directions which are regarding the
implementation of artificial intelligence, dataset curation, and
privacy-preserving interaction analysis. Some major observations are: the most
often used nonverbal cue, computational method, interaction environment, and
sensing approach are speaking activity, support vector machines, and meetings
composed of 3-4 persons equipped with microphones and cameras, respectively;
multimodal features are prominently performing better; deep learning
architectures showed improved performance in overall, but there exist many
phenomena whose detection has never been implemented through deep models. We
also identified several limitations such as the lack of scalable benchmarks,
annotation reliability tests, cross-dataset experiments, and explainability
analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the author's version of the work. It is posted here for your
  personal use. Not for redistribution. The definitive version was published in
  ACM Computing Surveys, https://doi.org/10.1145/3626516</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Break-A-Scene: Extracting Multiple Concepts from a Single Image <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, Dani Lischinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image model personalization aims to introduce a user-provided concept
to the model, allowing its synthesis in diverse contexts. However, current
methods primarily focus on the case of learning a single concept from multiple
images with variations in backgrounds and poses, and struggle when adapted to a
different scenario. In this work, we introduce the task of textual scene
decomposition: given a single image of a scene that may contain several
concepts, we aim to extract a distinct text token for each concept, enabling
fine-grained control over the generated scenes. To this end, we propose
augmenting the input image with masks that indicate the presence of target
concepts. These masks can be provided by the user or generated automatically by
a pre-trained segmentation model. We then present a novel two-phase
customization process that optimizes a set of dedicated textual embeddings
(handles), as well as the model weights, striking a delicate balance between
accurately capturing the concepts and avoiding overfitting. We employ a masked
diffusion loss to enable handles to generate their assigned concepts,
complemented by a novel loss on cross-attention maps to prevent entanglement.
We also introduce union-sampling, a training strategy aimed to improve the
ability of combining multiple concepts in generated images. We use several
automatic metrics to quantitatively compare our method against several
baselines, and further affirm the results using a user study. Finally, we
showcase several applications of our method. Project page is available at:
https://omriavrahami.com/break-a-scene/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2023. Project page: at:
  https://omriavrahami.com/break-a-scene/ Video:
  https://www.youtube.com/watch?v=-9EA-BhizgM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Foundation Model for General Moving Object Segmentation in Medical
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongnuo Yan, Tong Han, Yuhao Huang, Lian Liu, Han Zhou, Jiongquan Chen, Wenlong Shi, Yan Cao, Xin Yang, Dong Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation aims to delineate the anatomical or pathological
structures of interest, playing a crucial role in clinical diagnosis. A
substantial amount of high-quality annotated data is crucial for constructing
high-precision deep segmentation models. However, medical annotation is highly
cumbersome and time-consuming, especially for medical videos or 3D volumes, due
to the huge labeling space and poor inter-frame consistency. Recently, a
fundamental task named Moving Object Segmentation (MOS) has made significant
advancements in natural images. Its objective is to delineate moving objects
from the background within image sequences, requiring only minimal annotations.
In this paper, we propose the first foundation model, named iMOS, for MOS in
medical images. Extensive experiments on a large multi-modal medical dataset
validate the effectiveness of the proposed iMOS. Specifically, with the
annotation of only a small number of images in the sequence, iMOS can achieve
satisfactory tracking and segmentation performance of moving objects throughout
the entire sequence in bi-directions. We hope that the proposed iMOS can help
accelerate the annotation speed of experts, and boost the development of
medical foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Modeling through the Semi-dual Formulation of Unbalanced
  Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemoo Choi, Jaewoong Choi, Myungjoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal Transport (OT) problem investigates a transport map that bridges two
distributions while minimizing a given cost function. In this regard, OT
between tractable prior distribution and data has been utilized for generative
modeling tasks. However, OT-based methods are susceptible to outliers and face
optimization challenges during training. In this paper, we propose a novel
generative model based on the semi-dual formulation of Unbalanced Optimal
Transport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution
matching. This approach provides better robustness against outliers, stability
during training, and faster convergence. We validate these properties
empirically through experiments. Moreover, we study the theoretical upper-bound
of divergence between distributions in UOT. Our model outperforms existing
OT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 5.80
on CelebA-HQ-256. The code is available at
\url{https://github.com/Jae-Moo/UOTM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Med-Tuning: Parameter-Efficient Transfer Learning with Fine-Grained
  Feature Enhancement for Medical Volumetric Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Jiachen Shen, Chen Chen, Jianbo Jiao, Jing Liu, Yan Zhang, Shanshan Song, Jiangyun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based medical volumetric segmentation methods either train the
model from scratch or follow the standard "pre-training then fine-tuning"
paradigm. Although fine-tuning a pre-trained model on downstream tasks can
harness its representation power, the standard full fine-tuning is costly in
terms of computation and memory footprint. In this paper, we present the study
on parameter-efficient transfer learning for medical volumetric segmentation
and propose a new framework named Med-Tuning based on intra-stage feature
enhancement and inter-stage feature interaction. Additionally, aiming at
exploiting the intrinsic global properties of Fourier Transform for
parameter-efficient transfer learning, a new adapter block namely Med-Adapter
with a well-designed Fourier Transform branch is proposed for effectively and
efficiently modeling the crucial global context for medical volumetric
segmentation. Given a large-scale pre-trained model on 2D natural images, our
method can exploit both the crucial spatial multi-scale feature and volumetric
correlations along slices for accurate segmentation. Extensive experiments on
three benchmark datasets (including CT and MRI) show that our method can
achieve better results than previous parameter-efficient transfer learning
methods on segmentation tasks, with much less tuned parameter costs. Compared
to full fine-tuning, our method reduces the finetuned model parameters by up to
4x, with even better segmentation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LanguageBind: Extending Video-Language Pretraining to N-modality by
  Language-based Semantic Alignment <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01852v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01852v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The video-language (VL) pretraining has achieved remarkable improvement in
multiple downstream tasks. However, the current VL pretraining framework is
hard to extend to multiple modalities (N modalities, N>=3) beyond vision and
language. We thus propose LanguageBind, taking the language as the bind across
different modalities because the language modality is well-explored and
contains rich semantics. Specifically, we freeze the language encoder acquired
by VL pretraining, then train encoders for other modalities with contrastive
learning. As a result, all modalities are mapped to a shared feature space,
implementing multi-modal semantic alignment. While LanguageBind ensures that we
can extend VL modalities to N modalities, we also need a high-quality dataset
with alignment data pairs centered on language. We thus propose VIDAL-10M with
Video, Infrared, Depth, Audio and their corresponding Language, naming as
VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with
complete semantics rather than truncated segments from long videos, and all the
video, depth, infrared, and audio modalities are aligned to their textual
descriptions. After pretraining on VIDAL-10M, we outperform ImageBind by 1.2%
R@1 on the MSR-VTT dataset with only 15% of the parameters in the zero-shot
video-text retrieval, validating the high quality of our dataset. Beyond this,
our LanguageBind has achieved great improvement in the zero-shot video, audio,
depth, and infrared understanding tasks. For instance, on the LLVIP and NYU-D
datasets, LanguageBind outperforms ImageBind-huge with 23.8% and 11.1% top-1
accuracy. Code address: https://github.com/PKU-YuanGroup/LanguageBind.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review as a conference paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Object-Centric Neural Scattering Functions for Free-Viewpoint
  Relighting and Scene Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06138v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06138v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong-Xing Yu, Michelle Guo, Alireza Fathi, Yen-Yu Chang, Eric Ryan Chan, Ruohan Gao, Thomas Funkhouser, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photorealistic object appearance modeling from 2D images is a constant topic
in vision and graphics. While neural implicit methods (such as Neural Radiance
Fields) have shown high-fidelity view synthesis results, they cannot relight
the captured objects. More recent neural inverse rendering approaches have
enabled object relighting, but they represent surface properties as simple
BRDFs, and therefore cannot handle translucent objects. We propose
Object-Centric Neural Scattering Functions (OSFs) for learning to reconstruct
object appearance from only images. OSFs not only support free-viewpoint object
relighting, but also can model both opaque and translucent objects. While
accurately modeling subsurface light transport for translucent objects can be
highly complex and even intractable for neural methods, OSFs learn to
approximate the radiance transfer from a distant light to an outgoing direction
at any spatial location. This approximation avoids explicitly modeling complex
subsurface scattering, making learning a neural implicit model tractable.
Experiments on real and synthetic data show that OSFs accurately reconstruct
appearances for both opaque and translucent objects, allowing faithful
free-viewpoint relighting as well as scene composition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal extension of arXiv:2012.08503 (TMLR 2023). The first two
  authors contributed equally to this work. Project page:
  https://kovenyu.com/osf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective and Parameter-Efficient Reusing Fine-Tuned Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weisen Jiang, Baijiong Lin, Han Shi, Yu Zhang, Zhenguo Li, James T. Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many pre-trained large-scale models provided online have become highly
effective in transferring to downstream tasks. At the same time, various
task-specific models fine-tuned on these pre-trained models are available
online for public use. In practice, as collecting task-specific data is
labor-intensive and fine-tuning the large pre-trained models is computationally
expensive, one can reuse task-specific finetuned models to deal with downstream
tasks. However, using a model per task causes a heavy burden on storage and
serving. Recently, many training-free and parameter-efficient methods have been
proposed for reusing multiple fine-tuned task-specific models into a single
multi-task model. However, these methods exhibit a large accuracy gap compared
with using a fine-tuned model per task. In this paper, we propose
Parameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing
Fully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task
vector into a merged model by magnitude pruning. For reusing LoRA fine-tuned
models, we propose PERU-LoRA use a lower-rank matrix to approximate the LoRA
matrix by singular value decomposition. Both PERUFFT and PERU-LoRA are
training-free. Extensive experiments conducted on computer vision and natural
language process tasks demonstrate the effectiveness and parameter-efficiency
of the proposed methods. The proposed PERU-FFT and PERU-LoRA outperform
existing reusing model methods by a large margin and achieve comparable
performance to using a fine-tuned model per task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning in Open-vocabulary Classification with Complementary
  Memory Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Zhu, Weijie Lyu, Yao Xiao, Derek Hoiem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method for flexible and efficient continual learning in
open-vocabulary image classification, drawing inspiration from the
complementary learning systems observed in human cognition. Specifically, we
propose to combine predictions from a CLIP zero-shot model and the
exemplar-based model, using the zero-shot estimated probability that a sample's
class is within the exemplar classes. We also propose a "tree probe" method, an
adaption of lazy learning principles, which enables fast learning from new
examples with competitive accuracy to batch-trained linear models. We test in
data incremental, class incremental, and task incremental settings, as well as
ability to perform flexible inference on varying subsets of zero-shot and
learned categories. Our proposed method achieves a good balance of learning
speed, target task effectiveness, and zero-shot effectiveness. Code will be
available at https://github.com/jessemelpolio/TreeProbe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalize Segment Anything Model with One Shot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Xianzheng Ma, Hao Dong, Peng Gao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by large-data pre-training, Segment Anything Model (SAM) has been
demonstrated as a powerful and promptable framework, revolutionizing the
segmentation models. Despite the generality, customizing SAM for specific
visual concepts without man-powered prompting is under explored, e.g.,
automatically segmenting your pet dog in different images. In this paper, we
propose a training-free Personalization approach for SAM, termed as PerSAM.
Given only a single image with a reference mask, PerSAM first localizes the
target concept by a location prior, and segments it within other images or
videos via three techniques: target-guided attention, target-semantic
prompting, and cascaded post-refinement. In this way, we effectively adapt SAM
for private use without any training. To further alleviate the mask ambiguity,
we present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the
entire SAM, we introduce two learnable weights for multi-scale masks, only
training 2 parameters within 10 seconds for improved performance. To
demonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for
personalized evaluation, and test our methods on video object segmentation with
competitive performance. Besides, our approach can also enhance DreamBooth to
personalize Stable Diffusion for text-to-image generation, which discards the
background disturbance for better target appearance learning. Code is released
at https://github.com/ZrrSkywalker/Personalize-SAM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/ZrrSkywalker/Personalize-SAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeformUX-Net: Exploring a 3D Foundation Backbone for Medical Image
  Segmentation with Depthwise Deformable Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Hin Lee, Quan Liu, Qi Yang, Xin Yu, Shunxing Bao, Yuankai Huo, Bennett A. Landman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of 3D ViTs to medical image segmentation has seen remarkable
strides, somewhat overshadowing the budding advancements in Convolutional
Neural Network (CNN)-based models. Large kernel depthwise convolution has
emerged as a promising technique, showcasing capabilities akin to hierarchical
transformers and facilitating an expansive effective receptive field (ERF)
vital for dense predictions. Despite this, existing core operators, ranging
from global-local attention to large kernel convolution, exhibit inherent
trade-offs and limitations (e.g., global-local range trade-off, aggregating
attentional features). We hypothesize that deformable convolution can be an
exploratory alternative to combine all advantages from the previous operators,
providing long-range dependency, adaptive spatial aggregation and computational
efficiency as a foundation backbone. In this work, we introduce 3D
DeformUX-Net, a pioneering volumetric CNN model that adeptly navigates the
shortcomings traditionally associated with ViTs and large kernel convolution.
Specifically, we revisit volumetric deformable convolution in depth-wise
setting to adapt long-range dependency with computational efficiency. Inspired
by the concepts of structural re-parameterization for convolution kernel
weights, we further generate the deformable tri-planar offsets by adapting a
parallel branch (starting from $1\times1\times1$ convolution), providing
adaptive spatial aggregation across all channels. Our empirical evaluations
reveal that the 3D DeformUX-Net consistently outperforms existing
state-of-the-art ViTs and large kernel convolution models across four
challenging public datasets, spanning various scales from organs (KiTS: 0.680
to 0.720, MSD Pancreas: 0.676 to 0.717, AMOS: 0.871 to 0.902) to vessels (e.g.,
MSD hepatic vessels: 0.635 to 0.671) in mean Dice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, the source code with our pre-trained model is available at
  this https://github.com/MASILab/deform-uxnet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust Mobile Digital-Twin Tracking via An RGBD-based
  Transformer Model and A Comprehensive Mobile Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixun Huang, Keling Yao, Seth Z. Zhao, Chuanyu Pan, Tianjian Xu, Weiyu Feng, Allen Y. Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The potential of digital-twin technology, involving the creation of precise
digital replicas of physical objects, to reshape AR experiences in 3D object
tracking and localization scenarios is significant. However, enabling robust 3D
object tracking in dynamic mobile AR environments remains a formidable
challenge. These scenarios often require a more robust pose estimator capable
of handling the inherent sensor-level measurement noise. In this paper,
recognizing the challenges of comprehensive solutions in existing literature,
we propose a transformer-based 6DoF pose estimator designed to achieve
state-of-the-art accuracy under real-world noisy data. To systematically
validate the new solution's performance against the prior art, we also
introduce a novel RGBD dataset called Digital Twin Tracking Dataset (DTTD) v2,
which is focused on digital-twin object tracking scenarios. Expanded from an
existing DTTD v1, the new dataset adds digital-twin data captured using a
cutting-edge mobile RGBD sensor suite on Apple iPhone 14 Pro, expanding the
applicability of our approach to iPhone sensor data. Through extensive
experimentation and in-depth analysis, we illustrate the effectiveness of our
methods under significant depth data errors, surpassing the performance of
existing baselines. Code is made publicly available at:
https://github.com/augcog/Robust-Digital-Twin-Tracking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Clipping: Differentially Private Deep Learning Made Easier and
  Stronger <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07136v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07136v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, George Karypis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Per-example gradient clipping is a key algorithmic step that enables
practical differential private (DP) training for deep learning models. The
choice of clipping threshold R, however, is vital for achieving high accuracy
under DP. We propose an easy-to-use replacement, called automatic clipping,
that eliminates the need to tune R for any DP optimizers, including DP-SGD,
DP-Adam, DP-LAMB and many others. The automatic variants are as private and
computationally efficient as existing DP optimizers, but require no DP-specific
hyperparameters and thus make DP training as amenable as the standard
non-private training. We give a rigorous convergence analysis of automatic
DP-SGD in the non-convex setting, showing that it can enjoy an asymptotic
convergence rate that matches the standard SGD, under a symmetric gradient
noise assumption of the per-sample gradients (commonly used in the non-DP
literature). We demonstrate on various language and vision tasks that automatic
clipping outperforms or matches the state-of-the-art, and can be easily
employed with minimal changes to existing codebases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11932v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11932v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Kasahara, Shubham Agrawal, Selim Engin, Nikhil Chavan-Dafle, Shuran Song, Volkan Isler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General scene reconstruction refers to the task of estimating the full 3D
geometry and texture of a scene containing previously unseen objects. In many
practical applications such as AR/VR, autonomous navigation, and robotics, only
a single view of the scene may be available, making the scene reconstruction
task challenging. In this paper, we present a method for scene reconstruction
by structurally breaking the problem into two steps: rendering novel views via
inpainting and 2D to 3D scene lifting. Specifically, we leverage the
generalization capability of large visual language models (Dalle-2) to inpaint
the missing areas of scene color images rendered from different views. Next, we
lift these inpainted images to 3D by predicting normals of the inpainted image
and solving for the missing depth values. By predicting for normals instead of
depth directly, our method allows for robustness to changes in depth
distributions and scale. With rigorous quantitative evaluation, we show that
our method outperforms multiple baselines while providing generalization to
novel objects and scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffeomorphic Multi-Resolution Deep Learning Registration for
  Applications in Breast MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew G. French, Gonzalo D. Maso Talou, Thiranja P. Babarenda Gamage, Martyn P. Nash, Poul M. Nielsen, Anthony J. Doyle, Juan Eugenio Iglesias, Yaël Balbastre, Sean I. Young
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In breast surgical planning, accurate registration of MR images across
patient positions has the potential to improve the localisation of tumours
during breast cancer treatment. While learning-based registration methods have
recently become the state-of-the-art approach for most medical image
registration tasks, these methods have yet to make inroads into breast image
registration due to certain difficulties-the lack of rich texture information
in breast MR images and the need for the deformations to be diffeomophic. In
this work, we propose learning strategies for breast MR image registration that
are amenable to diffeomorphic constraints, together with early experimental
results from in-silico and in-vivo experiments. One key contribution of this
work is a registration network which produces superior registration outcomes
for breast images in addition to providing diffeomorphic guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrossGET: Cross-Guided Ensemble of Tokens for Accelerating
  Vision-Language Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17455v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17455v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dachuan Shi, Chaofan Tao, Anyi Rao, Zhendong Yang, Chun Yuan, Jiaqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent vision-language models have achieved tremendous progress far beyond
what we ever expected. However, their computational costs are also dramatically
growing with rapid development, especially for the large models. It makes model
acceleration exceedingly critical in a scenario of limited resources. Although
extensively studied for unimodal models, the acceleration for multimodal
models, especially the vision-language Transformers, is relatively
under-explored. To pursue more efficient and accessible vision-language
Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided
\textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal
acceleration framework for vision-language Transformers. This framework
adaptively combines tokens through real-time, cross-modal guidance, thereby
achieving substantial acceleration while keeping high performance.
\textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and
Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and
ensemble to exploit cross-modal information effectively, only introducing
cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph
Soft Matching}. In contrast to the existing bipartite soft matching approach,
\textit{CrossGET} introduces a complete-graph soft matching policy to achieve
more reliable token-matching results while maintaining parallelizability and
high efficiency. Extensive experiments are conducted on various vision-language
tasks, including image-text retrieval, visual reasoning, image captioning, and
visual question answering. Performance on both classic multimodal architectures
and emerging multimodal LLMs demonstrate the effectiveness and versatility of
the proposed \textit{CrossGET} framework. The code will be at
\url{https://github.com/sdc17/CrossGET}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Foreground Extraction via Deep Region Competition <span class="chip">NeurIPS 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15497v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15497v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyu Yu, Sirui Xie, Xiaojian Ma, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Deep Region Competition (DRC), an algorithm designed to extract
foreground objects from images in a fully unsupervised manner. Foreground
extraction can be viewed as a special case of generic image segmentation that
focuses on identifying and disentangling objects from the background. In this
work, we rethink the foreground extraction by reconciling energy-based prior
with generative image modeling in the form of Mixture of Experts (MoE), where
we further introduce the learned pixel re-assignment as the essential inductive
bias to capture the regularities of background regions. With this modeling, the
foreground-background partition can be naturally found through
Expectation-Maximization (EM). We show that the proposed method effectively
exploits the interaction between the mixture components during the partitioning
process, which closely connects to region competition, a seminal approach for
generic image segmentation. Experiments demonstrate that DRC exhibits more
competitive performances on complex real-world data and challenging
multi-object scenes compared with prior methods. Moreover, we show empirically
that DRC can potentially generalize to novel foreground objects even from
categories unseen during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modality Cycles with Masked Conditional Diffusion for Unsupervised
  Anomaly Segmentation in MRI <span class="chip">MICCAI
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyun Liang, Harry Anthony, Felix Wagner, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised anomaly segmentation aims to detect patterns that are distinct
from any patterns processed during training, commonly called abnormal or
out-of-distribution patterns, without providing any associated manual
segmentations. Since anomalies during deployment can lead to model failure,
detecting the anomaly can enhance the reliability of models, which is valuable
in high-risk domains like medical imaging. This paper introduces Masked
Modality Cycles with Conditional Diffusion (MMCCD), a method that enables
segmentation of anomalies across diverse patterns in multimodal MRI. The method
is based on two fundamental ideas. First, we propose the use of cyclic modality
translation as a mechanism for enabling abnormality detection.
Image-translation models learn tissue-specific modality mappings, which are
characteristic of tissue physiology. Thus, these learned mappings fail to
translate tissues or image patterns that have never been encountered during
training, and the error enables their segmentation. Furthermore, we combine
image translation with a masked conditional diffusion model, which attempts to
`imagine' what tissue exists under a masked area, further exposing unknown
patterns as the generative model fails to recreate them. We evaluate our method
on a proxy task by training on healthy-looking slices of BraTS2021
multi-modality MRIs and testing on slices with tumors. We show that our method
compares favorably to previous unsupervised approaches based on image
reconstruction and denoising with autoencoders and diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Multiscale Multimodal Medical Imaging workshop in MICCAI
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SYRAC: Synthesize, Rank, and Count 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adriano D'Alessandro, Ali Mahdavi-Amiri, Ghassan Hamarneh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowd counting is a critical task in computer vision, with several important
applications. However, existing counting methods rely on labor-intensive
density map annotations, necessitating the manual localization of each
individual pedestrian. While recent efforts have attempted to alleviate the
annotation burden through weakly or semi-supervised learning, these approaches
fall short of significantly reducing the workload. We propose a novel approach
to eliminate the annotation burden by leveraging latent diffusion models to
generate synthetic data. However, these models struggle to reliably understand
object quantities, leading to noisy annotations when prompted to produce images
with a specific quantity of objects. To address this, we use latent diffusion
models to create two types of synthetic data: one by removing pedestrians from
real images, which generates ranked image pairs with a weak but reliable object
quantity signal, and the other by generating synthetic images with a
predetermined number of objects, offering a strong but noisy counting signal.
Our method utilizes the ranking image pairs for pre-training and then fits a
linear layer to the noisy synthetic images using these crowd quantity features.
We report state-of-the-art results for unsupervised crowd counting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Numerical Weather Forecasting using Convolutional-LSTM with Attention
  and Context Matcher Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.00696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.00696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selim Furkan Tekin, Arda Fazla, Suleyman Serdar Kozat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerical weather forecasting using high-resolution physical models often
requires extensive computational resources on supercomputers, which diminishes
their wide usage in most real-life applications. As a remedy, applying deep
learning methods has revealed innovative solutions within this field. To this
end, we introduce a novel deep learning architecture for forecasting
high-resolution spatio-temporal weather data. Our approach extends the
conventional encoder-decoder structure by integrating Convolutional Long-short
Term Memory and Convolutional Neural Networks. In addition, we incorporate
attention and context matcher mechanisms into the model architecture. Our
Weather Model achieves significant performance improvements compared to
baseline deep learning models, including ConvLSTM, TrajGRU, and U-Net. Our
experimental evaluation involves high-scale, real-world benchmark numerical
weather datasets, namely the ERA5 hourly dataset on pressure levels and
WeatherBench. Our results demonstrate substantial improvements in identifying
spatial and temporal correlations with attention matrices focusing on distinct
parts of the input series to model atmospheric circulations. We also compare
our model with high-resolution physical models using the benchmark metrics and
show that our Weather Model is accurate and easy to interpret.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>- In our journal submission, we removed the integration of the
  observational data section since it was not used in the experiments. Thus, we
  also removed the authors from the paper who were responsible for that
  section. - In the second version, we also performed an experiment on
  WeatherBench. We compare our results with the Physical Weather Forecasting
  Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-image Alignment for Diffusion-based Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neehar Kondapaneni, Markus Marks, Manuel Knott, Rogério Guimarães, Pietro Perona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are generative models with impressive text-to-image
synthesis capabilities and have spurred a new wave of creative methods for
classical machine learning tasks. However, the best way to harness the
perceptual knowledge of these generative models for visual tasks is still an
open question. Specifically, it is unclear how to use the prompting interface
when applying diffusion backbones to vision tasks. We find that automatically
generated captions can improve text-image alignment and significantly enhance a
model's cross-attention maps, leading to better perceptual performance. Our
approach improves upon the current SOTA in diffusion-based semantic
segmentation on ADE20K and the current overall SOTA in depth estimation on
NYUv2. Furthermore, our method generalizes to the cross-domain setting; we use
model personalization and caption modifications to align our model to the
target domain and find improvements over unaligned baselines. Our object
detection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K.
Our segmentation method, trained on Cityscapes, achieves SOTA results on Dark
Zurich-val and Nighttime Driving. Project page:
https://www.vision.caltech.edu/tadp/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.vision.caltech.edu/tadp/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-10-15T05:15:45.348852374Z">
            2023-10-15 05:15:45 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
