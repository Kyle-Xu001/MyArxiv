{"2023-10-04T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2310.03026v1","updated":"2023-10-04T17:59:49Z","published":"2023-10-04T17:59:49Z","title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous\n  Driving","summary":"  Existing learning-based autonomous driving (AD) systems face challenges in\ncomprehending high-level information, generalizing to rare events, and\nproviding interpretability. To address these problems, this work employs Large\nLanguage Models (LLMs) as a decision-making component for complex AD scenarios\nthat require human commonsense understanding. We devise cognitive pathways to\nenable comprehensive reasoning with LLMs, and develop algorithms for\ntranslating LLM decisions into actionable driving commands. Through this\napproach, LLM decisions are seamlessly integrated with low-level controllers by\nguided parameter matrix adaptation. Extensive experiments demonstrate that our\nproposed method not only consistently surpasses baseline approaches in\nsingle-vehicle tasks, but also helps handle complex driving behaviors even\nmulti-vehicle coordination, thanks to the commonsense reasoning capabilities of\nLLMs. This paper presents an initial step toward leveraging LLMs as effective\ndecision-makers for intricate AD scenarios in terms of safety, efficiency,\ngeneralizability, and interoperability. We aspire for it to serve as\ninspiration for future research in this field. Project page:\nhttps://sites.google.com/view/llm-mpc\n","authors":["Hao Sha","Yao Mu","Yuxuan Jiang","Li Chen","Chenfeng Xu","Ping Luo","Shengbo Eben Li","Masayoshi Tomizuka","Wei Zhan","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2310.03026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03023v1","updated":"2023-10-04T17:59:38Z","published":"2023-10-04T17:59:38Z","title":"Human-oriented Representation Learning for Robotic Manipulation","summary":"  Humans inherently possess generalizable visual representations that empower\nthem to efficiently explore and interact with the environments in manipulation\ntasks. We advocate that such a representation automatically arises from\nsimultaneously learning about multiple simple perceptual skills that are\ncritical for everyday scenarios (e.g., hand detection, state estimate, etc.)\nand is better suited for learning robot manipulation policies compared to\ncurrent state-of-the-art visual representations purely based on self-supervised\nobjectives. We formalize this idea through the lens of human-oriented\nmulti-task fine-tuning on top of pre-trained visual encoders, where each task\nis a perceptual skill tied to human-environment interactions. We introduce Task\nFusion Decoder as a plug-and-play embedding translator that utilizes the\nunderlying relationships among these perceptual skills to guide the\nrepresentation learning towards encoding meaningful structure for what's\nimportant for all perceptual skills, ultimately empowering learning of\ndownstream robotic manipulation tasks. Extensive experiments across a range of\nrobotic tasks and embodiments, in both simulations and real-world environments,\nshow that our Task Fusion Decoder consistently improves the representation of\nthree state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for\ndownstream manipulation policy-learning. Project page:\nhttps://sites.google.com/view/human-oriented-robot-learning\n","authors":["Mingxiao Huo","Mingyu Ding","Chenfeng Xu","Thomas Tian","Xinghao Zhu","Yao Mu","Lingfeng Sun","Masayoshi Tomizuka","Wei Zhan"],"pdf_url":"https://arxiv.org/pdf/2310.03023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10657v2","updated":"2023-10-04T17:55:01Z","published":"2023-09-19T14:39:39Z","title":"Learning Adaptive Safety for Multi-Agent Systems","summary":"  Ensuring safety in dynamic multi-agent systems is challenging due to limited\ninformation about the other agents. Control Barrier Functions (CBFs) are\nshowing promise for safety assurance but current methods make strong\nassumptions about other agents and often rely on manual tuning to balance\nsafety, feasibility, and performance. In this work, we delve into the problem\nof adaptive safe learning for multi-agent systems with CBF. We show how\nemergent behavior can be profoundly influenced by the CBF configuration,\nhighlighting the necessity for a responsive and dynamic approach to CBF design.\nWe present ASRL, a novel adaptive safe RL framework, to fully automate the\noptimization of policy and CBF coefficients, to enhance safety and long-term\nperformance through reinforcement learning. By directly interacting with the\nother agents, ASRL learns to cope with diverse agent behaviours and maintains\nthe cost violations below a desired limit. We evaluate ASRL in a multi-robot\nsystem and a competitive multi-agent racing scenario, against learning-based\nand control-theoretic approaches. We empirically demonstrate the efficacy and\nflexibility of ASRL, and assess generalization and scalability to\nout-of-distribution scenarios. Code and supplementary material are public\nonline.\n","authors":["Luigi Berducci","Shuo Yang","Rahul Mangharam","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2309.10657v2.pdf","comment":"Update with appendix"},{"id":"http://arxiv.org/abs/2310.02944v1","updated":"2023-10-04T16:24:00Z","published":"2023-10-04T16:24:00Z","title":"Adaptive Landmark Color for AUV Docking in Visually Dynamic Environments","summary":"  Autonomous Underwater Vehicles (AUVs) conduct missions underwater without the\nneed for human intervention. A docking station (DS) can extend mission times of\nan AUV by providing a location for the AUV to recharge its batteries and\nreceive updated mission information. Various methods for locating and tracking\na DS exist, but most rely on expensive acoustic sensors, or are vision-based,\nwhich is significantly affected by water quality. In this \\doctype, we present\na vision-based method that utilizes adaptive color LED markers and dynamic\ncolor filtering to maximize landmark visibility in varying water conditions.\nBoth AUV and DS utilize cameras to determine the water background color in\norder to calculate the desired marker color. No communication between AUV and\nDS is needed to determine marker color. Experiments conducted in a pool and\nlake show our method performs 10 times better than static color thresholding\nmethods as background color varies. DS detection is possible at a range of 5\nmeters in clear water with minimal false positives.\n","authors":["Corey Knutson","Zhipeng Cao","Junaed Sattar"],"pdf_url":"https://arxiv.org/pdf/2310.02944v1.pdf","comment":"Submitted to ICRA 2024 for review"},{"id":"http://arxiv.org/abs/2202.01069v2","updated":"2023-10-04T16:14:51Z","published":"2022-02-02T15:00:44Z","title":"Image-based Navigation in Real-World Environments via Multiple Mid-level\n  Representations: Fusion Models, Benchmark and Efficient Evaluation","summary":"  Navigating complex indoor environments requires a deep understanding of the\nspace the robotic agent is acting into to correctly inform the navigation\nprocess of the agent towards the goal location. In recent learning-based\nnavigation approaches, the scene understanding and navigation abilities of the\nagent are achieved simultaneously by collecting the required experience in\nsimulation. Unfortunately, even if simulators represent an efficient tool to\ntrain navigation policies, the resulting models often fail when transferred\ninto the real world. One possible solution is to provide the navigation model\nwith mid-level visual representations containing important domain-invariant\nproperties of the scene. But, what are the best representations that facilitate\nthe transfer of a model to the real-world? How can they be combined? In this\nwork we address these issues by proposing a benchmark of Deep Learning\narchitectures to combine a range of mid-level visual representations, to\nperform a PointGoal navigation task following a Reinforcement Learning setup.\nAll the proposed navigation models have been trained with the Habitat simulator\non a synthetic office environment and have been tested on the same real-world\nenvironment using a real robotic platform. To efficiently assess their\nperformance in a real context, a validation tool has been proposed to generate\nrealistic navigation episodes inside the simulator. Our experiments showed that\nnavigation models can benefit from the multi-modal input and that our\nvalidation tool can provide good estimation of the expected navigation\nperformance in the real world, while saving time and resources. The acquired\nsynthetic and real 3D models of the environment, together with the code of our\nvalidation tool built on top of Habitat, are publicly available at the\nfollowing link: https://iplab.dmi.unict.it/EmbodiedVN/\n","authors":["Marco Rosano","Antonino Furnari","Luigi Gulino","Corrado Santoro","Giovanni Maria Farinella"],"pdf_url":"https://arxiv.org/pdf/2202.01069v2.pdf","comment":"Paper accepted for submission in Autonomous Robots"},{"id":"http://arxiv.org/abs/2310.02907v1","updated":"2023-10-04T15:45:16Z","published":"2023-10-04T15:45:16Z","title":"Whole-body MPC for highly redundant legged manipulators: experimental\n  evaluation with a 37 DoF dual-arm quadruped","summary":"  Recent progress in legged locomotion has rendered quadruped manipulators a\npromising solution for performing tasks that require both mobility and\nmanipulation (\\emph{loco-manipulation}). In the real world, task specifications\nand/or environment constraints may require the quadruped manipulator to be\nequipped with \\emph{high redundancy} as well as \\emph{whole-body} motion\ncoordination capabilities. This work presents an experimental evaluation of a\nwhole-body Model Predictive Control (MPC) framework achieving real-time\nperformance on a dual-arm quadruped platform consisting of 37 actuated joints.\nTo the best of our knowledge this is the legged manipulator with the highest\nnumber of joints to be controlled with real-time whole-body MPC so far. The\ncomputational efficiency of the MPC while considering the full robot kinematics\nand the centroidal dynamics model builds upon an open-source DDP-variant solver\nand a state-of-the-art optimal control problem formulation. Differently from\nprevious works on quadruped manipulators, the MPC is directly interfaced with\nthe low-level joint impedance controllers without the need of designing an\ninstantaneous whole-body controller. The feasibility on the real hardware is\nshowcased using the CENTAURO platform for the challenging task of picking a\nheavy object from the ground. Dynamic stepping (trotting) is also showcased for\nfirst time with this robot. The results highlight the potential of replanning\nwith whole-body information in a predictive control loop.\n","authors":["Ioannis Dadiotis","Arturo Laurenzi","Nikos Tsagarakis"],"pdf_url":"https://arxiv.org/pdf/2310.02907v1.pdf","comment":"Accepted at the 2023 IEEE-RAS International Conference on Humanoid\n  Robots (Humanoids 2023)"},{"id":"http://arxiv.org/abs/2209.09134v3","updated":"2023-10-04T15:42:34Z","published":"2022-09-19T15:56:54Z","title":"Safety Index Synthesis via Sum-of-Squares Programming","summary":"  Control systems often need to satisfy strict safety requirements. Safety\nindex provides a handy way to evaluate the safety level of the system and\nderive the resulting safe control policies. However, designing safety index\nfunctions under control limits is difficult and requires a great amount of\nexpert knowledge. This paper proposes a framework for synthesizing the safety\nindex for general control systems using sum-of-squares programming. Our\napproach is to show that ensuring the non-emptiness of safe control on the safe\nset boundary is equivalent to a local manifold positiveness problem. We then\nprove that this problem is equivalent to sum-of-squares programming via the\nPositivstellensatz of algebraic geometry. We validate the proposed method on\nrobot arms with different degrees of freedom and ground vehicles. The results\nshow that the synthesized safety index guarantees safety and our method is\neffective even in high-dimensional robot systems.\n","authors":["Weiye Zhao","Tairan He","Tianhao Wei","Simin Liu","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2209.09134v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02875v1","updated":"2023-10-04T15:09:44Z","published":"2023-10-04T15:09:44Z","title":"Approximating Robot Configuration Spaces with few Convex Sets using\n  Clique Covers of Visibility Graphs","summary":"  Many computations in robotics can be dramatically accelerated if the robot\nconfiguration space is described as a collection of simple sets. For example,\nrecently developed motion planners rely on a convex decomposition of the free\nspace to design collision-free trajectories using fast convex optimization. In\nthis work, we present an efficient method for approximately covering complex\nconfiguration spaces with a small number of polytopes. The approach constructs\na visibility graph using sampling and generates a clique cover of this graph to\nfind clusters of samples that have mutual line of sight. These clusters are\nthen inflated into large, full-dimensional, polytopes. We evaluate our method\non a variety of robotic systems and show that it consistently covers larger\nportions of free configuration space, with fewer polytopes, and in a fraction\nof the time compared to previous methods.\n","authors":["Peter Werner","Alexandre Amice","Tobia Marcucci","Daniela Rus","Russ Tedrake"],"pdf_url":"https://arxiv.org/pdf/2310.02875v1.pdf","comment":"7 pages, 6 figures, under review for possible publication at ICRA\n  2024"},{"id":"http://arxiv.org/abs/2310.02843v1","updated":"2023-10-04T14:20:50Z","published":"2023-10-04T14:20:50Z","title":"Incorporating Target Vehicle Trajectories Predicted by Deep Learning\n  Into Model Predictive Controlled Vehicles","summary":"  Model Predictive Control (MPC) has been widely applied to the motion planning\nof autonomous vehicles. An MPC-controlled vehicle is required to predict its\nown trajectories in a finite prediction horizon according to its model. Beyond\nthis, the vehicle should also incorporate the prediction of the trajectory of\nits nearby vehicles, or target vehicles (TVs) into its decision-making. The\nconventional trajectory prediction methods, such as the constant-speed-based\nones, are too trivial to accurately capture the potential collision risks. In\nthis report, we propose a novel MPC-based motion planning method for an\nautonomous vehicle with a set of risk-aware constraints. These constraints\nincorporate the predicted trajectory of a TV learned using a\ndeep-learning-based method. A recurrent neural network (RNN) is used to predict\nthe TV's future trajectory based on its historical data. Then, the predicted TV\ntrajectory is incorporated into the optimization of the MPC of the ego vehicle\nto generate collision-free motion. Simulation studies are conducted to showcase\nthe prediction accuracy of the RNN model and the collision-free trajectories\ngenerated by the MPC.\n","authors":["Ni Dang","Zengjie Zhang","Jizheng Liu","Marion Leibold","Martin Buss"],"pdf_url":"https://arxiv.org/pdf/2310.02843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.00406v2","updated":"2023-10-04T14:05:26Z","published":"2023-04-30T06:43:38Z","title":"LIMOT: A Tightly-Coupled System for LiDAR-Inertial Odometry and\n  Multi-Object Tracking","summary":"  Simultaneous localization and mapping (SLAM) is critical to the\nimplementation of autonomous driving. Most LiDAR-inertial SLAM algorithms\nassume a static environment, leading to unreliable localization in dynamic\nenvironments. Moreover, the accurate tracking of moving objects is of great\nsignificance for the control and planning of autonomous vehicles. This study\nproposes LIMOT, a tightly-coupled multi-object tracking and LiDAR-inertial\nodometry system that is capable of accurately estimating the poses of both\nego-vehicle and objects. We propose a trajectory-based dynamic feature\nfiltering method, which filters out features belonging to moving objects by\nleveraging tracking results before scan-matching. Factor graph-based\noptimization is then conducted to optimize the bias of the IMU and the poses of\nboth the ego-vehicle and surrounding objects in a sliding window. Experiments\nconducted on the KITTI tracking dataset and self-collected dataset show that\nour method achieves better pose and tracking accuracy than our previous work\nDL-SLOT and other baseline methods. Our open-source implementation is available\nat https://github.com/tiev-tongji/LIMOT.\n","authors":["Zhongyang Zhu","Junqiao Zhao","Kai Huang","Xuebo Tian","Jiaye Lin","Chen Ye"],"pdf_url":"https://arxiv.org/pdf/2305.00406v2.pdf","comment":"7 pages, 5 figures. This updated version mainly refines the\n  experiments. This work has been submitted to the IEEE for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible"},{"id":"http://arxiv.org/abs/2211.13024v2","updated":"2023-10-04T13:47:03Z","published":"2022-11-23T15:27:31Z","title":"Comparison of Motion Encoding Frameworks on Human Manipulation Actions","summary":"  Movement generation, and especially generalisation to unseen situations,\nplays an important role in robotics. Different types of movement generation\nmethods exist such as spline based methods, dynamical system based methods, and\nmethods based on Gaussian mixture models (GMMs). Using a large, new dataset on\nhuman manipulations, in this paper we provide a highly detailed comparison of\nthree most widely used movement encoding and generation frameworks: dynamic\nmovement primitives (DMPs), time based Gaussian mixture regression (tbGMR) and\nstable estimator of dynamical systems (SEDS). We compare these frameworks with\nrespect to their movement encoding efficiency, reconstruction accuracy, and\nmovement generalisation capabilities. The new dataset consists of nine object\nmanipulation actions performed by 12 humans: pick and place, put on top/take\ndown, put inside/take out, hide/uncover, and push/pull with a total of 7,652\nmovement examples. Our analysis shows that for movement encoding and\nreconstruction DMPs are the most efficient framework with respect to the number\nof parameters and reconstruction accuracy if a sufficient number of kernels is\nused. In case of movement generalisation to new start- and end-point\nsituations, DMPs and task parameterized GMM (TP-GMM, movement generalisation\nframework based on tbGMR) lead to similar performance and outperform SEDS.\nFurthermore we observe that TP-GMM and SEDS suffer from inaccurate convergence\nto the end-point as compared to DMPs. These different quantitative results will\nhelp designing trajectory representations in an improved task-dependent way in\nfuture robotic applications.\n","authors":["Lennart Jahn","Florentin Wörgötter","Tomas Kulvicius"],"pdf_url":"https://arxiv.org/pdf/2211.13024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02815v1","updated":"2023-10-04T13:38:53Z","published":"2023-10-04T13:38:53Z","title":"CoBEV: Elevating Roadside 3D Object Detection with Depth and Height\n  Complementarity","summary":"  Roadside camera-driven 3D object detection is a crucial task in intelligent\ntransportation systems, which extends the perception range beyond the\nlimitations of vision-centric vehicles and enhances road safety. While previous\nstudies have limitations in using only depth or height information, we find\nboth depth and height matter and they are in fact complementary. The depth\nfeature encompasses precise geometric cues, whereas the height feature is\nprimarily focused on distinguishing between various categories of height\nintervals, essentially providing semantic context. This insight motivates the\ndevelopment of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D\nobject detection framework that integrates depth and height to construct robust\nBEV representations. In essence, CoBEV estimates each pixel's depth and height\ndistribution and lifts the camera features into 3D space for lateral fusion\nusing the newly proposed two-stage complementary feature selection (CFS)\nmodule. A BEV feature distillation framework is also seamlessly integrated to\nfurther enhance the detection accuracy from the prior knowledge of the\nfusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D\ndetection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as\nthe private Supremind-Road dataset, demonstrating that CoBEV not only achieves\nthe accuracy of the new state-of-the-art, but also significantly advances the\nrobustness of previous methods in challenging long-distance scenarios and noisy\ncamera disturbance, and enhances generalization by a large margin in\nheterologous settings with drastic changes in scene and camera parameters. For\nthe first time, the vehicle AP score of a camera model reaches 80% on\nDAIR-V2X-I in terms of easy mode. The source code will be made publicly\navailable at https://github.com/MasterHow/CoBEV.\n","authors":["Hao Shi","Chengshan Pang","Jiaming Zhang","Kailun Yang","Yuhao Wu","Huajian Ni","Yining Lin","Rainer Stiefelhagen","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02815v1.pdf","comment":"The source code will be made publicly available at\n  https://github.com/MasterHow/CoBEV"},{"id":"http://arxiv.org/abs/2309.16264v2","updated":"2023-10-04T13:16:25Z","published":"2023-09-28T08:57:14Z","title":"GAMMA: Generalizable Articulation Modeling and Manipulation for\n  Articulated Objects","summary":"  Articulated objects like cabinets and doors are widespread in daily life.\nHowever, directly manipulating 3D articulated objects is challenging because\nthey have diverse geometrical shapes, semantic categories, and kinetic\nconstraints. Prior works mostly focused on recognizing and manipulating\narticulated objects with specific joint types. They can either estimate the\njoint parameters or distinguish suitable grasp poses to facilitate trajectory\nplanning. Although these approaches have succeeded in certain types of\narticulated objects, they lack generalizability to unseen objects, which\nsignificantly impedes their application in broader scenarios. In this paper, we\npropose a novel framework of Generalizable Articulation Modeling and\nManipulating for Articulated Objects (GAMMA), which learns both articulation\nmodeling and grasp pose affordance from diverse articulated objects with\ndifferent categories. In addition, GAMMA adopts adaptive manipulation to\niteratively reduce the modeling errors and enhance manipulation performance. We\ntrain GAMMA with the PartNet-Mobility dataset and evaluate with comprehensive\nexperiments in SAPIEN simulation and real-world Franka robot. Results show that\nGAMMA significantly outperforms SOTA articulation modeling and manipulation\nalgorithms in unseen and cross-category articulated objects. We will\nopen-source all codes and datasets in both simulation and real robots for\nreproduction in the final version. Images and videos are published on the\nproject website at: http://sites.google.com/view/gamma-articulation\n","authors":["Qiaojun Yu","Junbo Wang","Wenhai Liu","Ce Hao","Liu Liu","Lin Shao","Weiming Wang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2309.16264v2.pdf","comment":"8 pages, 5 figures, submitted to ICRA 2024"},{"id":"http://arxiv.org/abs/2310.02791v1","updated":"2023-10-04T13:10:47Z","published":"2023-10-04T13:10:47Z","title":"R-LGP: A Reachability-guided Logic-geometric Programming Framework for\n  Optimal Task and Motion Planning on Mobile Manipulators","summary":"  This paper presents an optimization-based solution to task and motion\nplanning (TAMP) on mobile manipulators. Logic-geometric programming (LGP) has\nshown promising capabilities for optimally dealing with hybrid TAMP problems\nthat involve abstract and geometric constraints. However, LGP does not scale\nwell to high-dimensional systems (e.g. mobile manipulators) and can suffer from\nobstacle avoidance issues. In this work, we extend LGP with a sampling-based\nreachability graph to enable solving optimal TAMP on high-DoF mobile\nmanipulators. The proposed reachability graph can incorporate environmental\ninformation (obstacles) to provide the planner with sufficient geometric\nconstraints. This reachability-aware heuristic efficiently prunes infeasible\nsequences of actions in the continuous domain, hence, it reduces replanning by\nsecuring feasibility at the final full trajectory optimization. Our framework\nproves to be time-efficient in computing optimal and collision-free solutions,\nwhile outperforming the current state of the art on metrics of success rate,\nplanning time, path length and number of steps. We validate our framework on\nthe physical Toyota HSR robot and report comparisons on a series of mobile\nmanipulation tasks of increasing difficulty.\n","authors":["Kim Tien Ly","Valeriy Semenov","Mattia Risiglione","Wolfgang Merkt","Ioannis Havoutis"],"pdf_url":"https://arxiv.org/pdf/2310.02791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11589v2","updated":"2023-10-04T12:52:52Z","published":"2023-05-19T10:58:12Z","title":"Vision-based DRL Autonomous Driving Agent with Sim2Real Transfer","summary":"  To achieve fully autonomous driving, vehicles must be capable of continuously\nperforming various driving tasks, including lane keeping and car following,\nboth of which are fundamental and well-studied driving ones. However, previous\nstudies have mainly focused on individual tasks, and car following tasks have\ntypically relied on complete leader-follower information to attain optimal\nperformance. To address this limitation, we propose a vision-based deep\nreinforcement learning (DRL) agent that can simultaneously perform lane keeping\nand car following maneuvers. To evaluate the performance of our DRL agent, we\ncompare it with a baseline controller and use various performance metrics for\nquantitative analysis. Furthermore, we conduct a real-world evaluation to\ndemonstrate the Sim2Real transfer capability of the trained DRL agent. To the\nbest of our knowledge, our vision-based car following and lane keeping agent\nwith Sim2Real transfer capability is the first of its kind.\n","authors":["Dianzhao Li","Ostap Okhrin"],"pdf_url":"https://arxiv.org/pdf/2305.11589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.12000v2","updated":"2023-10-04T11:51:11Z","published":"2022-08-25T11:10:29Z","title":"Data-driven Predictive Tracking Control based on Koopman Operators","summary":"  Constraint handling during tracking operations is at the core of many\nreal-world control implementations and is well understood when dynamic models\nof the underlying system exist, yet becomes more challenging when data-driven\nmodels are used to describe the nonlinear system at hand. We seek to combine\nthe nonlinear modeling capabilities of a wide class of neural networks with the\nconstraint-handling guarantees of model predictive control (MPC) in a rigorous\nand online computationally tractable framework. The class of networks\nconsidered can be captured using Koopman operators, and are integrated into a\nKoopman-based tracking MPC (KTMPC) for nonlinear systems to track piecewise\nconstant references. The effect of model mismatch between original nonlinear\ndynamics and its trained Koopman linear model is handled by using a constraint\ntightening approach in the proposed tracking MPC strategy. By choosing two\nLyapunov functions, we prove that solution is recursively feasible and\ninput-to-state stable to a neighborhood of both online and offline optimal\nreachable steady outputs in the presence of bounded modeling errors under mild\nassumptions. Finally, we demonstrate the results on a numerical example, before\napplying the proposed approach to the problem of reference tracking by an\nautonomous ground vehicle.\n","authors":["Ye Wang","Yujia Yang","Ye Pu","Chris Manzie"],"pdf_url":"https://arxiv.org/pdf/2208.12000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02726v1","updated":"2023-10-04T10:58:37Z","published":"2023-10-04T10:58:37Z","title":"Optimal Collaborative Transportation for Under-Capacitated Vehicle\n  Routing Problems using Aerial Drone Swarms","summary":"  Swarms of aerial drones have recently been considered for last-mile\ndeliveries in urban logistics or automated construction. At the same time,\ncollaborative transportation of payloads by multiple drones is another\nimportant area of recent research. However, efficient coordination algorithms\nfor collaborative transportation of many payloads by many drones remain to be\nconsidered. In this work, we formulate the collaborative transportation of\npayloads by a swarm of drones as a novel, under-capacitated generalization of\nvehicle routing problems (VRP), which may also be of separate interest. In\ncontrast to standard VRP and capacitated VRP, we must additionally consider\nwaiting times for payloads lifted cooperatively by multiple drones, and the\ncorresponding coordination. Algorithmically, we provide a solution encoding\nthat avoids deadlocks and formulate an appropriate alternating minimization\nscheme to solve the problem. On the hardware side, we integrate our algorithms\nwith collision avoidance and drone controllers. The approach and the impact of\nthe system integration are successfully verified empirically, both on a swarm\nof real nano-quadcopters and for large swarms in simulation. Overall, we\nprovide a framework for collaborative transportation with aerial drone swarms,\nthat uses only as many drones as necessary for the transportation of any single\npayload.\n","authors":["Akash Kopparam Sreedhara","Deepesh Padala","Shashank Mahesh","Kai Cui","Mengguang Li","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2310.02726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02696v1","updated":"2023-10-04T10:05:34Z","published":"2023-10-04T10:05:34Z","title":"Curve Trajectory Model for Human Preferred Path Planning of Automated\n  Vehicles","summary":"  Automated driving systems are often used for lane keeping tasks. By these\nsystems, a local path is planned ahead of the vehicle. However, these paths are\noften found unnatural by human drivers. We propose a linear driver model, which\ncan calculate node points that reflect the preferences of human drivers and\nbased on these node points a human driver preferred motion path can be designed\nfor autonomous driving. The model input is the road curvature. We apply this\nmodel to a self-developed Euler-curve-based curve fitting algorithm. Through a\ncase study, we show that the model based planned path can reproduce the average\nbehavior of human curve path selection. We analyze the performance of the\nproposed model through statistical analysis that shows the validity of the\ncaptured relations.\n","authors":["Gergo Igneczi","Erno Horvath","Roland Toth","Krisztian Nyilas"],"pdf_url":"https://arxiv.org/pdf/2310.02696v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02678v1","updated":"2023-10-04T09:37:27Z","published":"2023-10-04T09:37:27Z","title":"Open Gimbal: A 3 Degrees of Freedom Open Source Sensing and Testing\n  Platform for Nano and Micro UAVs","summary":"  Testing the aerodynamics of micro- and nano-UAVs without actually flying is\nhighly challenging. To address this issue, we introduce Open Gimbal, a\nspecially designed 3 Degrees of Freedom platform that caters to the unique\nrequirements of micro- and nano-UAVs. This platform allows for unrestricted and\nfree rotational motion, enabling comprehensive experimentation and evaluation\nof these UAVs. Our approach focuses on simplicity and accessibility. We\ndeveloped an open-source, 3D printable electro-mechanical design that has\nminimal size and low complexity. This design facilitates easy replication and\ncustomization, making it widely accessible to researchers and developers.\nAddressing the challenges of sensing flight dynamics at a small scale, we have\ndevised an integrated wireless batteryless sensor subsystem. Our innovative\nsolution eliminates the need for complex wiring and instead uses wireless power\ntransfer for sensor data reception. To validate the effectiveness of open\ngimbal, we thoroughly evaluate and test its communication link and sensing\nperformance using a typical nano-quadrotor. Through comprehensive testing, we\nverify the reliability and accuracy of open gimbal in real-world scenarios.\nThese advancements provide valuable tools and insights for researchers and\ndevelopers working with mUAVs and nUAVs, contributing to the progress of this\nrapidly evolving field.\n","authors":["Suryansh Sharma","Tristan Dijkstra","R. Venkatesha Prasad"],"pdf_url":"https://arxiv.org/pdf/2310.02678v1.pdf","comment":"Link to open source repository:\n  https://doi.org/10.5281/zenodo.8052218"},{"id":"http://arxiv.org/abs/2303.17355v2","updated":"2023-10-04T09:09:48Z","published":"2023-03-30T13:11:31Z","title":"Regression and Classification Methods for Learning Sound Wave Amplitude\n  Modulation in Soft Tactile Sensing","summary":"  This paper presents a novel soft tactile skin (STS) technology operating with\nsound waves. In this innovative approach, the sound waves generated by a\nspeaker travel in channels embedded in a soft membrane and get modulated due to\na deformation of the channel when pressed by an external force and received by\na microphone at the end of the channel. The sensor leverages regression and\nclassification methods for estimating the normal force and its contact\nlocation. Our sensor can be affixed to any robot part, e.g., end effectors or\narm. We tested several regression and classifier methods to learn the relation\nbetween sound wave modulation, the applied force, and its location,\nrespectively and picked the best-performing models for force and location\npredictions. Our novel tactile sensor yields 93% of the force estimation within\n1.5 N tolerances for a range of 0-30+1 N and estimates contact locations with\nover 96% accuracy. We also demonstrated the performance of STS technology for a\nreal-time gripping force control application.\n","authors":["Vishnu Rajendran S","Willow Mandil","Simon Parsons","Amir Ghalamzan E"],"pdf_url":"https://arxiv.org/pdf/2303.17355v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02650v1","updated":"2023-10-04T08:18:30Z","published":"2023-10-04T08:18:30Z","title":"Active Visual Localization for Multi-Agent Collaboration: A Data-Driven\n  Approach","summary":"  Rather than having each newly deployed robot create its own map of its\nsurroundings, the growing availability of SLAM-enabled devices provides the\noption of simply localizing in a map of another robot or device. In cases such\nas multi-robot or human-robot collaboration, localizing all agents in the same\nmap is even necessary. However, localizing e.g. a ground robot in the map of a\ndrone or head-mounted MR headset presents unique challenges due to viewpoint\nchanges. This work investigates how active visual localization can be used to\novercome such challenges of viewpoint changes. Specifically, we focus on the\nproblem of selecting the optimal viewpoint at a given location. We compare\nexisting approaches in the literature with additional proposed baselines and\npropose a novel data-driven approach. The result demonstrates the superior\nperformance of the data-driven approach when compared to existing methods, both\nin controlled simulation experiments and real-world deployment.\n","authors":["Matthew Hanlon","Boyang Sun","Marc Pollefeys","Hermann Blum"],"pdf_url":"https://arxiv.org/pdf/2310.02650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02648v1","updated":"2023-10-04T08:13:44Z","published":"2023-10-04T08:13:44Z","title":"Long-Term Dynamic Window Approach for Kinodynamic Local Planning in\n  Static and Crowd Environments","summary":"  Local planning for a differential wheeled robot is designed to generate\nkinodynamic feasible actions that guide the robot to a goal position along the\nnavigation path while avoiding obstacles. Reactive, predictive, and\nlearning-based methods are widely used in local planning. However, few of them\ncan fit static and crowd environments while satisfying kinodynamic constraints\nsimultaneously. To solve this problem, we propose a novel local planning\nmethod. The method applies a long-term dynamic window approach to generate an\ninitial trajectory and then optimizes it with graph optimization. The method\ncan plan actions under the robot's kinodynamic constraints in real time while\nallowing the generated actions to be safer and more jitterless. Experimental\nresults show that the proposed method adapts well to crowd and static\nenvironments and outperforms most SOTA approaches.\n","authors":["Zhiqiang Jian","Songyi Zhang","Lingfeng Sun","Wei Zhan","Nanning Zheng","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2310.02648v1.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.02635v1","updated":"2023-10-04T07:56:42Z","published":"2023-10-04T07:56:42Z","title":"Foundation Reinforcement Learning: towards Embodied Generalist Agents\n  with Foundation Prior Assistance","summary":"  Recently, people have shown that large-scale pre-training from internet-scale\ndata is the key to building generalist models, as witnessed in NLP. To build\nembodied generalist agents, we and many other researchers hypothesize that such\nfoundation prior is also an indispensable component. However, it is unclear\nwhat is the proper concrete form to represent those embodied foundation priors\nand how they should be used in the downstream task. In this paper, we propose\nan intuitive and effective set of embodied priors that consist of foundation\npolicy, value, and success reward. The proposed priors are based on the\ngoal-conditioned MDP. To verify their effectiveness, we instantiate an\nactor-critic method assisted by the priors, called Foundation Actor-Critic\n(FAC). We name our framework as Foundation Reinforcement Learning (FRL), since\nit completely relies on embodied foundation priors to explore, learn and\nreinforce. The benefits of FRL are threefold. (1) Sample efficient. With\nfoundation priors, FAC learns significantly faster than traditional RL. Our\nevaluation on the Meta-World has proved that FAC can achieve 100% success rates\nfor 7/8 tasks under less than 200k frames, which outperforms the baseline\nmethod with careful manual-designed rewards under 1M frames. (2) Robust to\nnoisy priors. Our method tolerates the unavoidable noise in embodied foundation\nmodels. We show that FAC works well even under heavy noise or quantization\nerrors. (3) Minimal human intervention: FAC completely learns from the\nfoundation priors, without the need of human-specified dense reward, or\nproviding teleoperated demos. Thus, FAC can be easily scaled up. We believe our\nFRL framework could enable the future robot to autonomously explore and learn\nwithout human intervention in the physical world. In summary, our proposed FRL\nis a novel and powerful learning paradigm, towards achieving embodied\ngeneralist agents.\n","authors":["Weirui Ye","Yunsheng Zhang","Mengchen Wang","Shengjie Wang","Xianfan Gu","Pieter Abbeel","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2310.02635v1.pdf","comment":"Submitted to ICLR 2024"},{"id":"http://arxiv.org/abs/2310.02625v1","updated":"2023-10-04T07:34:21Z","published":"2023-10-04T07:34:21Z","title":"Adaptive Spatio-Temporal Voxels Based Trajectory Planning for Autonomous\n  Driving in Highway Traffic Flow","summary":"  Trajectory planning is crucial for the safe driving of autonomous vehicles in\nhighway traffic flow. Currently, some advanced trajectory planning methods\nutilize spatio-temporal voxels to construct feasible regions and then convert\ntrajectory planning into optimization problem solving based on the feasible\nregions. However, these feasible region construction methods cannot adapt to\nthe changes in dynamic environments, making them difficult to apply in complex\ntraffic flow. In this paper, we propose a trajectory planning method based on\nadaptive spatio-temporal voxels which improves the construction of feasible\nregions and trajectory optimization while maintaining the quadratic programming\nform. The method can adjust feasible regions and trajectory planning according\nto real-time traffic flow and environmental changes, realizing vehicles to\ndrive safely in complex traffic flow. The proposed method has been tested in\nboth open-loop and closed-loop environments, and the test results show that our\nmethod outperforms the current planning methods.\n","authors":["Zhiqiang Jian","Songyi Zhang","Lingfeng Sun","Wei Zhan","Masayoshi Tomizuka","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.02625v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2209.01774v2","updated":"2023-10-04T07:31:41Z","published":"2022-09-05T05:54:35Z","title":"ElasticROS: An Elastically Collaborative Robot Operation System for Fog\n  and Cloud Robotics","summary":"  Robots are integrating more huge-size models to enrich functions and improve\naccuracy, which leads to out-of-control computing pressure. And thus robots are\nencountering bottlenecks in computing power and battery capacity. Fog or cloud\nrobotics is one of the most anticipated theories to address these issues.\nApproaches of cloud robotics have developed from system-level to node-level.\nHowever, the present node-level systems are not flexible enough to dynamically\nadapt to changing conditions. To address this, we present ElasticROS, which\nevolves the present node-level systems into an algorithm-level one. ElasticROS\nis based on ROS and ROS2. For fog and cloud robotics, it is the first robot\noperating system with algorithm-level collaborative computing. ElasticROS\ndevelops elastic collaborative computing to achieve adaptability to dynamic\nconditions. The collaborative computing algorithm is the core and challenge of\nElasticROS. We abstract the problem and then propose an algorithm named\nElasAction to address. It is a dynamic action decision algorithm based on\nonline learning, which determines how robots and servers cooperate. The\nalgorithm dynamically updates parameters to adapt to changes of conditions\nwhere the robot is currently in. It achieves elastically distributing of\ncomputing tasks to robots and servers according to configurations. In addition,\nwe prove that the regret upper bound of the ElasAction is sublinear, which\nguarantees its convergence and thus enables ElasticROS to be stable in its\nelasticity. Finally, we conducted experiments with ElasticROS on common tasks\nof robotics, including SLAM, grasping and human-robot dialogue, and then\nmeasured its performances in latency, CPU usage and power consumption. The\nalgorithm-level ElasticROS performs significantly better than the present\nnode-level system.\n","authors":["Boyi Liu"],"pdf_url":"https://arxiv.org/pdf/2209.01774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02583v1","updated":"2023-10-04T05:01:47Z","published":"2023-10-04T05:01:47Z","title":"Machine Learning-Enabled Precision Position Control and Thermal\n  Regulation in Advanced Thermal Actuators","summary":"  With their unique combination of characteristics - an energy density almost\n100 times that of human muscle, and a power density of 5.3 kW/kg, similar to a\njet engine's output - Nylon artificial muscles stand out as particularly apt\nfor robotics applications. However, the necessity of integrating sensors and\ncontrollers poses a limitation to their practical usage. Here we report a\nconstant power open-loop controller based on machine learning. We show that we\ncan control the position of a nylon artificial muscle without external sensors.\nTo this end, we construct a mapping from a desired displacement trajectory to a\nrequired power using an ensemble encoder-style feed-forward neural network. The\nneural controller is carefully trained on a physics-based denoised dataset and\ncan be fine-tuned to accommodate various types of thermal artificial muscles,\nirrespective of the presence or absence of hysteresis.\n","authors":["Seyed Mo Mirvakili","Ehsan Haghighat","Douglas Sim"],"pdf_url":"https://arxiv.org/pdf/2310.02583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01866v2","updated":"2023-10-04T04:46:57Z","published":"2023-10-03T08:08:38Z","title":"Route Design in Sheepdog System--Traveling Salesman Problem Formulation\n  and Evolutionary Computation Solution--","summary":"  In this study, we consider the guidance control problem of the sheepdog\nsystem, which involves the guidance of the flock using the characteristics of\nthe sheepdog and sheep. Sheepdog systems require a strategy to guide sheep\nagents to a target value using a small number of sheepdog agents, and various\nmethods have been proposed. Previous studies have proposed a guidance control\nlaw to guide a herd of sheep reliably, but the movement distance of a sheepdog\nrequired for guidance has not been considered. Therefore, in this study, we\npropose a novel guidance algorithm in which a supposedly efficient route for\nguiding a flock of sheep is designed via Traveling Salesman Problem and\nevolutionary computation. Numerical simulations were performed to confirm\nwhether sheep flocks could be guided and controlled using the obtained guidance\nroutes. We specifically revealed that the proposed method reduces both the\nguidance failure rate and the guidance distance.\n","authors":["Wataru Imahayashi","Yusuke Tsunoda","Masaki Ogura"],"pdf_url":"https://arxiv.org/pdf/2310.01866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02573v1","updated":"2023-10-04T04:18:23Z","published":"2023-10-04T04:18:23Z","title":"Robust Collision Detection for Robots with Variable Stiffness Actuation\n  by Using MAD-CNN: Modularized-Attention-Dilated Convolutional Neural Network","summary":"  Ensuring safety is paramount in the field of collaborative robotics to\nmitigate the risks of human injury and environmental damage. Apart from\ncollision avoidance, it is crucial for robots to rapidly detect and respond to\nunexpected collisions. While several learning-based collision detection methods\nhave been introduced as alternatives to purely model-based detection\ntechniques, there is currently a lack of such methods designed for\ncollaborative robots equipped with variable stiffness actuators. Moreover,\nthere is potential for further enhancing the network's robustness and improving\nthe efficiency of data training. In this paper, we propose a new network, the\nModularized Attention-Dilated Convolutional Neural Network (MAD-CNN), for\ncollision detection in robots equipped with variable stiffness actuators. Our\nmodel incorporates a dual inductive bias mechanism and an attention module to\nenhance data efficiency and improve robustness. In particular, MAD-CNN is\ntrained using only a four-minute collision dataset focusing on the highest\nlevel of joint stiffness. Despite limited training data, MAD-CNN robustly\ndetects all collisions with minimal detection delay across various stiffness\nconditions. Moreover, it exhibits a higher level of collision sensitivity,\nwhich is beneficial for effectively handling false positives, which is a common\nissue in learning-based methods. Experimental results demonstrate that the\nproposed MAD-CNN model outperforms existing state-of-the-art models in terms of\ncollision sensitivity and robustness.\n","authors":["Zhenwei Niu","Lyes Saad Saoud","Irfan Hussain"],"pdf_url":"https://arxiv.org/pdf/2310.02573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02565v1","updated":"2023-10-04T03:55:43Z","published":"2023-10-04T03:55:43Z","title":"Improving Drumming Robot Via Attention Transformer Network","summary":"  Robotic technology has been widely used in nowadays society, which has made\ngreat progress in various fields such as agriculture, manufacturing and\nentertainment. In this paper, we focus on the topic of drumming robots in\nentertainment. To this end, we introduce an improving drumming robot that can\nautomatically complete music transcription based on the popular vision\ntransformer network based on the attention mechanism. Equipped with the\nattention transformer network, our method can efficiently handle the sequential\naudio embedding input and model their global long-range dependencies. Massive\nexperimental results demonstrate that the improving algorithm can help the\ndrumming robot promote drum classification performance, which can also help the\nrobot to enjoy a variety of smart applications and services.\n","authors":["Yang Yi","Zonghan Li"],"pdf_url":"https://arxiv.org/pdf/2310.02565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02031v2","updated":"2023-10-04T03:35:33Z","published":"2023-10-03T13:17:35Z","title":"OceanGPT: A Large Language Model for Ocean Science Tasks","summary":"  Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reason may be the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean\ndomain, which is expert in various ocean science tasks. We propose DoInstruct,\na novel framework to automatically obtain a large volume of ocean domain\ninstruction data, which generates instructions based on multi-agent\ncollaboration. Additionally, we construct the first oceanography benchmark,\nOceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though\ncomprehensive experiments, OceanGPT not only shows a higher level of knowledge\nexpertise for oceans science tasks but also gains preliminary embodied\nintelligence capabilities in ocean technology. Codes, data and checkpoints will\nsoon be available at https://github.com/zjunlp/KnowLM.\n","authors":["Zhen Bi","Ningyu Zhang","Yida Xue","Yixin Ou","Daxiong Ji","Guozhou Zheng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02031v2.pdf","comment":"Work in progress. Project Website:\n  https://zjunlp.github.io/project/OceanGPT/"},{"id":"http://arxiv.org/abs/2310.02542v1","updated":"2023-10-04T02:53:10Z","published":"2023-10-04T02:53:10Z","title":"Tightly Joining Positioning and Control for Trustworthy Unmanned Aerial\n  Vehicles Based on Factor Graph Optimization in Urban Transportation","summary":"  Unmanned aerial vehicles (UAV) showed great potential in improving the\nefficiency of parcel delivery applications in the coming smart cities era.\nUnfortunately, the trustworthy positioning and control algorithms of the UAV\nare significantly challenged in complex urban areas. For example, the\nubiquitous global navigation satellite system (GNSS) positioning can be\ndegraded by the signal reflections from surrounding high-rising buildings,\nleading to significantly increased positioning uncertainty. An additional\nchallenge is introduced to the control algorithm due to the complex wind\ndisturbances in urban canyons. Given the fact that the system positioning and\ncontrol are highly correlated with each other, for example, the system dynamics\nof the control can largely help with the positioning, this paper proposed a\njoint positioning and control method (JPCM) based on factor graph optimization\n(FGO), which combines sensors' measurements and control intention. In\nparticular, the positioning measurements are formulated as the factors in the\nfactor graph model, such as the positioning from the GNSS. The model predictive\ncontrol (MPC) is also formulated as the additional factors in the factor graph\nmodel. By solving the factor graph contributed by both the positioning factor\nand the MPC-based factors, the complementariness of positioning and control can\nbe fully explored. To guarantee reliable system dynamic parameters, we validate\nthe effectiveness of the proposed method using a simulated quadrotor system\nwhich showed significantly improved trajectory following performance. To\nbenefit the research community, we open-source our code and make it available\nat https://github.com/RoboticsPolyu/IPN_MPC.\n","authors":["Peiwen Yang","Weisong Wen"],"pdf_url":"https://arxiv.org/pdf/2310.02542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06606v2","updated":"2023-10-04T02:00:51Z","published":"2023-09-12T21:12:21Z","title":"Probabilistic Differentiable Filters Enable Ubiquitous Robot Control\n  with Smartwatches","summary":"  Ubiquitous robot control and human-robot collaboration using smart devices\nposes a challenging problem primarily due to strict accuracy requirements and\nsparse information. This paper presents a novel approach that incorporates a\nprobabilistic differentiable filter, specifically the Differentiable Ensemble\nKalman Filter (DEnKF), to facilitate robot control solely using Inertial\nMeasurement Units (IMUs) from a smartwatch and a smartphone. The implemented\nsystem is cost-effective and achieves accurate estimation of the human pose\nstate. Experiment results from human-robot handover tasks underscore that smart\ndevices allow versatile and ubiquitous robot control. The code for this paper\nis available at https://github.com/ir-lab/DEnKF and\nhttps://github.com/wearable-motion-capture.\n","authors":["Fabian C Weigend","Xiao Liu","Heni Ben Amor"],"pdf_url":"https://arxiv.org/pdf/2309.06606v2.pdf","comment":"DiffPropRob Workshop IROS 2023 (Oral)"},{"id":"http://arxiv.org/abs/2310.02506v1","updated":"2023-10-04T00:50:21Z","published":"2023-10-04T00:50:21Z","title":"Proactive Human-Robot Interaction using Visuo-Lingual Transformers","summary":"  Humans possess the innate ability to extract latent visuo-lingual cues to\ninfer context through human interaction. During collaboration, this enables\nproactive prediction of the underlying intention of a series of tasks. In\ncontrast, robotic agents collaborating with humans naively follow elementary\ninstructions to complete tasks or use specific hand-crafted triggers to\ninitiate proactive collaboration when working towards the completion of a goal.\nEndowing such robots with the ability to reason about the end goal and\nproactively suggest intermediate tasks will engender a much more intuitive\nmethod for human-robot collaboration. To this end, we propose a learning-based\nmethod that uses visual cues from the scene, lingual commands from a user and\nknowledge of prior object-object interaction to identify and proactively\npredict the underlying goal the user intends to achieve. Specifically, we\npropose ViLing-MMT, a vision-language multimodal transformer-based architecture\nthat captures inter and intra-modal dependencies to provide accurate scene\ndescriptions and proactively suggest tasks where applicable. We evaluate our\nproposed model in simulation and real-world scenarios.\n","authors":["Pranay Mathur"],"pdf_url":"https://arxiv.org/pdf/2310.02506v1.pdf","comment":"Accepted to IROS'23 Workshop: Geriatronics: AI and Robotics for\n  Health & Well-Being in Older Age and Workshop: Assistive Robotics for\n  Citizens"},{"id":"http://arxiv.org/abs/2310.03191v1","updated":"2023-10-04T22:32:35Z","published":"2023-10-04T22:32:35Z","title":"Sim-to-Real Learning for Humanoid Box Loco-Manipulation","summary":"  In this work we propose a learning-based approach to box loco-manipulation\nfor a humanoid robot. This is a particularly challenging problem due to the\nneed for whole-body coordination in order to lift boxes of varying weight,\nposition, and orientation while maintaining balance. To address this challenge,\nwe present a sim-to-real reinforcement learning approach for training general\nbox pickup and carrying skills for the bipedal robot Digit. Our reward\nfunctions are designed to produce the desired interactions with the box while\nalso valuing balance and gait quality. We combine the learned skills into a\nfull system for box loco-manipulation to achieve the task of moving boxes from\none table to another with a variety of sizes, weights, and initial\nconfigurations. In addition to quantitative simulation results, we demonstrate\nsuccessful sim-to-real transfer on the humanoid r\n","authors":["Jeremy Dao","Helei Duan","Alan Fern"],"pdf_url":"https://arxiv.org/pdf/2310.03191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03179v1","updated":"2023-10-04T21:48:35Z","published":"2023-10-04T21:48:35Z","title":"Multi-Domain Walking with Reduced-Order Models of Locomotion","summary":"  Drawing inspiration from human multi-domain walking, this work presents a\nnovel reduced-order model based framework for realizing multi-domain robotic\nwalking. At the core of our approach is the viewpoint that human walking can be\nrepresented by a hybrid dynamical system, with continuous phases that are\nfully-actuated, under-actuated, and over-actuated and discrete changes in\nactuation type occurring with changes in contact. Leveraging this perspective,\nwe synthesize a multi-domain linear inverted pendulum (MLIP) model of\nlocomotion. Utilizing the step-to-step dynamics of the MLIP model, we\nsuccessfully demonstrate multi-domain walking behaviors on the bipedal robot\nCassie -- a high degree of freedom 3D bipedal robot. Thus, we show the ability\nto bridge the gap between multi-domain reduced order models and full-order\nmulti-contact locomotion. Additionally, our results showcase the ability of the\nproposed method to achieve versatile speed-tracking performance and robust push\nrecovery behaviors.\n","authors":["Min Dai","Jaemin Lee","Aaron D. Ames"],"pdf_url":"https://arxiv.org/pdf/2310.03179v1.pdf","comment":"submitted to ACC 2024"},{"id":"http://arxiv.org/abs/2310.03172v1","updated":"2023-10-04T21:35:00Z","published":"2023-10-04T21:35:00Z","title":"Optimization and Evaluation of Multi Robot Surface Inspection Through\n  Particle Swarm Optimization","summary":"  Robot swarms can be tasked with a variety of automated sensing and inspection\napplications in aerial, aquatic, and surface environments. In this paper, we\nstudy a simplified two-outcome surface inspection task. We task a group of\nrobots to inspect and collectively classify a 2D surface section based on a\nbinary pattern projected on the surface. We use a decentralized Bayesian\ndecision-making algorithm and deploy a swarm of miniature 3-cm sized wheeled\nrobots to inspect randomized black and white tiles of $1m\\times 1m$. We first\ndescribe the model parameters that characterize our simulated environment, the\nrobot swarm, and the inspection algorithm. We then employ a noise-resistant\nheuristic optimization scheme based on the Particle Swarm Optimization (PSO)\nusing a fitness evaluation that combines decision accuracy and decision time.\nWe use our fitness measure definition to asses the optimized parameters through\n100 randomized simulations that vary surface pattern and initial robot poses.\nThe optimized algorithm parameters show up to a 55% improvement in median of\nfitness evaluations against an empirically chosen parameter set.\n","authors":["Darren Chiu","Radhika Nagpal","Bahar Haghighat"],"pdf_url":"https://arxiv.org/pdf/2310.03172v1.pdf","comment":"6 pages, 8 figures"},{"id":"http://arxiv.org/abs/2305.08262v2","updated":"2023-10-04T20:31:52Z","published":"2023-05-14T21:51:26Z","title":"A Stochastic Compound Failure Model for Testing Resilience of Autonomous\n  Fixed-Wing Aircraft I: Formulation and Simulation","summary":"  This paper presents a Markov chain model to dynamically emulate the effects\nof adverse (failure) flight conditions on fixed-wing, autonomous aircraft\nsystem actuators. It implements a PX4 Autopilot flight stack module that\nperturbs the attitude control inputs to the plane's actuator mixer. We apply\nthis approach in simulation on a fixed-wing autonomous aircraft to test the\ncontroller response to stochastic compound failures on a range of turning\nradii. Statistical measures of the differences between target and simulated\nflight paths demonstrate that a well-tuned PID controller remains competitive\nwith adaptive control in a cascading, compound, transient failure regime.\n","authors":["Thelonious Cooper","Sai Ravela"],"pdf_url":"https://arxiv.org/pdf/2305.08262v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2309.01807v2","updated":"2023-10-04T20:17:46Z","published":"2023-09-04T20:52:04Z","title":"Marginalized Importance Sampling for Off-Environment Policy Evaluation","summary":"  Reinforcement Learning (RL) methods are typically sample-inefficient, making\nit challenging to train and deploy RL-policies in real world robots. Even a\nrobust policy trained in simulation requires a real-world deployment to assess\ntheir performance. This paper proposes a new approach to evaluate the\nreal-world performance of agent policies prior to deploying them in the real\nworld. Our approach incorporates a simulator along with real-world offline data\nto evaluate the performance of any policy using the framework of Marginalized\nImportance Sampling (MIS). Existing MIS methods face two challenges: (1) large\ndensity ratios that deviate from a reasonable range and (2) indirect\nsupervision, where the ratio needs to be inferred indirectly, thus exacerbating\nestimation error. Our approach addresses these challenges by introducing the\ntarget policy's occupancy in the simulator as an intermediate variable and\nlearning the density ratio as the product of two terms that can be learned\nseparately. The first term is learned with direct supervision and the second\nterm has a small magnitude, thus making it computationally efficient. We\nanalyze the sample complexity as well as error propagation of our two\nstep-procedure. Furthermore, we empirically evaluate our approach on Sim2Sim\nenvironments such as Cartpole, Reacher, and Half-Cheetah. Our results show that\nour method generalizes well across a variety of Sim2Sim gap, target policies\nand offline data collection policies. We also demonstrate the performance of\nour algorithm on a Sim2Real task of validating the performance of a 7 DoF\nrobotic arm using offline data along with the Gazebo simulator.\n","authors":["Pulkit Katdare","Nan Jiang","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2309.01807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03137v1","updated":"2023-10-04T19:57:52Z","published":"2023-10-04T19:57:52Z","title":"Speech-Based Human-Exoskeleton Interaction for Lower Limb Motion\n  Planning","summary":"  This study presents a speech-based motion planning strategy (SBMP) developed\nfor lower limb exoskeletons to facilitate safe and compliant human-robot\ninteraction. A speech processing system, finite state machine, and central\npattern generator are the building blocks of the proposed strategy for online\nplanning of the exoskeleton's trajectory. According to experimental\nevaluations, this speech-processing system achieved low levels of word and\nintent errors. Regarding locomotion, the completion time for users with voice\ncommands was 54% faster than that using a mobile app interface. With the\nproposed SBMP, users are able to maintain their postural stability with both\nhands-free. This supports its use as an effective motion planning method for\nthe assistance and rehabilitation of individuals with lower-limb impairments.\n","authors":["Eddie Guo","Christopher Perlette","Mojtaba Sharifi","Lukas Grasse","Matthew Tata","Vivian K. Mushahwar","Mahdi Tavakoli"],"pdf_url":"https://arxiv.org/pdf/2310.03137v1.pdf","comment":"13 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2310.01791v2","updated":"2023-10-04T19:51:56Z","published":"2023-10-03T04:40:38Z","title":"Online POMDP Planning with Anytime Deterministic Guarantees","summary":"  Autonomous agents operating in real-world scenarios frequently encounter\nuncertainty and make decisions based on incomplete information. Planning under\nuncertainty can be mathematically formalized using partially observable Markov\ndecision processes (POMDPs). However, finding an optimal plan for POMDPs can be\ncomputationally expensive and is feasible only for small tasks. In recent\nyears, approximate algorithms, such as tree search and sample-based\nmethodologies, have emerged as state-of-the-art POMDP solvers for larger\nproblems. Despite their effectiveness, these algorithms offer only\nprobabilistic and often asymptotic guarantees toward the optimal solution due\nto their dependence on sampling. To address these limitations, we derive a\ndeterministic relationship between a simplified solution that is easier to\nobtain and the theoretically optimal one. First, we derive bounds for selecting\na subset of the observations to branch from while computing a complete belief\nat each posterior node. Then, since a complete belief update may be\ncomputationally demanding, we extend the bounds to support reduction of both\nthe state and the observation spaces. We demonstrate how our guarantees can be\nintegrated with existing state-of-the-art solvers that sample a subset of\nstates and observations. As a result, the returned solution holds deterministic\nbounds relative to the optimal policy. Lastly, we substantiate our findings\nwith supporting experimental results.\n","authors":["Moran Barenboim","Vadim Indelman"],"pdf_url":"https://arxiv.org/pdf/2310.01791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03132v1","updated":"2023-10-04T19:44:45Z","published":"2023-10-04T19:44:45Z","title":"Application-Oriented Co-Design of Motors and Motions for a 6DOF Robot\n  Manipulator","summary":"  This work investigates an application-driven co-design problem where the\nmotion and motors of a six degrees of freedom robotic manipulator are optimized\nsimultaneously, and the application is characterized by a set of tasks. Unlike\nthe state-of-the-art which selects motors from a product catalogue and performs\nco-design for a single task, this work designs the motor geometry as well as\nmotion for a specific application. Contributions are made towards solving the\nproposed co-design problem in a computationally-efficient manner. First, a\ntwo-step process is proposed, where multiple motor designs are identified by\noptimizing motions and motors for multiple tasks one by one, and then are\nreconciled to determine the final motor design. Second, magnetic equivalent\ncircuit modeling is exploited to establish the analytic mapping from motor\ndesign parameters to dynamic models and objective functions to facilitate the\nsubsequent differentiable simulation. Third, a direct-collocation-based\ndifferentiable simulator of motor and robotic arm dynamics is developed to\nbalance the computational complexity and numerical stability. Simulation\nverifies that higher performance for a specific application can be achieved\nwith the multi-task method, compared to several benchmark co-design methods.\n","authors":["Adrian Stein","Yebin Wang","Yusuke Sakamoto","Bingnan Wang","Huazhen Fang"],"pdf_url":"https://arxiv.org/pdf/2310.03132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06125v2","updated":"2023-10-04T19:05:58Z","published":"2023-07-12T12:25:33Z","title":"Learning Hierarchical Interactive Multi-Object Search for Mobile\n  Manipulation","summary":"  Existing object-search approaches enable robots to search through free\npathways, however, robots operating in unstructured human-centered environments\nfrequently also have to manipulate the environment to their needs. In this\nwork, we introduce a novel interactive multi-object search task in which a\nrobot has to open doors to navigate rooms and search inside cabinets and\ndrawers to find target objects. These new challenges require combining\nmanipulation and navigation skills in unexplored environments. We present\nHIMOS, a hierarchical reinforcement learning approach that learns to compose\nexploration, navigation, and manipulation skills. To achieve this, we design an\nabstract high-level action space around a semantic map memory and leverage the\nexplored environment as instance navigation points. We perform extensive\nexperiments in simulation and the real world that demonstrate that, with\naccurate perception, the decision making of HIMOS effectively transfers to new\nenvironments in a zero-shot manner. It shows robustness to unseen subpolicies,\nfailures in their execution, and different robot kinematics. These capabilities\nopen the door to a wide range of downstream tasks across embodied AI and\nreal-world use cases.\n","authors":["Fabian Schmalstieg","Daniel Honerkamp","Tim Welschehold","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2307.06125v2.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.02907v1","updated":"2023-10-04T15:45:16Z","published":"2023-10-04T15:45:16Z","title":"Whole-body MPC for highly redundant legged manipulators: experimental\n  evaluation with a 37 DoF dual-arm quadruped","summary":"  Recent progress in legged locomotion has rendered quadruped manipulators a\npromising solution for performing tasks that require both mobility and\nmanipulation (loco-manipulation). In the real world, task specifications and/or\nenvironment constraints may require the quadruped manipulator to be equipped\nwith high redundancy as well as whole-body motion coordination capabilities.\nThis work presents an experimental evaluation of a whole-body Model Predictive\nControl (MPC) framework achieving real-time performance on a dual-arm quadruped\nplatform consisting of 37 actuated joints. To the best of our knowledge this is\nthe legged manipulator with the highest number of joints to be controlled with\nreal-time whole-body MPC so far. The computational efficiency of the MPC while\nconsidering the full robot kinematics and the centroidal dynamics model builds\nupon an open-source DDP-variant solver and a state-of-the-art optimal control\nproblem formulation. Differently from previous works on quadruped manipulators,\nthe MPC is directly interfaced with the low-level joint impedance controllers\nwithout the need of designing an instantaneous whole-body controller. The\nfeasibility on the real hardware is showcased using the CENTAURO platform for\nthe challenging task of picking a heavy object from the ground. Dynamic\nstepping (trotting) is also showcased for first time with this robot. The\nresults highlight the potential of replanning with whole-body information in a\npredictive control loop.\n","authors":["Ioannis Dadiotis","Arturo Laurenzi","Nikos Tsagarakis"],"pdf_url":"https://arxiv.org/pdf/2310.02907v1.pdf","comment":"Accepted at the 2023 IEEE-RAS International Conference on Humanoid\n  Robots (Humanoids 2023)"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.03026v1","updated":"2023-10-04T17:59:49Z","published":"2023-10-04T17:59:49Z","title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous\n  Driving","summary":"  Existing learning-based autonomous driving (AD) systems face challenges in\ncomprehending high-level information, generalizing to rare events, and\nproviding interpretability. To address these problems, this work employs Large\nLanguage Models (LLMs) as a decision-making component for complex AD scenarios\nthat require human commonsense understanding. We devise cognitive pathways to\nenable comprehensive reasoning with LLMs, and develop algorithms for\ntranslating LLM decisions into actionable driving commands. Through this\napproach, LLM decisions are seamlessly integrated with low-level controllers by\nguided parameter matrix adaptation. Extensive experiments demonstrate that our\nproposed method not only consistently surpasses baseline approaches in\nsingle-vehicle tasks, but also helps handle complex driving behaviors even\nmulti-vehicle coordination, thanks to the commonsense reasoning capabilities of\nLLMs. This paper presents an initial step toward leveraging LLMs as effective\ndecision-makers for intricate AD scenarios in terms of safety, efficiency,\ngeneralizability, and interoperability. We aspire for it to serve as\ninspiration for future research in this field. Project page:\nhttps://sites.google.com/view/llm-mpc\n","authors":["Hao Sha","Yao Mu","Yuxuan Jiang","Li Chen","Chenfeng Xu","Ping Luo","Shengbo Eben Li","Masayoshi Tomizuka","Wei Zhan","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2310.03026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03023v1","updated":"2023-10-04T17:59:38Z","published":"2023-10-04T17:59:38Z","title":"Human-oriented Representation Learning for Robotic Manipulation","summary":"  Humans inherently possess generalizable visual representations that empower\nthem to efficiently explore and interact with the environments in manipulation\ntasks. We advocate that such a representation automatically arises from\nsimultaneously learning about multiple simple perceptual skills that are\ncritical for everyday scenarios (e.g., hand detection, state estimate, etc.)\nand is better suited for learning robot manipulation policies compared to\ncurrent state-of-the-art visual representations purely based on self-supervised\nobjectives. We formalize this idea through the lens of human-oriented\nmulti-task fine-tuning on top of pre-trained visual encoders, where each task\nis a perceptual skill tied to human-environment interactions. We introduce Task\nFusion Decoder as a plug-and-play embedding translator that utilizes the\nunderlying relationships among these perceptual skills to guide the\nrepresentation learning towards encoding meaningful structure for what's\nimportant for all perceptual skills, ultimately empowering learning of\ndownstream robotic manipulation tasks. Extensive experiments across a range of\nrobotic tasks and embodiments, in both simulations and real-world environments,\nshow that our Task Fusion Decoder consistently improves the representation of\nthree state-of-the-art visual encoders including R3M, MVP, and EgoVLP, for\ndownstream manipulation policy-learning. Project page:\nhttps://sites.google.com/view/human-oriented-robot-learning\n","authors":["Mingxiao Huo","Mingyu Ding","Chenfeng Xu","Thomas Tian","Xinghao Zhu","Yao Mu","Lingfeng Sun","Masayoshi Tomizuka","Wei Zhan"],"pdf_url":"https://arxiv.org/pdf/2310.03023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03020v1","updated":"2023-10-04T17:58:57Z","published":"2023-10-04T17:58:57Z","title":"Consistent-1-to-3: Consistent Image to 3D View Synthesis via\n  Geometry-aware Diffusion Models","summary":"  Zero-shot novel view synthesis (NVS) from a single image is an essential\nproblem in 3D object understanding. While recent approaches that leverage\npre-trained generative models can synthesize high-quality novel views from\nin-the-wild inputs, they still struggle to maintain 3D consistency across\ndifferent views. In this paper, we present Consistent-1-to-3, which is a\ngenerative framework that significantly mitigate this issue. Specifically, we\ndecompose the NVS task into two stages: (i) transforming observed regions to a\nnovel view, and (ii) hallucinating unseen regions. We design a scene\nrepresentation transformer and view-conditioned diffusion model for performing\nthese two stages respectively. Inside the models, to enforce 3D consistency, we\npropose to employ epipolor-guided attention to incorporate geometry\nconstraints, and multi-view attention to better aggregate multi-view\ninformation. Finally, we design a hierarchy generation paradigm to generate\nlong sequences of consistent views, allowing a full 360 observation of the\nprovided object image. Qualitative and quantitative evaluation over multiple\ndatasets demonstrate the effectiveness of the proposed mechanisms against\nstate-of-the-art approaches. Our project page is at\nhttps://jianglongye.com/consistent123/\n","authors":["Jianglong Ye","Peng Wang","Kejie Li","Yichun Shi","Heng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.03020v1.pdf","comment":"Project page: https://jianglongye.com/consistent123/"},{"id":"http://arxiv.org/abs/2310.03015v1","updated":"2023-10-04T17:57:07Z","published":"2023-10-04T17:57:07Z","title":"Efficient-3DiM: Learning a Generalizable Single-image Novel-view\n  Synthesizer in One Day","summary":"  The task of novel view synthesis aims to generate unseen perspectives of an\nobject or scene from a limited set of input images. Nevertheless, synthesizing\nnovel views from a single image still remains a significant challenge in the\nrealm of computer vision. Previous approaches tackle this problem by adopting\nmesh prediction, multi-plain image construction, or more advanced techniques\nsuch as neural radiance fields. Recently, a pre-trained diffusion model that is\nspecifically designed for 2D image synthesis has demonstrated its capability in\nproducing photorealistic novel views, if sufficiently optimized on a 3D\nfinetuning task. Although the fidelity and generalizability are greatly\nimproved, training such a powerful diffusion model requires a vast volume of\ntraining data and model parameters, resulting in a notoriously long time and\nhigh computational costs. To tackle this issue, we propose Efficient-3DiM, a\nsimple but effective framework to learn a single-image novel-view synthesizer.\nMotivated by our in-depth analysis of the inference process of diffusion\nmodels, we propose several pragmatic strategies to reduce the training overhead\nto a manageable scale, including a crafted timestep sampling strategy, a\nsuperior 3D feature extractor, and an enhanced training scheme. When combined,\nour framework is able to reduce the total training time from 10 days to less\nthan 1 day, significantly accelerating the training process under the same\ncomputational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive\nexperiments are conducted to demonstrate the efficiency and generalizability of\nour proposed method.\n","authors":["Yifan Jiang","Hao Tang","Jen-Hao Rick Chang","Liangchen Song","Zhangyang Wang","Liangliang Cao"],"pdf_url":"https://arxiv.org/pdf/2310.03015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03007v1","updated":"2023-10-04T17:51:02Z","published":"2023-10-04T17:51:02Z","title":"Towards Domain-Specific Features Disentanglement for Domain\n  Generalization","summary":"  Distributional shift between domains poses great challenges to modern machine\nlearning algorithms. The domain generalization (DG) signifies a popular line\ntargeting this issue, where these methods intend to uncover universal patterns\nacross disparate distributions. Noted, the crucial challenge behind DG is the\nexistence of irrelevant domain features, and most prior works overlook this\ninformation. Motivated by this, we propose a novel contrastive-based\ndisentanglement method CDDG, to effectively utilize the disentangled features\nto exploit the over-looked domain-specific features, and thus facilitating the\nextraction of the desired cross-domain category features for DG tasks.\nSpecifically, CDDG learns to decouple inherent mutually exclusive features by\nleveraging them in the latent space, thus making the learning discriminative.\nExtensive experiments conducted on various benchmark datasets demonstrate the\nsuperiority of our method compared to other state-of-the-art approaches.\nFurthermore, visualization evaluations confirm the potential of our method in\nachieving effective feature disentanglement.\n","authors":["Hao Chen","Qi Zhang","Zenan Huang","Haobo Wang","Junbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.03007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03006v1","updated":"2023-10-04T17:49:48Z","published":"2023-10-04T17:49:48Z","title":"COOLer: Class-Incremental Learning for Appearance-Based Multiple Object\n  Tracking","summary":"  Continual learning allows a model to learn multiple tasks sequentially while\nretaining the old knowledge without the training data of the preceding tasks.\nThis paper extends the scope of continual learning research to\nclass-incremental learning for \\ac{mot}, which is desirable to accommodate the\ncontinuously evolving needs of autonomous systems. Previous solutions for\ncontinual learning of object detectors do not address the data association\nstage of appearance-based trackers, leading to catastrophic forgetting of\nprevious classes' re-identification features. We introduce COOLer, a\nCOntrastive- and cOntinual-Learning-based tracker, which incrementally learns\nto track new categories while preserving past knowledge by training on a\ncombination of currently available ground truth labels and pseudo-labels\ngenerated by the past tracker. To further exacerbate the disentanglement of\ninstance representations, we introduce a novel contrastive class-incremental\ninstance representation learning technique. Finally, we propose a practical\nevaluation protocol for continual learning for MOT and conduct experiments on\nthe \\bdd and \\shift datasets. Experimental results demonstrate that COOLer\ncontinually learns while effectively addressing catastrophic forgetting of both\ntracking and detection. The code is available at\n\\url{https://github.com/BoSmallEar/COOLer}.\n","authors":["Zhizheng Liu","Mattia Segu","Fisher Yu"],"pdf_url":"https://arxiv.org/pdf/2310.03006v1.pdf","comment":"GCPR 2023 Oral"},{"id":"http://arxiv.org/abs/2310.03005v1","updated":"2023-10-04T17:48:23Z","published":"2023-10-04T17:48:23Z","title":"Reversing Deep Face Embeddings with Probable Privacy Protection","summary":"  Generally, privacy-enhancing face recognition systems are designed to offer\npermanent protection of face embeddings. Recently, so-called soft-biometric\nprivacy-enhancement approaches have been introduced with the aim of canceling\nsoft-biometric attributes. These methods limit the amount of soft-biometric\ninformation (gender or skin-colour) that can be inferred from face embeddings.\nPrevious work has underlined the need for research into rigorous evaluations\nand standardised evaluation protocols when assessing privacy protection\ncapabilities. Motivated by this fact, this paper explores to what extent the\nnon-invertibility requirement can be met by methods that claim to provide\nsoft-biometric privacy protection. Additionally, a detailed vulnerability\nassessment of state-of-the-art face embedding extractors is analysed in terms\nof the transformation complexity used for privacy protection. In this context,\na well-known state-of-the-art face image reconstruction approach has been\nevaluated on protected face embeddings to break soft biometric privacy\nprotection. Experimental results show that biometric privacy-enhanced face\nembeddings can be reconstructed with an accuracy of up to approximately 98%,\ndepending on the complexity of the protection algorithm.\n","authors":["Daile Osorio-Roig","Paul A. Gerlitz","Christian Rathgeb","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2310.03005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03004v1","updated":"2023-10-04T17:45:14Z","published":"2023-10-04T17:45:14Z","title":"Soft Convex Quantization: Revisiting Vector Quantization with Convex\n  Optimization","summary":"  Vector Quantization (VQ) is a well-known technique in deep learning for\nextracting informative discrete latent representations. VQ-embedded models have\nshown impressive results in a range of applications including image and speech\ngeneration. VQ operates as a parametric K-means algorithm that quantizes inputs\nusing a single codebook vector in the forward pass. While powerful, this\ntechnique faces practical challenges including codebook collapse,\nnon-differentiability and lossy compression. To mitigate the aforementioned\nissues, we propose Soft Convex Quantization (SCQ) as a direct substitute for\nVQ. SCQ works like a differentiable convex optimization (DCO) layer: in the\nforward pass, we solve for the optimal convex combination of codebook vectors\nthat quantize the inputs. In the backward pass, we leverage differentiability\nthrough the optimality conditions of the forward solution. We then introduce a\nscalable relaxation of the SCQ optimization and demonstrate its efficacy on the\nCIFAR-10, GTSRB and LSUN datasets. We train powerful SCQ autoencoder models\nthat significantly outperform matched VQ-based architectures, observing an\norder of magnitude better image reconstruction and codebook usage with\ncomparable quantization runtime.\n","authors":["Tanmay Gautam","Reid Pryzant","Ziyi Yang","Chenguang Zhu","Somayeh Sojoudi"],"pdf_url":"https://arxiv.org/pdf/2310.03004v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.02998v1","updated":"2023-10-04T17:34:00Z","published":"2023-10-04T17:34:00Z","title":"ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language\n  Models","summary":"  Large Vision-Language Models (LVLMs) can understand the world comprehensively\nby integrating rich information from different modalities, achieving remarkable\nperformance improvements on various multimodal downstream tasks. However,\ndeploying LVLMs is often problematic due to their massive computational/energy\ncosts and carbon consumption. Such issues make it infeasible to adopt\nconventional iterative global pruning, which is costly due to computing the\nHessian matrix of the entire large model for sparsification. Alternatively,\nseveral studies have recently proposed layer-wise pruning approaches to avoid\nthe expensive computation of global pruning and efficiently compress model\nweights according to their importance within a layer. However, these methods\noften suffer from suboptimal model compression due to their lack of a global\nperspective. To address this limitation in recent efficient pruning methods for\nlarge models, we propose Efficient Coarse-to-Fine Layer-Wise Pruning (ECoFLaP),\na two-stage coarse-to-fine weight pruning approach for LVLMs. We first\ndetermine the sparsity ratios of different layers or blocks by leveraging the\nglobal importance score, which is efficiently computed based on the\nzeroth-order approximation of the global model gradients. Then, the multimodal\nmodel performs local layer-wise unstructured weight pruning based on\nglobally-informed sparsity ratios. We validate our proposed method across\nvarious multimodal and unimodal models and datasets, demonstrating significant\nperformance improvements over prevalent pruning techniques in the high-sparsity\nregime.\n","authors":["Yi-Lin Sung","Jaehong Yoon","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2310.02998v1.pdf","comment":"Project page: https://ecoflap.github.io/"},{"id":"http://arxiv.org/abs/2310.02997v1","updated":"2023-10-04T17:32:32Z","published":"2023-10-04T17:32:32Z","title":"Optimizing Key-Selection for Face-based One-Time Biometrics via Morphing","summary":"  Nowadays, facial recognition systems are still vulnerable to adversarial\nattacks. These attacks vary from simple perturbations of the input image to\nmodifying the parameters of the recognition model to impersonate an authorised\nsubject. So-called privacy-enhancing facial recognition systems have been\nmostly developed to provide protection of stored biometric reference data, i.e.\ntemplates. In the literature, privacy-enhancing facial recognition approaches\nhave focused solely on conventional security threats at the template level,\nignoring the growing concern related to adversarial attacks. Up to now, few\nworks have provided mechanisms to protect face recognition against adversarial\nattacks while maintaining high security at the template level. In this paper,\nwe propose different key selection strategies to improve the security of a\ncompetitive cancelable scheme operating at the signal level. Experimental\nresults show that certain strategies based on signal-level key selection can\nlead to complete blocking of the adversarial attack based on an iterative\noptimization for the most secure threshold, while for the most practical\nthreshold, the attack success chance can be decreased to approximately 5.0%.\n","authors":["Daile Osorio-Roig","Mahdi Ghafourian","Christian Rathgeb","Ruben Vera-Rodriguez","Christoph Busch","Julian Fierrez"],"pdf_url":"https://arxiv.org/pdf/2310.02997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02992v1","updated":"2023-10-04T17:28:44Z","published":"2023-10-04T17:28:44Z","title":"Kosmos-G: Generating Images in Context with Multimodal Large Language\n  Models","summary":"  Recent advancements in text-to-image (T2I) and vision-language-to-image\n(VL2I) generation have made significant strides. However, the generation from\ngeneralized vision-language inputs, especially involving multiple images,\nremains under-explored. This paper presents Kosmos-G, a model that leverages\nthe advanced perception capabilities of Multimodal Large Language Models\n(MLLMs) to tackle the aforementioned challenge. Our approach aligns the output\nspace of MLLM with CLIP using the textual modality as an anchor and performs\ncompositional instruction tuning on curated data. Kosmos-G demonstrates a\nunique capability of zero-shot multi-entity subject-driven generation. Notably,\nthe score distillation instruction tuning requires no modifications to the\nimage decoder. This allows for a seamless substitution of CLIP and effortless\nintegration with a myriad of U-Net techniques ranging from fine-grained\ncontrols to personalized image decoder variants. We posit Kosmos-G as an\ninitial attempt towards the goal of \"image as a foreign language in image\ngeneration.\"\n","authors":["Xichen Pan","Li Dong","Shaohan Huang","Zhiliang Peng","Wenhu Chen","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2310.02992v1.pdf","comment":"Code: https://aka.ms/Kosmos-G Project Page:\n  https://xichenpan.github.io/kosmosg"},{"id":"http://arxiv.org/abs/2310.02988v1","updated":"2023-10-04T17:25:10Z","published":"2023-10-04T17:25:10Z","title":"Probing Intersectional Biases in Vision-Language Models with\n  Counterfactual Examples","summary":"  While vision-language models (VLMs) have achieved remarkable performance\nimprovements recently, there is growing evidence that these models also posses\nharmful biases with respect to social attributes such as gender and race. Prior\nstudies have primarily focused on probing such bias attributes individually\nwhile ignoring biases associated with intersections between social attributes.\nThis could be due to the difficulty of collecting an exhaustive set of\nimage-text pairs for various combinations of social attributes from existing\ndatasets. To address this challenge, we employ text-to-image diffusion models\nto produce counterfactual examples for probing intserctional social biases at\nscale. Our approach utilizes Stable Diffusion with cross attention control to\nproduce sets of counterfactual image-text pairs that are highly similar in\ntheir depiction of a subject (e.g., a given occupation) while differing only in\ntheir depiction of intersectional social attributes (e.g., race & gender). We\nconduct extensive experiments using our generated dataset which reveal the\nintersectional social biases present in state-of-the-art VLMs.\n","authors":["Phillip Howard","Avinash Madasu","Tiep Le","Gustavo Lujan Moreno","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2310.02988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02977v1","updated":"2023-10-04T17:12:18Z","published":"2023-10-04T17:12:18Z","title":"T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation","summary":"  Recent methods in text-to-3D leverage powerful pretrained diffusion models to\noptimize NeRF. Notably, these methods are able to produce high-quality 3D\nscenes without training on 3D data. Due to the open-ended nature of the task,\nmost studies evaluate their results with subjective case studies and user\nexperiments, thereby presenting a challenge in quantitatively addressing the\nquestion: How has current progress in Text-to-3D gone so far? In this paper, we\nintroduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing\ndiverse text prompts of three increasing complexity levels that are specially\ndesigned for 3D generation. To assess both the subjective quality and the text\nalignment, we propose two automatic metrics based on multi-view images produced\nby the 3D contents. The quality metric combines multi-view text-image scores\nand regional convolution to detect quality and view inconsistency. The\nalignment metric uses multi-view captioning and Large Language Model (LLM)\nevaluation to measure text-3D consistency. Both metrics closely correlate with\ndifferent dimensions of human judgments, providing a paradigm for efficiently\nevaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal\nperformance differences among six prevalent text-to-3D methods. Our analysis\nfurther highlights the common struggles for current methods on generating\nsurroundings and multi-object scenes, as well as the bottleneck of leveraging\n2D guidance for 3D generation. Our project page is available at:\nhttps://t3bench.com.\n","authors":["Yuze He","Yushi Bai","Matthieu Lin","Wang Zhao","Yubin Hu","Jenny Sheng","Ran Yi","Juanzi Li","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2310.02977v1.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2310.02972v1","updated":"2023-10-04T17:10:13Z","published":"2023-10-04T17:10:13Z","title":"Fully Automatic Segmentation of Gross Target Volume and Organs-at-Risk\n  for Radiotherapy Planning of Nasopharyngeal Carcinoma","summary":"  Target segmentation in CT images of Head&Neck (H&N) region is challenging due\nto low contrast between adjacent soft tissue. The SegRap 2023 challenge has\nbeen focused on benchmarking the segmentation algorithms of Nasopharyngeal\nCarcinoma (NPC) which would be employed as auto-contouring tools for radiation\ntreatment planning purposes. We propose a fully-automatic framework and develop\ntwo models for a) segmentation of 45 Organs at Risk (OARs) and b) two Gross\nTumor Volumes (GTVs). To this end, we preprocess the image volumes by\nharmonizing the intensity distributions and then automatically cropping the\nvolumes around the target regions. The preprocessed volumes were employed to\ntrain a standard 3D U-Net model for each task, separately. Our method took\nsecond place for each of the tasks in the validation phase of the challenge.\nThe proposed framework is available at https://github.com/Astarakee/segrap2023\n","authors":["Mehdi Astaraki","Simone Bendazzoli","Iuliana Toma-Dasu"],"pdf_url":"https://arxiv.org/pdf/2310.02972v1.pdf","comment":"9 pages, 5 figures, 3 tables, MICCAI SegRap challenge contribution"},{"id":"http://arxiv.org/abs/2310.02960v1","updated":"2023-10-04T16:50:51Z","published":"2023-10-04T16:50:51Z","title":"CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for\n  Open-vocabulary 3D Object Detection","summary":"  Open-vocabulary 3D Object Detection (OV-3DDet) aims to detect objects from an\narbitrary list of categories within a 3D scene, which remains seldom explored\nin the literature. There are primarily two fundamental problems in OV-3DDet,\ni.e., localizing and classifying novel objects. This paper aims at addressing\nthe two problems simultaneously via a unified framework, under the condition of\nlimited base categories. To localize novel 3D objects, we propose an effective\n3D Novel Object Discovery strategy, which utilizes both the 3D box geometry\npriors and 2D semantic open-vocabulary priors to generate pseudo box labels of\nthe novel objects. To classify novel object boxes, we further develop a\ncross-modal alignment module based on discovered novel boxes, to align feature\nspaces between 3D point cloud and image/text modalities. Specifically, the\nalignment process contains a class-agnostic and a class-discriminative\nalignment, incorporating not only the base objects with annotations but also\nthe increasingly discovered novel objects, resulting in an iteratively enhanced\nalignment. The novel box discovery and crossmodal alignment are jointly learned\nto collaboratively benefit each other. The novel object discovery can directly\nimpact the cross-modal alignment, while a better feature alignment can, in\nturn, boost the localization capability, leading to a unified OV-3DDet\nframework, named CoDA, for simultaneous novel object localization and\nclassification. Extensive experiments on two challenging datasets (i.e.,\nSUN-RGBD and ScanNet) demonstrate the effectiveness of our method and also show\na significant mAP improvement upon the best-performing alternative method by\n80%. Codes and pre-trained models are released on the project page.\n","authors":["Yang Cao","Yihan Zeng","Hang Xu","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2310.02960v1.pdf","comment":"Accepted by NeurIPS 2023. Project Page:\n  https://yangcaoai.github.io/publications/CoDA.html"},{"id":"http://arxiv.org/abs/2310.00357v2","updated":"2023-10-04T16:34:58Z","published":"2023-09-30T12:27:53Z","title":"Structural Adversarial Objectives for Self-Supervised Representation\n  Learning","summary":"  Within the framework of generative adversarial networks (GANs), we propose\nobjectives that task the discriminator for self-supervised representation\nlearning via additional structural modeling responsibilities. In combination\nwith an efficient smoothness regularizer imposed on the network, these\nobjectives guide the discriminator to learn to extract informative\nrepresentations, while maintaining a generator capable of sampling from the\ndomain. Specifically, our objectives encourage the discriminator to structure\nfeatures at two levels of granularity: aligning distribution characteristics,\nsuch as mean and variance, at coarse scales, and grouping features into local\nclusters at finer scales. Operating as a feature learner within the GAN\nframework frees our self-supervised system from the reliance on hand-crafted\ndata augmentation schemes that are prevalent across contrastive representation\nlearning methods. Across CIFAR-10/100 and an ImageNet subset, experiments\ndemonstrate that equipping GANs with our self-supervised objectives suffices to\nproduce discriminators which, evaluated in terms of representation learning,\ncompete with networks trained by contrastive learning approaches.\n","authors":["Xiao Zhang","Michael Maire"],"pdf_url":"https://arxiv.org/pdf/2310.00357v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02944v1","updated":"2023-10-04T16:24:00Z","published":"2023-10-04T16:24:00Z","title":"Adaptive Landmark Color for AUV Docking in Visually Dynamic Environments","summary":"  Autonomous Underwater Vehicles (AUVs) conduct missions underwater without the\nneed for human intervention. A docking station (DS) can extend mission times of\nan AUV by providing a location for the AUV to recharge its batteries and\nreceive updated mission information. Various methods for locating and tracking\na DS exist, but most rely on expensive acoustic sensors, or are vision-based,\nwhich is significantly affected by water quality. In this \\doctype, we present\na vision-based method that utilizes adaptive color LED markers and dynamic\ncolor filtering to maximize landmark visibility in varying water conditions.\nBoth AUV and DS utilize cameras to determine the water background color in\norder to calculate the desired marker color. No communication between AUV and\nDS is needed to determine marker color. Experiments conducted in a pool and\nlake show our method performs 10 times better than static color thresholding\nmethods as background color varies. DS detection is possible at a range of 5\nmeters in clear water with minimal false positives.\n","authors":["Corey Knutson","Zhipeng Cao","Junaed Sattar"],"pdf_url":"https://arxiv.org/pdf/2310.02944v1.pdf","comment":"Submitted to ICRA 2024 for review"},{"id":"http://arxiv.org/abs/2202.01069v2","updated":"2023-10-04T16:14:51Z","published":"2022-02-02T15:00:44Z","title":"Image-based Navigation in Real-World Environments via Multiple Mid-level\n  Representations: Fusion Models, Benchmark and Efficient Evaluation","summary":"  Navigating complex indoor environments requires a deep understanding of the\nspace the robotic agent is acting into to correctly inform the navigation\nprocess of the agent towards the goal location. In recent learning-based\nnavigation approaches, the scene understanding and navigation abilities of the\nagent are achieved simultaneously by collecting the required experience in\nsimulation. Unfortunately, even if simulators represent an efficient tool to\ntrain navigation policies, the resulting models often fail when transferred\ninto the real world. One possible solution is to provide the navigation model\nwith mid-level visual representations containing important domain-invariant\nproperties of the scene. But, what are the best representations that facilitate\nthe transfer of a model to the real-world? How can they be combined? In this\nwork we address these issues by proposing a benchmark of Deep Learning\narchitectures to combine a range of mid-level visual representations, to\nperform a PointGoal navigation task following a Reinforcement Learning setup.\nAll the proposed navigation models have been trained with the Habitat simulator\non a synthetic office environment and have been tested on the same real-world\nenvironment using a real robotic platform. To efficiently assess their\nperformance in a real context, a validation tool has been proposed to generate\nrealistic navigation episodes inside the simulator. Our experiments showed that\nnavigation models can benefit from the multi-modal input and that our\nvalidation tool can provide good estimation of the expected navigation\nperformance in the real world, while saving time and resources. The acquired\nsynthetic and real 3D models of the environment, together with the code of our\nvalidation tool built on top of Habitat, are publicly available at the\nfollowing link: https://iplab.dmi.unict.it/EmbodiedVN/\n","authors":["Marco Rosano","Antonino Furnari","Luigi Gulino","Corrado Santoro","Giovanni Maria Farinella"],"pdf_url":"https://arxiv.org/pdf/2202.01069v2.pdf","comment":"Paper accepted for submission in Autonomous Robots"},{"id":"http://arxiv.org/abs/2310.02931v1","updated":"2023-10-04T16:09:35Z","published":"2023-10-04T16:09:35Z","title":"Graph data modelling for outcome prediction in oropharyngeal cancer\n  patients","summary":"  Graph neural networks (GNNs) are becoming increasingly popular in the medical\ndomain for the tasks of disease classification and outcome prediction. Since\npatient data is not readily available as a graph, most existing methods either\nmanually define a patient graph, or learn a latent graph based on pairwise\nsimilarities between the patients. There are also hypergraph neural network\n(HGNN)-based methods that were introduced recently to exploit potential higher\norder associations between the patients by representing them as a hypergraph.\nIn this work, we propose a patient hypergraph network (PHGN), which has been\ninvestigated in an inductive learning setup for binary outcome prediction in\noropharyngeal cancer (OPC) patients using computed tomography (CT)-based\nradiomic features for the first time. Additionally, the proposed model was\nextended to perform time-to-event analyses, and compared with GNN and baseline\nlinear models.\n","authors":["Nithya Bhasker","Stefan Leger","Alexander Zwanenburg","Chethan Babu Reddy","Sebastian Bodenstedt","Steffen Löck","Stefanie Speidel"],"pdf_url":"https://arxiv.org/pdf/2310.02931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18171v2","updated":"2023-10-04T15:55:04Z","published":"2023-05-29T16:02:09Z","title":"Improved Probabilistic Image-Text Representations","summary":"  Image-Text Matching (ITM) task, a fundamental vision-language (VL) task,\nsuffers from the inherent ambiguity arising from multiplicity and imperfect\nannotations. Deterministic functions are not sufficiently powerful to capture\nambiguity, prompting the exploration of probabilistic embeddings to tackle the\nchallenge. However, the existing probabilistic ITM approach encounters two key\nshortcomings; the burden of heavy computations due to the Monte Carlo\napproximation, and the loss saturation issue in the face of abundant false\nnegatives. To overcome the issues, this paper presents an improved\nProbabilistic Cross-Modal Embeddings (named PCME++) by introducing a new\nprobabilistic distance with a closed-form solution. In addition, two\noptimization techniques are proposed to enhance PCME++ further; first, the\nincorporation of pseudo-positives to prevent the loss saturation problem under\nmassive false negatives; second, mixed sample data augmentation for\nprobabilistic matching. Experimental results on MS-COCO Caption and two\nextended benchmarks, CxC and ECCV Caption, demonstrate the effectiveness of\nPCME++ compared to state-of-the-art ITM methods. The robustness of PCME++ is\nalso evaluated under noisy image-text correspondences. In addition, the\npotential applicability of PCME++ in automatic prompt tuning for zero-shot\nclassification is shown. The code is available at\nhttps://naver-ai.github.io/pcmepp/.\n","authors":["Sanghyuk Chun"],"pdf_url":"https://arxiv.org/pdf/2305.18171v2.pdf","comment":"Code: https://github.com/naver-ai/pcmepp. Project page:\n  https://naver-ai.github.io/pcmepp/. 26 pages, 1.2 MB"},{"id":"http://arxiv.org/abs/2303.11916v2","updated":"2023-10-04T15:54:30Z","published":"2023-03-21T15:06:35Z","title":"CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion","summary":"  This paper proposes a novel diffusion-based model, CompoDiff, for solving\nComposed Image Retrieval (CIR) with latent diffusion and presents a newly\ncreated dataset, named SynthTriplets18M, of 18 million reference images,\nconditions, and corresponding target image triplets to train the model.\nCompoDiff and SynthTriplets18M tackle the shortages of the previous CIR\napproaches, such as poor generalizability due to the small dataset scale and\nthe limited types of conditions. CompoDiff not only achieves a new zero-shot\nstate-of-the-art on four CIR benchmarks, including FashionIQ, CIRR, CIRCO, and\nGeneCIS, but also enables a more versatile and controllable CIR by accepting\nvarious conditions, such as negative text and image mask conditions, and the\ncontrollability to the importance between multiple queries or the trade-off\nbetween inference speed and the performance which are unavailable with existing\nCIR methods. The code and dataset are available at\nhttps://github.com/navervision/CompoDiff\n","authors":["Geonmo Gu","Sanghyuk Chun","Wonjae Kim","HeeJae Jun","Yoohoon Kang","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2303.11916v2.pdf","comment":"First two authors contributed equally; 26 pages, 4.1MB"},{"id":"http://arxiv.org/abs/2308.06887v2","updated":"2023-10-04T15:45:47Z","published":"2023-08-14T01:47:26Z","title":"Robustified ANNs Reveal Wormholes Between Human Category Percepts","summary":"  The visual object category reports of artificial neural networks (ANNs) are\nnotoriously sensitive to tiny, adversarial image perturbations. Because human\ncategory reports (aka human percepts) are thought to be insensitive to those\nsame small-norm perturbations -- and locally stable in general -- this argues\nthat ANNs are incomplete scientific models of human visual perception.\nConsistent with this, we show that when small-norm image perturbations are\ngenerated by standard ANN models, human object category percepts are indeed\nhighly stable. However, in this very same \"human-presumed-stable\" regime, we\nfind that robustified ANNs reliably discover low-norm image perturbations that\nstrongly disrupt human percepts. These previously undetectable human perceptual\ndisruptions are massive in amplitude, approaching the same level of sensitivity\nseen in robustified ANNs. Further, we show that robustified ANNs support\nprecise perceptual state interventions: they guide the construction of low-norm\nimage perturbations that strongly alter human category percepts toward specific\nprescribed percepts. These observations suggest that for arbitrary starting\npoints in image space, there exists a set of nearby \"wormholes\", each leading\nthe subject from their current category perceptual state into a semantically\nvery different state. Moreover, contemporary ANN models of biological visual\nprocessing are now accurate enough to consistently guide us to those portals.\n","authors":["Guy Gaziv","Michael J. Lee","James J. DiCarlo"],"pdf_url":"https://arxiv.org/pdf/2308.06887v2.pdf","comment":"In NeurIPS 2023. Code: https://github.com/ggaziv/Wormholes Project\n  Webpage: https://himjl.github.io/pwormholes"},{"id":"http://arxiv.org/abs/2310.02906v1","updated":"2023-10-04T15:43:26Z","published":"2023-10-04T15:43:26Z","title":"Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with\n  Visual and Textual Prompts","summary":"  Image synthesis approaches, e.g., generative adversarial networks, have been\npopular as a form of data augmentation in medical image analysis tasks. It is\nprimarily beneficial to overcome the shortage of publicly accessible data and\nassociated quality annotations. However, the current techniques often lack\ncontrol over the detailed contents in generated images, e.g., the type of\ndisease patterns, the location of lesions, and attributes of the diagnosis. In\nthis work, we adapt the latest advance in the generative model, i.e., the\ndiffusion model, with the added control flow using lesion-specific visual and\ntextual prompts for generating dermatoscopic images. We further demonstrate the\nadvantage of our diffusion model-based framework over the classical generation\nmodels in both the image quality and boosting the segmentation performance on\nskin lesions. It can achieve a 9% increase in the SSIM image quality measure\nand an over 5% increase in Dice coefficients over the prior arts.\n","authors":["Shiyi Du","Xiaosong Wang","Yongyi Lu","Yuyin Zhou","Shaoting Zhang","Alan Yuille","Kang Li","Zongwei Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.02906v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.02901v1","updated":"2023-10-04T15:39:57Z","published":"2023-10-04T15:39:57Z","title":"Computationally Efficient Quadratic Neural Networks","summary":"  Higher order artificial neurons whose outputs are computed by applying an\nactivation function to a higher order multinomial function of the inputs have\nbeen considered in the past, but did not gain acceptance due to the extra\nparameters and computational cost. However, higher order neurons have\nsignificantly greater learning capabilities since the decision boundaries of\nhigher order neurons can be complex surfaces instead of just hyperplanes. The\nboundary of a single quadratic neuron can be a general hyper-quadric surface\nallowing it to learn many nonlinearly separable datasets. Since quadratic forms\ncan be represented by symmetric matrices, only $\\frac{n(n+1)}{2}$ additional\nparameters are needed instead of $n^2$. A quadratic Logistic regression model\nis first presented. Solutions to the XOR problem with a single quadratic neuron\nare considered. The complete vectorized equations for both forward and backward\npropagation in feedforward networks composed of quadratic neurons are derived.\nA reduced parameter quadratic neural network model with just $ n $ additional\nparameters per neuron that provides a compromise between learning ability and\ncomputational cost is presented. Comparison on benchmark classification\ndatasets are used to demonstrate that a final layer of quadratic neurons\nenables networks to achieve higher accuracy with significantly fewer hidden\nlayer neurons. In particular this paper shows that any dataset composed of $C$\nbounded clusters can be separated with only a single layer of $C$ quadratic\nneurons.\n","authors":["Mathew Mithra Noel","Venkataraman Muthiah-Nakarajan"],"pdf_url":"https://arxiv.org/pdf/2310.02901v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2310.02894v1","updated":"2023-10-04T15:31:02Z","published":"2023-10-04T15:31:02Z","title":"Human-centric Behavior Description in Videos: New Benchmark and Model","summary":"  In the domain of video surveillance, describing the behavior of each\nindividual within the video is becoming increasingly essential, especially in\ncomplex scenarios with multiple individuals present. This is because describing\neach individual's behavior provides more detailed situational analysis,\nenabling accurate assessment and response to potential risks, ensuring the\nsafety and harmony of public places. Currently, video-level captioning datasets\ncannot provide fine-grained descriptions for each individual's specific\nbehavior. However, mere descriptions at the video-level fail to provide an\nin-depth interpretation of individual behaviors, making it challenging to\naccurately determine the specific identity of each individual. To address this\nchallenge, we construct a human-centric video surveillance captioning dataset,\nwhich provides detailed descriptions of the dynamic behaviors of 7,820\nindividuals. Specifically, we have labeled several aspects of each person, such\nas location, clothing, and interactions with other elements in the scene, and\nthese people are distributed across 1,012 videos. Based on this dataset, we can\nlink individuals to their respective behaviors, allowing for further analysis\nof each person's behavior in surveillance videos. Besides the dataset, we\npropose a novel video captioning approach that can describe individual behavior\nin detail on a person-level basis, achieving state-of-the-art results. To\nfacilitate further research in this field, we intend to release our dataset and\ncode.\n","authors":["Lingru Zhou","Yiqi Gao","Manqing Zhang","Peng Wu","Peng Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.02894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02887v1","updated":"2023-10-04T15:24:00Z","published":"2023-10-04T15:24:00Z","title":"A Grammatical Compositional Model for Video Action Detection","summary":"  Analysis of human actions in videos demands understanding complex human\ndynamics, as well as the interaction between actors and context. However, these\ninteraction relationships usually exhibit large intra-class variations from\ndiverse human poses or object manipulations, and fine-grained inter-class\ndifferences between similar actions. Thus the performance of existing methods\nis severely limited. Motivated by the observation that interactive actions can\nbe decomposed into actor dynamics and participating objects or humans, we\npropose to investigate the composite property of them. In this paper, we\npresent a novel Grammatical Compositional Model (GCM) for action detection\nbased on typical And-Or graphs. Our model exploits the intrinsic structures and\nlatent relationships of actions in a hierarchical manner to harness both the\ncompositionality of grammar models and the capability of expressing rich\nfeatures of DNNs. The proposed model can be readily embodied into a neural\nnetwork module for efficient optimization in an end-to-end manner. Extensive\nexperiments are conducted on the AVA dataset and the Something-Else task to\ndemonstrate the superiority of our model, meanwhile the interpretability is\nenhanced through an inference parsing procedure.\n","authors":["Zhijun Zhang","Xu Zou","Jiahuan Zhou","Sheng Zhong","Ying Wu"],"pdf_url":"https://arxiv.org/pdf/2310.02887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16364v2","updated":"2023-10-04T14:51:01Z","published":"2023-09-28T12:05:08Z","title":"FG-NeRF: Flow-GAN based Probabilistic Neural Radiance Field for\n  Independence-Assumption-Free Uncertainty Estimation","summary":"  Neural radiance fields with stochasticity have garnered significant interest\nby enabling the sampling of plausible radiance fields and quantifying\nuncertainty for downstream tasks. Existing works rely on the independence\nassumption of points in the radiance field or the pixels in input views to\nobtain tractable forms of the probability density function. However, this\nassumption inadvertently impacts performance when dealing with intricate\ngeometry and texture. In this work, we propose an independence-assumption-free\nprobabilistic neural radiance field based on Flow-GAN. By combining the\ngenerative capability of adversarial learning and the powerful expressivity of\nnormalizing flow, our method explicitly models the density-radiance\ndistribution of the whole scene. We represent our probabilistic NeRF as a\nmean-shifted probabilistic residual neural model. Our model is trained without\nan explicit likelihood function, thereby avoiding the independence assumption.\nSpecifically, We downsample the training images with different strides and\ncenters to form fixed-size patches which are used to train the generator with\npatch-based adversarial learning. Through extensive experiments, our method\ndemonstrates state-of-the-art performance by predicting lower rendering errors\nand more reliable uncertainty on both synthetic and real-world datasets.\n","authors":["Songlin Wei","Jiazhao Zhang","Yang Wang","Fanbo Xiang","Hao Su","He Wang"],"pdf_url":"https://arxiv.org/pdf/2309.16364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02855v1","updated":"2023-10-04T14:42:45Z","published":"2023-10-04T14:42:45Z","title":"Multi-Resolution Fusion for Fully Automatic Cephalometric Landmark\n  Detection","summary":"  Cephalometric landmark detection on lateral skull X-ray images plays a\ncrucial role in the diagnosis of certain dental diseases. Accurate and\neffective identification of these landmarks presents a significant challenge.\nBased on extensive data observations and quantitative analyses, we discovered\nthat visual features from different receptive fields affect the detection\naccuracy of various landmarks differently. As a result, we employed an image\npyramid structure, integrating multiple resolutions as input to train a series\nof models with different receptive fields, aiming to achieve the optimal\nfeature combination for each landmark. Moreover, we applied several data\naugmentation techniques during training to enhance the model's robustness\nacross various devices and measurement alternatives. We implemented this method\nin the Cephalometric Landmark Detection in Lateral X-ray Images 2023 Challenge\nand achieved a Mean Radial Error (MRE) of 1.62 mm and a Success Detection Rate\n(SDR) 2.0mm of 74.18% in the final testing phase.\n","authors":["Dongqian Guo","Wencheng Han"],"pdf_url":"https://arxiv.org/pdf/2310.02855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02848v1","updated":"2023-10-04T14:34:11Z","published":"2023-10-04T14:34:11Z","title":"Magicremover: Tuning-free Text-guided Image inpainting with Diffusion\n  Models","summary":"  Image inpainting aims to fill in the missing pixels with visually coherent\nand semantically plausible content. Despite the great progress brought from\ndeep generative models, this task still suffers from i. the difficulties in\nlarge-scale realistic data collection and costly model training; and ii. the\nintrinsic limitations in the traditionally user-defined binary masks on objects\nwith unclear boundaries or transparent texture. In this paper, we propose\nMagicRemover, a tuning-free method that leverages the powerful diffusion models\nfor text-guided image inpainting. We introduce an attention guidance strategy\nto constrain the sampling process of diffusion models, enabling the erasing of\ninstructed areas and the restoration of occluded content. We further propose a\nclassifier optimization algorithm to facilitate the denoising stability within\nless sampling steps. Extensive comparisons are conducted among our MagicRemover\nand state-of-the-art methods including quantitative evaluation and user study,\ndemonstrating the significant improvement of MagicRemover on high-quality image\ninpainting. We will release our code at https://github.com/exisas/Magicremover.\n","authors":["Siyuan Yang","Lu Zhang","Liqian Ma","Yu Liu","JingJing Fu","You He"],"pdf_url":"https://arxiv.org/pdf/2310.02848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10650v2","updated":"2023-10-04T14:24:09Z","published":"2023-09-19T14:30:14Z","title":"MUSTANG: Multi-Stain Self-Attention Graph Multiple Instance Learning\n  Pipeline for Histopathology Whole Slide Images","summary":"  Whole Slide Images (WSIs) present a challenging computer vision task due to\ntheir gigapixel size and presence of numerous artefacts. Yet they are a\nvaluable resource for patient diagnosis and stratification, often representing\nthe gold standard for diagnostic tasks. Real-world clinical datasets tend to\ncome as sets of heterogeneous WSIs with labels present at the patient-level,\nwith poor to no annotations. Weakly supervised attention-based multiple\ninstance learning approaches have been developed in recent years to address\nthese challenges, but can fail to resolve both long and short-range\ndependencies. Here we propose an end-to-end multi-stain self-attention graph\n(MUSTANG) multiple instance learning pipeline, which is designed to solve a\nweakly-supervised gigapixel multi-image classification task, where the label is\nassigned at the patient-level, but no slide-level labels or region annotations\nare available. The pipeline uses a self-attention based approach by restricting\nthe operations to a highly sparse k-Nearest Neighbour Graph of embedded WSI\npatches based on the Euclidean distance. We show this approach achieves a\nstate-of-the-art F1-score/AUC of 0.89/0.92, outperforming the widely used CLAM\nmodel. Our approach is highly modular and can easily be modified to suit\ndifferent clinical datasets, as it only requires a patient-level label without\nannotations and accepts WSI sets of different sizes, as the graphs can be of\nvarying sizes and structures. The source code can be found at\nhttps://github.com/AmayaGS/MUSTANG.\n","authors":["Amaya Gallagher-Syed","Luca Rossi","Felice Rivellese","Costantino Pitzalis","Myles Lewis","Michael Barnes","Gregory Slabaugh"],"pdf_url":"https://arxiv.org/pdf/2309.10650v2.pdf","comment":"Accepted for publication at BMVC 2023"},{"id":"http://arxiv.org/abs/2310.02835v1","updated":"2023-10-04T14:01:55Z","published":"2023-10-04T14:01:55Z","title":"Delving into CLIP latent space for Video Anomaly Recognition","summary":"  We tackle the complex problem of detecting and recognising anomalies in\nsurveillance videos at the frame level, utilising only video-level supervision.\nWe introduce the novel method AnomalyCLIP, the first to combine Large Language\nand Vision (LLV) models, such as CLIP, with multiple instance learning for\njoint video anomaly detection and classification. Our approach specifically\ninvolves manipulating the latent CLIP feature space to identify the normal\nevent subspace, which in turn allows us to effectively learn text-driven\ndirections for abnormal events. When anomalous frames are projected onto these\ndirections, they exhibit a large feature magnitude if they belong to a\nparticular class. We also introduce a computationally efficient Transformer\narchitecture to model short- and long-term temporal dependencies between\nframes, ultimately producing the final anomaly score and class prediction\nprobabilities. We compare AnomalyCLIP against state-of-the-art methods\nconsidering three major anomaly detection benchmarks, i.e. ShanghaiTech,\nUCF-Crime, and XD-Violence, and empirically show that it outperforms baselines\nin recognising video anomalies.\n","authors":["Luca Zanella","Benedetta Liberatori","Willi Menapace","Fabio Poiesi","Yiming Wang","Elisa Ricci"],"pdf_url":"https://arxiv.org/pdf/2310.02835v1.pdf","comment":"submitted to Computer Vision and Image Understanding, project website\n  and code are available at https://luca-zanella-dvl.github.io/AnomalyCLIP/"},{"id":"http://arxiv.org/abs/2310.02829v1","updated":"2023-10-04T13:56:32Z","published":"2023-10-04T13:56:32Z","title":"All Sizes Matter: Improving Volumetric Brain Segmentation on Small\n  Lesions","summary":"  Brain metastases (BMs) are the most frequently occurring brain tumors. The\ntreatment of patients having multiple BMs with stereo tactic radiosurgery\nnecessitates accurate localization of the metastases. Neural networks can\nassist in this time-consuming and costly task that is typically performed by\nhuman experts. Particularly challenging is the detection of small lesions since\nthey are often underrepresented in exist ing approaches. Yet, lesion detection\nis equally important for all sizes. In this work, we develop an ensemble of\nneural networks explicitly fo cused on detecting and segmenting small BMs. To\naccomplish this task, we trained several neural networks focusing on individual\naspects of the BM segmentation problem: We use blob loss that specifically\naddresses the imbalance of lesion instances in terms of size and texture and\nis, therefore, not biased towards larger lesions. In addition, a model using a\nsubtraction sequence between the T1 and T1 contrast-enhanced sequence focuses\non low-contrast lesions. Furthermore, we train additional models only on small\nlesions. Our experiments demonstrate the utility of the ad ditional blob loss\nand the subtraction sequence. However, including the specialized small lesion\nmodels in the ensemble deteriorates segmentation results. We also find\ndomain-knowledge-inspired postprocessing steps to drastically increase our\nperformance in most experiments. Our approach enables us to submit a\ncompetitive challenge entry to the ASNR-MICCAI BraTS Brain Metastasis Challenge\n2023.\n","authors":["Ayhan Can Erdur","Daniel Scholz","Josef A. Buchner","Stephanie E. Combs","Daniel Rueckert","Jan C. Peeken"],"pdf_url":"https://arxiv.org/pdf/2310.02829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02821v1","updated":"2023-10-04T13:44:56Z","published":"2023-10-04T13:44:56Z","title":"Improving Vision Anomaly Detection with the Guidance of Language\n  Modality","summary":"  Recent years have seen a surge of interest in anomaly detection for tackling\nindustrial defect detection, event detection, etc. However, existing\nunsupervised anomaly detectors, particularly those for the vision modality,\nface significant challenges due to redundant information and sparse latent\nspace. Conversely, the language modality performs well due to its relatively\nsingle data. This paper tackles the aforementioned challenges for vision\nmodality from a multimodal point of view. Specifically, we propose Cross-modal\nGuidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) and\nCross-modal Linear Embedding (CMLE), to tackle the redundant information issue\nand sparse space issue, respectively. CMER masks parts of the raw image and\ncomputes the matching score with the text. Then, CMER discards irrelevant\npixels to make the detector focus on critical contents. To learn a more compact\nlatent space for the vision anomaly detector, CMLE learns a correlation\nstructure matrix from the language modality, and then the latent space of\nvision modality will be learned with the guidance of the matrix. Thereafter,\nthe vision latent space will get semantically similar images closer. Extensive\nexperiments demonstrate the effectiveness of the proposed methods.\nParticularly, CMG outperforms the baseline that only uses images by 16.81%.\nAblation experiments further confirm the synergy among the proposed methods, as\neach component depends on the other to achieve optimal performance.\n","authors":["Dong Chen","Kaihang Pan","Guoming Wang","Yueting Zhuang","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2310.02821v1.pdf","comment":"9 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.02815v1","updated":"2023-10-04T13:38:53Z","published":"2023-10-04T13:38:53Z","title":"CoBEV: Elevating Roadside 3D Object Detection with Depth and Height\n  Complementarity","summary":"  Roadside camera-driven 3D object detection is a crucial task in intelligent\ntransportation systems, which extends the perception range beyond the\nlimitations of vision-centric vehicles and enhances road safety. While previous\nstudies have limitations in using only depth or height information, we find\nboth depth and height matter and they are in fact complementary. The depth\nfeature encompasses precise geometric cues, whereas the height feature is\nprimarily focused on distinguishing between various categories of height\nintervals, essentially providing semantic context. This insight motivates the\ndevelopment of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D\nobject detection framework that integrates depth and height to construct robust\nBEV representations. In essence, CoBEV estimates each pixel's depth and height\ndistribution and lifts the camera features into 3D space for lateral fusion\nusing the newly proposed two-stage complementary feature selection (CFS)\nmodule. A BEV feature distillation framework is also seamlessly integrated to\nfurther enhance the detection accuracy from the prior knowledge of the\nfusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D\ndetection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as\nthe private Supremind-Road dataset, demonstrating that CoBEV not only achieves\nthe accuracy of the new state-of-the-art, but also significantly advances the\nrobustness of previous methods in challenging long-distance scenarios and noisy\ncamera disturbance, and enhances generalization by a large margin in\nheterologous settings with drastic changes in scene and camera parameters. For\nthe first time, the vehicle AP score of a camera model reaches 80% on\nDAIR-V2X-I in terms of easy mode. The source code will be made publicly\navailable at https://github.com/MasterHow/CoBEV.\n","authors":["Hao Shi","Chengshan Pang","Jiaming Zhang","Kailun Yang","Yuhao Wu","Huajian Ni","Yining Lin","Rainer Stiefelhagen","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02815v1.pdf","comment":"The source code will be made publicly available at\n  https://github.com/MasterHow/CoBEV"},{"id":"http://arxiv.org/abs/2211.13976v5","updated":"2023-10-04T13:37:58Z","published":"2022-11-25T09:38:22Z","title":"Expanding Small-Scale Datasets with Guided Imagination","summary":"  The power of DNNs relies heavily on the quantity and quality of training\ndata. However, collecting and annotating data on a large scale is often\nexpensive and time-consuming. To address this issue, we explore a new task,\ntermed dataset expansion, aimed at expanding a ready-to-use small dataset by\nautomatically creating new labeled samples. To this end, we present a Guided\nImagination Framework (GIF) that leverages cutting-edge generative models like\nDALL-E2 and Stable Diffusion (SD) to \"imagine\" and create informative new data\nfrom the input seed data. Specifically, GIF conducts data imagination by\noptimizing the latent features of the seed data in the semantically meaningful\nspace of the prior model, resulting in the creation of photo-realistic images\nwith new content. To guide the imagination towards creating informative samples\nfor model training, we introduce two key criteria, i.e., class-maintained\ninformation boosting and sample diversity promotion. These criteria are\nverified to be essential for effective dataset expansion: GIF-SD obtains 13.5%\nhigher model accuracy on natural image datasets than unguided expansion with\nSD. With these essential criteria, GIF successfully expands small datasets in\nvarious scenarios, boosting model accuracy by 36.9% on average over six natural\nimage datasets and by 13.5% on average over three medical datasets. The source\ncode is available at https://github.com/Vanint/DatasetExpansion.\n","authors":["Yifan Zhang","Daquan Zhou","Bryan Hooi","Kai Wang","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2211.13976v5.pdf","comment":"NeurIPS 2023. Source code: https://github.com/Vanint/DatasetExpansion"},{"id":"http://arxiv.org/abs/2303.13278v2","updated":"2023-10-04T13:35:36Z","published":"2023-03-23T13:59:57Z","title":"Improved Anisotropic Gaussian Filters","summary":"  Elongated anisotropic Gaussian filters are used for the orientation\nestimation of fibers. In cases where computed tomography images are noisy,\nroughly resolved, and of low contrast, they are the method of choice even if\nbeing efficient only in virtual 2D slices. However, minor inaccuracies in the\nanisotropic Gaussian filters can carry over to the orientation estimation.\nTherefore, this paper proposes a modified algorithm for 2D anisotropic Gaussian\nfilters and shows that this improves their precision. Applied to synthetic\nimages of fiber bundles, it is more accurate and robust to noise. Finally, the\neffectiveness of the approach is shown by applying it to real-world images of\nsheet molding compounds.\n","authors":["Alex Keilmann","Michael Godehardt","Ali Moghiseh","Claudia Redenbach","Katja Schladitz"],"pdf_url":"https://arxiv.org/pdf/2303.13278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02804v1","updated":"2023-10-04T13:29:47Z","published":"2023-10-04T13:29:47Z","title":"DOMINO: A Dual-System for Multi-step Visual Language Reasoning","summary":"  Visual language reasoning requires a system to extract text or numbers from\ninformation-dense images like charts or plots and perform logical or arithmetic\nreasoning to arrive at an answer. To tackle this task, existing work relies on\neither (1) an end-to-end vision-language model trained on a large amount of\ndata, or (2) a two-stage pipeline where a captioning model converts the image\ninto text that is further read by another large language model to deduce the\nanswer. However, the former approach forces the model to answer a complex\nquestion with one single step, and the latter approach is prone to inaccurate\nor distracting information in the converted text that can confuse the language\nmodel. In this work, we propose a dual-system for multi-step multimodal\nreasoning, which consists of a \"System-1\" step for visual information\nextraction and a \"System-2\" step for deliberate reasoning. Given an input,\nSystem-2 breaks down the question into atomic sub-steps, each guiding System-1\nto extract the information required for reasoning from the image. Experiments\non chart and plot datasets show that our method with a pre-trained System-2\nmodule performs competitively compared to prior work on in- and\nout-of-distribution data. By fine-tuning the System-2 module (LLaMA-2 70B) on\nonly a small amount of data on multi-step reasoning, the accuracy of our method\nis further improved and surpasses the best fully-supervised end-to-end approach\nby 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on a challenging\ndataset with human-authored questions.\n","authors":["Peifang Wang","Olga Golovneva","Armen Aghajanyan","Xiang Ren","Muhao Chen","Asli Celikyilmaz","Maryam Fazel-Zarandi"],"pdf_url":"https://arxiv.org/pdf/2310.02804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.00510v4","updated":"2023-10-04T13:26:14Z","published":"2021-12-01T14:13:11Z","title":"Trimap-guided Feature Mining and Fusion Network for Natural Image\n  Matting","summary":"  Utilizing trimap guidance and fusing multi-level features are two important\nissues for trimap-based matting with pixel-level prediction. To utilize trimap\nguidance, most existing approaches simply concatenate trimaps and images\ntogether to feed a deep network or apply an extra network to extract more\ntrimap guidance, which meets the conflict between efficiency and effectiveness.\nFor emerging content-based feature fusion, most existing matting methods only\nfocus on local features which lack the guidance of a global feature with strong\nsemantic information related to the interesting object. In this paper, we\npropose a trimap-guided feature mining and fusion network consisting of our\ntrimap-guided non-background multi-scale pooling (TMP) module and global-local\ncontext-aware fusion (GLF) modules. Considering that trimap provides strong\nsemantic guidance, our TMP module focuses effective feature mining on\ninteresting objects under the guidance of trimap without extra parameters.\nFurthermore, our GLF modules use global semantic information of interesting\nobjects mined by our TMP module to guide an effective global-local\ncontext-aware multi-level feature fusion. In addition, we build a common\ninteresting object matting (CIOM) dataset to advance high-quality image\nmatting. Particularly, results on the Composition-1k and our CIOM show that our\nTMFNet achieves 13% and 25% relative improvement on SAD, respectively, against\na strong baseline with fewer parameters and 14% fewer FLOPs. Experimental\nresults on the Composition-1k test set, Alphamatting benchmark, and our CIOM\ntest set demonstrate that our method outperforms state-of-the-art approaches.\nOur code and models are available at\nhttps://github.com/Serge-weihao/TMF-Matting.\n","authors":["Weihao Jiang","Dongdong Yu","Zhaozhi Xie","Yaoyi Li","Zehuan Yuan","Hongtao Lu"],"pdf_url":"https://arxiv.org/pdf/2112.00510v4.pdf","comment":"Accepted to Computer Vision and Image Understanding"},{"id":"http://arxiv.org/abs/2309.16264v2","updated":"2023-10-04T13:16:25Z","published":"2023-09-28T08:57:14Z","title":"GAMMA: Generalizable Articulation Modeling and Manipulation for\n  Articulated Objects","summary":"  Articulated objects like cabinets and doors are widespread in daily life.\nHowever, directly manipulating 3D articulated objects is challenging because\nthey have diverse geometrical shapes, semantic categories, and kinetic\nconstraints. Prior works mostly focused on recognizing and manipulating\narticulated objects with specific joint types. They can either estimate the\njoint parameters or distinguish suitable grasp poses to facilitate trajectory\nplanning. Although these approaches have succeeded in certain types of\narticulated objects, they lack generalizability to unseen objects, which\nsignificantly impedes their application in broader scenarios. In this paper, we\npropose a novel framework of Generalizable Articulation Modeling and\nManipulating for Articulated Objects (GAMMA), which learns both articulation\nmodeling and grasp pose affordance from diverse articulated objects with\ndifferent categories. In addition, GAMMA adopts adaptive manipulation to\niteratively reduce the modeling errors and enhance manipulation performance. We\ntrain GAMMA with the PartNet-Mobility dataset and evaluate with comprehensive\nexperiments in SAPIEN simulation and real-world Franka robot. Results show that\nGAMMA significantly outperforms SOTA articulation modeling and manipulation\nalgorithms in unseen and cross-category articulated objects. We will\nopen-source all codes and datasets in both simulation and real robots for\nreproduction in the final version. Images and videos are published on the\nproject website at: http://sites.google.com/view/gamma-articulation\n","authors":["Qiaojun Yu","Junbo Wang","Wenhai Liu","Ce Hao","Liu Liu","Lin Shao","Weiming Wang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2309.16264v2.pdf","comment":"8 pages, 5 figures, submitted to ICRA 2024"},{"id":"http://arxiv.org/abs/2310.02792v1","updated":"2023-10-04T13:11:20Z","published":"2023-10-04T13:11:20Z","title":"Tracking Anything in Heart All at Once","summary":"  Myocardial motion tracking stands as an essential clinical tool in the\nprevention and detection of Cardiovascular Diseases (CVDs), the foremost cause\nof death globally. However, current techniques suffer incomplete and inaccurate\nmotion estimation of the myocardium both in spatial and temporal dimensions,\nhindering the early identification of myocardial dysfunction. In addressing\nthese challenges, this paper introduces the Neural Cardiac Motion Field\n(NeuralCMF). NeuralCMF leverages the implicit neural representation (INR) to\nmodel the 3D structure and the comprehensive 6D forward/backward motion of the\nheart. This approach offers memory-efficient storage and continuous capability\nto query the precise shape and motion of the myocardium throughout the cardiac\ncycle at any specific point. Notably, NeuralCMF operates without the need for\npaired datasets, and its optimization is self-supervised through the physics\nknowledge priors both in space and time dimensions, ensuring compatibility with\nboth 2D and 3D echocardiogram video inputs. Experimental validations across\nthree representative datasets support the robustness and innovative nature of\nthe NeuralCMF, marking significant advantages over existing state-of-the-arts\nin cardiac imaging and motion tracking.\n","authors":["Chengkang Shen","Hao Zhu","You Zhou","Yu Liu","Si Yi","Lili Dong","Weipeng Zhao","David J. Brady","Xun Cao","Zhan Ma","Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2310.02792v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.02781v1","updated":"2023-10-04T12:52:38Z","published":"2023-10-04T12:52:38Z","title":"LROC-PANGU-GAN: Closing the Simulation Gap in Learning Crater\n  Segmentation with Planetary Simulators","summary":"  It is critical for probes landing on foreign planetary bodies to be able to\nrobustly identify and avoid hazards - as, for example, steep cliffs or deep\ncraters can pose significant risks to a probe's landing and operational\nsuccess. Recent applications of deep learning to this problem show promising\nresults. These models are, however, often learned with explicit supervision\nover annotated datasets. These human-labelled crater databases, such as from\nthe Lunar Reconnaissance Orbiter Camera (LROC), may lack in consistency and\nquality, undermining model performance - as incomplete and/or inaccurate labels\nintroduce noise into the supervisory signal, which encourages the model to\nlearn incorrect associations and results in the model making unreliable\npredictions. Physics-based simulators, such as the Planet and Asteroid Natural\nScene Generation Utility, have, in contrast, perfect ground truth, as the\ninternal state that they use to render scenes is known with exactness. However,\nthey introduce a serious simulation-to-real domain gap - because of fundamental\ndifferences between the simulated environment and the real-world arising from\nmodelling assumptions, unaccounted for physical interactions, environmental\nvariability, etc. Therefore, models trained on their outputs suffer when\ndeployed in the face of realism they have not encountered in their training\ndata distributions. In this paper, we therefore introduce a system to close\nthis \"realism\" gap while retaining label fidelity. We train a CycleGAN model to\nsynthesise LROC from Planet and Asteroid Natural Scene Generation Utility\n(PANGU) images. We show that these improve the training of a downstream crater\nsegmentation network, with segmentation performance on a test set of real LROC\nimages improved as compared to using only simulated PANGU images.\n","authors":["Jaewon La","Jaime Phadke","Matt Hutton","Marius Schwinning","Gabriele De Canio","Florian Renk","Lars Kunze","Matthew Gadd"],"pdf_url":"https://arxiv.org/pdf/2310.02781v1.pdf","comment":"17th Symposium on Advanced Space Technologies in Robotics and\n  Automation"},{"id":"http://arxiv.org/abs/2310.02776v1","updated":"2023-10-04T12:47:48Z","published":"2023-10-04T12:47:48Z","title":"Dynamic Shuffle: An Efficient Channel Mixture Method","summary":"  The redundancy of Convolutional neural networks not only depends on weights\nbut also depends on inputs. Shuffling is an efficient operation for mixing\nchannel information but the shuffle order is usually pre-defined. To reduce the\ndata-dependent redundancy, we devise a dynamic shuffle module to generate\ndata-dependent permutation matrices for shuffling. Since the dimension of\npermutation matrix is proportional to the square of the number of input\nchannels, to make the generation process efficiently, we divide the channels\ninto groups and generate two shared small permutation matrices for each group,\nand utilize Kronecker product and cross group shuffle to obtain the final\npermutation matrices. To make the generation process learnable, based on\ntheoretical analysis, softmax, orthogonal regularization, and binarization are\nemployed to asymptotically approximate the permutation matrix. Dynamic shuffle\nadaptively mixes channel information with negligible extra computation and\nmemory occupancy. Experiment results on image classification benchmark datasets\nCIFAR-10, CIFAR-100, Tiny ImageNet and ImageNet have shown that our method\nsignificantly increases ShuffleNets' performance. Adding dynamic generated\nmatrix with learnable static matrix, we further propose static-dynamic-shuffle\nand show that it can serve as a lightweight replacement of ordinary pointwise\nconvolution.\n","authors":["Kaijun Gong","Zhuowen Yin","Yushu Li","Kailing Guo","Xiangmin Xu"],"pdf_url":"https://arxiv.org/pdf/2310.02776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15842v2","updated":"2023-10-04T12:37:00Z","published":"2023-05-25T08:32:41Z","title":"Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion\n  Data and Natural Language","summary":"  Due to recent advances in pose-estimation methods, human motion can be\nextracted from a common video in the form of 3D skeleton sequences. Despite\nwonderful application opportunities, effective and efficient content-based\naccess to large volumes of such spatio-temporal skeleton data still remains a\nchallenging problem. In this paper, we propose a novel content-based\ntext-to-motion retrieval task, which aims at retrieving relevant motions based\non a specified natural-language textual description. To define baselines for\nthis uncharted task, we employ the BERT and CLIP language representations to\nencode the text modality and successful spatio-temporal models to encode the\nmotion modality. We additionally introduce our transformer-based approach,\ncalled Motion Transformer (MoT), which employs divided space-time attention to\neffectively aggregate the different skeleton joints in space and time. Inspired\nby the recent progress in text-to-image/video matching, we experiment with two\nwidely-adopted metric-learning loss functions. Finally, we set up a common\nevaluation protocol by defining qualitative metrics for assessing the quality\nof the retrieved motions, targeting the two recently-introduced KIT\nMotion-Language and HumanML3D datasets. The code for reproducing our results is\navailable at https://github.com/mesnico/text-to-motion-retrieval.\n","authors":["Nicola Messina","Jan Sedmidubsky","Fabrizio Falchi","Tomáš Rebok"],"pdf_url":"https://arxiv.org/pdf/2305.15842v2.pdf","comment":"SIGIR 2023 (best short paper honorable mention)"},{"id":"http://arxiv.org/abs/2309.13438v2","updated":"2023-10-04T12:13:53Z","published":"2023-09-23T17:29:38Z","title":"Rethinking superpixel segmentation from biologically inspired mechanisms","summary":"  Recently, advancements in deep learning-based superpixel segmentation methods\nhave brought about improvements in both the efficiency and the performance of\nsegmentation. However, a significant challenge remains in generating\nsuperpixels that strictly adhere to object boundaries while conveying rich\nvisual significance, especially when cross-surface color correlations may\ninterfere with objects. Drawing inspiration from neural structure and visual\nmechanisms, we propose a biological network architecture comprising an Enhanced\nScreening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel\nsegmentation. The ESM enhances semantic information by simulating the\ninteractive projection mechanisms of the visual cortex. Additionally, the BAL\nemulates the spatial frequency characteristics of visual cortical cells to\nfacilitate the generation of superpixels with strong boundary adherence. We\ndemonstrate the effectiveness of our approach through evaluations on both the\nBSDS500 dataset and the NYUv2 dataset.\n","authors":["TingYu Zhao","Bo Peng","Yuan Sun","DaiPeng Yang","ZhenGuang Zhange","Xi Wu"],"pdf_url":"https://arxiv.org/pdf/2309.13438v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15521v2","updated":"2023-10-04T11:54:08Z","published":"2023-09-27T09:39:45Z","title":"MLOps for Scarce Image Data: A Use Case in Microscopic Image Analysis","summary":"  Nowadays, Machine Learning (ML) is experiencing tremendous popularity that\nhas never been seen before. The operationalization of ML models is governed by\na set of concepts and methods referred to as Machine Learning Operations\n(MLOps). Nevertheless, researchers, as well as professionals, often focus more\non the automation aspect and neglect the continuous deployment and monitoring\naspects of MLOps. As a result, there is a lack of continuous learning through\nthe flow of feedback from production to development, causing unexpected model\ndeterioration over time due to concept drifts, particularly when dealing with\nscarce data. This work explores the complete application of MLOps in the\ncontext of scarce data analysis. The paper proposes a new holistic approach to\nenhance biomedical image analysis. Our method includes: a fingerprinting\nprocess that enables selecting the best models, datasets, and model development\nstrategy relative to the image analysis task at hand; an automated model\ndevelopment stage; and a continuous deployment and monitoring process to ensure\ncontinuous learning. For preliminary results, we perform a proof of concept for\nfingerprinting in microscopic image datasets.\n","authors":["Angelo Yamachui Sitcheu","Nils Friederich","Simon Baeuerle","Oliver Neumann","Markus Reischl","Ralf Mikut"],"pdf_url":"https://arxiv.org/pdf/2309.15521v2.pdf","comment":"21 pages, 5 figures , 33. Workshop on Computational Intelligence\n  Berlin Germany"},{"id":"http://arxiv.org/abs/2310.02753v1","updated":"2023-10-04T11:44:20Z","published":"2023-10-04T11:44:20Z","title":"MUNCH: Modelling Unique 'N Controllable Heads","summary":"  The automated generation of 3D human heads has been an intriguing and\nchallenging task for computer vision researchers. Prevailing methods synthesize\nrealistic avatars but with limited control over the diversity and quality of\nrendered outputs and suffer from limited correlation between shape and texture\nof the character. We propose a method that offers quality, diversity, control,\nand realism along with explainable network design, all desirable features to\ngame-design artists in the domain. First, our proposed Geometry Generator\nidentifies disentangled latent directions and generate novel and diverse\nsamples. A Render Map Generator then learns to synthesize multiply high-fidelty\nphysically-based render maps including Albedo, Glossiness, Specular, and\nNormals. For artists preferring fine-grained control over the output, we\nintroduce a novel Color Transformer Model that allows semantic color control\nover generated maps. We also introduce quantifiable metrics called Uniqueness\nand Novelty and a combined metric to test the overall performance of our model.\nDemo for both shapes and textures can be found:\nhttps://munch-seven.vercel.app/. We will release our model along with the\nsynthetic dataset.\n","authors":["Debayan Deb","Suvidha Tripathi","Pranit Puri"],"pdf_url":"https://arxiv.org/pdf/2310.02753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02751v1","updated":"2023-10-04T11:43:08Z","published":"2023-10-04T11:43:08Z","title":"SHOT: Suppressing the Hessian along the Optimization Trajectory for\n  Gradient-Based Meta-Learning","summary":"  In this paper, we hypothesize that gradient-based meta-learning (GBML)\nimplicitly suppresses the Hessian along the optimization trajectory in the\ninner loop. Based on this hypothesis, we introduce an algorithm called SHOT\n(Suppressing the Hessian along the Optimization Trajectory) that minimizes the\ndistance between the parameters of the target and reference models to suppress\nthe Hessian in the inner loop. Despite dealing with high-order terms, SHOT does\nnot increase the computational complexity of the baseline model much. It is\nagnostic to both the algorithm and architecture used in GBML, making it highly\nversatile and applicable to any GBML baseline. To validate the effectiveness of\nSHOT, we conduct empirical tests on standard few-shot learning tasks and\nqualitatively analyze its dynamics. We confirm our hypothesis empirically and\ndemonstrate that SHOT outperforms the corresponding baseline. Code is available\nat: https://github.com/JunHoo-Lee/SHOT\n","authors":["JunHoo Lee","Jayeon Yoo","Nojun Kwak"],"pdf_url":"https://arxiv.org/pdf/2310.02751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12114v2","updated":"2023-10-04T10:50:42Z","published":"2023-09-21T14:34:17Z","title":"AutoPET Challenge 2023: Sliding Window-based Optimization of U-Net","summary":"  Tumor segmentation in medical imaging is crucial and relies on precise\ndelineation. Fluorodeoxyglucose Positron-Emission Tomography (FDG-PET) is\nwidely used in clinical practice to detect metabolically active tumors.\nHowever, FDG-PET scans may misinterpret irregular glucose consumption in\nhealthy or benign tissues as cancer. Combining PET with Computed Tomography\n(CT) can enhance tumor segmentation by integrating metabolic and anatomic\ninformation. FDG-PET/CT scans are pivotal for cancer staging and reassessment,\nutilizing radiolabeled fluorodeoxyglucose to highlight metabolically active\nregions. Accurately distinguishing tumor-specific uptake from physiological\nuptake in normal tissues is a challenging aspect of precise tumor segmentation.\nThe AutoPET challenge addresses this by providing a dataset of 1014 FDG-PET/CT\nstudies, encouraging advancements in accurate tumor segmentation and analysis\nwithin the FDG-PET/CT domain. Code:\nhttps://github.com/matt3o/AutoPET2-Submission/\n","authors":["Matthias Hadlich","Zdravko Marinov","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2309.12114v2.pdf","comment":"9 pages, 1 figure, MICCAI 2023 - AutoPET Challenge Submission Version\n  2: Added all results on the preliminary test set"},{"id":"http://arxiv.org/abs/2310.02719v1","updated":"2023-10-04T10:45:55Z","published":"2023-10-04T10:45:55Z","title":"Condition numbers in multiview geometry, instability in relative pose\n  estimation, and RANSAC","summary":"  In this paper we introduce a general framework for analyzing the numerical\nconditioning of minimal problems in multiple view geometry, using tools from\ncomputational algebra and Riemannian geometry. Special motivation comes from\nthe fact that relative pose estimation, based on standard 5-point or 7-point\nRandom Sample Consensus (RANSAC) algorithms, can fail even when no outliers are\npresent and there is enough data to support a hypothesis. We argue that these\ncases arise due to the intrinsic instability of the 5- and 7-point minimal\nproblems. We apply our framework to characterize the instabilities, both in\nterms of the world scenes that lead to infinite condition number, and directly\nin terms of ill-conditioned image data. The approach produces computational\ntests for assessing the condition number before solving the minimal problem.\nLastly synthetic and real data experiments suggest that RANSAC serves not only\nto remove outliers, but also to select for well-conditioned image data, as\npredicted by our theory.\n","authors":["Hongyi Fan","Joe Kileel","Benjamin Kimia"],"pdf_url":"https://arxiv.org/pdf/2310.02719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02718v1","updated":"2023-10-04T10:41:21Z","published":"2023-10-04T10:41:21Z","title":"Understanding Pan-Sharpening via Generalized Inverse","summary":"  Pan-sharpening algorithm utilizes panchromatic image and multispectral image\nto obtain a high spatial and high spectral image. However, the optimizations of\nthe algorithms are designed with different standards. We adopt the simple\nmatrix equation to describe the Pan-sharpening problem. The solution existence\ncondition and the acquirement of spectral and spatial resolution are discussed.\nA down-sampling enhancement method was introduced for better acquiring the\nspatial and spectral down-sample matrices. By the generalized inverse theory,\nwe derived two forms of general inverse matrix formulations that can correspond\nto the two prominent classes of Pan-sharpening methods, that is, component\nsubstitution and multi-resolution analysis methods. Specifically, the Gram\nSchmidt Adaptive(GSA) was proved to follow the general inverse matrix\nformulation of component substitution. A model prior to the general inverse\nmatrix of the spectral function was rendered. The theoretical errors are\nanalyzed. Synthetic experiments and real data experiments are implemented. The\nproposed methods are better and sharper than other methods qualitatively in\nboth synthetic and real experiments. The down-sample enhancement effect is\nshown of better results both quantitatively and qualitatively in real\nexperiments. The generalized inverse matrix theory help us better understand\nthe Pan-sharpening.\n","authors":["Shiqi Liu","Yutong Bai","Xinyang Han","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2310.02718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03531v2","updated":"2023-10-04T10:39:47Z","published":"2023-06-06T09:28:37Z","title":"A Unified Concept-Based System for Local, Global, and Misclassification\n  Explanations","summary":"  Explainability of Deep Neural Networks (DNNs) has been garnering increasing\nattention in recent years. Of the various explainability approaches,\nconcept-based techniques stand out for their ability to utilize\nhuman-meaningful concepts instead of focusing solely on individual pixels.\nHowever, there is a scarcity of methods that consistently provide both local\nand global explanations. Moreover, most of the methods have no offer to explain\nmisclassification cases. Considering these challenges, we present a unified\nconcept-based system for unsupervised learning of both local and global\nconcepts. Our primary objective is to uncover the intrinsic concepts underlying\neach data category by training surrogate explainer networks to estimate the\nimportance of the concepts. Our experimental results substantiated the efficacy\nof the discovered concepts through diverse quantitative and qualitative\nassessments, encompassing faithfulness, completeness, and generality.\nFurthermore, our approach facilitates the explanation of both accurate and\nerroneous predictions, rendering it a valuable tool for comprehending the\ncharacteristics of the target objects and classes.\n","authors":["Fatemeh Aghaeipoor","Dorsa Asgarian","Mohammad Sabokrou"],"pdf_url":"https://arxiv.org/pdf/2306.03531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04995v3","updated":"2023-10-04T10:39:24Z","published":"2023-03-09T02:38:32Z","title":"Text-Visual Prompting for Efficient 2D Temporal Video Grounding","summary":"  In this paper, we study the problem of temporal video grounding (TVG), which\naims to predict the starting/ending time points of moments described by a text\nsentence within a long untrimmed video. Benefiting from fine-grained 3D visual\nfeatures, the TVG techniques have achieved remarkable progress in recent years.\nHowever, the high complexity of 3D convolutional neural networks (CNNs) makes\nextracting dense 3D visual features time-consuming, which calls for intensive\nmemory and computing resources. Towards efficient TVG, we propose a novel\ntext-visual prompting (TVP) framework, which incorporates optimized\nperturbation patterns (that we call 'prompts') into both visual inputs and\ntextual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP\nallows us to effectively co-train vision encoder and language encoder in a 2D\nTVG model and improves the performance of crossmodal feature fusion using only\nlow-complexity sparse 2D visual features. Further, we propose a\nTemporal-Distance IoU (TDIoU) loss for efficient learning of TVG. Experiments\non two benchmark datasets, Charades-STA and ActivityNet Captions datasets,\nempirically show that the proposed TVP significantly boosts the performance of\n2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on\nActivityNet Captions) and achieves 5x inference acceleration over TVG using 3D\nvisual features. Codes are available at Open.Intel.\n","authors":["Yimeng Zhang","Xin Chen","Jinghan Jia","Sijia Liu","Ke Ding"],"pdf_url":"https://arxiv.org/pdf/2303.04995v3.pdf","comment":"Accepted to the CVPR 2023 and code released\n  (https://github.com/intel/TVP)"},{"id":"http://arxiv.org/abs/2310.02714v1","updated":"2023-10-04T10:30:24Z","published":"2023-10-04T10:30:24Z","title":"GETAvatar: Generative Textured Meshes for Animatable Human Avatars","summary":"  We study the problem of 3D-aware full-body human generation, aiming at\ncreating animatable human avatars with high-quality textures and geometries.\nGenerally, two challenges remain in this field: i) existing methods struggle to\ngenerate geometries with rich realistic details such as the wrinkles of\ngarments; ii) they typically utilize volumetric radiance fields and neural\nrenderers in the synthesis process, making high-resolution rendering\nnon-trivial. To overcome these problems, we propose GETAvatar, a Generative\nmodel that directly generates Explicit Textured 3D meshes for animatable human\nAvatar, with photo-realistic appearance and fine geometric details.\nSpecifically, we first design an articulated 3D human representation with\nexplicit surface modeling, and enrich the generated humans with realistic\nsurface details by learning from the 2D normal maps of 3D scan data. Second,\nwith the explicit mesh representation, we can use a rasterization-based\nrenderer to perform surface rendering, allowing us to achieve high-resolution\nimage generation efficiently. Extensive experiments demonstrate that GETAvatar\nachieves state-of-the-art performance on 3D-aware human generation both in\nappearance and geometry quality. Notably, GETAvatar can generate images at\n512x512 resolution with 17FPS and 1024x1024 resolution with 14FPS, improving\nupon previous methods by 2x. Our code and models will be available.\n","authors":["Xuanmeng Zhang","Jianfeng Zhang","Rohan Chacko","Hongyi Xu","Guoxian Song","Yi Yang","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2310.02714v1.pdf","comment":"Accepted by ICCV2023. Project Page: https://getavatar.github.io/"},{"id":"http://arxiv.org/abs/2310.02712v1","updated":"2023-10-04T10:28:38Z","published":"2023-10-04T10:28:38Z","title":"ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space\n  NeRF","summary":"  Recently, there has been a significant advancement in text-to-image diffusion\nmodels, leading to groundbreaking performance in 2D image generation. These\nadvancements have been extended to 3D models, enabling the generation of novel\n3D objects from textual descriptions. This has evolved into NeRF editing\nmethods, which allow the manipulation of existing 3D objects through textual\nconditioning. However, existing NeRF editing techniques have faced limitations\nin their performance due to slow training speeds and the use of loss functions\nthat do not adequately consider editing. To address this, here we present a\nnovel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding\nreal-world scenes into the latent space of the latent diffusion model (LDM)\nthrough a unique refinement layer. This approach enables us to obtain a NeRF\nbackbone that is not only faster but also more amenable to editing compared to\ntraditional image space NeRF editing. Furthermore, we propose an improved loss\nfunction tailored for editing by migrating the delta denoising score (DDS)\ndistillation loss, originally used in 2D image editing to the three-dimensional\ndomain. This novel loss function surpasses the well-known score distillation\nsampling (SDS) loss in terms of suitability for editing purposes. Our\nexperimental results demonstrate that ED-NeRF achieves faster editing speed\nwhile producing improved output quality compared to state-of-the-art 3D editing\nmodels.\n","authors":["Jangho Park","Gihyun Kwon","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2310.02712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02692v1","updated":"2023-10-04T10:03:07Z","published":"2023-10-04T10:03:07Z","title":"Bridging the Domain Gap by Clustering-based Image-Text Graph Matching","summary":"  Learning domain-invariant representations is important to train a model that\ncan generalize well to unseen target task domains. Text descriptions inherently\ncontain semantic structures of concepts and such auxiliary semantic cues can be\nused as effective pivot embedding for domain generalization problems. Here, we\nuse multimodal graph representations, fusing images and text, to get\ndomain-invariant pivot embeddings by considering the inherent semantic\nstructure between local images and text descriptors. Specifically, we aim to\nlearn domain-invariant features by (i) representing the image and text\ndescriptions with graphs, and by (ii) clustering and matching the graph-based\nimage node features into textual graphs simultaneously. We experiment with\nlarge-scale public datasets, such as CUB-DG and DomainBed, and our model\nachieves matched or better state-of-the-art performance on these datasets. Our\ncode will be publicly available upon publication.\n","authors":["Nokyung Park","Daewon Chae","Jeongyong Shim","Sangpil Kim","Eun-Sol Kim","Jinkyu Kim"],"pdf_url":"https://arxiv.org/pdf/2310.02692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15410v2","updated":"2023-10-04T10:02:45Z","published":"2023-06-27T12:11:22Z","title":"AutoGraph: Predicting Lane Graphs from Traffic Observations","summary":"  Lane graph estimation is a long-standing problem in the context of autonomous\ndriving. Previous works aimed at solving this problem by relying on\nlarge-scale, hand-annotated lane graphs, introducing a data bottleneck for\ntraining models to solve this task. To overcome this limitation, we propose to\nuse the motion patterns of traffic participants as lane graph annotations. In\nour AutoGraph approach, we employ a pre-trained object tracker to collect the\ntracklets of traffic participants such as vehicles and trucks. Based on the\nlocation of these tracklets, we predict the successor lane graph from an\ninitial position using overhead RGB images only, not requiring any human\nsupervision. In a subsequent stage, we show how the individual successor\npredictions can be aggregated into a consistent lane graph. We demonstrate the\nefficacy of our approach on the UrbanLaneGraph dataset and perform extensive\nquantitative and qualitative evaluations, indicating that AutoGraph is on par\nwith models trained on hand-annotated graph data. Model and dataset will be\nmade available at redacted-for-review.\n","authors":["Jannik Zürn","Ingmar Posner","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2306.15410v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.02690v1","updated":"2023-10-04T10:02:04Z","published":"2023-10-04T10:02:04Z","title":"Multi-Dimension-Embedding-Aware Modality Fusion Transformer for\n  Psychiatric Disorder Clasification","summary":"  Deep learning approaches, together with neuroimaging techniques, play an\nimportant role in psychiatric disorders classification. Previous studies on\npsychiatric disorders diagnosis mainly focus on using functional connectivity\nmatrices of resting-state functional magnetic resonance imaging (rs-fMRI) as\ninput, which still needs to fully utilize the rich temporal information of the\ntime series of rs-fMRI data. In this work, we proposed a\nmulti-dimension-embedding-aware modality fusion transformer (MFFormer) for\nschizophrenia and bipolar disorder classification using rs-fMRI and T1 weighted\nstructural MRI (T1w sMRI). Concretely, to fully utilize the temporal\ninformation of rs-fMRI and spatial information of sMRI, we constructed a deep\nlearning architecture that takes as input 2D time series of rs-fMRI and 3D\nvolumes T1w. Furthermore, to promote intra-modality attention and information\nfusion across different modalities, a fusion transformer module (FTM) is\ndesigned through extensive self-attention of hybrid feature maps of\nmulti-modality. In addition, a dimension-up and dimension-down strategy is\nsuggested to properly align feature maps of multi-dimensional from different\nmodalities. Experimental results on our private and public OpenfMRI datasets\nshow that our proposed MFFormer performs better than that using a single\nmodality or multi-modality MRI on schizophrenia and bipolar disorder diagnosis.\n","authors":["Guoxin Wang","Xuyang Cao","Shan An","Fengmei Fan","Chao Zhang","Jinsong Wang","Feng Yu","Zhiren Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02687v1","updated":"2023-10-04T09:51:58Z","published":"2023-10-04T09:51:58Z","title":"USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields","summary":"  Neural Radiance Fields (NeRF) has received much attention recently due to its\nimpressive capability to represent 3D scene and synthesize novel view images.\nExisting works usually assume that the input images are captured by a global\nshutter camera. Thus, rolling shutter (RS) images cannot be trivially applied\nto an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter\neffect would also affect the accuracy of the camera pose estimation (e.g. via\nCOLMAP), which further prevents the success of NeRF algorithm with RS images.\nIn this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance\nFields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and\nrecover accurate camera motion trajectory simultaneously under the framework of\nNeRF, by modeling the physical image formation process of a RS camera.\nExperimental results demonstrate that USB-NeRF achieves better performance\ncompared to prior works, in terms of RS effect removal, novel view image\nsynthesis as well as camera motion estimation. Furthermore, our algorithm can\nalso be used to recover high-fidelity high frame-rate global shutter video from\na sequence of RS images.\n","authors":["Moyang Li","Peng Wang","Lingzhe Zhao","Bangyan Liao","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2310.02687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05613v2","updated":"2023-10-04T09:42:43Z","published":"2023-09-11T16:54:34Z","title":"Learning the Geodesic Embedding with Graph Neural Networks","summary":"  We present GeGnn, a learning-based method for computing the approximate\ngeodesic distance between two arbitrary points on discrete polyhedra surfaces\nwith constant time complexity after fast precomputation. Previous relevant\nmethods either focus on computing the geodesic distance between a single source\nand all destinations, which has linear complexity at least or require a long\nprecomputation time. Our key idea is to train a graph neural network to embed\nan input mesh into a high-dimensional embedding space and compute the geodesic\ndistance between a pair of points using the corresponding embedding vectors and\na lightweight decoding function. To facilitate the learning of the embedding,\nwe propose novel graph convolution and graph pooling modules that incorporate\nlocal geodesic information and are verified to be much more effective than\nprevious designs. After training, our method requires only one forward pass of\nthe network per mesh as precomputation. Then, we can compute the geodesic\ndistance between a pair of points using our decoding function, which requires\nonly several matrix multiplications and can be massively parallelized on GPUs.\nWe verify the efficiency and effectiveness of our method on ShapeNet and\ndemonstrate that our method is faster than existing methods by orders of\nmagnitude while achieving comparable or better accuracy. Additionally, our\nmethod exhibits robustness on noisy and incomplete meshes and strong\ngeneralization ability on out-of-distribution meshes. The code and pretrained\nmodel can be found on https://github.com/IntelligentGeometry/GeGnn.\n","authors":["Bo Pang","Zhongtian Zheng","Guoping Wang","Peng-Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2309.05613v2.pdf","comment":"SIGGRAPH Asia 2023, Journal Track"},{"id":"http://arxiv.org/abs/2211.14118v2","updated":"2023-10-04T09:29:07Z","published":"2022-11-25T14:01:54Z","title":"MS-PS: A Multi-Scale Network for Photometric Stereo With a New\n  Comprehensive Training Dataset","summary":"  The photometric stereo (PS) problem consists in reconstructing the 3D-surface\nof an object, thanks to a set of photographs taken under different lighting\ndirections. In this paper, we propose a multi-scale architecture for PS which,\ncombined with a new dataset, yields state-of-the-art results. Our proposed\narchitecture is flexible: it permits to consider a variable number of images as\nwell as variable image size without loss of performance. In addition, we define\na set of constraints to allow the generation of a relevant synthetic dataset to\ntrain convolutional neural networks for the PS problem. Our proposed dataset is\nmuch larger than pre-existing ones, and contains many objects with challenging\nmaterials having anisotropic reflectance (e.g. metals, glass). We show on\npublicly available benchmarks that the combination of both these contributions\ndrastically improves the accuracy of the estimated normal field, in comparison\nwith previous state-of-the-art methods.\n","authors":["Clément Hardy","Yvain Quéau","David Tschumperlé"],"pdf_url":"https://arxiv.org/pdf/2211.14118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02676v1","updated":"2023-10-04T09:27:39Z","published":"2023-10-04T09:27:39Z","title":"PostRainBench: A comprehensive benchmark and a new model for\n  precipitation forecasting","summary":"  Accurate precipitation forecasting is a vital challenge of both scientific\nand societal importance. Data-driven approaches have emerged as a widely used\nsolution for addressing this challenge. However, solely relying on data-driven\napproaches has limitations in modeling the underlying physics, making accurate\npredictions difficult. Coupling AI-based post-processing techniques with\ntraditional Numerical Weather Prediction (NWP) methods offers a more effective\nsolution for improving forecasting accuracy. Despite previous post-processing\nefforts, accurately predicting heavy rainfall remains challenging due to the\nimbalanced precipitation data across locations and complex relationships\nbetween multiple meteorological variables. To address these limitations, we\nintroduce the PostRainBench, a comprehensive multi-variable NWP post-processing\nbenchmark consisting of three datasets for NWP post-processing-based\nprecipitation forecasting. We propose CAMT, a simple yet effective Channel\nAttention Enhanced Multi-task Learning framework with a specially designed\nweighted loss function. Its flexible design allows for easy plug-and-play\nintegration with various backbones. Extensive experimental results on the\nproposed benchmark show that our method outperforms state-of-the-art methods by\n6.3%, 4.7%, and 26.8% in rain CSI on the three datasets respectively. Most\nnotably, our model is the first deep learning-based method to outperform\ntraditional Numerical Weather Prediction (NWP) approaches in extreme\nprecipitation conditions. It shows improvements of 15.6%, 17.4%, and 31.8% over\nNWP predictions in heavy rain CSI on respective datasets. These results\nhighlight the potential impact of our model in reducing the severe consequences\nof extreme weather events.\n","authors":["Yujin Tang","Jiaming Zhou","Xiang Pan","Zeying Gong","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2310.02676v1.pdf","comment":"16 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:2105.05537, arXiv:2206.15241 by other authors"},{"id":"http://arxiv.org/abs/2310.02674v1","updated":"2023-10-04T09:26:44Z","published":"2023-10-04T09:26:44Z","title":"Land-cover change detection using paired OpenStreetMap data and optical\n  high-resolution imagery via object-guided Transformer","summary":"  Optical high-resolution imagery and OpenStreetMap (OSM) data are two\nimportant data sources for land-cover change detection. Previous studies in\nthese two data sources focus on utilizing the information in OSM data to aid\nthe change detection on multi-temporal optical high-resolution images. This\npaper pioneers the direct detection of land-cover changes utilizing paired OSM\ndata and optical imagery, thereby broadening the horizons of change detection\ntasks to encompass more dynamic earth observations. To this end, we propose an\nobject-guided Transformer (ObjFormer) architecture by naturally combining the\nprevalent object-based image analysis (OBIA) technique with the advanced vision\nTransformer architecture. The introduction of OBIA can significantly reduce the\ncomputational overhead and memory burden in the self-attention module.\nSpecifically, the proposed ObjFormer has a hierarchical pseudo-siamese encoder\nconsisting of object-guided self-attention modules that extract representative\nfeatures of different levels from OSM data and optical images; a decoder\nconsisting of object-guided cross-attention modules can progressively recover\nthe land-cover changes from the extracted heterogeneous features. In addition\nto the basic supervised binary change detection task, this paper raises a new\nsemi-supervised semantic change detection task that does not require any\nmanually annotated land-cover labels of optical images to train semantic change\ndetectors. Two lightweight semantic decoders are added to ObjFormer to\naccomplish this task efficiently. A converse cross-entropy loss is designed to\nfully utilize the negative samples, thereby contributing to the great\nperformance improvement in this task. The first large-scale benchmark dataset\ncontaining 1,287 map-image pairs (1024$\\times$ 1024 pixels for each sample)\ncovering 40 regions on six continents ...(see the manuscript for the full\nabstract)\n","authors":["Hongruixuan Chen","Cuiling Lan","Jian Song","Clifford Broni-Bediako","Junshi Xia","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2310.02674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.05727v2","updated":"2023-10-04T09:11:39Z","published":"2023-04-12T09:34:13Z","title":"Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks","summary":"  Explainable AI has become a popular tool for validating machine learning\nmodels. Mismatches between the explained model's decision strategy and the\nuser's domain knowledge (e.g. Clever Hans effects) have also been recognized as\na starting point for improving faulty models. However, it is less clear what to\ndo when the user and the explanation agree. In this paper, we demonstrate that\nacceptance of explanations by the user is not a guarantee for a machine\nlearning model to function well, in particular, some Clever Hans effects may\nremain undetected. Such hidden flaws of the model can nevertheless be\nmitigated, and we demonstrate this by contributing a new method,\nExplanation-Guided Exposure Minimization (EGEM), that preemptively prunes\nvariations in the ML model that have not been the subject of positive\nexplanation feedback. Experiments on natural image data demonstrate that our\napproach leads to models that strongly reduce their reliance on hidden Clever\nHans strategies, and consequently achieve higher accuracy on new data.\n","authors":["Lorenz Linhardt","Klaus-Robert Müller","Grégoire Montavon"],"pdf_url":"https://arxiv.org/pdf/2304.05727v2.pdf","comment":"18 pages + supplement"},{"id":"http://arxiv.org/abs/2306.06991v2","updated":"2023-10-04T09:10:03Z","published":"2023-06-12T09:38:04Z","title":"Fast Diffusion Model","summary":"  Diffusion models (DMs) have been adopted across diverse fields with its\nremarkable abilities in capturing intricate data distributions. In this paper,\nwe propose a Fast Diffusion Model (FDM) to significantly speed up DMs from a\nstochastic optimization perspective for both faster training and sampling. We\nfirst find that the diffusion process of DMs accords with the stochastic\noptimization process of stochastic gradient descent (SGD) on a stochastic\ntime-variant problem. Then, inspired by momentum SGD that uses both gradient\nand an extra momentum to achieve faster and more stable convergence than SGD,\nwe integrate momentum into the diffusion process of DMs. This comes with a\nunique challenge of deriving the noise perturbation kernel from the\nmomentum-based diffusion process. To this end, we frame the process as a Damped\nOscillation system whose critically damped state -- the kernel solution --\navoids oscillation and yields a faster convergence speed of the diffusion\nprocess. Empirical results show that our FDM can be applied to several popular\nDM frameworks, e.g., VP, VE, and EDM, and reduces their training cost by about\n50% with comparable image synthesis performance on CIFAR-10, FFHQ, and AFHQv2\ndatasets. Moreover, FDM decreases their sampling steps by about 3x to achieve\nsimilar performance under the same samplers. The code is available at\nhttps://github.com/sail-sg/FDM.\n","authors":["Zike Wu","Pan Zhou","Kenji Kawaguchi","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.06991v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16066v3","updated":"2023-10-04T09:05:24Z","published":"2023-05-25T13:56:30Z","title":"Guided Attention for Next Active Object @ EGO4D STA Challenge","summary":"  In this technical report, we describe the Guided-Attention mechanism based\nsolution for the short-term anticipation (STA) challenge for the EGO4D\nchallenge. It combines the object detections, and the spatiotemporal features\nextracted from video clips, enhancing the motion and contextual information,\nand further decoding the object-centric and motion-centric information to\naddress the problem of STA in egocentric videos. For the challenge, we build\nour model on top of StillFast with Guided Attention applied on fast network.\nOur model obtains better performance on the validation set and also achieves\nstate-of-the-art (SOTA) results on the challenge test set for EGO4D Short-Term\nObject Interaction Anticipation Challenge.\n","authors":["Sanket Thakur","Cigdem Beyan","Pietro Morerio","Vittorio Murino","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2305.16066v3.pdf","comment":"Winner of CVPR@2023 Ego4D STA challenge. arXiv admin note:\n  substantial text overlap with arXiv:2305.12953"},{"id":"http://arxiv.org/abs/2310.00527v3","updated":"2023-10-04T09:05:17Z","published":"2023-10-01T00:13:06Z","title":"Self-supervised Learning of Contextualized Local Visual Embeddings","summary":"  We present Contextualized Local Visual Embeddings (CLoVE), a self-supervised\nconvolutional-based method that learns representations suited for dense\nprediction tasks. CLoVE deviates from current methods and optimizes a single\nloss function that operates at the level of contextualized local embeddings\nlearned from output feature maps of convolution neural network (CNN) encoders.\nTo learn contextualized embeddings, CLoVE proposes a normalized mult-head\nself-attention layer that combines local features from different parts of an\nimage based on similarity. We extensively benchmark CLoVE's pre-trained\nrepresentations on multiple datasets. CLoVE reaches state-of-the-art\nperformance for CNN-based architectures in 4 dense prediction downstream tasks,\nincluding object detection, instance segmentation, keypoint detection, and\ndense pose estimation.\n","authors":["Thalles Santos Silva","Helio Pedrini","Adín Ramírez Rivera"],"pdf_url":"https://arxiv.org/pdf/2310.00527v3.pdf","comment":"Pre-print. 4th Visual Inductive Priors for Data-Efficient Deep\n  Learning Workshop ICCV 2023. Code at https://github.com/sthalles/CLoVE"},{"id":"http://arxiv.org/abs/2308.08303v2","updated":"2023-10-04T09:04:44Z","published":"2023-08-16T12:07:02Z","title":"Leveraging Next-Active Objects for Context-Aware Anticipation in\n  Egocentric Videos","summary":"  Objects are crucial for understanding human-object interactions. By\nidentifying the relevant objects, one can also predict potential future\ninteractions or actions that may occur with these objects. In this paper, we\nstudy the problem of Short-Term Object interaction anticipation (STA) and\npropose NAOGAT (Next-Active-Object Guided Anticipation Transformer), a\nmulti-modal end-to-end transformer network, that attends to objects in observed\nframes in order to anticipate the next-active-object (NAO) and, eventually, to\nguide the model to predict context-aware future actions. The task is\nchallenging since it requires anticipating future action along with the object\nwith which the action occurs and the time after which the interaction will\nbegin, a.k.a. the time to contact (TTC). Compared to existing video modeling\narchitectures for action anticipation, NAOGAT captures the relationship between\nobjects and the global scene context in order to predict detections for the\nnext active object and anticipate relevant future actions given these\ndetections, leveraging the objects' dynamics to improve accuracy. One of the\nkey strengths of our approach, in fact, is its ability to exploit the motion\ndynamics of objects within a given clip, which is often ignored by other\nmodels, and separately decoding the object-centric and motion-centric\ninformation. Through our experiments, we show that our model outperforms\nexisting methods on two separate datasets, Ego4D and EpicKitchens-100 (\"Unseen\nSet\"), as measured by several additional metrics, such as time to contact, and\nnext-active-object localization. The code will be available upon acceptance.\n","authors":["Sanket Thakur","Cigdem Beyan","Pietro Morerio","Vittorio Murino","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2308.08303v2.pdf","comment":"Accepted in WACV'24"},{"id":"http://arxiv.org/abs/2310.02664v1","updated":"2023-10-04T09:04:20Z","published":"2023-10-04T09:04:20Z","title":"On Memorization in Diffusion Models","summary":"  Due to their capacity to generate novel and high-quality samples, diffusion\nmodels have attracted significant research interest in recent years. Notably,\nthe typical training objective of diffusion models, i.e., denoising score\nmatching, has a closed-form optimal solution that can only generate training\ndata replicating samples. This indicates that a memorization behavior is\ntheoretically expected, which contradicts the common generalization ability of\nstate-of-the-art diffusion models, and thus calls for a deeper understanding.\nLooking into this, we first observe that memorization behaviors tend to occur\non smaller-sized datasets, which motivates our definition of effective model\nmemorization (EMM), a metric measuring the maximum size of training data at\nwhich a learned diffusion model approximates its theoretical optimum. Then, we\nquantify the impact of the influential factors on these memorization behaviors\nin terms of EMM, focusing primarily on data distribution, model configuration,\nand training procedure. Besides comprehensive empirical results identifying the\ninfluential factors, we surprisingly find that conditioning training data on\nuninformative random labels can significantly trigger the memorization in\ndiffusion models. Our study holds practical significance for diffusion model\nusers and offers clues to theoretical research in deep generative models. Code\nis available at https://github.com/sail-sg/DiffMemorize.\n","authors":["Xiangming Gu","Chao Du","Tianyu Pang","Chongxuan Li","Min Lin","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02663v1","updated":"2023-10-04T08:58:23Z","published":"2023-10-04T08:58:23Z","title":"MedPrompt: Cross-Modal Prompting for Multi-Task Medical Image\n  Translation","summary":"  Cross-modal medical image translation is an essential task for synthesizing\nmissing modality data for clinical diagnosis. However, current learning-based\ntechniques have limitations in capturing cross-modal and global features,\nrestricting their suitability to specific pairs of modalities. This lack of\nversatility undermines their practical usefulness, particularly considering\nthat the missing modality may vary for different cases. In this study, we\npresent MedPrompt, a multi-task framework that efficiently translates different\nmodalities. Specifically, we propose the Self-adaptive Prompt Block, which\ndynamically guides the translation network towards distinct modalities. Within\nthis framework, we introduce the Prompt Extraction Block and the Prompt Fusion\nBlock to efficiently encode the cross-modal prompt. To enhance the extraction\nof global features across diverse modalities, we incorporate the Transformer\nmodel. Extensive experimental results involving five datasets and four pairs of\nmodalities demonstrate that our proposed model achieves state-of-the-art visual\nquality and exhibits excellent generalization capability.\n","authors":["Xuhang Chen","Chi-Man Pun","Shuqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02650v1","updated":"2023-10-04T08:18:30Z","published":"2023-10-04T08:18:30Z","title":"Active Visual Localization for Multi-Agent Collaboration: A Data-Driven\n  Approach","summary":"  Rather than having each newly deployed robot create its own map of its\nsurroundings, the growing availability of SLAM-enabled devices provides the\noption of simply localizing in a map of another robot or device. In cases such\nas multi-robot or human-robot collaboration, localizing all agents in the same\nmap is even necessary. However, localizing e.g. a ground robot in the map of a\ndrone or head-mounted MR headset presents unique challenges due to viewpoint\nchanges. This work investigates how active visual localization can be used to\novercome such challenges of viewpoint changes. Specifically, we focus on the\nproblem of selecting the optimal viewpoint at a given location. We compare\nexisting approaches in the literature with additional proposed baselines and\npropose a novel data-driven approach. The result demonstrates the superior\nperformance of the data-driven approach when compared to existing methods, both\nin controlled simulation experiments and real-world deployment.\n","authors":["Matthew Hanlon","Boyang Sun","Marc Pollefeys","Hermann Blum"],"pdf_url":"https://arxiv.org/pdf/2310.02650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02642v1","updated":"2023-10-04T08:02:33Z","published":"2023-10-04T08:02:33Z","title":"GET: Group Event Transformer for Event-Based Vision","summary":"  Event cameras are a type of novel neuromorphic sen-sor that has been gaining\nincreasing attention. Existing event-based backbones mainly rely on image-based\ndesigns to extract spatial information within the image transformed from\nevents, overlooking important event properties like time and polarity. To\naddress this issue, we propose a novel Group-based vision Transformer backbone\nfor Event-based vision, called Group Event Transformer (GET), which de-couples\ntemporal-polarity information from spatial infor-mation throughout the feature\nextraction process. Specifi-cally, we first propose a new event representation\nfor GET, named Group Token, which groups asynchronous events based on their\ntimestamps and polarities. Then, GET ap-plies the Event Dual Self-Attention\nblock, and Group Token Aggregation module to facilitate effective feature\ncommu-nication and integration in both the spatial and temporal-polarity\ndomains. After that, GET can be integrated with different downstream tasks by\nconnecting it with vari-ous heads. We evaluate our method on four event-based\nclassification datasets (Cifar10-DVS, N-MNIST, N-CARS, and DVS128Gesture) and\ntwo event-based object detection datasets (1Mpx and Gen1), and the results\ndemonstrate that GET outperforms other state-of-the-art methods. The code is\navailable at https://github.com/Peterande/GET-Group-Event-Transformer.\n","authors":["Yansong Peng","Yueyi Zhang","Zhiwei Xiong","Xiaoyan Sun","Feng Wu"],"pdf_url":"https://arxiv.org/pdf/2310.02642v1.pdf","comment":"This paper is accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2310.02641v1","updated":"2023-10-04T08:01:36Z","published":"2023-10-04T08:01:36Z","title":"Deformation-Invariant Neural Network and Its Applications in Distorted\n  Image Restoration and Analysis","summary":"  Images degraded by geometric distortions pose a significant challenge to\nimaging and computer vision tasks such as object recognition. Deep\nlearning-based imaging models usually fail to give accurate performance for\ngeometrically distorted images. In this paper, we propose the\ndeformation-invariant neural network (DINN), a framework to address the problem\nof imaging tasks for geometrically distorted images. The DINN outputs\nconsistent latent features for images that are geometrically distorted but\nrepresent the same underlying object or scene. The idea of DINN is to\nincorporate a simple component, called the quasiconformal transformer network\n(QCTN), into other existing deep networks for imaging tasks. The QCTN is a deep\nneural network that outputs a quasiconformal map, which can be used to\ntransform a geometrically distorted image into an improved version that is\ncloser to the distribution of natural or good images. It first outputs a\nBeltrami coefficient, which measures the quasiconformality of the output\ndeformation map. By controlling the Beltrami coefficient, the local geometric\ndistortion under the quasiconformal mapping can be controlled. The QCTN is\nlightweight and simple, which can be readily integrated into other existing\ndeep neural networks to enhance their performance. Leveraging our framework, we\nhave developed an image classification network that achieves accurate\nclassification of distorted images. Our proposed framework has been applied to\nrestore geometrically distorted images by atmospheric turbulence and water\nturbulence. DINN outperforms existing GAN-based restoration methods under these\nscenarios, demonstrating the effectiveness of the proposed framework.\nAdditionally, we apply our proposed framework to the 1-1 verification of human\nface images under atmospheric turbulence and achieve satisfactory performance,\nfurther demonstrating the efficacy of our approach.\n","authors":["Han Zhang","Qiguang Chen","Lok Ming Lui"],"pdf_url":"https://arxiv.org/pdf/2310.02641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02638v1","updated":"2023-10-04T08:00:05Z","published":"2023-10-04T08:00:05Z","title":"P2CADNet: An End-to-End Reconstruction Network for Parametric 3D CAD\n  Model from Point Clouds","summary":"  Computer Aided Design (CAD), especially the feature-based parametric CAD,\nplays an important role in modern industry and society. However, the\nreconstruction of featured CAD model is more challenging than the\nreconstruction of other CAD models. To this end, this paper proposes an\nend-to-end network to reconstruct featured CAD model from point cloud\n(P2CADNet). Initially, the proposed P2CADNet architecture combines a point\ncloud feature extractor, a CAD sequence reconstructor and a parameter\noptimizer. Subsequently, in order to reconstruct the featured CAD model in an\nautoregressive way, the CAD sequence reconstructor applies two transformer\ndecoders, one with target mask and the other without mask. Finally, for\npredicting parameters more precisely, we design a parameter optimizer with\ncross-attention mechanism to further refine the CAD feature parameters. We\nevaluate P2CADNet on the public dataset, and the experimental results show that\nP2CADNet has excellent reconstruction quality and accuracy. To our best\nknowledge, P2CADNet is the first end-to-end network to reconstruct featured CAD\nmodel from point cloud, and can be regarded as baseline for future works.\nTherefore, we open the source code at https://github.com/Blice0415/P2CADNet.\n","authors":["Zhihao Zong","Fazhi He","Rubin Fan","Yuxin Liu"],"pdf_url":"https://arxiv.org/pdf/2310.02638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.10574v2","updated":"2023-10-04T07:52:19Z","published":"2022-07-20T13:37:57Z","title":"Co-Located Human-Human Interaction Analysis using Nonverbal Cues: A\n  Survey","summary":"  Automated co-located human-human interaction analysis has been addressed by\nthe use of nonverbal communication as measurable evidence of social and\npsychological phenomena. We survey the computing studies (since 2010) detecting\nphenomena related to social traits (e.g., leadership, dominance, personality\ntraits), social roles/relations, and interaction dynamics (e.g., group\ncohesion, engagement, rapport). Our target is to identify the nonverbal cues\nand computational methodologies resulting in effective performance. This survey\ndiffers from its counterparts by involving the widest spectrum of social\nphenomena and interaction settings (free-standing conversations, meetings,\ndyads, and crowds). We also present a comprehensive summary of the related\ndatasets and outline future research directions which are regarding the\nimplementation of artificial intelligence, dataset curation, and\nprivacy-preserving interaction analysis. Some major observations are: the most\noften used nonverbal cue, computational method, interaction environment, and\nsensing approach are speaking activity, support vector machines, and meetings\ncomposed of 3-4 persons equipped with microphones and cameras, respectively;\nmultimodal features are prominently performing better; deep learning\narchitectures showed improved performance in overall, but there exist many\nphenomena whose detection has never been implemented through deep models. We\nalso identified several limitations such as the lack of scalable benchmarks,\nannotation reliability tests, cross-dataset experiments, and explainability\nanalysis.\n","authors":["Cigdem Beyan","Alessandro Vinciarelli","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2207.10574v2.pdf","comment":"This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive version was published in\n  ACM Computing Surveys, https://doi.org/10.1145/3626516"},{"id":"http://arxiv.org/abs/2305.16311v2","updated":"2023-10-04T07:38:36Z","published":"2023-05-25T17:59:04Z","title":"Break-A-Scene: Extracting Multiple Concepts from a Single Image","summary":"  Text-to-image model personalization aims to introduce a user-provided concept\nto the model, allowing its synthesis in diverse contexts. However, current\nmethods primarily focus on the case of learning a single concept from multiple\nimages with variations in backgrounds and poses, and struggle when adapted to a\ndifferent scenario. In this work, we introduce the task of textual scene\ndecomposition: given a single image of a scene that may contain several\nconcepts, we aim to extract a distinct text token for each concept, enabling\nfine-grained control over the generated scenes. To this end, we propose\naugmenting the input image with masks that indicate the presence of target\nconcepts. These masks can be provided by the user or generated automatically by\na pre-trained segmentation model. We then present a novel two-phase\ncustomization process that optimizes a set of dedicated textual embeddings\n(handles), as well as the model weights, striking a delicate balance between\naccurately capturing the concepts and avoiding overfitting. We employ a masked\ndiffusion loss to enable handles to generate their assigned concepts,\ncomplemented by a novel loss on cross-attention maps to prevent entanglement.\nWe also introduce union-sampling, a training strategy aimed to improve the\nability of combining multiple concepts in generated images. We use several\nautomatic metrics to quantitatively compare our method against several\nbaselines, and further affirm the results using a user study. Finally, we\nshowcase several applications of our method. Project page is available at:\nhttps://omriavrahami.com/break-a-scene/\n","authors":["Omri Avrahami","Kfir Aberman","Ohad Fried","Daniel Cohen-Or","Dani Lischinski"],"pdf_url":"https://arxiv.org/pdf/2305.16311v2.pdf","comment":"SIGGRAPH Asia 2023. Project page: at:\n  https://omriavrahami.com/break-a-scene/ Video:\n  https://www.youtube.com/watch?v=-9EA-BhizgM"},{"id":"http://arxiv.org/abs/2309.17264v2","updated":"2023-10-04T07:37:28Z","published":"2023-09-29T14:17:24Z","title":"A Foundation Model for General Moving Object Segmentation in Medical\n  Images","summary":"  Medical image segmentation aims to delineate the anatomical or pathological\nstructures of interest, playing a crucial role in clinical diagnosis. A\nsubstantial amount of high-quality annotated data is crucial for constructing\nhigh-precision deep segmentation models. However, medical annotation is highly\ncumbersome and time-consuming, especially for medical videos or 3D volumes, due\nto the huge labeling space and poor inter-frame consistency. Recently, a\nfundamental task named Moving Object Segmentation (MOS) has made significant\nadvancements in natural images. Its objective is to delineate moving objects\nfrom the background within image sequences, requiring only minimal annotations.\nIn this paper, we propose the first foundation model, named iMOS, for MOS in\nmedical images. Extensive experiments on a large multi-modal medical dataset\nvalidate the effectiveness of the proposed iMOS. Specifically, with the\nannotation of only a small number of images in the sequence, iMOS can achieve\nsatisfactory tracking and segmentation performance of moving objects throughout\nthe entire sequence in bi-directions. We hope that the proposed iMOS can help\naccelerate the annotation speed of experts, and boost the development of\nmedical foundation models.\n","authors":["Zhongnuo Yan","Tong Han","Yuhao Huang","Lian Liu","Han Zhou","Jiongquan Chen","Wenlong Shi","Yan Cao","Xin Yang","Dong Ni"],"pdf_url":"https://arxiv.org/pdf/2309.17264v2.pdf","comment":"6 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.02611v1","updated":"2023-10-04T06:52:03Z","published":"2023-10-04T06:52:03Z","title":"Analyzing and Improving OT-based Adversarial Networks","summary":"  Optimal Transport (OT) problem aims to find a transport plan that bridges two\ndistributions while minimizing a given cost function. OT theory has been widely\nutilized in generative modeling. In the beginning, OT distance has been used as\na measure for assessing the distance between data and generated distributions.\nRecently, OT transport map between data and prior distributions has been\nutilized as a generative model. These OT-based generative models share a\nsimilar adversarial training objective. In this paper, we begin by unifying\nthese OT-based adversarial methods within a single framework. Then, we\nelucidate the role of each component in training dynamics through a\ncomprehensive analysis of this unified framework. Moreover, we suggest a simple\nbut novel method that improves the previously best-performing OT-based model.\nIntuitively, our approach conducts a gradual refinement of the generated\ndistribution, progressively aligning it with the data distribution. Our\napproach achieves a FID score of 2.51 on CIFAR-10, outperforming unified\nOT-based adversarial approaches.\n","authors":["Jaemoo Choi","Jaewoong Choi","Myungjoo Kang"],"pdf_url":"https://arxiv.org/pdf/2310.02611v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2310.02601v1","updated":"2023-10-04T06:14:06Z","published":"2023-10-04T06:14:06Z","title":"MagicDrive: Street View Generation with Diverse 3D Geometry Control","summary":"  Recent advancements in diffusion models have significantly enhanced the data\nsynthesis with 2D control. Yet, precise 3D control in street view generation,\ncrucial for 3D perception tasks, remains elusive. Specifically, utilizing\nBird's-Eye View (BEV) as the primary condition often leads to challenges in\ngeometry control (e.g., height), affecting the representation of object shapes,\nocclusion patterns, and road surface elevations, all of which are essential to\nperception data synthesis, especially for 3D object detection tasks. In this\npaper, we introduce MagicDrive, a novel street view generation framework\noffering diverse 3D geometry controls, including camera poses, road maps, and\n3D bounding boxes, together with textual descriptions, achieved through\ntailored encoding strategies. Besides, our design incorporates a cross-view\nattention module, ensuring consistency across multiple camera views. With\nMagicDrive, we achieve high-fidelity street-view synthesis that captures\nnuanced 3D geometry and various scene descriptions, enhancing tasks like BEV\nsegmentation and 3D object detection.\n","authors":["Ruiyuan Gao","Kai Chen","Enze Xie","Lanqing Hong","Zhenguo Li","Dit-Yan Yeung","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.02601v1.pdf","comment":"Project Page: https://flymin.github.io/magicdrive"},{"id":"http://arxiv.org/abs/2305.14777v2","updated":"2023-10-04T06:10:48Z","published":"2023-05-24T06:31:05Z","title":"Generative Modeling through the Semi-dual Formulation of Unbalanced\n  Optimal Transport","summary":"  Optimal Transport (OT) problem investigates a transport map that bridges two\ndistributions while minimizing a given cost function. In this regard, OT\nbetween tractable prior distribution and data has been utilized for generative\nmodeling tasks. However, OT-based methods are susceptible to outliers and face\noptimization challenges during training. In this paper, we propose a novel\ngenerative model based on the semi-dual formulation of Unbalanced Optimal\nTransport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution\nmatching. This approach provides better robustness against outliers, stability\nduring training, and faster convergence. We validate these properties\nempirically through experiments. Moreover, we study the theoretical upper-bound\nof divergence between distributions in UOT. Our model outperforms existing\nOT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 5.80\non CelebA-HQ-256. The code is available at\n\\url{https://github.com/Jae-Moo/UOTM}.\n","authors":["Jaemoo Choi","Jaewoong Choi","Myungjoo Kang"],"pdf_url":"https://arxiv.org/pdf/2305.14777v2.pdf","comment":"23 pages, 15 figures"},{"id":"http://arxiv.org/abs/2310.02596v1","updated":"2023-10-04T05:59:50Z","published":"2023-10-04T05:59:50Z","title":"SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent\n  Text-to-3D","summary":"  It is inherently ambiguous to lift 2D results from pre-trained diffusion\nmodels to a 3D world for text-to-3D generation. 2D diffusion models solely\nlearn view-agnostic priors and thus lack 3D knowledge during the lifting,\nleading to the multi-view inconsistency problem. We find that this problem\nprimarily stems from geometric inconsistency, and avoiding misplaced geometric\nstructures substantially mitigates the problem in the final outputs. Therefore,\nwe improve the consistency by aligning the 2D geometric priors in diffusion\nmodels with well-defined 3D shapes during the lifting, addressing the vast\nmajority of the problem. This is achieved by fine-tuning the 2D diffusion model\nto be viewpoint-aware and to produce view-specific coordinate maps of\ncanonically oriented 3D objects. In our process, only coarse 3D information is\nused for aligning. This \"coarse\" alignment not only resolves the multi-view\ninconsistency in geometries but also retains the ability in 2D diffusion models\nto generate detailed and diversified high-quality objects unseen in the 3D\ndatasets. Furthermore, our aligned geometric priors (AGP) are generic and can\nbe seamlessly integrated into various state-of-the-art pipelines, obtaining\nhigh generalizability in terms of unseen shapes and visual appearance while\ngreatly alleviating the multi-view inconsistency problem. Our method represents\na new state-of-the-art performance with an 85+% consistency rate by human\nevaluation, while many previous methods are around 30%. Our project page is\nhttps://sweetdreamer3d.github.io/\n","authors":["Weiyu Li","Rui Chen","Xuelin Chen","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2310.02596v1.pdf","comment":"Project page: https://sweetdreamer3d.github.io/"},{"id":"http://arxiv.org/abs/2304.10880v2","updated":"2023-10-04T05:55:05Z","published":"2023-04-21T10:47:13Z","title":"Med-Tuning: Parameter-Efficient Transfer Learning with Fine-Grained\n  Feature Enhancement for Medical Volumetric Segmentation","summary":"  Deep learning-based medical volumetric segmentation methods either train the\nmodel from scratch or follow the standard \"pre-training then fine-tuning\"\nparadigm. Although fine-tuning a pre-trained model on downstream tasks can\nharness its representation power, the standard full fine-tuning is costly in\nterms of computation and memory footprint. In this paper, we present the study\non parameter-efficient transfer learning for medical volumetric segmentation\nand propose a new framework named Med-Tuning based on intra-stage feature\nenhancement and inter-stage feature interaction. Additionally, aiming at\nexploiting the intrinsic global properties of Fourier Transform for\nparameter-efficient transfer learning, a new adapter block namely Med-Adapter\nwith a well-designed Fourier Transform branch is proposed for effectively and\nefficiently modeling the crucial global context for medical volumetric\nsegmentation. Given a large-scale pre-trained model on 2D natural images, our\nmethod can exploit both the crucial spatial multi-scale feature and volumetric\ncorrelations along slices for accurate segmentation. Extensive experiments on\nthree benchmark datasets (including CT and MRI) show that our method can\nachieve better results than previous parameter-efficient transfer learning\nmethods on segmentation tasks, with much less tuned parameter costs. Compared\nto full fine-tuning, our method reduces the finetuned model parameters by up to\n4x, with even better segmentation performance.\n","authors":["Wenxuan Wang","Jiachen Shen","Chen Chen","Jianbo Jiao","Jing Liu","Yan Zhang","Shanshan Song","Jiangyun Li"],"pdf_url":"https://arxiv.org/pdf/2304.10880v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02588v1","updated":"2023-10-04T05:09:50Z","published":"2023-10-04T05:09:50Z","title":"ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for\n  Vision Transformer","summary":"  This paper presents a novel approach to address the challenges of\nunderstanding the prediction process and debugging prediction errors in Vision\nTransformers (ViT), which have demonstrated superior performance in various\ncomputer vision tasks such as image classification and object detection. While\nseveral visual explainability techniques, such as CAM, Grad-CAM, Score-CAM, and\nRecipro-CAM, have been extensively researched for Convolutional Neural Networks\n(CNNs), limited research has been conducted on ViT. Current state-of-the-art\nsolutions for ViT rely on class agnostic Attention-Rollout and Relevance\ntechniques. In this work, we propose a new gradient-free visual explanation\nmethod for ViT, called ViT-ReciproCAM, which does not require attention matrix\nand gradient information. ViT-ReciproCAM utilizes token masking and generated\nnew layer outputs from the target layer's input to exploit the correlation\nbetween activated tokens and network predictions for target classes. Our\nproposed method outperforms the state-of-the-art Relevance method in the\nAverage Drop-Coherence-Complexity (ADCC) metric by $4.58\\%$ to $5.80\\%$ and\ngenerates more localized saliency maps. Our experiments demonstrate the\neffectiveness of ViT-ReciproCAM and showcase its potential for understanding\nand debugging ViT models. Our proposed method provides an efficient and\neasy-to-implement alternative for generating visual explanations, without\nrequiring attention and gradient information, which can be beneficial for\nvarious applications in the field of computer vision.\n","authors":["Seok-Yong Byun","Wonju Lee"],"pdf_url":"https://arxiv.org/pdf/2310.02588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02576v1","updated":"2023-10-04T04:27:16Z","published":"2023-10-04T04:27:16Z","title":"A Prototype-Based Neural Network for Image Anomaly Detection and\n  Localization","summary":"  Image anomaly detection and localization perform not only image-level anomaly\nclassification but also locate pixel-level anomaly regions. Recently, it has\nreceived much research attention due to its wide application in various fields.\nThis paper proposes ProtoAD, a prototype-based neural network for image anomaly\ndetection and localization. First, the patch features of normal images are\nextracted by a deep network pre-trained on nature images. Then, the prototypes\nof the normal patch features are learned by non-parametric clustering. Finally,\nwe construct an image anomaly localization network (ProtoAD) by appending the\nfeature extraction network with $L2$ feature normalization, a $1\\times1$\nconvolutional layer, a channel max-pooling, and a subtraction operation. We use\nthe prototypes as the kernels of the $1\\times1$ convolutional layer; therefore,\nour neural network does not need a training phase and can conduct anomaly\ndetection and localization in an end-to-end manner. Extensive experiments on\ntwo challenging industrial anomaly detection datasets, MVTec AD and BTAD,\ndemonstrate that ProtoAD achieves competitive performance compared to the\nstate-of-the-art methods with a higher inference speed. The source code is\navailable at: https://github.com/98chao/ProtoAD.\n","authors":["Chao Huang","Zhao Kang","Hong Wu"],"pdf_url":"https://arxiv.org/pdf/2310.02576v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.02575v1","updated":"2023-10-04T04:26:33Z","published":"2023-10-04T04:26:33Z","title":"AdaMerging: Adaptive Model Merging for Multi-Task Learning","summary":"  Multi-task learning (MTL) aims to empower a model to tackle multiple tasks\nsimultaneously. A recent development known as task arithmetic has revealed that\nseveral models, each fine-tuned for distinct tasks, can be directly merged into\na single model to execute MTL without necessitating a retraining process using\nthe initial training data. Nevertheless, this direct addition of models often\nleads to a significant deterioration in the overall performance of the merged\nmodel. This decline occurs due to potential conflicts and intricate\ncorrelations among the multiple tasks. Consequently, the challenge emerges of\nhow to merge pre-trained models more effectively without using their original\ntraining data. This paper introduces an innovative technique called Adaptive\nModel Merging (AdaMerging). This approach aims to autonomously learn the\ncoefficients for model merging, either in a task-wise or layer-wise manner,\nwithout relying on the original training data. Specifically, our AdaMerging\nmethod operates as an automatic, unsupervised task arithmetic scheme. It\nleverages entropy minimization on unlabeled test samples from the multi-task\nsetup as a surrogate objective function to iteratively refine the merging\ncoefficients of the multiple models. Our experimental findings across eight\ntasks demonstrate the efficacy of the AdaMerging scheme we put forth. Compared\nto the current state-of-the-art task arithmetic merging scheme, AdaMerging\nshowcases a remarkable 11\\% improvement in performance. Notably, AdaMerging\nalso exhibits superior generalization capabilities when applied to unseen\ndownstream tasks. Furthermore, it displays a significantly enhanced robustness\nto data distribution shifts that may occur during the testing phase.\n","authors":["Enneng Yang","Zhenyi Wang","Li Shen","Shiwei Liu","Guibing Guo","Xingwei Wang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2310.02575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02569v1","updated":"2023-10-04T04:07:37Z","published":"2023-10-04T04:07:37Z","title":"ReForm-Eval: Evaluating Large Vision Language Models via Unified\n  Re-Formulation of Task-Oriented Benchmarks","summary":"  Recent years have witnessed remarkable progress in the development of large\nvision-language models (LVLMs). Benefiting from the strong language backbones\nand efficient cross-modal alignment strategies, LVLMs exhibit surprising\ncapabilities to perceive visual signals and perform visually grounded\nreasoning. However, the capabilities of LVLMs have not been comprehensively and\nquantitatively evaluate. Most existing multi-modal benchmarks require\ntask-oriented input-output formats, posing great challenges to automatically\nassess the free-form text output of LVLMs. To effectively leverage the\nannotations available in existing benchmarks and reduce the manual effort\nrequired for constructing new benchmarks, we propose to re-formulate existing\nbenchmarks into unified LVLM-compatible formats. Through systematic data\ncollection and reformulation, we present the ReForm-Eval benchmark, offering\nsubstantial data for evaluating various capabilities of LVLMs. Based on\nReForm-Eval, we conduct extensive experiments, thoroughly analyze the strengths\nand weaknesses of existing LVLMs, and identify the underlying factors. Our\nbenchmark and evaluation framework will be open-sourced as a cornerstone for\nadvancing the development of LVLMs.\n","authors":["Zejun Li","Ye Wang","Mengfei Du","Qingwen Liu","Binhao Wu","Jiwen Zhang","Chengxing Zhou","Zhihao Fan","Jie Fu","Jingjing Chen","Xuanjing Huang","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2310.02569v1.pdf","comment":"38 pages, 11 figures, 24 tables"},{"id":"http://arxiv.org/abs/2310.02567v1","updated":"2023-10-04T03:59:57Z","published":"2023-10-04T03:59:57Z","title":"Improving Automatic VQA Evaluation Using Large Language Models","summary":"  8 years after the visual question answering (VQA) task was proposed, accuracy\nremains the primary metric for automatic evaluation. VQA Accuracy has been\neffective so far in the IID evaluation setting. However, our community is\nundergoing a shift towards open-ended generative models and OOD evaluation. In\nthis new paradigm, the existing VQA Accuracy metric is overly stringent and\nunderestimates the performance of VQA systems. Thus, there is a need to develop\nmore robust automatic VQA metrics that serve as a proxy for human judgment. In\nthis work, we propose to leverage the in-context learning capabilities of\ninstruction-tuned large language models (LLMs) to build a better VQA metric. We\nformulate VQA evaluation as an answer-rating task where the LLM is instructed\nto score the accuracy of a candidate answer given a set of reference answers.\nWe demonstrate the proposed metric better correlates with human judgment\ncompared to existing metrics across several VQA models and benchmarks. We hope\nwide adoption of our metric will contribute to better estimating the research\nprogress on the VQA task.\n","authors":["Oscar Mañas","Benno Krojer","Aishwarya Agrawal"],"pdf_url":"https://arxiv.org/pdf/2310.02567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01852v2","updated":"2023-10-04T03:48:19Z","published":"2023-10-03T07:33:27Z","title":"LanguageBind: Extending Video-Language Pretraining to N-modality by\n  Language-based Semantic Alignment","summary":"  The video-language (VL) pretraining has achieved remarkable improvement in\nmultiple downstream tasks. However, the current VL pretraining framework is\nhard to extend to multiple modalities (N modalities, N>=3) beyond vision and\nlanguage. We thus propose LanguageBind, taking the language as the bind across\ndifferent modalities because the language modality is well-explored and\ncontains rich semantics. Specifically, we freeze the language encoder acquired\nby VL pretraining, then train encoders for other modalities with contrastive\nlearning. As a result, all modalities are mapped to a shared feature space,\nimplementing multi-modal semantic alignment. While LanguageBind ensures that we\ncan extend VL modalities to N modalities, we also need a high-quality dataset\nwith alignment data pairs centered on language. We thus propose VIDAL-10M with\nVideo, Infrared, Depth, Audio and their corresponding Language, naming as\nVIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with\ncomplete semantics rather than truncated segments from long videos, and all the\nvideo, depth, infrared, and audio modalities are aligned to their textual\ndescriptions. After pretraining on VIDAL-10M, we outperform ImageBind by 1.2%\nR@1 on the MSR-VTT dataset with only 15% of the parameters in the zero-shot\nvideo-text retrieval, validating the high quality of our dataset. Beyond this,\nour LanguageBind has achieved great improvement in the zero-shot video, audio,\ndepth, and infrared understanding tasks. For instance, on the LLVIP and NYU-D\ndatasets, LanguageBind outperforms ImageBind-huge with 23.8% and 11.1% top-1\naccuracy. Code address: https://github.com/PKU-YuanGroup/LanguageBind.\n","authors":["Bin Zhu","Bin Lin","Munan Ning","Yang Yan","Jiaxi Cui","HongFa Wang","Yatian Pang","Wenhao Jiang","Junwu Zhang","Zongwei Li","Wancai Zhang","Zhifeng Li","Wei Liu","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2310.01852v2.pdf","comment":"Under review as a conference paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2310.02557v1","updated":"2023-10-04T03:30:32Z","published":"2023-10-04T03:30:32Z","title":"Generalization in diffusion models arises from geometry-adaptive\n  harmonic representation","summary":"  High-quality samples generated with score-based reverse diffusion algorithms\nprovide evidence that deep neural networks (DNN) trained for denoising can\nlearn high-dimensional densities, despite the curse of dimensionality. However,\nrecent reports of memorization of the training set raise the question of\nwhether these networks are learning the \"true\" continuous density of the data.\nHere, we show that two denoising DNNs trained on non-overlapping subsets of a\ndataset learn nearly the same score function, and thus the same density, with a\nsurprisingly small number of training images. This strong generalization\ndemonstrates an alignment of powerful inductive biases in the DNN architecture\nand/or training algorithm with properties of the data distribution. We analyze\nthese, demonstrating that the denoiser performs a shrinkage operation in a\nbasis adapted to the underlying image. Examination of these bases reveals\noscillating harmonic structures along contours and in homogeneous image\nregions. We show that trained denoisers are inductively biased towards these\ngeometry-adaptive harmonic representations by demonstrating that they arise\neven when the network is trained on image classes such as low-dimensional\nmanifolds, for which the harmonic basis is suboptimal. Additionally, we show\nthat the denoising performance of the networks is near-optimal when trained on\nregular image classes for which the optimal basis is known to be\ngeometry-adaptive and harmonic.\n","authors":["Zahra Kadkhodaie","Florentin Guth","Eero P. Simoncelli","Stéphane Mallat"],"pdf_url":"https://arxiv.org/pdf/2310.02557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02556v1","updated":"2023-10-04T03:30:24Z","published":"2023-10-04T03:30:24Z","title":"NOLA: Networks as Linear Combination of Low Rank Random Basis","summary":"  Large Language Models (LLMs) have recently gained popularity due to their\nimpressive few-shot performance across various downstream tasks. However,\nfine-tuning all parameters and storing a unique model for each downstream task\nor domain becomes impractical because of the massive size of checkpoints (e.g.,\n350GB in GPT-3). Current literature, such as LoRA, showcases the potential of\nlow-rank modifications to the original weights of an LLM, enabling efficient\nadaptation and storage for task-specific models. These methods can reduce the\nnumber of parameters needed to fine-tune an LLM by several orders of magnitude.\nYet, these methods face two primary limitations: 1) the parameter reduction is\nlower-bounded by the rank one decomposition, and 2) the extent of reduction is\nheavily influenced by both the model architecture and the chosen rank. For\ninstance, in larger models, even a rank one decomposition might exceed the\nnumber of parameters truly needed for adaptation. In this paper, we introduce\nNOLA, which overcomes the rank one lower bound present in LoRA. It achieves\nthis by re-parameterizing the low-rank matrices in LoRA using linear\ncombinations of randomly generated matrices (basis) and optimizing the linear\nmixture coefficients only. This approach allows us to decouple the number of\ntrainable parameters from both the choice of rank and the network architecture.\nWe present adaptation results using GPT-2 and ViT in natural language and\ncomputer vision tasks. NOLA performs as well as, or better than models with\nequivalent parameter counts. Furthermore, we demonstrate that we can halve the\nparameters in larger models compared to LoRA with rank one, without sacrificing\nperformance.\n","authors":["Soroush Abbasi Koohpayegani","KL Navaneet","Parsa Nooralinejad","Soheil Kolouri","Hamed Pirsiavash"],"pdf_url":"https://arxiv.org/pdf/2310.02556v1.pdf","comment":"Our code is available here: https://github.com/UCDvision/NOLA"},{"id":"http://arxiv.org/abs/2303.06138v4","updated":"2023-10-04T03:17:03Z","published":"2023-03-10T18:55:46Z","title":"Learning Object-Centric Neural Scattering Functions for Free-Viewpoint\n  Relighting and Scene Composition","summary":"  Photorealistic object appearance modeling from 2D images is a constant topic\nin vision and graphics. While neural implicit methods (such as Neural Radiance\nFields) have shown high-fidelity view synthesis results, they cannot relight\nthe captured objects. More recent neural inverse rendering approaches have\nenabled object relighting, but they represent surface properties as simple\nBRDFs, and therefore cannot handle translucent objects. We propose\nObject-Centric Neural Scattering Functions (OSFs) for learning to reconstruct\nobject appearance from only images. OSFs not only support free-viewpoint object\nrelighting, but also can model both opaque and translucent objects. While\naccurately modeling subsurface light transport for translucent objects can be\nhighly complex and even intractable for neural methods, OSFs learn to\napproximate the radiance transfer from a distant light to an outgoing direction\nat any spatial location. This approximation avoids explicitly modeling complex\nsubsurface scattering, making learning a neural implicit model tractable.\nExperiments on real and synthetic data show that OSFs accurately reconstruct\nappearances for both opaque and translucent objects, allowing faithful\nfree-viewpoint relighting as well as scene composition.\n","authors":["Hong-Xing Yu","Michelle Guo","Alireza Fathi","Yen-Yu Chang","Eric Ryan Chan","Ruohan Gao","Thomas Funkhouser","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2303.06138v4.pdf","comment":"Journal extension of arXiv:2012.08503 (TMLR 2023). The first two\n  authors contributed equally to this work. Project page:\n  https://kovenyu.com/osf/"},{"id":"http://arxiv.org/abs/2310.02544v1","updated":"2023-10-04T02:57:01Z","published":"2023-10-04T02:57:01Z","title":"SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy\n  Efficiency of Inference Efficient Vision Transformers","summary":"  Recently, there has been a lot of progress in reducing the computation of\ndeep models at inference time. These methods can reduce both the computational\nneeds and power usage of deep models. Some of these approaches adaptively scale\nthe compute based on the input instance. We show that such models can be\nvulnerable to a universal adversarial patch attack, where the attacker\noptimizes for a patch that when pasted on any image, can increase the compute\nand power consumption of the model. We run experiments with three different\nefficient vision transformer methods showing that in some cases, the attacker\ncan increase the computation to the maximum possible level by simply pasting a\npatch that occupies only 8\\% of the image area. We also show that a standard\nadversarial training defense method can reduce some of the attack's success. We\nbelieve adaptive efficient methods will be necessary for the future to lower\nthe power usage of deep models, so we hope our paper encourages the community\nto study the robustness of these methods and develop better defense methods for\nthe proposed attack.\n","authors":["KL Navaneet","Soroush Abbasi Koohpayegani","Essam Sleiman","Hamed Pirsiavash"],"pdf_url":"https://arxiv.org/pdf/2310.02544v1.pdf","comment":"Code is available at https://github.com/UCDvision/SlowFormer"},{"id":"http://arxiv.org/abs/2310.01886v2","updated":"2023-10-04T02:30:27Z","published":"2023-10-03T08:39:33Z","title":"Effective and Parameter-Efficient Reusing Fine-Tuned Models","summary":"  Many pre-trained large-scale models provided online have become highly\neffective in transferring to downstream tasks. At the same time, various\ntask-specific models fine-tuned on these pre-trained models are available\nonline for public use. In practice, as collecting task-specific data is\nlabor-intensive and fine-tuning the large pre-trained models is computationally\nexpensive, one can reuse task-specific finetuned models to deal with downstream\ntasks. However, using a model per task causes a heavy burden on storage and\nserving. Recently, many training-free and parameter-efficient methods have been\nproposed for reusing multiple fine-tuned task-specific models into a single\nmulti-task model. However, these methods exhibit a large accuracy gap compared\nwith using a fine-tuned model per task. In this paper, we propose\nParameter-Efficient methods for ReUsing (PERU) fine-tuned models. For reusing\nFully Fine-Tuned (FFT) models, we propose PERU-FFT by injecting a sparse task\nvector into a merged model by magnitude pruning. For reusing LoRA fine-tuned\nmodels, we propose PERU-LoRA use a lower-rank matrix to approximate the LoRA\nmatrix by singular value decomposition. Both PERUFFT and PERU-LoRA are\ntraining-free. Extensive experiments conducted on computer vision and natural\nlanguage process tasks demonstrate the effectiveness and parameter-efficiency\nof the proposed methods. The proposed PERU-FFT and PERU-LoRA outperform\nexisting reusing model methods by a large margin and achieve comparable\nperformance to using a fine-tuned model per task.\n","authors":["Weisen Jiang","Baijiong Lin","Han Shi","Yu Zhang","Zhenguo Li","James T. Kwok"],"pdf_url":"https://arxiv.org/pdf/2310.01886v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2310.02532v1","updated":"2023-10-04T02:17:59Z","published":"2023-10-04T02:17:59Z","title":"ShaSTA-Fuse: Camera-LiDAR Sensor Fusion to Model Shape and\n  Spatio-Temporal Affinities for 3D Multi-Object Tracking","summary":"  3D multi-object tracking (MOT) is essential for an autonomous mobile agent to\nsafely navigate a scene. In order to maximize the perception capabilities of\nthe autonomous agent, we aim to develop a 3D MOT framework that fuses camera\nand LiDAR sensor information. Building on our prior LiDAR-only work, ShaSTA,\nwhich models shape and spatio-temporal affinities for 3D MOT, we propose a\nnovel camera-LiDAR fusion approach for learning affinities. At its core, this\nwork proposes a fusion technique that generates a rich sensory signal\nincorporating information about depth and distant objects to enhance affinity\nestimation for improved data association, track lifecycle management,\nfalse-positive elimination, false-negative propagation, and track confidence\nscore refinement. Our main contributions include a novel fusion approach for\ncombining camera and LiDAR sensory signals to learn affinities, and a\nfirst-of-its-kind multimodal sequential track confidence refinement technique\nthat fuses 2D and 3D detections. Additionally, we perform an ablative analysis\non each fusion step to demonstrate the added benefits of incorporating the\ncamera sensor, particular for small, distant objects that tend to suffer from\nthe depth-sensing limits and sparsity of LiDAR sensors. In sum, our technique\nachieves state-of-the-art performance on the nuScenes benchmark amongst\nmultimodal 3D MOT algorithms using CenterPoint detections.\n","authors":["Tara Sadjadpour","Rares Ambrus","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2310.02532v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2310.02528v1","updated":"2023-10-04T02:06:48Z","published":"2023-10-04T02:06:48Z","title":"On the Cognition of Visual Question Answering Models and Human\n  Intelligence: A Comparative Study","summary":"  Visual Question Answering (VQA) is a challenging task that requires\ncross-modal understanding and reasoning of visual image and natural language\nquestion. To inspect the association of VQA models to human cognition, we\ndesigned a survey to record human thinking process and analyzed VQA models by\ncomparing the outputs and attention maps with those of humans. We found that\nalthough the VQA models resemble human cognition in architecture and performs\nsimilarly with human on the recognition-level, they still struggle with\ncognitive inferences. The analysis of human thinking procedure serves to direct\nfuture research and introduce more cognitive capacity into modeling features\nand architectures.\n","authors":["Liben Chen","Long Chen","Tian Ellison-Chen","Zhuoyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2310.02528v1.pdf","comment":"16 pages, 11 figures"},{"id":"http://arxiv.org/abs/2307.01430v2","updated":"2023-10-04T01:56:32Z","published":"2023-07-04T01:47:34Z","title":"Continual Learning in Open-vocabulary Classification with Complementary\n  Memory Systems","summary":"  We introduce a method for flexible and efficient continual learning in\nopen-vocabulary image classification, drawing inspiration from the\ncomplementary learning systems observed in human cognition. Specifically, we\npropose to combine predictions from a CLIP zero-shot model and the\nexemplar-based model, using the zero-shot estimated probability that a sample's\nclass is within the exemplar classes. We also propose a \"tree probe\" method, an\nadaption of lazy learning principles, which enables fast learning from new\nexamples with competitive accuracy to batch-trained linear models. We test in\ndata incremental, class incremental, and task incremental settings, as well as\nability to perform flexible inference on varying subsets of zero-shot and\nlearned categories. Our proposed method achieves a good balance of learning\nspeed, target task effectiveness, and zero-shot effectiveness. Code will be\navailable at https://github.com/jessemelpolio/TreeProbe.\n","authors":["Zhen Zhu","Weijie Lyu","Yao Xiao","Derek Hoiem"],"pdf_url":"https://arxiv.org/pdf/2307.01430v2.pdf","comment":"In review"},{"id":"http://arxiv.org/abs/2310.02523v1","updated":"2023-10-04T01:47:36Z","published":"2023-10-04T01:47:36Z","title":"A Spatio-Temporal Attention-Based Method for Detecting Student Classroom\n  Behaviors","summary":"  Accurately detecting student behavior from classroom videos is beneficial for\nanalyzing their classroom status and improving teaching efficiency. However,\nlow accuracy in student classroom behavior detection is a prevalent issue. To\naddress this issue, we propose a Spatio-Temporal Attention-Based Method for\nDetecting Student Classroom Behaviors (BDSTA). Firstly, the SlowFast network is\nused to generate motion and environmental information feature maps from the\nvideo. Then, the spatio-temporal attention module is applied to the feature\nmaps, including information aggregation, compression and stimulation processes.\nSubsequently, attention maps in the time, channel and space dimensions are\nobtained, and multi-label behavior classification is performed based on these\nattention maps. To solve the long-tail data problem that exists in student\nclassroom behavior datasets, we use an improved focal loss function to assign\nmore weight to the tail class data during training. Experimental results are\nconducted on a self-made student classroom behavior dataset named STSCB.\nCompared with the SlowFast model, the average accuracy of student behavior\nclassification detection improves by 8.94\\% using BDSTA.\n","authors":["Fan Yang"],"pdf_url":"https://arxiv.org/pdf/2310.02523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02522v1","updated":"2023-10-04T01:43:46Z","published":"2023-10-04T01:43:46Z","title":"SCB-Dataset3: A Benchmark for Detecting Student Classroom Behavior","summary":"  The use of deep learning methods to automatically detect students' classroom\nbehavior is a promising approach for analyzing their class performance and\nimproving teaching effectiveness. However, the lack of publicly available\ndatasets on student behavior poses a challenge for researchers in this field.\nTo address this issue, we propose the Student Classroom Behavior dataset\n(SCB-dataset3), which represents real-life scenarios. Our dataset comprises\n5686 images with 45578 labels, focusing on six behaviors: hand-raising,\nreading, writing, using a phone, bowing the head, and leaning over the table.\nWe evaluated the dataset using the YOLOv5, YOLOv7, and YOLOv8 algorithms,\nachieving a mean average precision (map) of up to 80.3$\\%$. We believe that our\ndataset can serve as a robust foundation for future research in student\nbehavior detection and contribute to advancements in this field. Our\nSCB-dataset3 is available for download at:\nhttps://github.com/Whiffe/SCB-dataset\n","authors":["Fan Yang","Tao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02522v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2304.02488,\n  arXiv:2306.03318"},{"id":"http://arxiv.org/abs/2305.03048v2","updated":"2023-10-04T01:15:21Z","published":"2023-05-04T17:59:36Z","title":"Personalize Segment Anything Model with One Shot","summary":"  Driven by large-data pre-training, Segment Anything Model (SAM) has been\ndemonstrated as a powerful and promptable framework, revolutionizing the\nsegmentation models. Despite the generality, customizing SAM for specific\nvisual concepts without man-powered prompting is under explored, e.g.,\nautomatically segmenting your pet dog in different images. In this paper, we\npropose a training-free Personalization approach for SAM, termed as PerSAM.\nGiven only a single image with a reference mask, PerSAM first localizes the\ntarget concept by a location prior, and segments it within other images or\nvideos via three techniques: target-guided attention, target-semantic\nprompting, and cascaded post-refinement. In this way, we effectively adapt SAM\nfor private use without any training. To further alleviate the mask ambiguity,\nwe present an efficient one-shot fine-tuning variant, PerSAM-F. Freezing the\nentire SAM, we introduce two learnable weights for multi-scale masks, only\ntraining 2 parameters within 10 seconds for improved performance. To\ndemonstrate our efficacy, we construct a new segmentation dataset, PerSeg, for\npersonalized evaluation, and test our methods on video object segmentation with\ncompetitive performance. Besides, our approach can also enhance DreamBooth to\npersonalize Stable Diffusion for text-to-image generation, which discards the\nbackground disturbance for better target appearance learning. Code is released\nat https://github.com/ZrrSkywalker/Personalize-SAM\n","authors":["Renrui Zhang","Zhengkai Jiang","Ziyu Guo","Shilin Yan","Junting Pan","Xianzheng Ma","Hao Dong","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2305.03048v2.pdf","comment":"Code is available at https://github.com/ZrrSkywalker/Personalize-SAM"},{"id":"http://arxiv.org/abs/2310.00199v2","updated":"2023-10-04T01:00:42Z","published":"2023-09-30T00:33:41Z","title":"DeformUX-Net: Exploring a 3D Foundation Backbone for Medical Image\n  Segmentation with Depthwise Deformable Convolution","summary":"  The application of 3D ViTs to medical image segmentation has seen remarkable\nstrides, somewhat overshadowing the budding advancements in Convolutional\nNeural Network (CNN)-based models. Large kernel depthwise convolution has\nemerged as a promising technique, showcasing capabilities akin to hierarchical\ntransformers and facilitating an expansive effective receptive field (ERF)\nvital for dense predictions. Despite this, existing core operators, ranging\nfrom global-local attention to large kernel convolution, exhibit inherent\ntrade-offs and limitations (e.g., global-local range trade-off, aggregating\nattentional features). We hypothesize that deformable convolution can be an\nexploratory alternative to combine all advantages from the previous operators,\nproviding long-range dependency, adaptive spatial aggregation and computational\nefficiency as a foundation backbone. In this work, we introduce 3D\nDeformUX-Net, a pioneering volumetric CNN model that adeptly navigates the\nshortcomings traditionally associated with ViTs and large kernel convolution.\nSpecifically, we revisit volumetric deformable convolution in depth-wise\nsetting to adapt long-range dependency with computational efficiency. Inspired\nby the concepts of structural re-parameterization for convolution kernel\nweights, we further generate the deformable tri-planar offsets by adapting a\nparallel branch (starting from $1\\times1\\times1$ convolution), providing\nadaptive spatial aggregation across all channels. Our empirical evaluations\nreveal that the 3D DeformUX-Net consistently outperforms existing\nstate-of-the-art ViTs and large kernel convolution models across four\nchallenging public datasets, spanning various scales from organs (KiTS: 0.680\nto 0.720, MSD Pancreas: 0.676 to 0.717, AMOS: 0.871 to 0.902) to vessels (e.g.,\nMSD hepatic vessels: 0.635 to 0.671) in mean Dice.\n","authors":["Ho Hin Lee","Quan Liu","Qi Yang","Xin Yu","Shunxing Bao","Yuankai Huo","Bennett A. Landman"],"pdf_url":"https://arxiv.org/pdf/2310.00199v2.pdf","comment":"14 pages, the source code with our pre-trained model is available at\n  this https://github.com/MASILab/deform-uxnet"},{"id":"http://arxiv.org/abs/2309.13570v2","updated":"2023-10-04T00:51:32Z","published":"2023-09-24T07:06:45Z","title":"Towards Robust Mobile Digital-Twin Tracking via An RGBD-based\n  Transformer Model and A Comprehensive Mobile Dataset","summary":"  The potential of digital-twin technology, involving the creation of precise\ndigital replicas of physical objects, to reshape AR experiences in 3D object\ntracking and localization scenarios is significant. However, enabling robust 3D\nobject tracking in dynamic mobile AR environments remains a formidable\nchallenge. These scenarios often require a more robust pose estimator capable\nof handling the inherent sensor-level measurement noise. In this paper,\nrecognizing the challenges of comprehensive solutions in existing literature,\nwe propose a transformer-based 6DoF pose estimator designed to achieve\nstate-of-the-art accuracy under real-world noisy data. To systematically\nvalidate the new solution's performance against the prior art, we also\nintroduce a novel RGBD dataset called Digital Twin Tracking Dataset (DTTD) v2,\nwhich is focused on digital-twin object tracking scenarios. Expanded from an\nexisting DTTD v1, the new dataset adds digital-twin data captured using a\ncutting-edge mobile RGBD sensor suite on Apple iPhone 14 Pro, expanding the\napplicability of our approach to iPhone sensor data. Through extensive\nexperimentation and in-depth analysis, we illustrate the effectiveness of our\nmethods under significant depth data errors, surpassing the performance of\nexisting baselines. Code is made publicly available at:\nhttps://github.com/augcog/Robust-Digital-Twin-Tracking.\n","authors":["Zixun Huang","Keling Yao","Seth Z. Zhao","Chuanyu Pan","Tianjian Xu","Weiyu Feng","Allen Y. Yang"],"pdf_url":"https://arxiv.org/pdf/2309.13570v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07136v3","updated":"2023-10-04T00:15:50Z","published":"2022-06-14T19:49:44Z","title":"Automatic Clipping: Differentially Private Deep Learning Made Easier and\n  Stronger","summary":"  Per-example gradient clipping is a key algorithmic step that enables\npractical differential private (DP) training for deep learning models. The\nchoice of clipping threshold R, however, is vital for achieving high accuracy\nunder DP. We propose an easy-to-use replacement, called automatic clipping,\nthat eliminates the need to tune R for any DP optimizers, including DP-SGD,\nDP-Adam, DP-LAMB and many others. The automatic variants are as private and\ncomputationally efficient as existing DP optimizers, but require no DP-specific\nhyperparameters and thus make DP training as amenable as the standard\nnon-private training. We give a rigorous convergence analysis of automatic\nDP-SGD in the non-convex setting, showing that it can enjoy an asymptotic\nconvergence rate that matches the standard SGD, under a symmetric gradient\nnoise assumption of the per-sample gradients (commonly used in the non-DP\nliterature). We demonstrate on various language and vision tasks that automatic\nclipping outperforms or matches the state-of-the-art, and can be easily\nemployed with minimal changes to existing codebases.\n","authors":["Zhiqi Bu","Yu-Xiang Wang","Sheng Zha","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2206.07136v3.pdf","comment":"accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.03211v1","updated":"2023-10-04T23:33:36Z","published":"2023-10-04T23:33:36Z","title":"On the Performance of Multimodal Language Models","summary":"  Instruction-tuned large language models (LLMs) have demonstrated promising\nzero-shot generalization capabilities across various downstream tasks. Recent\nresearch has introduced multimodal capabilities to LLMs by integrating\nindependently pretrained vision encoders through model grafting. These\nmultimodal variants undergo instruction tuning, similar to LLMs, enabling\neffective zero-shot generalization for multimodal tasks. This study conducts a\ncomparative analysis of different multimodal instruction tuning approaches and\nevaluates their performance across a range of tasks, including complex\nreasoning, conversation, image captioning, multiple-choice questions (MCQs),\nand binary classification. Through rigorous benchmarking and ablation\nexperiments, we reveal key insights for guiding architectural choices when\nincorporating multimodal capabilities into LLMs. However, current approaches\nhave limitations; they do not sufficiently address the need for a diverse\nmultimodal instruction dataset, which is crucial for enhancing task\ngeneralization. Additionally, they overlook issues related to truthfulness and\nfactuality when generating responses. These findings illuminate current\nmethodological constraints in adapting language models for image comprehension\nand provide valuable guidance for researchers and practitioners seeking to\nharness multimodal versions of LLMs.\n","authors":["Utsav Garg","Erhan Bas"],"pdf_url":"https://arxiv.org/pdf/2310.03211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03205v1","updated":"2023-10-04T23:24:22Z","published":"2023-10-04T23:24:22Z","title":"A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized\n  Optimization","summary":"  We propose NeuFace, a 3D face mesh pseudo annotation method on videos via\nneural re-parameterized optimization. Despite the huge progress in 3D face\nreconstruction methods, generating reliable 3D face labels for in-the-wild\ndynamic videos remains challenging. Using NeuFace optimization, we annotate the\nper-view/-frame accurate and consistent face meshes on large-scale face videos,\ncalled the NeuFace-dataset. We investigate how neural re-parameterization helps\nto reconstruct image-aligned facial details on 3D meshes via gradient analysis.\nBy exploiting the naturalness and diversity of 3D faces in our dataset, we\ndemonstrate the usefulness of our dataset for 3D face-related tasks: improving\nthe reconstruction accuracy of an existing 3D face reconstruction model and\nlearning 3D facial motion prior. Code and datasets will be available at\nhttps://neuface-dataset.github.\n","authors":["Kim Youwang","Lee Hyun","Kim Sung-Bin","Suekyeong Nam","Janghoon Ju","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2310.03205v1.pdf","comment":"9 pages, 7 figures, and 3 tables for the main paper. 8 pages, 6\n  figures and 3 tables for the appendix"},{"id":"http://arxiv.org/abs/2307.11932v2","updated":"2023-10-04T22:57:04Z","published":"2023-07-21T22:39:41Z","title":"RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction","summary":"  General scene reconstruction refers to the task of estimating the full 3D\ngeometry and texture of a scene containing previously unseen objects. In many\npractical applications such as AR/VR, autonomous navigation, and robotics, only\na single view of the scene may be available, making the scene reconstruction\ntask challenging. In this paper, we present a method for scene reconstruction\nby structurally breaking the problem into two steps: rendering novel views via\ninpainting and 2D to 3D scene lifting. Specifically, we leverage the\ngeneralization capability of large visual language models (Dalle-2) to inpaint\nthe missing areas of scene color images rendered from different views. Next, we\nlift these inpainted images to 3D by predicting normals of the inpainted image\nand solving for the missing depth values. By predicting for normals instead of\ndepth directly, our method allows for robustness to changes in depth\ndistributions and scale. With rigorous quantitative evaluation, we show that\nour method outperforms multiple baselines while providing generalization to\nnovel objects and scenes.\n","authors":["Isaac Kasahara","Shubham Agrawal","Selim Engin","Nikhil Chavan-Dafle","Shuran Song","Volkan Isler"],"pdf_url":"https://arxiv.org/pdf/2307.11932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13777v2","updated":"2023-10-04T22:40:47Z","published":"2023-09-24T23:16:38Z","title":"Diffeomorphic Multi-Resolution Deep Learning Registration for\n  Applications in Breast MRI","summary":"  In breast surgical planning, accurate registration of MR images across\npatient positions has the potential to improve the localisation of tumours\nduring breast cancer treatment. While learning-based registration methods have\nrecently become the state-of-the-art approach for most medical image\nregistration tasks, these methods have yet to make inroads into breast image\nregistration due to certain difficulties-the lack of rich texture information\nin breast MR images and the need for the deformations to be diffeomophic. In\nthis work, we propose learning strategies for breast MR image registration that\nare amenable to diffeomorphic constraints, together with early experimental\nresults from in-silico and in-vivo experiments. One key contribution of this\nwork is a registration network which produces superior registration outcomes\nfor breast images in addition to providing diffeomorphic guarantees.\n","authors":["Matthew G. French","Gonzalo D. Maso Talou","Thiranja P. Babarenda Gamage","Martyn P. Nash","Poul M. Nielsen","Anthony J. Doyle","Juan Eugenio Iglesias","Yaël Balbastre","Sean I. Young"],"pdf_url":"https://arxiv.org/pdf/2309.13777v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17455v2","updated":"2023-10-04T22:11:50Z","published":"2023-05-27T12:07:21Z","title":"CrossGET: Cross-Guided Ensemble of Tokens for Accelerating\n  Vision-Language Transformers","summary":"  Recent vision-language models have achieved tremendous progress far beyond\nwhat we ever expected. However, their computational costs are also dramatically\ngrowing with rapid development, especially for the large models. It makes model\nacceleration exceedingly critical in a scenario of limited resources. Although\nextensively studied for unimodal models, the acceleration for multimodal\nmodels, especially the vision-language Transformers, is relatively\nunder-explored. To pursue more efficient and accessible vision-language\nTransformers, this paper introduces \\textbf{Cross}-\\textbf{G}uided\n\\textbf{E}nsemble of \\textbf{T}okens (\\textbf{\\emph{CrossGET}}), a universal\nacceleration framework for vision-language Transformers. This framework\nadaptively combines tokens through real-time, cross-modal guidance, thereby\nachieving substantial acceleration while keeping high performance.\n\\textit{CrossGET} has two key innovations: 1) \\textit{Cross-Guided Matching and\nEnsemble}. \\textit{CrossGET} incorporates cross-modal guided token matching and\nensemble to exploit cross-modal information effectively, only introducing\ncross-modal tokens with negligible extra parameters. 2) \\textit{Complete-Graph\nSoft Matching}. In contrast to the existing bipartite soft matching approach,\n\\textit{CrossGET} introduces a complete-graph soft matching policy to achieve\nmore reliable token-matching results while maintaining parallelizability and\nhigh efficiency. Extensive experiments are conducted on various vision-language\ntasks, including image-text retrieval, visual reasoning, image captioning, and\nvisual question answering. Performance on both classic multimodal architectures\nand emerging multimodal LLMs demonstrate the effectiveness and versatility of\nthe proposed \\textit{CrossGET} framework. The code will be at\n\\url{https://github.com/sdc17/CrossGET}.\n","authors":["Dachuan Shi","Chaofan Tao","Anyi Rao","Zhendong Yang","Chun Yuan","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2305.17455v2.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2110.15497v4","updated":"2023-10-04T22:05:42Z","published":"2021-10-29T02:32:44Z","title":"Unsupervised Foreground Extraction via Deep Region Competition","summary":"  We present Deep Region Competition (DRC), an algorithm designed to extract\nforeground objects from images in a fully unsupervised manner. Foreground\nextraction can be viewed as a special case of generic image segmentation that\nfocuses on identifying and disentangling objects from the background. In this\nwork, we rethink the foreground extraction by reconciling energy-based prior\nwith generative image modeling in the form of Mixture of Experts (MoE), where\nwe further introduce the learned pixel re-assignment as the essential inductive\nbias to capture the regularities of background regions. With this modeling, the\nforeground-background partition can be naturally found through\nExpectation-Maximization (EM). We show that the proposed method effectively\nexploits the interaction between the mixture components during the partitioning\nprocess, which closely connects to region competition, a seminal approach for\ngeneric image segmentation. Experiments demonstrate that DRC exhibits more\ncompetitive performances on complex real-world data and challenging\nmulti-object scenes compared with prior methods. Moreover, we show empirically\nthat DRC can potentially generalize to novel foreground objects even from\ncategories unseen during training.\n","authors":["Peiyu Yu","Sirui Xie","Xiaojian Ma","Yixin Zhu","Ying Nian Wu","Song-Chun Zhu"],"pdf_url":"https://arxiv.org/pdf/2110.15497v4.pdf","comment":"NeurIPS 2021"},{"id":"http://arxiv.org/abs/2310.03182v1","updated":"2023-10-04T21:57:09Z","published":"2023-10-04T21:57:09Z","title":"Robust and Interpretable Medical Image Classifiers via Concept\n  Bottleneck Models","summary":"  Medical image classification is a critical problem for healthcare, with the\npotential to alleviate the workload of doctors and facilitate diagnoses of\npatients. However, two challenges arise when deploying deep learning models to\nreal-world healthcare applications. First, neural models tend to learn spurious\ncorrelations instead of desired features, which could fall short when\ngeneralizing to new domains (e.g., patients with different ages). Second, these\nblack-box models lack interpretability. When making diagnostic predictions, it\nis important to understand why a model makes a decision for trustworthy and\nsafety considerations. In this paper, to address these two limitations, we\npropose a new paradigm to build robust and interpretable medical image\nclassifiers with natural language concepts. Specifically, we first query\nclinical concepts from GPT-4, then transform latent image features into\nexplicit concepts with a vision-language model. We systematically evaluate our\nmethod on eight medical image classification datasets to verify its\neffectiveness. On challenging datasets with strong confounding factors, our\nmethod can mitigate spurious correlations thus substantially outperform\nstandard visual encoders and other baselines. Finally, we show how\nclassification with a small number of concepts brings a level of\ninterpretability for understanding model decisions through case studies in real\nmedical data.\n","authors":["An Yan","Yu Wang","Yiwu Zhong","Zexue He","Petros Karypis","Zihan Wang","Chengyu Dong","Amilcare Gentili","Chun-Nan Hsu","Jingbo Shang","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2310.03182v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2308.16150v2","updated":"2023-10-04T21:20:18Z","published":"2023-08-30T17:16:02Z","title":"Modality Cycles with Masked Conditional Diffusion for Unsupervised\n  Anomaly Segmentation in MRI","summary":"  Unsupervised anomaly segmentation aims to detect patterns that are distinct\nfrom any patterns processed during training, commonly called abnormal or\nout-of-distribution patterns, without providing any associated manual\nsegmentations. Since anomalies during deployment can lead to model failure,\ndetecting the anomaly can enhance the reliability of models, which is valuable\nin high-risk domains like medical imaging. This paper introduces Masked\nModality Cycles with Conditional Diffusion (MMCCD), a method that enables\nsegmentation of anomalies across diverse patterns in multimodal MRI. The method\nis based on two fundamental ideas. First, we propose the use of cyclic modality\ntranslation as a mechanism for enabling abnormality detection.\nImage-translation models learn tissue-specific modality mappings, which are\ncharacteristic of tissue physiology. Thus, these learned mappings fail to\ntranslate tissues or image patterns that have never been encountered during\ntraining, and the error enables their segmentation. Furthermore, we combine\nimage translation with a masked conditional diffusion model, which attempts to\n`imagine' what tissue exists under a masked area, further exposing unknown\npatterns as the generative model fails to recreate them. We evaluate our method\non a proxy task by training on healthy-looking slices of BraTS2021\nmulti-modality MRIs and testing on slices with tumors. We show that our method\ncompares favorably to previous unsupervised approaches based on image\nreconstruction and denoising with autoencoders and diffusion models.\n","authors":["Ziyun Liang","Harry Anthony","Felix Wagner","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2308.16150v2.pdf","comment":"Accepted in Multiscale Multimodal Medical Imaging workshop in MICCAI\n  2023"},{"id":"http://arxiv.org/abs/2310.01662v2","updated":"2023-10-04T20:39:08Z","published":"2023-10-02T21:52:47Z","title":"SYRAC: Synthesize, Rank, and Count","summary":"  Crowd counting is a critical task in computer vision, with several important\napplications. However, existing counting methods rely on labor-intensive\ndensity map annotations, necessitating the manual localization of each\nindividual pedestrian. While recent efforts have attempted to alleviate the\nannotation burden through weakly or semi-supervised learning, these approaches\nfall short of significantly reducing the workload. We propose a novel approach\nto eliminate the annotation burden by leveraging latent diffusion models to\ngenerate synthetic data. However, these models struggle to reliably understand\nobject quantities, leading to noisy annotations when prompted to produce images\nwith a specific quantity of objects. To address this, we use latent diffusion\nmodels to create two types of synthetic data: one by removing pedestrians from\nreal images, which generates ranked image pairs with a weak but reliable object\nquantity signal, and the other by generating synthetic images with a\npredetermined number of objects, offering a strong but noisy counting signal.\nOur method utilizes the ranking image pairs for pre-training and then fits a\nlinear layer to the noisy synthetic images using these crowd quantity features.\nWe report state-of-the-art results for unsupervised crowd counting.\n","authors":["Adriano D'Alessandro","Ali Mahdavi-Amiri","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2310.01662v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03149v1","updated":"2023-10-04T20:26:59Z","published":"2023-10-04T20:26:59Z","title":"Attributing Learned Concepts in Neural Networks to Training Data","summary":"  By now there is substantial evidence that deep learning models learn certain\nhuman-interpretable features as part of their internal representations of data.\nAs having the right (or wrong) concepts is critical to trustworthy machine\nlearning systems, it is natural to ask which inputs from the model's original\ntraining set were most important for learning a concept at a given layer. To\nanswer this, we combine data attribution methods with methods for probing the\nconcepts learned by a model. Training network and probe ensembles for two\nconcept datasets on a range of network layers, we use the recently developed\nTRAK method for large-scale data attribution. We find some evidence for\nconvergence, where removing the 10,000 top attributing images for a concept and\nretraining the model does not change the location of the concept in the network\nnor the probing sparsity of the concept. This suggests that rather than being\nhighly dependent on a few specific examples, the features that inform the\ndevelopment of a concept are spread in a more diffuse manner across its\nexemplars, implying robustness in concept formation.\n","authors":["Nicholas Konz","Charles Godfrey","Madelyn Shapiro","Jonathan Tu","Henry Kvinge","Davis Brown"],"pdf_url":"https://arxiv.org/pdf/2310.03149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03140v1","updated":"2023-10-04T20:05:40Z","published":"2023-10-04T20:05:40Z","title":"ViFiT: Reconstructing Vision Trajectories from IMU and Wi-Fi Fine Time\n  Measurements","summary":"  Tracking subjects in videos is one of the most widely used functions in\ncamera-based IoT applications such as security surveillance, smart city traffic\nsafety enhancement, vehicle to pedestrian communication and so on. In the\ncomputer vision domain, tracking is usually achieved by first detecting\nsubjects with bounding boxes, then associating detected bounding boxes across\nvideo frames. For many IoT systems, images captured by cameras are usually sent\nover the network to be processed at a different site that has more powerful\ncomputing resources than edge devices. However, sending entire frames through\nthe network causes significant bandwidth consumption that may exceed the system\nbandwidth constraints. To tackle this problem, we propose ViFiT, a\ntransformer-based model that reconstructs vision bounding box trajectories from\nphone data (IMU and Fine Time Measurements). It leverages a transformer ability\nof better modeling long-term time series data. ViFiT is evaluated on Vi-Fi\nDataset, a large-scale multimodal dataset in 5 diverse real world scenes,\nincluding indoor and outdoor environments. To fill the gap of proper metrics of\njointly capturing the system characteristics of both tracking quality and video\nbandwidth reduction, we propose a novel evaluation framework dubbed Minimum\nRequired Frames (MRF) and Minimum Required Frames Ratio (MRFR). ViFiT achieves\nan MRFR of 0.65 that outperforms the state-of-the-art approach for cross-modal\nreconstruction in LSTM Encoder-Decoder architecture X-Translator of 0.98,\nresulting in a high frame reduction rate as 97.76%.\n","authors":["Bryan Bo Cao","Abrar Alali","Hansi Liu","Nicholas Meegan","Marco Gruteser","Kristin Dana","Ashwin Ashok","Shubham Jain"],"pdf_url":"https://arxiv.org/pdf/2310.03140v1.pdf","comment":"22 pages, 12 figures, 9 tables. MobiCom 2023 ISACom"},{"id":"http://arxiv.org/abs/2310.03125v1","updated":"2023-10-04T19:35:56Z","published":"2023-10-04T19:35:56Z","title":"Shielding the Unseen: Privacy Protection through Poisoning NeRF with\n  Spatial Deformation","summary":"  In this paper, we introduce an innovative method of safeguarding user privacy\nagainst the generative capabilities of Neural Radiance Fields (NeRF) models.\nOur novel poisoning attack method induces changes to observed views that are\nimperceptible to the human eye, yet potent enough to disrupt NeRF's ability to\naccurately reconstruct a 3D scene. To achieve this, we devise a bi-level\noptimization algorithm incorporating a Projected Gradient Descent (PGD)-based\nspatial deformation. We extensively test our approach on two common NeRF\nbenchmark datasets consisting of 29 real-world scenes with high-quality images.\nOur results compellingly demonstrate that our privacy-preserving method\nsignificantly impairs NeRF's performance across these benchmark datasets.\nAdditionally, we show that our method is adaptable and versatile, functioning\nacross various perturbation strengths and NeRF architectures. This work offers\nvaluable insights into NeRF's vulnerabilities and emphasizes the need to\naccount for such potential privacy risks when developing robust 3D scene\nreconstruction algorithms. Our study contributes to the larger conversation\nsurrounding responsible AI and generative machine learning, aiming to protect\nuser privacy and respect creative ownership in the digital age.\n","authors":["Yihan Wu","Brandon Y. Feng","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2310.03125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03118v1","updated":"2023-10-04T19:13:16Z","published":"2023-10-04T19:13:16Z","title":"Blind CT Image Quality Assessment Using DDPM-derived Content and\n  Transformer-based Evaluator","summary":"  Lowering radiation dose per view and utilizing sparse views per scan are two\ncommon CT scan modes, albeit often leading to distorted images characterized by\nnoise and streak artifacts. Blind image quality assessment (BIQA) strives to\nevaluate perceptual quality in alignment with what radiologists perceive, which\nplays an important role in advancing low-dose CT reconstruction techniques. An\nintriguing direction involves developing BIQA methods that mimic the\noperational characteristic of the human visual system (HVS). The internal\ngenerative mechanism (IGM) theory reveals that the HVS actively deduces primary\ncontent to enhance comprehension. In this study, we introduce an innovative\nBIQA metric that emulates the active inference process of IGM. Initially, an\nactive inference module, implemented as a denoising diffusion probabilistic\nmodel (DDPM), is constructed to anticipate the primary content. Then, the\ndissimilarity map is derived by assessing the interrelation between the\ndistorted image and its primary content. Subsequently, the distorted image and\ndissimilarity map are combined into a multi-channel image, which is inputted\ninto a transformer-based image quality evaluator. Remarkably, by exclusively\nutilizing this transformer-based quality evaluator, we won the second place in\nthe MICCAI 2023 low-dose computed tomography perceptual image quality\nassessment grand challenge. Leveraging the DDPM-derived primary content, our\napproach further improves the performance on the challenge dataset.\n","authors":["Yongyi Shi","Wenjun Xia","Ge Wang","Xuanqin Mou"],"pdf_url":"https://arxiv.org/pdf/2310.03118v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.03108v1","updated":"2023-10-04T18:58:47Z","published":"2023-10-04T18:58:47Z","title":"Reinforcement Learning-based Mixture of Vision Transformers for Video\n  Violence Recognition","summary":"  Video violence recognition based on deep learning concerns accurate yet\nscalable human violence recognition. Currently, most state-of-the-art video\nviolence recognition studies use CNN-based models to represent and categorize\nvideos. However, recent studies suggest that pre-trained transformers are more\naccurate than CNN-based models on various video analysis benchmarks. Yet these\nmodels are not thoroughly evaluated for video violence recognition. This paper\nintroduces a novel transformer-based Mixture of Experts (MoE) video violence\nrecognition system. Through an intelligent combination of large vision\ntransformers and efficient transformer architectures, the proposed system not\nonly takes advantage of the vision transformer architecture but also reduces\nthe cost of utilizing large vision transformers. The proposed architecture\nmaximizes violence recognition system accuracy while actively reducing\ncomputational costs through a reinforcement learning-based router. The\nempirical results show the proposed MoE architecture's superiority over\nCNN-based models by achieving 92.4% accuracy on the RWF dataset.\n","authors":["Hamid Mohammadi","Ehsan Nazerfard","Tahereh Firoozi"],"pdf_url":"https://arxiv.org/pdf/2310.03108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.00696v2","updated":"2023-10-04T18:56:52Z","published":"2021-02-01T08:30:42Z","title":"Numerical Weather Forecasting using Convolutional-LSTM with Attention\n  and Context Matcher Mechanisms","summary":"  Numerical weather forecasting using high-resolution physical models often\nrequires extensive computational resources on supercomputers, which diminishes\ntheir wide usage in most real-life applications. As a remedy, applying deep\nlearning methods has revealed innovative solutions within this field. To this\nend, we introduce a novel deep learning architecture for forecasting\nhigh-resolution spatio-temporal weather data. Our approach extends the\nconventional encoder-decoder structure by integrating Convolutional Long-short\nTerm Memory and Convolutional Neural Networks. In addition, we incorporate\nattention and context matcher mechanisms into the model architecture. Our\nWeather Model achieves significant performance improvements compared to\nbaseline deep learning models, including ConvLSTM, TrajGRU, and U-Net. Our\nexperimental evaluation involves high-scale, real-world benchmark numerical\nweather datasets, namely the ERA5 hourly dataset on pressure levels and\nWeatherBench. Our results demonstrate substantial improvements in identifying\nspatial and temporal correlations with attention matrices focusing on distinct\nparts of the input series to model atmospheric circulations. We also compare\nour model with high-resolution physical models using the benchmark metrics and\nshow that our Weather Model is accurate and easy to interpret.\n","authors":["Selim Furkan Tekin","Arda Fazla","Suleyman Serdar Kozat"],"pdf_url":"https://arxiv.org/pdf/2102.00696v2.pdf","comment":"- In our journal submission, we removed the integration of the\n  observational data section since it was not used in the experiments. Thus, we\n  also removed the authors from the paper who were responsible for that\n  section. - In the second version, we also performed an experiment on\n  WeatherBench. We compare our results with the Physical Weather Forecasting\n  Models"},{"id":"http://arxiv.org/abs/2310.03106v1","updated":"2023-10-04T18:51:25Z","published":"2023-10-04T18:51:25Z","title":"Creating an Atlas of Normal Tissue for Pruning WSI Patching Through\n  Anomaly Detection","summary":"  Patching gigapixel whole slide images (WSIs) is an important task in\ncomputational pathology. Some methods have been proposed to select a subset of\npatches as WSI representation for downstream tasks. While most of the\ncomputational pathology tasks are designed to classify or detect the presence\nof pathological lesions in each WSI, the confounding role and redundant nature\nof normal histology in tissue samples are generally overlooked in WSI\nrepresentations. In this paper, we propose and validate the concept of an\n\"atlas of normal tissue\" solely using samples of WSIs obtained from normal\ntissue biopsies. Such atlases can be employed to eliminate normal fragments of\ntissue samples and hence increase the representativeness collection of patches.\nWe tested our proposed method by establishing a normal atlas using 107 normal\nskin WSIs and demonstrated how established indexes and search engines like\nYottixel can be improved. We used 553 WSIs of cutaneous squamous cell carcinoma\n(cSCC) to show the advantage. We also validated our method applied to an\nexternal dataset of 451 breast WSIs. The number of selected WSI patches was\nreduced by 30% to 50% after utilizing the proposed normal atlas while\nmaintaining the same indexing and search performance in leave-one-patinet-out\nvalidation for both datasets. We show that the proposed normal atlas shows\npromise for unsupervised selection of the most representative patches of the\nabnormal/malignant WSI lesions.\n","authors":["Peyman Nejat","Areej Alsaafin","Ghazal Alabtah","Nneka Comfere","Aaron Mangold","Dennis Murphree","Patricija Zot","Saba Yasir","Joaquin J. Garcia","H. R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2310.03106v1.pdf","comment":"13 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.03091v1","updated":"2023-10-04T18:18:24Z","published":"2023-10-04T18:18:24Z","title":"Privacy-preserving Multi-biometric Indexing based on Frequent Binary\n  Patterns","summary":"  The development of large-scale identification systems that ensure the privacy\nprotection of enrolled subjects represents a major challenge. Biometric\ndeployments that provide interoperability and usability by including efficient\nmulti-biometric solutions are a recent requirement. In the context of privacy\nprotection, several template protection schemes have been proposed in the past.\nHowever, these schemes seem inadequate for indexing (workload reduction) in\nbiometric identification systems. More specifically, they have been used in\nidentification systems that perform exhaustive searches, leading to a\ndegradation of computational efficiency. To overcome these limitations, we\npropose an efficient privacy-preserving multi-biometric identification system\nthat retrieves protected deep cancelable templates and is agnostic with respect\nto biometric characteristics and biometric template protection schemes. To this\nend, a multi-biometric binning scheme is designed to exploit the low\nintra-class variation properties contained in the frequent binary patterns\nextracted from different types of biometric characteristics. Experimental\nresults reported on publicly available databases using state-of-the-art Deep\nNeural Network (DNN)-based embedding extractors show that the protected\nmulti-biometric identification system can reduce the computational workload to\napproximately 57\\% (indexing up to three types of biometric characteristics)\nand 53% (indexing up to two types of biometric characteristics), while\nsimultaneously improving the biometric performance of the baseline biometric\nsystem at the high-security thresholds. The source code of the proposed\nmulti-biometric indexing approach together with the composed multi-biometric\ndataset, will be made available to the research community once the article is\naccepted.\n","authors":["Daile Osorio-Roig","Lazaro J. Gonzalez-Soler","Christian Rathgeb","Christoph Busch"],"pdf_url":"https://arxiv.org/pdf/2310.03091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03059v1","updated":"2023-10-04T16:49:36Z","published":"2023-10-04T16:49:36Z","title":"Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models","summary":"  The popularity of pre-trained large models has revolutionized downstream\ntasks across diverse fields, such as language, vision, and multi-modality. To\nminimize the adaption cost for downstream tasks, many Parameter-Efficient\nFine-Tuning (PEFT) techniques are proposed for language and 2D image\npre-trained models. However, the specialized PEFT method for 3D pre-trained\nmodels is still under-explored. To this end, we introduce Point-PEFT, a novel\nframework for adapting point cloud pre-trained models with minimal learnable\nparameters. Specifically, for a pre-trained 3D model, we freeze most of its\nparameters, and only tune the newly added PEFT modules on downstream tasks,\nwhich consist of a Point-prior Prompt and a Geometry-aware Adapter. The\nPoint-prior Prompt adopts a set of learnable prompt tokens, for which we\npropose to construct a memory bank with domain-specific knowledge, and utilize\na parameter-free attention to enhance the prompt tokens. The Geometry-aware\nAdapter aims to aggregate point cloud features within spatial neighborhoods to\ncapture fine-grained geometric information through local interactions.\nExtensive experiments indicate that our Point-PEFT can achieve better\nperformance than the full fine-tuning on various downstream tasks, while using\nonly 5% of the trainable parameters, demonstrating the efficiency and\neffectiveness of our approach. Code will be released at\nhttps://github.com/EvenJoker/Point-PEFT.\n","authors":["Ivan Tang","Eric Zhang","Ray Gu"],"pdf_url":"https://arxiv.org/pdf/2310.03059v1.pdf","comment":"10 pages. The specialized PEFT framework for 3D pre-trained models,\n  which achieves competitive performance to full fine-tuning, and significantly\n  reduces the computational resources. Project page:\n  https://github.com/EvenJoker/Point-PEFT"},{"id":"http://arxiv.org/abs/2310.00031v2","updated":"2023-10-04T14:57:09Z","published":"2023-09-29T05:16:41Z","title":"Text-image Alignment for Diffusion-based Perception","summary":"  Diffusion models are generative models with impressive text-to-image\nsynthesis capabilities and have spurred a new wave of creative methods for\nclassical machine learning tasks. However, the best way to harness the\nperceptual knowledge of these generative models for visual tasks is still an\nopen question. Specifically, it is unclear how to use the prompting interface\nwhen applying diffusion backbones to vision tasks. We find that automatically\ngenerated captions can improve text-image alignment and significantly enhance a\nmodel's cross-attention maps, leading to better perceptual performance. Our\napproach improves upon the current SOTA in diffusion-based semantic\nsegmentation on ADE20K and the current overall SOTA in depth estimation on\nNYUv2. Furthermore, our method generalizes to the cross-domain setting; we use\nmodel personalization and caption modifications to align our model to the\ntarget domain and find improvements over unaligned baselines. Our object\ndetection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K.\nOur segmentation method, trained on Cityscapes, achieves SOTA results on Dark\nZurich-val and Nighttime Driving. Project page:\nhttps://www.vision.caltech.edu/tadp/\n","authors":["Neehar Kondapaneni","Markus Marks","Manuel Knott","Rogério Guimarães","Pietro Perona"],"pdf_url":"https://arxiv.org/pdf/2310.00031v2.pdf","comment":"Project page: https://www.vision.caltech.edu/tadp/"}]},"2023-10-05T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2310.03743v1","updated":"2023-10-05T17:59:55Z","published":"2023-10-05T17:59:55Z","title":"The Un-Kidnappable Robot: Acoustic Localization of Sneaking People","summary":"  How easy is it to sneak up on a robot? We examine whether we can detect\npeople using only the incidental sounds they produce as they move, even when\nthey try to be quiet. We collect a robotic dataset of high-quality 4-channel\naudio paired with 360 degree RGB data of people moving in different indoor\nsettings. We train models that predict if there is a moving person nearby and\ntheir location using only audio. We implement our method on a robot, allowing\nit to track a single person moving quietly with only passive audio sensing. For\ndemonstration videos, see our project page:\nhttps://sites.google.com/view/unkidnappable-robot\n","authors":["Mengyu Yang","Patrick Grady","Samarth Brahmbhatt","Arun Balajee Vasudevan","Charles C. Kemp","James Hays"],"pdf_url":"https://arxiv.org/pdf/2310.03743v1.pdf","comment":"Project page: https://sites.google.com/view/unkidnappable-robot"},{"id":"http://arxiv.org/abs/2310.03740v1","updated":"2023-10-05T17:59:45Z","published":"2023-10-05T17:59:45Z","title":"ContactGen: Generative Contact Modeling for Grasp Generation","summary":"  This paper presents a novel object-centric contact representation ContactGen\nfor hand-object interaction. The ContactGen comprises three components: a\ncontact map indicates the contact location, a part map represents the contact\nhand part, and a direction map tells the contact direction within each part.\nGiven an input object, we propose a conditional generative model to predict\nContactGen and adopt model-based optimization to predict diverse and\ngeometrically feasible grasps. Experimental results demonstrate our method can\ngenerate high-fidelity and diverse human grasps for various objects. Project\npage: https://stevenlsw.github.io/contactgen/\n","authors":["Shaowei Liu","Yang Zhou","Jimei Yang","Saurabh Gupta","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.03740v1.pdf","comment":"Accepted to ICCV 2023. Website:\n  https://stevenlsw.github.io/contactgen/"},{"id":"http://arxiv.org/abs/2310.03739v1","updated":"2023-10-05T17:59:18Z","published":"2023-10-05T17:59:18Z","title":"Aligning Text-to-Image Diffusion Models with Reward Backpropagation","summary":"  Text-to-image diffusion models have recently emerged at the forefront of\nimage generation, powered by very large-scale unsupervised or weakly supervised\ntext-to-image training datasets. Due to their unsupervised training,\ncontrolling their behavior in downstream tasks, such as maximizing\nhuman-perceived image quality, image-text alignment, or ethical image\ngeneration, is difficult. Recent works finetune diffusion models to downstream\nreward functions using vanilla reinforcement learning, notorious for the high\nvariance of the gradient estimators. In this paper, we propose AlignProp, a\nmethod that aligns diffusion models to downstream reward functions using\nend-to-end backpropagation of the reward gradient through the denoising\nprocess. While naive implementation of such backpropagation would require\nprohibitive memory resources for storing the partial derivatives of modern\ntext-to-image models, AlignProp finetunes low-rank adapter weight modules and\nuses gradient checkpointing, to render its memory usage viable. We test\nAlignProp in finetuning diffusion models to various objectives, such as\nimage-text semantic alignment, aesthetics, compressibility and controllability\nof the number of objects present, as well as their combinations. We show\nAlignProp achieves higher rewards in fewer training steps than alternatives,\nwhile being conceptually simpler, making it a straightforward choice for\noptimizing diffusion models for differentiable reward functions of interest.\nCode and Visualization results are available at https://align-prop.github.io/.\n","authors":["Mihir Prabhudesai","Anirudh Goyal","Deepak Pathak","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2310.03739v1.pdf","comment":"Code available at https://align-prop.github.io/"},{"id":"http://arxiv.org/abs/2310.03687v1","updated":"2023-10-05T17:07:25Z","published":"2023-10-05T17:07:25Z","title":"Probabilistic Generative Modeling for Procedural Roundabout Generation\n  for Developing Countries","summary":"  Due to limited resources and fast economic growth, designing optimal\ntransportation road networks with traffic simulation and validation in a\ncost-effective manner is vital for developing countries, where extensive manual\ntesting is expensive and often infeasible. Current rule-based road design\ngenerators lack diversity, a key feature for design robustness. Generative Flow\nNetworks (GFlowNets) learn stochastic policies to sample from an unnormalized\nreward distribution, thus generating high-quality solutions while preserving\ntheir diversity. In this work, we formulate the problem of linking incident\nroads to the circular junction of a roundabout by a Markov decision process,\nand we leverage GFlowNets as the Junction-Art road generator. We compare our\nmethod with related methods and our empirical results show that our method\nachieves better diversity while preserving a high validity score.\n","authors":["Zarif Ikram","Ling Pan","Dianbo Liu"],"pdf_url":"https://arxiv.org/pdf/2310.03687v1.pdf","comment":"6 pages. Submitted to ReALML@NeurIPS (2023)"},{"id":"http://arxiv.org/abs/2310.03676v1","updated":"2023-10-05T16:52:59Z","published":"2023-10-05T16:52:59Z","title":"PV-OSIMr: A Lowest Order Complexity Algorithm for Computing the Delassus\n  Matrix","summary":"  We present PV-OSIMr, an efficient algorithm for computing the Delassus matrix\n(also known as the inverse operational space inertia matrix) for a kinematic\ntree, with the lowest order computational complexity known in literature.\nPV-OSIMr is derived by optimizing the Popov-Vereshchagin (PV) solver\ncomputations using the compositionality of the force and motion propagators. It\nhas a computational complexity of O(n + m^2 ) compared to O(n + m^2d) of the\noriginal PV-OSIM algorithm and O(n+md+m^2 ) of the extended force propagator\nalgorithm (EFPA), where n is the number of joints, m is the number of\nconstraints and d is the depth of the kinematic tree. Since Delassus matrix\ncomputation requires constructing an m x m sized matrix and must consider all\nthe n joints at least once, the asymptotic computational complexity of PV-OSIMr\nis optimal. We further benchmark our algorithm and find it to be often more\nefficient than the PV-OSIM and EFPA in practice.\n","authors":["Ajay Suresha Sathya","Wilm Decre","Jan Swevers"],"pdf_url":"https://arxiv.org/pdf/2310.03676v1.pdf","comment":"8 pages, submitted for review"},{"id":"http://arxiv.org/abs/2107.08086v2","updated":"2023-10-05T16:28:20Z","published":"2021-07-16T19:21:43Z","title":"An Information-state based Approach to the Optimal Output Feedback\n  Control of Nonlinear Systems","summary":"  This paper develops a data-based approach to the closed-loop output feedback\ncontrol of nonlinear dynamical systems with a partial nonlinear observation\nmodel. We propose an information state based approach to rigorously transform\nthe partially observed problem into a fully observed problem where the\ninformation state consists of the past several observations and control inputs.\nWe further show the equivalence of the transformed and the initial partially\nobserved optimal control problems and provide the conditions to solve for the\ndeterministic optimal solution. We develop a data based generalization of the\niterative Linear Quadratic Regulator (iLQR) to partially observed systems using\na local linear time varying model of the information state dynamics\napproximated by an Autoregressive moving average (ARMA) model, that is\ngenerated using only the input-output data. This open-loop trajectory\noptimization solution is then used to design a local feedback control law, and\nthe composite law then provides an optimum solution to the partially observed\nfeedback design problem. The efficacy of the developed method is shown by\ncontrolling complex high dimensional nonlinear dynamical systems in the\npresence of model and sensing uncertainty.\n","authors":["Raman Goyal","Ran Wang","Mohamed Naveed Gul Mohamed","Aayushman Sharma","Suman Chakravorty"],"pdf_url":"https://arxiv.org/pdf/2107.08086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10840v2","updated":"2023-10-05T16:13:17Z","published":"2023-06-19T10:40:09Z","title":"RedMotion: Motion Prediction via Redundancy Reduction","summary":"  Predicting the future motion of traffic agents is vital for self-driving\nvehicles to ensure their safe operation. We introduce RedMotion, a transformer\nmodel for motion prediction that incorporates two types of redundancy\nreduction. The first type of redundancy reduction is induced by an internal\ntransformer decoder and reduces a variable-sized set of road environment\ntokens, such as road graphs with agent data, to a fixed-sized embedding. The\nsecond type of redundancy reduction is a self-supervised learning objective and\napplies the redundancy reduction principle to embeddings generated from\naugmented views of road environments. Our experiments reveal that our\nrepresentation learning approach can outperform PreTraM, Traj-MAE, and\nGraphDINO in a semi-supervised setting. Our RedMotion model achieves results\nthat are competitive with those of Scene Transformer or MTR++. We provide an\nopen source implementation that is accessible via GitHub\n(https://github.com/kit-mrt/red-motion) and Colab\n(https://colab.research.google.com/drive/1Q-Z9VdiqvfPfctNG8oqzPcgm0lP3y1il).\n","authors":["Royden Wagner","Omer Sahin Tas","Marvin Klemp","Carlos Fernandez Lopez"],"pdf_url":"https://arxiv.org/pdf/2306.10840v2.pdf","comment":"Technical report, 13 pages, 8 figures; v2: focus on transformer model"},{"id":"http://arxiv.org/abs/2310.03624v1","updated":"2023-10-05T16:01:29Z","published":"2023-10-05T16:01:29Z","title":"High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling\n  and Motion Planning","summary":"  A robot self-model is a task-agnostic representation of the robot's physical\nmorphology that can be used for motion planning tasks in absence of classical\ngeometric kinematic models. In particular, when the latter are hard to engineer\nor the robot's kinematics change unexpectedly, human-free self-modeling is a\nnecessary feature of truly autonomous agents. In this work, we leverage neural\nfields to allow a robot to self-model its kinematics as a neural-implicit query\nmodel learned only from 2D images annotated with camera poses and\nconfigurations. This enables significantly greater applicability than existing\napproaches which have been dependent on depth images or geometry knowledge. To\nthis end, alongside a curricular data sampling strategy, we propose a new\nencoder-based neural density field architecture for dynamic object-centric\nscenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF\nrobot test setup, the learned self-model achieves a Chamfer-L2 distance of 2%\nof the robot's workspace dimension. We demonstrate the capabilities of this\nmodel on a motion planning task as an exemplary downstream application.\n","authors":["Lennart Schulze","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2310.03624v1.pdf","comment":"ICCV 2023 Workshop on Neural Fields for Autonomous Driving and\n  Robotics (oral)"},{"id":"http://arxiv.org/abs/2310.03586v1","updated":"2023-10-05T15:11:10Z","published":"2023-10-05T15:11:10Z","title":"A Suspended Aerial Manipulation Avatar for Physical Interaction in\n  Unstructured Environments","summary":"  This paper presents a floating robot capable of performing physically\ninteractive tasks in unstructured environments with human-like dexterity under\nhuman supervision. The robot consists of a humanoid torso attached to a\nhexacopter. A two-degree-of-freedom head and two five-degree-of-freedom arms\nequipped with softhands provide the requisite dexterity to allow human\noperators to carry out various tasks. A robust tendon-driven structure is\npurposefully designed for the arms, considerably reducing the impact of arm\ninertia on the floating base in motion. In addition, tendons provide\nflexibility to the joints, which enhances the robustness of the arm preventing\ndamage in interaction with the environment. To increase the payload of the\naerial system and the battery life, we use the concept of Suspended Aerial\nManipulation, i.e., the flying humanoid can be connected with a tether to a\nstructure, e.g., a larger airborne carrier or a supporting crane. Importantly,\nto maximize portability and applicability, we adopt a modular approach\nexploiting commercial components for the drone hardware and autopilot, while\ndeveloping a whole-body outer control loop to stabilize the robot attitude,\ncompensating for the tether force and for the humanoid head and arm motions.\nThe humanoid can be controlled by a remote operator, thus effectively realizing\na Suspended Aerial Manipulation Avatar. The proposed system is validated\nthrough experiments in indoor scenarios reproducing post-disaster tasks.\n","authors":["Fanyi Kong","Grazia Zambella","Simone Monteleone","Giorgio Grioli","Manuel G. Catalano","Antonio Bicchi"],"pdf_url":"https://arxiv.org/pdf/2310.03586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03581v1","updated":"2023-10-05T15:01:31Z","published":"2023-10-05T15:01:31Z","title":"Resilient Legged Local Navigation: Learning to Traverse with Compromised\n  Perception End-to-End","summary":"  Autonomous robots must navigate reliably in unknown environments even under\ncompromised exteroceptive perception, or perception failures. Such failures\noften occur when harsh environments lead to degraded sensing, or when the\nperception algorithm misinterprets the scene due to limited generalization. In\nthis paper, we model perception failures as invisible obstacles and pits, and\ntrain a reinforcement learning (RL) based local navigation policy to guide our\nlegged robot. Unlike previous works relying on heuristics and anomaly detection\nto update navigational information, we train our navigation policy to\nreconstruct the environment information in the latent space from corrupted\nperception and react to perception failures end-to-end. To this end, we\nincorporate both proprioception and exteroception into our policy inputs,\nthereby enabling the policy to sense collisions on different body parts and\npits, prompting corresponding reactions. We validate our approach in simulation\nand on the real quadruped robot ANYmal running in real-time (<10 ms CPU\ninference). In a quantitative comparison with existing heuristic-based locally\nreactive planners, our policy increases the success rate over 30% when facing\nperception failures. Project Page: https://bit.ly/45NBTuh.\n","authors":["Jin Jin","Chong Zhang","Jonas Frey","Nikita Rudin","Matias Mattamala","Cesar Cadena","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2310.03581v1.pdf","comment":"Website and videos are available at our Project Page:\n  https://bit.ly/45NBTuh"},{"id":"http://arxiv.org/abs/2310.03563v1","updated":"2023-10-05T14:27:06Z","published":"2023-10-05T14:27:06Z","title":"BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance\n  Fields","summary":"  We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which\ndefines the image pose estimation problem as a NeRF based iterative linear\noptimization. NeRFs are novel neural space representation models that can\nsynthesize photorealistic novel views of real-world scenes or objects. Our\ncontributions are as follows: we extend the localization optimization objective\nwith a depth-based loss function, we introduce a multi-image based loss\nfunction where a sequence of images with known relative poses are used without\nincreasing the computational complexity, we omit hierarchical sampling during\nvolumetric rendering, meaning only the coarse model is used for pose\nestimation, and we how that by extending the sampling interval convergence can\nbe achieved even or higher initial pose estimate errors. With the proposed\nmodifications the convergence speed is significantly improved, and the basin of\nconvergence is substantially extended.\n","authors":["Ágoston István Csehi","Csaba Máté Józsa"],"pdf_url":"https://arxiv.org/pdf/2310.03563v1.pdf","comment":"Accepted to Nerf4ADR workshop of ICCV23 conference"},{"id":"http://arxiv.org/abs/2310.03534v1","updated":"2023-10-05T13:34:07Z","published":"2023-10-05T13:34:07Z","title":"3D-Aware Hypothesis & Verification for Generalizable Relative Object\n  Pose Estimation","summary":"  Prior methods that tackle the problem of generalizable object pose estimation\nhighly rely on having dense views of the unseen object. By contrast, we address\nthe scenario where only a single reference view of the object is available. Our\ngoal then is to estimate the relative object pose between this reference view\nand a query image that depicts the object in a different pose. In this\nscenario, robust generalization is imperative due to the presence of unseen\nobjects during testing and the large-scale object pose variation between the\nreference and the query. To this end, we present a new\nhypothesis-and-verification framework, in which we generate and evaluate\nmultiple pose hypotheses, ultimately selecting the most reliable one as the\nrelative object pose. To measure reliability, we introduce a 3D-aware\nverification that explicitly applies 3D transformations to the 3D object\nrepresentations learned from the two input images. Our comprehensive\nexperiments on the Objaverse, LINEMOD, and CO3D datasets evidence the superior\naccuracy of our approach in relative pose estimation and its robustness in\nlarge-scale pose variations, when dealing with unseen objects.\n","authors":["Chen Zhao","Tong Zhang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2310.03534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03505v1","updated":"2023-10-05T12:35:09Z","published":"2023-10-05T12:35:09Z","title":"RadaRays: Real-time Simulation of Rotating FMCW Radar for Mobile\n  Robotics via Hardware-accelerated Ray Tracing","summary":"  RadaRays allows for the accurate modeling and simulation of rotating FMCW\nradar sensors in complex environments, including the simulation of reflection,\nrefraction, and scattering of radar waves. Our software is able to handle large\nnumbers of objects and materials, making it suitable for use in a variety of\nmobile robotics applications. We demonstrate the effectiveness of RadaRays\nthrough a series of experiments and show that it can more accurately reproduce\nthe behavior of FMCW radar sensors in a variety of environments, compared to\nthe ray casting-based lidar-like simulations that are commonly used in\nsimulators for autonomous driving such as CARLA. Our experiments additionally\nserve as valuable reference point for researchers to evaluate their own radar\nsimulations. By using RadaRays, developers can significantly reduce the time\nand cost associated with prototyping and testing FMCW radar-based algorithms.\nWe also provide a Gazebo plugin that makes our work accessible to the mobile\nrobotics community.\n","authors":["Alexander Mock","Martin Magnusson","Joachim Hertzberg"],"pdf_url":"https://arxiv.org/pdf/2310.03505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03478v1","updated":"2023-10-05T11:46:09Z","published":"2023-10-05T11:46:09Z","title":"RGBManip: Monocular Image-based Robotic Manipulation through Active\n  Object Pose Estimation","summary":"  Robotic manipulation requires accurate perception of the environment, which\nposes a significant challenge due to its inherent complexity and constantly\nchanging nature. In this context, RGB image and point-cloud observations are\ntwo commonly used modalities in visual-based robotic manipulation, but each of\nthese modalities have their own limitations. Commercial point-cloud\nobservations often suffer from issues like sparse sampling and noisy output due\nto the limits of the emission-reception imaging principle. On the other hand,\nRGB images, while rich in texture information, lack essential depth and 3D\ninformation crucial for robotic manipulation. To mitigate these challenges, we\npropose an image-only robotic manipulation framework that leverages an\neye-on-hand monocular camera installed on the robot's parallel gripper. By\nmoving with the robot gripper, this camera gains the ability to actively\nperceive object from multiple perspectives during the manipulation process.\nThis enables the estimation of 6D object poses, which can be utilized for\nmanipulation. While, obtaining images from more and diverse viewpoints\ntypically improves pose estimation, it also increases the manipulation time. To\naddress this trade-off, we employ a reinforcement learning policy to\nsynchronize the manipulation strategy with active perception, achieving a\nbalance between 6D pose accuracy and manipulation efficiency. Our experimental\nresults in both simulated and real-world environments showcase the\nstate-of-the-art effectiveness of our approach. %, which, to the best of our\nknowledge, is the first to achieve robust real-world robotic manipulation\nthrough active pose estimation. We believe that our method will inspire further\nresearch on real-world-oriented robotic manipulation.\n","authors":["Boshi An","Yiran Geng","Kai Chen","Xiaoqi Li","Qi Dou","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2310.03478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03470v1","updated":"2023-10-05T11:26:17Z","published":"2023-10-05T11:26:17Z","title":"Cyber Physical System Information Collection: Robot Location and\n  Navigation Method Based on QR Code","summary":"  In this paper, we propose a method to estimate the exact location of a camera\nin a cyber-physical system using the exact geographic coordinates of four\nfeature points stored in QR codes(Quick response codes) and the pixel\ncoordinates of four feature points analyzed from the QR code images taken by\nthe camera. Firstly, the P4P(Perspective 4 Points) algorithm is designed to\nuniquely determine the initial pose estimation value of the QR coordinate\nsystem relative to the camera coordinate system by using the four feature\npoints of the selected QR code. In the second step, the manifold gradient\noptimization algorithm is designed. The rotation matrix and displacement vector\nare taken as the initial values of iteration, and the iterative optimization is\ncarried out to improve the positioning accuracy and obtain the rotation matrix\nand displacement vector with higher accuracy. The third step is to convert the\npose of the QR coordinate system with respect to the camera coordinate system\nto the pose of the AGV(Automated Guided Vehicle) with respect to the world\ncoordinate system. Finally, the performance of manifold gradient optimization\nalgorithm and P4P analytical algorithm are simulated and compared under the\nsame conditions.One can see that the performance of the manifold gradient\noptimization algorithm proposed in this paper is much better than that of the\nP4P analytic algorithm when the signal-to-noise ratio is small.With the\nincrease of the signal-to-noise ratio,the performance of the P4P analytic\nalgorithm approaches that of the manifold gradient optimization algorithm.when\nthe noise is same,the performance of manifold gradient optimization algorithm\nis better when there are more feature points.\n","authors":["Hongwei Li","Tao Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.03470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17260v3","updated":"2023-10-05T10:13:58Z","published":"2023-09-29T14:12:54Z","title":"PlaceNav: Topological Navigation through Place Recognition","summary":"  Recent results suggest that splitting topological navigation into\nrobot-independent and robot-specific components improves navigation performance\nby enabling the robot-independent part to be trained with data collected by\ndifferent robot types. However, the navigation methods are still limited by the\nscarcity of suitable training data and suffer from poor computational scaling.\nIn this work, we present PlaceNav, subdividing the robot-independent part into\nnavigation-specific and generic computer vision components. We utilize visual\nplace recognition for the subgoal selection of the topological navigation\npipeline. This makes subgoal selection more efficient and enables leveraging\nlarge-scale datasets from non-robotics sources, increasing training data\navailability. Bayesian filtering, enabled by place recognition, further\nimproves navigation performance by increasing the temporal consistency of\nsubgoals. Our experimental results verify the design and the new model obtains\na 76% higher success rate in indoor and 23% higher in outdoor navigation tasks\nwith higher computational efficiency.\n","authors":["Lauri Suomela","Jussi Kalliola","Harry Edelman","Joni-Kristian Kämäräinen"],"pdf_url":"https://arxiv.org/pdf/2309.17260v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.00047v2","updated":"2023-10-05T10:02:31Z","published":"2023-02-28T19:43:23Z","title":"Online On-Demand Multi-Robot Coverage Path Planning","summary":"  We present an online centralized path planning algorithm to cover a large,\ncomplex, unknown workspace with multiple homogeneous mobile robots. Our\nalgorithm is horizon-based, synchronous, and on-demand. The recently proposed\nhorizon-based synchronous algorithms compute all the robots' paths in each\nhorizon, significantly increasing the computation burden in large workspaces\nwith many robots. As a remedy, we propose an algorithm that computes the paths\nfor a subset of robots that have traversed previously computed paths entirely\n(thus on-demand) and reuses the remaining paths for the other robots. We\nformally prove that the algorithm guarantees complete coverage of the unknown\nworkspace. Experimental results on several standard benchmark workspaces show\nthat our algorithm scales to hundreds of robots in large complex workspaces and\nconsistently beats a state-of-the-art online centralized multi-robot coverage\npath planning algorithm in terms of the time needed to achieve complete\ncoverage. For its validation, we perform ROS+Gazebo simulations in five 2D grid\nbenchmark workspaces with 10 Quadcopters and 10 TurtleBots, respectively. Also,\nto demonstrate its practical feasibility, we conduct one indoor experiment with\ntwo real TurtleBot2 robots and one outdoor experiment with three real\nQuadcopters.\n","authors":["Ratijit Mitra","Indranil Saha"],"pdf_url":"https://arxiv.org/pdf/2303.00047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03406v1","updated":"2023-10-05T09:22:16Z","published":"2023-10-05T09:22:16Z","title":"RUSOpt: Robotic UltraSound Probe Normalization with Bayesian\n  Optimization for In-plane and Out-plane Scanning","summary":"  The one of the significant challenges faced by autonomous robotic ultrasound\nsystems is acquiring high-quality images across different patients. The proper\norientation of the robotized probe plays a crucial role in governing the\nquality of ultrasound images. To address this challenge, we propose a\nsample-efficient method to automatically adjust the orientation of the\nultrasound probe normal to the point of contact on the scanning surface,\nthereby improving the acoustic coupling of the probe and resulting image\nquality. Our method utilizes Bayesian Optimization (BO) based search on the\nscanning surface to efficiently search for the normalized probe orientation. We\nformulate a novel objective function for BO that leverages the contact force\nmeasurements and underlying mechanics to identify the normal. We further\nincorporate a regularization scheme in BO to handle the noisy objective\nfunction. The performance of the proposed strategy has been assessed through\nexperiments on urinary bladder phantoms. These phantoms included planar,\ntilted, and rough surfaces, and were examined using both linear and convex\nprobes with varying search space limits. Further, simulation-based studies have\nbeen carried out using 3D human mesh models. The results demonstrate that the\nmean ($\\pm$SD) absolute angular error averaged over all phantoms and 3D models\nis $\\boldsymbol{2.4\\pm0.7^\\circ}$ and $\\boldsymbol{2.1\\pm1.3^\\circ}$,\nrespectively.\n","authors":["Deepak Raina","Abhishek Mathur","Richard M. Voyles","Juan Wachs","SH Chandrashekhara","Subir Kumar Saha"],"pdf_url":"https://arxiv.org/pdf/2310.03406v1.pdf","comment":"Accepted in IEEE International Conference on Automation Science and\n  Engineering (CASE) 2023"},{"id":"http://arxiv.org/abs/2307.08493v2","updated":"2023-10-05T09:16:44Z","published":"2023-07-17T13:56:28Z","title":"Occupancy Grid Mapping without Ray-Casting for High-resolution LiDAR\n  Sensors","summary":"  Occupancy mapping is a fundamental component of robotic systems to reason\nabout the unknown and known regions of the environment. This article presents\nan efficient occupancy mapping framework for high-resolution LiDAR sensors,\ntermed D-Map. The framework introduces three main novelties to address the\ncomputational efficiency challenges of occupancy mapping. Firstly, we use a\ndepth image to determine the occupancy state of regions instead of the\ntraditional ray-casting method. Secondly, we introduce an efficient on-tree\nupdate strategy on a tree-based map structure. These two techniques avoid\nredundant visits to small cells, significantly reducing the number of cells to\nbe updated. Thirdly, we remove known cells from the map at each update by\nleveraging the low false alarm rate of LiDAR sensors. This approach not only\nenhances our framework's update efficiency by reducing map size but also endows\nit with an interesting decremental property, which we have named D-Map. To\nsupport our design, we provide theoretical analyses of the accuracy of the\ndepth image projection and time complexity of occupancy updates. Furthermore,\nwe conduct extensive benchmark experiments on various LiDAR sensors in both\npublic and private datasets. Our framework demonstrates superior efficiency in\ncomparison with other state-of-the-art methods while maintaining comparable\nmapping accuracy and high memory efficiency. We demonstrate two real-world\napplications of D-Map for real-time occupancy mapping on a handle device and an\naerial platform carrying a high-resolution LiDAR. In addition, we open-source\nthe implementation of D-Map on GitHub to benefit society:\ngithub.com/hku-mars/D-Map.\n","authors":["Yixi Cai","Fanze Kong","Yunfan Ren","Fangcheng Zhu","Jiarong Lin","Fu Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.08493v2.pdf","comment":"Supplementary material included. Accepted for publication in IEEE\n  Transactions on Robotics (T-RO)"},{"id":"http://arxiv.org/abs/2310.03394v1","updated":"2023-10-05T09:02:22Z","published":"2023-10-05T09:02:22Z","title":"Kinodynamic Motion Planning for a Team of Multirotors Transporting a\n  Cable-Suspended Payload in Cluttered Environments","summary":"  We propose a motion planner for cable-driven payload transportation using\nmultiple unmanned aerial vehicles (UAVs) in an environment cluttered with\nobstacles. Our planner is kinodynamic, i.e., it considers the full dynamics\nmodel of the transporting system including actuation constraints. Due to the\nhigh dimensionality of the planning problem, we use a hierarchical approach\nwhere we first solve the geometric motion planning using a sampling-based\nmethod with a novel sampler, followed by constrained trajectory optimization\nthat considers the full dynamics of the system. Both planning stages consider\ninter-robot and robot/obstacle collisions. We demonstrate in a\nsoftware-in-the-loop simulation that there is a significant benefit in\nkinodynamic motion planning for such payload transport systems with respect to\npayload tracking error and energy consumption compared to the standard methods\nof planning for the payload alone. Notably, we observe a significantly higher\nsuccess rate in scenarios where the team formation changes are needed to move\nthrough tight spaces.\n","authors":["Khaled Wahba","Joaquim Ortiz-Haro","Marc Toussaint","Wolfgang Hönig"],"pdf_url":"https://arxiv.org/pdf/2310.03394v1.pdf","comment":"Submitted to ICRA, 2024"},{"id":"http://arxiv.org/abs/2310.03379v1","updated":"2023-10-05T08:29:35Z","published":"2023-10-05T08:29:35Z","title":"Progressive Adaptive Chance-Constrained Safeguards for Reinforcement\n  Learning","summary":"  Safety assurance of Reinforcement Learning (RL) is critical for exploration\nin real-world scenarios. In handling the Constrained Markov Decision Process,\ncurrent approaches experience intrinsic difficulties in trading-off between\noptimality and feasibility. Direct optimization methods cannot strictly\nguarantee state-wise in-training safety while projection-based methods are\nusually inefficient and correct actions through lengthy iterations. To address\nthese two challenges, this paper proposes an adaptive surrogate chance\nconstraint for the safety cost, and a hierarchical architecture that corrects\nactions produced by the upper policy layer via a fast Quasi-Newton method.\nTheoretical analysis indicates that the relaxed probabilistic constraint can\nsufficiently guarantee forward invariance to the safe set. We validate the\nproposed method on 4 simulated and real-world safety-critical robotic tasks.\nResults indicate that the proposed method can efficiently enforce safety\n(nearly zero-violation), while preserving optimality (+23.8%), robustness and\ngeneralizability to stochastic real-world settings.\n","authors":["Zhaorun Chen","Binhao Chen","Tairan He","Liang Gong","Chengliang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.03379v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.03374v1","updated":"2023-10-05T08:23:17Z","published":"2023-10-05T08:23:17Z","title":"Design Optimizer for Planar Soft-Growing Robot Manipulators","summary":"  Soft-growing robots are innovative devices that feature plant-inspired growth\nto navigate environments. Thanks to their embodied intelligence of adapting to\ntheir surroundings and the latest innovation in actuation and manufacturing, it\nis possible to employ them for specific manipulation tasks. The applications of\nthese devices include exploration of delicate/dangerous environments,\nmanipulation of items, or assistance in domestic environments.\n  This work presents a novel approach for design optimization of soft-growing\nrobots, which will be used prior to manufacturing to suggest engineers -- or\nrobot designer enthusiasts -- the optimal dimension of the robot to be built\nfor solving a specific task. I modeled the design process as a multi-objective\noptimization problem, in which I optimize the kinematic chain of a soft\nmanipulator to reach targets and avoid unnecessary overuse of material and\nresources. The method exploits the advantages of population-based optimization\nalgorithms, in particular evolutionary algorithms, to transform the problem\nfrom multi-objective into a single-objective thanks to an efficient\nmathematical formulation, the novel rank-partitioning algorithm, and obstacle\navoidance integrated within the optimizer operators.\n  I tested the proposed method on different tasks to access its optimality,\nwhich showed significant performance in solving the problem. Finally,\ncomparative experiments showed that the proposed method works better than the\none existing in the literature in terms of precision, resource consumption, and\nrun time.\n","authors":["Fabio Stroppa"],"pdf_url":"https://arxiv.org/pdf/2310.03374v1.pdf","comment":"50 pages, 15 figures"},{"id":"http://arxiv.org/abs/2310.03359v1","updated":"2023-10-05T07:30:02Z","published":"2023-10-05T07:30:02Z","title":"Time-Optimal Trajectory Planning in Highway Scenarios using Basis-Spline\n  Parameterization","summary":"  Basis splines enable a time-continuous feasibility check with a finite number\nof constraints. Constraints apply to the whole trajectory for motion planning\napplications that require a collision-free and dynamically feasible trajectory.\nExisting motion planners that rely on gradient-based optimization apply time\nscaling to implement a shrinking planning horizon. They neither guarantee a\nrecursively feasible trajectory nor enable reaching two terminal manifold parts\nat different time scales. This paper proposes a nonlinear optimization problem\nthat addresses the drawbacks of existing approaches. Therefore, the spline\nbreakpoints are included in the optimization variables. Transformations between\nspline bases are implemented so a sparse problem formulation is achieved. A\nstrategy for breakpoint removal enables the convergence into a terminal\nmanifold. The evaluation in an overtaking scenario shows the influence of the\nbreakpoint number on the solution quality and the time required for\noptimization.\n","authors":["Philip Dorpmüller","Thomas Schmitz","Naveen Bejagam","Torsten Bertram"],"pdf_url":"https://arxiv.org/pdf/2310.03359v1.pdf","comment":"Accepted for 2023 IEEE 26th International Conference on Intelligent\n  Transportation Systems (ITSC)"},{"id":"http://arxiv.org/abs/2310.03344v1","updated":"2023-10-05T06:50:11Z","published":"2023-10-05T06:50:11Z","title":"Generalized Benders Decomposition with Continual Learning for Hybrid\n  Model Predictive Control in Dynamic Environment","summary":"  Hybrid model predictive control (MPC) with both continuous and discrete\nvariables is widely applicable to robotic control tasks, especially those\ninvolving contact with the environment. Due to the combinatorial complexity,\nthe solving speed of hybrid MPC can be insufficient for real-time applications.\nIn this paper, we proposed a hybrid MPC solver based on Generalized Benders\nDecomposition (GBD) with continual learning. The algorithm accumulates cutting\nplanes from the invariant dual space of the subproblems. After a short\ncold-start phase, the accumulated cuts provide warm-starts for the new problem\ninstances to increase the solving speed. Despite the randomly changing\nenvironment that the control is unprepared for, the solving speed maintains. We\nverified our solver on controlling a cart-pole system with randomly moving soft\ncontact walls and show that the solving speed is 2-3 times faster than the\noff-the-shelf solver Gurobi.\n","authors":["Lin Xuan"],"pdf_url":"https://arxiv.org/pdf/2310.03344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03314v1","updated":"2023-10-05T05:12:14Z","published":"2023-10-05T05:12:14Z","title":"Enhanced Human-Robot Collaboration using Constrained Probabilistic\n  Human-Motion Prediction","summary":"  Human motion prediction is an essential step for efficient and safe\nhuman-robot collaboration. Current methods either purely rely on representing\nthe human joints in some form of neural network-based architecture or use\nregression models offline to fit hyper-parameters in the hope of capturing a\nmodel encompassing human motion. While these methods provide good initial\nresults, they are missing out on leveraging well-studied human body kinematic\nmodels as well as body and scene constraints which can help boost the efficacy\nof these prediction frameworks while also explicitly avoiding implausible human\njoint configurations. We propose a novel human motion prediction framework that\nincorporates human joint constraints and scene constraints in a Gaussian\nProcess Regression (GPR) model to predict human motion over a set time horizon.\nThis formulation is combined with an online context-aware constraints model to\nleverage task-dependent motions. It is tested on a human arm kinematic model\nand implemented on a human-robot collaborative setup with a UR5 robot arm to\ndemonstrate the real-time capability of our approach. Simulations were also\nperformed on datasets like HA4M and ANDY. The simulation and experimental\nresults demonstrate considerable improvements in a Gaussian Process framework\nwhen these constraints are explicitly considered.\n","authors":["Aadi Kothari","Tony Tohme","Xiaotong Zhang","Kamal Youcef-Toumi"],"pdf_url":"https://arxiv.org/pdf/2310.03314v1.pdf","comment":"7 pages, 5 figures. Associated video demonstration can be found at\n  https://www.youtube.com/@MITMechatronics"},{"id":"http://arxiv.org/abs/2310.03303v1","updated":"2023-10-05T04:10:56Z","published":"2023-10-05T04:10:56Z","title":"A Two-stage Based Social Preference Recognition in Multi-Agent\n  Autonomous Driving System","summary":"  Multi-Agent Reinforcement Learning (MARL) has become a promising solution for\nconstructing a multi-agent autonomous driving system (MADS) in complex and\ndense scenarios. But most methods consider agents acting selfishly, which leads\nto conflict behaviors. Some existing works incorporate the concept of social\nvalue orientation (SVO) to promote coordination, but they lack the knowledge of\nother agents' SVOs, resulting in conservative maneuvers. In this paper, we aim\nto tackle the mentioned problem by enabling the agents to understand other\nagents' SVOs. To accomplish this, we propose a two-stage system framework.\nFirstly, we train a policy by allowing the agents to share their ground truth\nSVOs to establish a coordinated traffic flow. Secondly, we develop a\nrecognition network that estimates agents' SVOs and integrates it with the\npolicy trained in the first stage. Experiments demonstrate that our developed\nmethod significantly improves the performance of the driving policy in MADS\ncompared to two state-of-the-art MARL algorithms.\n","authors":["Jintao Xue","Dongkun Zhang","Rong Xiong","Yue Wang","Eryun Liu"],"pdf_url":"https://arxiv.org/pdf/2310.03303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03246v1","updated":"2023-10-05T01:31:45Z","published":"2023-10-05T01:31:45Z","title":"${\\tt MORALS}$: Analysis of High-Dimensional Robot Controllers via\n  Topological Tools in a Latent Space","summary":"  Estimating the region of attraction (${\\tt RoA}$) for a robotic system's\ncontroller is essential for safe application and controller composition. Many\nexisting methods require access to a closed-form expression that limit\napplicability to data-driven controllers. Methods that operate only over\ntrajectory rollouts tend to be data-hungry. In prior work, we have demonstrated\nthat topological tools based on Morse Graphs offer data-efficient ${\\tt RoA}$\nestimation without needing an analytical model. They struggle, however, with\nhigh-dimensional systems as they operate over a discretization of the state\nspace. This paper presents ${\\it Mo}$rse Graph-aided discovery of ${\\it\nR}$egions of ${\\it A}$ttraction in a learned ${\\it L}$atent ${\\it S}$pace\n(${\\tt MORALS}$). The approach combines autoencoding neural networks with Morse\nGraphs. ${\\tt MORALS}$ shows promising predictive capabilities in estimating\nattractors and their ${\\tt RoA}$s for data-driven controllers operating over\nhigh-dimensional systems, including a 67-dim humanoid robot and a 96-dim\n3-fingered manipulator. It first projects the dynamics of the controlled system\ninto a learned latent space. Then, it constructs a reduced form of Morse Graphs\nrepresenting the bistability of the underlying dynamics, i.e., detecting when\nthe controller results in a desired versus an undesired behavior. The\nevaluation on high-dimensional robotic datasets indicates the data efficiency\nof the approach in ${\\tt RoA}$ estimation.\n","authors":["Ewerton R. Vieira","Aravind Sivaramakrishnan","Sumanth Tangirala","Edgar Granados","Konstantin Mischaikow","Kostas E. Bekris"],"pdf_url":"https://arxiv.org/pdf/2310.03246v1.pdf","comment":"The first two authors contributed equally to this paper"},{"id":"http://arxiv.org/abs/2310.03239v1","updated":"2023-10-05T01:21:33Z","published":"2023-10-05T01:21:33Z","title":"Roadmaps with Gaps over Controllers: Achieving Efficiency in Planning\n  under Dynamics","summary":"  This paper aims to improve the computational efficiency of motion planning\nfor mobile robots with non-trivial dynamics by taking advantage of learned\ncontrollers. It adopts a decoupled strategy, where a system-specific controller\nis first trained offline in an empty environment to deal with the system's\ndynamics. For an environment, the proposed approach constructs offline a data\nstructure, a \"Roadmap with Gaps,\" to approximately learn how to solve planning\nqueries in this environment using the learned controller. Its nodes correspond\nto local regions and edges correspond to applications of the learned control\npolicy that approximately connect these regions. Gaps arise due to the\ncontroller not perfectly connecting pairs of individual states along edges.\nOnline, given a query, a tree sampling-based motion planner uses the roadmap so\nthat the tree's expansion is informed towards the goal region. The tree\nexpansion selects local subgoals given a wavefront on the roadmap that guides\ntowards the goal. When the controller cannot reach a subgoal region, the\nplanner resorts to random exploration to maintain probabilistic completeness\nand asymptotic optimality. The experimental evaluation shows that the approach\nsignificantly improves the computational efficiency of motion planning on\nvarious benchmarks, including physics-based vehicular models on uneven and\nvarying friction terrains as well as a quadrotor under air pressure effects.\n","authors":["Aravind Sivaramakrishnan","Noah R. Carver","Sumanth Tangirala","Kostas E. Bekris"],"pdf_url":"https://arxiv.org/pdf/2310.03239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03225v1","updated":"2023-10-05T00:47:09Z","published":"2023-10-05T00:47:09Z","title":"Safe Exploration in Reinforcement Learning: A Generalized Formulation\n  and Algorithms","summary":"  Safe exploration is essential for the practical use of reinforcement learning\n(RL) in many real-world scenarios. In this paper, we present a generalized safe\nexploration (GSE) problem as a unified formulation of common safe exploration\nproblems. We then propose a solution of the GSE problem in the form of a\nmeta-algorithm for safe exploration, MASE, which combines an unconstrained RL\nalgorithm with an uncertainty quantifier to guarantee safety in the current\nepisode while properly penalizing unsafe explorations before actual safety\nviolation to discourage them in future episodes. The advantage of MASE is that\nwe can optimize a policy while guaranteeing with a high probability that no\nsafety constraint will be violated under proper assumptions. Specifically, we\npresent two variants of MASE with different constructions of the uncertainty\nquantifier: one based on generalized linear models with theoretical guarantees\nof safety and near-optimality, and another that combines a Gaussian process to\nensure safety with a deep RL algorithm to maximize the reward. Finally, we\ndemonstrate that our proposed algorithm achieves better performance than\nstate-of-the-art algorithms on grid-world and Safety Gym benchmarks without\nviolating any safety constraints, even during training.\n","authors":["Akifumi Wachi","Wataru Hashimoto","Xun Shen","Kazumune Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2310.03225v1.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.03932v1","updated":"2023-10-05T22:32:35Z","published":"2023-10-05T22:32:35Z","title":"Bridging Low-level Geometry to High-level Concepts in Visual Servoing of\n  Robot Manipulation Task Using Event Knowledge Graphs and Vision-Language\n  Models","summary":"  In this paper, we propose a framework of building knowledgeable robot control\nin the scope of smart human-robot interaction, by empowering a basic\nuncalibrated visual servoing controller with contextual knowledge through the\njoint usage of event knowledge graphs (EKGs) and large-scale pretrained\nvision-language models (VLMs). The framework is expanded in twofold: first, we\ninterpret low-level image geometry as high-level concepts, allowing us to\nprompt VLMs and to select geometric features of points and lines for motor\ncontrol skills; then, we create an event knowledge graph (EKG) to conceptualize\na robot manipulation task of interest, where the main body of the EKG is\ncharacterized by an executable behavior tree, and the leaves by semantic\nconcepts relevant to the manipulation context. We demonstrate, in an\nuncalibrated environment with real robot trials, that our method lowers the\nreliance of human annotation during task interfacing, allows the robot to\nperform activities of daily living more easily by treating low-level\ngeometric-based motor control skills as high-level concepts, and is beneficial\nin building cognitive thinking for smart robot applications.\n","authors":["Chen Jiang","Martin Jagersand"],"pdf_url":"https://arxiv.org/pdf/2310.03932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03923v1","updated":"2023-10-05T21:57:36Z","published":"2023-10-05T21:57:36Z","title":"Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene\n  Representation","summary":"  Precise 3D environmental mapping is pivotal in robotics. Existing methods\noften rely on predefined concepts during training or are time-intensive when\ngenerating semantic maps. This paper presents Open-Fusion, a groundbreaking\napproach for real-time open-vocabulary 3D mapping and queryable scene\nrepresentation using RGB-D data. Open-Fusion harnesses the power of a\npre-trained vision-language foundation model (VLFM) for open-set semantic\ncomprehension and employs the Truncated Signed Distance Function (TSDF) for\nswift 3D scene reconstruction. By leveraging the VLFM, we extract region-based\nembeddings and their associated confidence maps. These are then integrated with\n3D knowledge from TSDF using an enhanced Hungarian-based feature-matching\nmechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D\nsegmentation for open-vocabulary without necessitating additional 3D training.\nBenchmark tests on the ScanNet dataset against leading zero-shot methods\nhighlight Open-Fusion's superiority. Furthermore, it seamlessly combines the\nstrengths of region-based VLFM and TSDF, facilitating real-time 3D scene\ncomprehension that includes object concepts and open-world semantics. We\nencourage the readers to view the demos on our project page:\nhttps://uark-aicv.github.io/OpenFusion\n","authors":["Kashu Yamazaki","Taisei Hanyu","Khoa Vo","Thang Pham","Minh Tran","Gianfranco Doretto","Anh Nguyen","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2310.03923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03913v1","updated":"2023-10-05T21:40:39Z","published":"2023-10-05T21:40:39Z","title":"TRAIL Team Description Paper for RoboCup@Home 2023","summary":"  Our team, TRAIL, consists of AI/ML laboratory members from The University of\nTokyo. We leverage our extensive research experience in state-of-the-art\nmachine learning to build general-purpose in-home service robots. We previously\nparticipated in two competitions using Human Support Robot (HSR): RoboCup@Home\nJapan Open 2020 (DSPL) and World Robot Summit 2020, equivalent to RoboCup World\nTournament. Throughout the competitions, we showed that a data-driven approach\nis effective for performing in-home tasks. Aiming for further development of\nbuilding a versatile and fast-adaptable system, in RoboCup @Home 2023, we unify\nthree technologies that have recently been evaluated as components in the\nfields of deep learning and robot learning into a real household robot system.\nIn addition, to stimulate research all over the RoboCup@Home community, we\nbuild a platform that manages data collected from each site belonging to the\ncommunity around the world, taking advantage of the characteristics of the\ncommunity.\n","authors":["Chikaha Tsuji","Dai Komukai","Mimo Shirasaka","Hikaru Wada","Tsunekazu Omija","Aoi Horo","Daiki Furuta","Saki Yamaguchi","So Ikoma","Soshi Tsunashima","Masato Kobayashi","Koki Ishimoto","Yuya Ikeda","Tatsuya Matsushima","Yusuke Iwasawa","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2310.03913v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.03893v4","updated":"2023-10-05T21:26:48Z","published":"2021-04-08T17:01:19Z","title":"Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in\n  Prosthetic Hand Control","summary":"  Objective: For lower arm amputees, robotic prosthetic hands promise to regain\nthe capability to perform daily living activities. Current control methods\nbased on physiological signals such as electromyography (EMG) are prone to\nyielding poor inference outcomes due to motion artifacts, muscle fatigue, and\nmany more. Vision sensors are a major source of information about the\nenvironment state and can play a vital role in inferring feasible and intended\ngestures. However, visual evidence is also susceptible to its own artifacts,\nmost often due to object occlusion, lighting changes, etc. Multimodal evidence\nfusion using physiological and vision sensor measurements is a natural approach\ndue to the complementary strengths of these modalities. Methods: In this paper,\nwe present a Bayesian evidence fusion framework for grasp intent inference\nusing eye-view video, eye-gaze, and EMG from the forearm processed by neural\nnetwork models. We analyze individual and fused performance as a function of\ntime as the hand approaches the object to grasp it. For this purpose, we have\nalso developed novel data processing and augmentation techniques to train\nneural network components. Results: Our results indicate that, on average,\nfusion improves the instantaneous upcoming grasp type classification accuracy\nwhile in the reaching phase by 13.66% and 14.8%, relative to EMG and visual\nevidence individually, resulting in an overall fusion accuracy of 95.3%.\nConclusion: Our experimental data analyses demonstrate that EMG and visual\nevidence show complementary strengths, and as a consequence, fusion of\nmultimodal evidence can outperform each individual evidence modality at any\ngiven time.\n","authors":["Mehrshad Zandigohar","Mo Han","Mohammadreza Sharif","Sezen Yagmur Gunay","Mariusz P. Furmanek","Mathew Yarossi","Paolo Bonato","Cagdas Onal","Taskin Padir","Deniz Erdogmus","Gunar Schirner"],"pdf_url":"https://arxiv.org/pdf/2104.03893v4.pdf","comment":"This work has been submitted to Frontiers for possible publication"},{"id":"http://arxiv.org/abs/2310.03895v1","updated":"2023-10-05T21:01:04Z","published":"2023-10-05T21:01:04Z","title":"TWICE Dataset: Digital Twin of Test Scenarios in a Controlled\n  Environment","summary":"  Ensuring the safe and reliable operation of autonomous vehicles under adverse\nweather remains a significant challenge. To address this, we have developed a\ncomprehensive dataset composed of sensor data acquired in a real test track and\nreproduced in the laboratory for the same test scenarios. The provided dataset\nincludes camera, radar, LiDAR, inertial measurement unit (IMU), and GPS data\nrecorded under adverse weather conditions (rainy, night-time, and snowy\nconditions). We recorded test scenarios using objects of interest such as car,\ncyclist, truck and pedestrian -- some of which are inspired by EURONCAP\n(European New Car Assessment Programme). The sensor data generated in the\nlaboratory is acquired by the execution of simulation-based tests in\nhardware-in-the-loop environment with the digital twin of each real test\nscenario. The dataset contains more than 2 hours of recording, which totals\nmore than 280GB of data. Therefore, it is a valuable resource for researchers\nin the field of autonomous vehicles to test and improve their algorithms in\nadverse weather conditions, as well as explore the simulation-to-reality gap.\nThe dataset is available for download at: https://twicedataset.github.io/site/\n","authors":["Leonardo Novicki Neto","Fabio Reway","Yuri Poledna","Maikol Funk Drechsler","Eduardo Parente Ribeiro","Werner Huber","Christian Icking"],"pdf_url":"https://arxiv.org/pdf/2310.03895v1.pdf","comment":"8 pages, 13 figures, submitted to IEEE Sensors Journal"},{"id":"http://arxiv.org/abs/2310.03888v1","updated":"2023-10-05T20:48:25Z","published":"2023-10-05T20:48:25Z","title":"Frequency Domain Analysis of Nonlinear Series Elastic Actuator via\n  Describing Function","summary":"  Nonlinear stiffness SEAs (NSEAs) inspired by biological muscles offer promise\nin achieving adaptable stiffness for assistive robots. While assistive robots\nare often designed and compared based on torque capability and control\nbandwidth, NSEAs have not been systematically designed in the frequency domain\ndue to their nonlinearity. The describing function, an analytical concept for\nnonlinear systems, offers a means to understand their behavior in the frequency\ndomain. This paper introduces a frequency domain analysis of nonlinear series\nelastic actuators using the describing function method. This framework aims to\nequip researchers and engineers with tools for improved design and control in\nassistive robotics.\n","authors":["Motohiro Hirao","Burak Kurkcu","Alireza Ghanbarpour","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2310.03888v1.pdf","comment":"accepted by 2023 IEEE ROBIO conference"},{"id":"http://arxiv.org/abs/2310.03873v1","updated":"2023-10-05T20:05:47Z","published":"2023-10-05T20:05:47Z","title":"Neuromorphic Robust Framework for Concurrent Estimation and Control in\n  Dynamical Systems using Spiking Neural Networks","summary":"  Concurrent estimation and control of robotic systems remains an ongoing\nchallenge, where controllers rely on data extracted from states/parameters\nriddled with uncertainties and noises. Framework suitability hinges on task\ncomplexity and computational constraints, demanding a balance between\ncomputational efficiency and mission-critical accuracy. This study leverages\nrecent advancements in neuromorphic computing, particularly spiking neural\nnetworks (SNNs), for estimation and control applications. Our presented\nframework employs a recurrent network of leaky integrate-and-fire (LIF)\nneurons, mimicking a linear quadratic regulator (LQR) through a robust\nfiltering strategy, a modified sliding innovation filter (MSIF). Benefiting\nfrom both the robustness of MSIF and the computational efficiency of SNN, our\nframework customizes SNN weight matrices to match the desired system model\nwithout requiring training. Additionally, the network employs a biologically\nplausible firing rule similar to predictive coding. In the presence of\nuncertainties, we compare the SNN-LQR-MSIF with non-spiking LQR-MSIF and the\noptimal linear quadratic Gaussian (LQG) strategy. Evaluation across a workbench\nlinear problem and a satellite rendezvous maneuver, implementing the\nClohessy-Wiltshire (CW) model in space robotics, demonstrates that the\nSNN-LQR-MSIF achieves acceptable performance in computational efficiency,\nrobustness, and accuracy. This positions it as a promising solution for\naddressing dynamic systems' concurrent estimation and control challenges in\ndynamic systems.\n","authors":["Reza Ahmadvand","Sarah Safura Sharif","Yaser Mike Banad"],"pdf_url":"https://arxiv.org/pdf/2310.03873v1.pdf","comment":"12 pages, 15 figures"},{"id":"http://arxiv.org/abs/2310.03821v1","updated":"2023-10-05T18:17:07Z","published":"2023-10-05T18:17:07Z","title":"WLST: Weak Labels Guided Self-training for Weakly-supervised Domain\n  Adaptation on 3D Object Detection","summary":"  In the field of domain adaptation (DA) on 3D object detection, most of the\nwork is dedicated to unsupervised domain adaptation (UDA). Yet, without any\ntarget annotations, the performance gap between the UDA approaches and the\nfully-supervised approach is still noticeable, which is impractical for\nreal-world applications. On the other hand, weakly-supervised domain adaptation\n(WDA) is an underexplored yet practical task that only requires few labeling\neffort on the target domain. To improve the DA performance in a cost-effective\nway, we propose a general weak labels guided self-training framework, WLST,\ndesigned for WDA on 3D object detection. By incorporating autolabeler, which\ncan generate 3D pseudo labels from 2D bounding boxes, into the existing\nself-training pipeline, our method is able to generate more robust and\nconsistent pseudo labels that would benefit the training process on the target\ndomain. Extensive experiments demonstrate the effectiveness, robustness, and\ndetector-agnosticism of our WLST framework. Notably, it outperforms previous\nstate-of-the-art methods on all evaluation tasks.\n","authors":["Tsung-Lin Tsou","Tsung-Han Wu","Winston H. Hsu"],"pdf_url":"https://arxiv.org/pdf/2310.03821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06247v4","updated":"2023-10-05T18:09:53Z","published":"2023-03-10T23:45:30Z","title":"Task and Motion Planning with Large Language Models for Object\n  Rearrangement","summary":"  Multi-object rearrangement is a crucial skill for service robots, and\ncommonsense reasoning is frequently needed in this process. However, achieving\ncommonsense arrangements requires knowledge about objects, which is hard to\ntransfer to robots. Large language models (LLMs) are one potential source of\nthis knowledge, but they do not naively capture information about plausible\nphysical arrangements of the world. We propose LLM-GROP, which uses prompting\nto extract commonsense knowledge about semantically valid object configurations\nfrom an LLM and instantiates them with a task and motion planner in order to\ngeneralize to varying scene geometry. LLM-GROP allows us to go from\nnatural-language commands to human-aligned object rearrangement in varied\nenvironments. Based on human evaluations, our approach achieves the highest\nrating while outperforming competitive baselines in terms of success rate while\nmaintaining comparable cumulative action costs. Finally, we demonstrate a\npractical implementation of LLM-GROP on a mobile manipulator in real-world\nscenarios. Supplementary materials are available at:\nhttps://sites.google.com/view/llm-grop\n","authors":["Yan Ding","Xiaohan Zhang","Chris Paxton","Shiqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.06247v4.pdf","comment":"Accpted by IEEE IROS 2023"},{"id":"http://arxiv.org/abs/2305.17590v2","updated":"2023-10-05T18:05:51Z","published":"2023-05-27T22:30:15Z","title":"Integrating Action Knowledge and LLMs for Task Planning and Situation\n  Handling in Open Worlds","summary":"  Task planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/\n","authors":["Yan Ding","Xiaohan Zhang","Saeid Amiri","Nieqing Cao","Hao Yang","Andy Kaminski","Chad Esselink","Shiqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.17590v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2210.01287"},{"id":"http://arxiv.org/abs/2310.03779v1","updated":"2023-10-05T16:14:46Z","published":"2023-10-05T16:14:46Z","title":"HandMeThat: Human-Robot Communication in Physical and Social\n  Environments","summary":"  We introduce HandMeThat, a benchmark for a holistic evaluation of instruction\nunderstanding and following in physical and social environments. While previous\ndatasets primarily focused on language grounding and planning, HandMeThat\nconsiders the resolution of human instructions with ambiguities based on the\nphysical (object states and relations) and social (human actions and goals)\ninformation. HandMeThat contains 10,000 episodes of human-robot interactions.\nIn each episode, the robot first observes a trajectory of human actions towards\nher internal goal. Next, the robot receives a human instruction and should take\nactions to accomplish the subgoal set through the instruction. In this paper,\nwe present a textual interface for our benchmark, where the robot interacts\nwith a virtual environment through textual commands. We evaluate several\nbaseline models on HandMeThat, and show that both offline and online\nreinforcement learning algorithms perform poorly on HandMeThat, suggesting\nsignificant room for future work on physical and social human-robot\ncommunications and interactions.\n","authors":["Yanming Wan","Jiayuan Mao","Joshua B. Tenenbaum"],"pdf_url":"https://arxiv.org/pdf/2310.03779v1.pdf","comment":"NeurIPS 2022 (Dataset and Benchmark Track). First two authors\n  contributed equally. Project page: http://handmethat.csail.mit.edu/"},{"id":"http://arxiv.org/abs/2310.04459v1","updated":"2023-10-05T05:38:36Z","published":"2023-10-05T05:38:36Z","title":"Extended Kalman Filter State Estimation for Autonomous Competition\n  Robots","summary":"  Autonomous mobile robot competitions judge based on a robot's ability to\nquickly and accurately navigate the game field. This means accurate\nlocalization is crucial for creating an autonomous competition robot. Two\ncommon localization methods are odometry and computer vision landmark\ndetection. Odometry provides frequent velocity measurements, while landmark\ndetection provides infrequent position measurements. The state can also be\npredicted with a physics model. These three types of localization can be\n\"fused\" to create a more accurate state estimate using an Extended Kalman\nFilter (EKF). The EKF is a nonlinear full-state estimator that approximates the\nstate estimate with the lowest covariance error when given the sensor\nmeasurements, the model prediction, and their variances. In this paper, we\ndemonstrate the effectiveness of the EKF by implementing it on a 4-wheel\nmecanum-drive robot simulation. The position and velocity accuracy of fusing\ntogether various combinations of these three data sources are compared. We also\ndiscuss the assumptions and limitations of an EKF.\n","authors":["Ethan Kou","Acshi Haggenmiller"],"pdf_url":"https://arxiv.org/pdf/2310.04459v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.03744v1","updated":"2023-10-05T17:59:56Z","published":"2023-10-05T17:59:56Z","title":"Improved Baselines with Visual Instruction Tuning","summary":"  Large multimodal models (LMM) have recently shown encouraging progress with\nvisual instruction tuning. In this note, we show that the fully-connected\nvision-language cross-modal connector in LLaVA is surprisingly powerful and\ndata-efficient. With simple modifications to LLaVA, namely, using\nCLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA\ndata with simple response formatting prompts, we establish stronger baselines\nthat achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint\nuses merely 1.2M publicly available data, and finishes full training in ~1 day\non a single 8-A100 node. We hope this can make state-of-the-art LMM research\nmore accessible. Code and model will be publicly available.\n","authors":["Haotian Liu","Chunyuan Li","Yuheng Li","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2310.03744v1.pdf","comment":"Tech report, 4 pages. LLaVA project page: https://llava-vl.github.io"},{"id":"http://arxiv.org/abs/2310.03740v1","updated":"2023-10-05T17:59:45Z","published":"2023-10-05T17:59:45Z","title":"ContactGen: Generative Contact Modeling for Grasp Generation","summary":"  This paper presents a novel object-centric contact representation ContactGen\nfor hand-object interaction. The ContactGen comprises three components: a\ncontact map indicates the contact location, a part map represents the contact\nhand part, and a direction map tells the contact direction within each part.\nGiven an input object, we propose a conditional generative model to predict\nContactGen and adopt model-based optimization to predict diverse and\ngeometrically feasible grasps. Experimental results demonstrate our method can\ngenerate high-fidelity and diverse human grasps for various objects. Project\npage: https://stevenlsw.github.io/contactgen/\n","authors":["Shaowei Liu","Yang Zhou","Jimei Yang","Saurabh Gupta","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.03740v1.pdf","comment":"Accepted to ICCV 2023. Website:\n  https://stevenlsw.github.io/contactgen/"},{"id":"http://arxiv.org/abs/2310.03739v1","updated":"2023-10-05T17:59:18Z","published":"2023-10-05T17:59:18Z","title":"Aligning Text-to-Image Diffusion Models with Reward Backpropagation","summary":"  Text-to-image diffusion models have recently emerged at the forefront of\nimage generation, powered by very large-scale unsupervised or weakly supervised\ntext-to-image training datasets. Due to their unsupervised training,\ncontrolling their behavior in downstream tasks, such as maximizing\nhuman-perceived image quality, image-text alignment, or ethical image\ngeneration, is difficult. Recent works finetune diffusion models to downstream\nreward functions using vanilla reinforcement learning, notorious for the high\nvariance of the gradient estimators. In this paper, we propose AlignProp, a\nmethod that aligns diffusion models to downstream reward functions using\nend-to-end backpropagation of the reward gradient through the denoising\nprocess. While naive implementation of such backpropagation would require\nprohibitive memory resources for storing the partial derivatives of modern\ntext-to-image models, AlignProp finetunes low-rank adapter weight modules and\nuses gradient checkpointing, to render its memory usage viable. We test\nAlignProp in finetuning diffusion models to various objectives, such as\nimage-text semantic alignment, aesthetics, compressibility and controllability\nof the number of objects present, as well as their combinations. We show\nAlignProp achieves higher rewards in fewer training steps than alternatives,\nwhile being conceptually simpler, making it a straightforward choice for\noptimizing diffusion models for differentiable reward functions of interest.\nCode and Visualization results are available at https://align-prop.github.io/.\n","authors":["Mihir Prabhudesai","Anirudh Goyal","Deepak Pathak","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2310.03739v1.pdf","comment":"Code available at https://align-prop.github.io/"},{"id":"http://arxiv.org/abs/2212.02648v2","updated":"2023-10-05T17:59:06Z","published":"2022-12-05T23:15:43Z","title":"Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases","summary":"  We present a simple but effective method to measure and mitigate model biases\ncaused by reliance on spurious cues. Instead of requiring costly changes to\none's data or model training, our method better utilizes the data one already\nhas by sorting them. Specifically, we rank images within their classes based on\nspuriosity (the degree to which common spurious cues are present), proxied via\ndeep neural features of an interpretable network. With spuriosity rankings, it\nis easy to identify minority subpopulations (i.e. low spuriosity images) and\nassess model bias as the gap in accuracy between high and low spuriosity\nimages. One can even efficiently remove a model's bias at little cost to\naccuracy by finetuning its classification head on low spuriosity images,\nresulting in fairer treatment of samples regardless of spuriosity. We\ndemonstrate our method on ImageNet, annotating $5000$ class-feature\ndependencies ($630$ of which we find to be spurious) and generating a dataset\nof $325k$ soft segmentations for these features along the way. Having computed\nspuriosity rankings via the identified spurious neural features, we assess\nbiases for $89$ diverse models and find that class-wise biases are highly\ncorrelated across models. Our results suggest that model bias due to spurious\nfeature reliance is influenced far more by what the model is trained on than\nhow it is trained.\n","authors":["Mazda Moayeri","Wenxiao Wang","Sahil Singla","Soheil Feizi"],"pdf_url":"https://arxiv.org/pdf/2212.02648v2.pdf","comment":"Accepted to NeurIPS '23 (Spotlight)"},{"id":"http://arxiv.org/abs/2310.03738v1","updated":"2023-10-05T17:58:32Z","published":"2023-10-05T17:58:32Z","title":"Stylist: Style-Driven Feature Ranking for Robust Novelty Detection","summary":"  Novelty detection aims at finding samples that differ in some form from the\ndistribution of seen samples. But not all changes are created equal. Data can\nsuffer a multitude of distribution shifts, and we might want to detect only\nsome types of relevant changes. Similar to works in out-of-distribution\ngeneralization, we propose to use the formalization of separating into semantic\nor content changes, that are relevant to our task, and style changes, that are\nirrelevant. Within this formalization, we define the robust novelty detection\nas the task of finding semantic changes while being robust to style\ndistributional shifts. Leveraging pretrained, large-scale model\nrepresentations, we introduce Stylist, a novel method that focuses on dropping\nenvironment-biased features. First, we compute a per-feature score based on the\nfeature distribution distances between environments. Next, we show that our\nselection manages to remove features responsible for spurious correlations and\nimprove novelty detection performance. For evaluation, we adapt domain\ngeneralization datasets to our task and analyze the methods behaviors. We\nadditionally built a large synthetic dataset where we have control over the\nspurious correlations degree. We prove that our selection mechanism improves\nnovelty detection algorithms across multiple datasets, containing both\nstylistic and content shifts.\n","authors":["Stefan Smeu","Elena Burceanu","Emanuela Haller","Andrei Liviu Nicolicioiu"],"pdf_url":"https://arxiv.org/pdf/2310.03738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03734v1","updated":"2023-10-05T17:55:19Z","published":"2023-10-05T17:55:19Z","title":"Leveraging Unpaired Data for Vision-Language Generative Models via Cycle\n  Consistency","summary":"  Current vision-language generative models rely on expansive corpora of paired\nimage-text data to attain optimal performance and generalization capabilities.\nHowever, automatically collecting such data (e.g. via large-scale web scraping)\nleads to low quality and poor image-text correlation, while human annotation is\nmore accurate but requires significant manual effort and expense. We introduce\n$\\textbf{ITIT}$ ($\\textbf{I}$n$\\textbf{T}$egrating $\\textbf{I}$mage\n$\\textbf{T}$ext): an innovative training paradigm grounded in the concept of\ncycle consistency which allows vision-language training on unpaired image and\ntext data. ITIT is comprised of a joint image-text encoder with disjoint image\nand text decoders that enable bidirectional image-to-text and text-to-image\ngeneration in a single framework. During training, ITIT leverages a small set\nof paired image-text data to ensure its output matches the input reasonably\nwell in both directions. Simultaneously, the model is also trained on much\nlarger datasets containing only images or texts. This is achieved by enforcing\ncycle consistency between the original unpaired samples and the cycle-generated\ncounterparts. For instance, it generates a caption for a given input image and\nthen uses the caption to create an output image, and enforces similarity\nbetween the input and output images. Our experiments show that ITIT with\nunpaired datasets exhibits similar scaling behavior as using high-quality\npaired data. We demonstrate image generation and captioning performance on par\nwith state-of-the-art text-to-image and image-to-text models with orders of\nmagnitude fewer (only 3M) paired image-text data.\n","authors":["Tianhong Li","Sangnie Bhardwaj","Yonglong Tian","Han Zhang","Jarred Barber","Dina Katabi","Guillaume Lajoie","Huiwen Chang","Dilip Krishnan"],"pdf_url":"https://arxiv.org/pdf/2310.03734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03731v1","updated":"2023-10-05T17:52:09Z","published":"2023-10-05T17:52:09Z","title":"MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical\n  Reasoning","summary":"  The recently released GPT-4 Code Interpreter has demonstrated remarkable\nproficiency in solving challenging math problems, primarily attributed to its\nability to seamlessly reason with natural language, generate code, execute\ncode, and continue reasoning based on the execution output. In this paper, we\npresent a method to fine-tune open-source language models, enabling them to use\ncode for modeling and deriving math equations and, consequently, enhancing\ntheir mathematical reasoning abilities. We propose a method of generating novel\nand high-quality datasets with math problems and their code-based solutions,\nreferred to as MathCodeInstruct. Each solution interleaves natural language,\ncode, and execution results. We also introduce a customized supervised\nfine-tuning and inference approach. This approach yields the MathCoder models,\na family of models capable of generating code-based solutions for solving\nchallenging math problems. Impressively, the MathCoder models achieve\nstate-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K\n(83.9%) datasets, substantially outperforming other open-source alternatives.\nNotably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K\nand MATH but also outperforms GPT-4 on the competition-level MATH dataset. The\ndataset and models will be released at https://github.com/mathllm/MathCoder.\n","authors":["Ke Wang","Houxing Ren","Aojun Zhou","Zimu Lu","Sichun Luo","Weikang Shi","Renrui Zhang","Linqi Song","Mingjie Zhan","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2310.03731v1.pdf","comment":"The state-of-the-art open-source language models for mathematical\n  reasoning"},{"id":"http://arxiv.org/abs/2308.12960v2","updated":"2023-10-05T17:46:18Z","published":"2023-08-24T17:56:46Z","title":"Towards Realistic Zero-Shot Classification via Self Structural Semantic\n  Alignment","summary":"  Large-scale pre-trained Vision Language Models (VLMs) have proven effective\nfor zero-shot classification. Despite the success, most traditional VLMs-based\nmethods are restricted by the assumption of partial source supervision or ideal\nvocabularies, which rarely satisfy the open-world scenario. In this paper, we\naim at a more challenging setting, Realistic Zero-Shot Classification, which\nassumes no annotation but instead a broad vocabulary. To address this\nchallenge, we propose the Self Structural Semantic Alignment (S^3A) framework,\nwhich extracts the structural semantic information from unlabeled data while\nsimultaneously self-learning. Our S^3A framework adopts a unique\nCluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups\nunlabeled data to derive structural semantics for pseudo-supervision. Our CVPR\nprocess includes iterative clustering on images, voting within each cluster to\nidentify initial class candidates from the vocabulary, generating\ndiscriminative prompts with large language models to discern confusing\ncandidates, and realigning images and the vocabulary as structural semantic\nalignment. Finally, we propose to self-learn the CLIP image encoder with both\nindividual and structural semantic alignment through a teacher-student learning\nstrategy. Our comprehensive experiments across various generic and fine-grained\nbenchmarks demonstrate that the S^3A method offers substantial improvements\nover existing VLMs-based approaches, achieving a more than 15% accuracy\nimprovement over CLIP on average. Our codes, models, and prompts are publicly\nreleased at https://github.com/sheng-eatamath/S3A.\n","authors":["Sheng Zhang","Muzammal Naseer","Guangyi Chen","Zhiqiang Shen","Salman Khan","Kun Zhang","Fahad Khan"],"pdf_url":"https://arxiv.org/pdf/2308.12960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03707v1","updated":"2023-10-05T17:34:47Z","published":"2023-10-05T17:34:47Z","title":"OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable\n  Evasion Attacks","summary":"  Evasion Attacks (EA) are used to test the robustness of trained neural\nnetworks by distorting input data to misguide the model into incorrect\nclassifications. Creating these attacks is a challenging task, especially with\nthe ever-increasing complexity of models and datasets. In this work, we\nintroduce a self-supervised, computationally economical method for generating\nadversarial examples, designed for the unseen black-box setting. Adapting\ntechniques from representation learning, our method generates on-manifold EAs\nthat are encouraged to resemble the data distribution. These attacks are\ncomparable in effectiveness compared to the state-of-the-art when attacking the\nmodel trained on, but are significantly more effective when attacking unseen\nmodels, as the attacks are more related to the data rather than the model\nitself. Our experiments consistently demonstrate the method is effective across\nvarious models, unseen data categories, and even defended models, suggesting a\nsignificant role for on-manifold EAs when targeting unseen models.\n","authors":["Ofir Bar Tal","Adi Haviv","Amit H. Bermano"],"pdf_url":"https://arxiv.org/pdf/2310.03707v1.pdf","comment":"ICCV 2023, AROW Workshop"},{"id":"http://arxiv.org/abs/2303.16887v2","updated":"2023-10-05T17:32:26Z","published":"2023-03-29T17:56:36Z","title":"Towards Understanding the Effect of Pretraining Label Granularity","summary":"  In this paper, we study how the granularity of pretraining labels affects the\ngeneralization of deep neural networks in image classification tasks. We focus\non the \"fine-to-coarse\" transfer learning setting, where the pretraining label\nspace is more fine-grained than that of the target problem. Empirically, we\nshow that pretraining on the leaf labels of ImageNet21k produces better\ntransfer results on ImageNet1k than pretraining on other coarser granularity\nlevels, which supports the common practice used in the community.\nTheoretically, we explain the benefit of fine-grained pretraining by proving\nthat, for a data distribution satisfying certain hierarchy conditions, 1)\ncoarse-grained pretraining only allows a neural network to learn the \"common\"\nor \"easy-to-learn\" features well, while 2) fine-grained pretraining helps the\nnetwork learn the \"rarer\" or \"fine-grained\" features in addition to the common\nones, thus improving its accuracy on hard downstream test samples in which\ncommon features are missing or weak in strength. Furthermore, we perform\ncomprehensive experiments using the label hierarchies of iNaturalist 2021 and\nobserve that the following conditions, in addition to proper choice of label\ngranularity, enable the transfer to work well in practice: 1) the pretraining\ndataset needs to have a meaningful label hierarchy, and 2) the pretraining and\ntarget label functions need to align well.\n","authors":["Guan Zhe Hong","Yin Cui","Ariel Fuxman","Stanley H. Chan","Enming Luo"],"pdf_url":"https://arxiv.org/pdf/2303.16887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03704v1","updated":"2023-10-05T17:24:36Z","published":"2023-10-05T17:24:36Z","title":"Drag View: Generalizable Novel View Synthesis with Unposed Imagery","summary":"  We introduce DragView, a novel and interactive framework for generating novel\nviews of unseen scenes. DragView initializes the new view from a single source\nimage, and the rendering is supported by a sparse set of unposed multi-view\nimages, all seamlessly executed within a single feed-forward pass. Our approach\nbegins with users dragging a source view through a local relative coordinate\nsystem. Pixel-aligned features are obtained by projecting the sampled 3D points\nalong the target ray onto the source view. We then incorporate a view-dependent\nmodulation layer to effectively handle occlusion during the projection.\nAdditionally, we broaden the epipolar attention mechanism to encompass all\nsource pixels, facilitating the aggregation of initialized coordinate-aligned\npoint features from other unposed views. Finally, we employ another transformer\nto decode ray features into final pixel intensities. Crucially, our framework\ndoes not rely on either 2D prior models or the explicit estimation of camera\nposes. During testing, DragView showcases the capability to generalize to new\nscenes unseen during training, also utilizing only unposed support images,\nenabling the generation of photo-realistic new views characterized by flexible\ncamera trajectories. In our experiments, we conduct a comprehensive comparison\nof the performance of DragView with recent scene representation networks\noperating under pose-free conditions, as well as with generalizable NeRFs\nsubject to noisy test camera poses. DragView consistently demonstrates its\nsuperior performance in view synthesis quality, while also being more\nuser-friendly. Project page: https://zhiwenfan.github.io/DragView/.\n","authors":["Zhiwen Fan","Panwang Pan","Peihao Wang","Yifan Jiang","Hanwen Jiang","Dejia Xu","Zehao Zhu","Dilin Wang","Zhangyang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.03704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03669v1","updated":"2023-10-05T16:43:28Z","published":"2023-10-05T16:43:28Z","title":"LumiNet: The Bright Side of Perceptual Knowledge Distillation","summary":"  In knowledge distillation research, feature-based methods have dominated due\nto their ability to effectively tap into extensive teacher models. In contrast,\nlogit-based approaches are considered to be less adept at extracting hidden\n'dark knowledge' from teachers. To bridge this gap, we present LumiNet, a novel\nknowledge-transfer algorithm designed to enhance logit-based distillation. We\nintroduce a perception matrix that aims to recalibrate logits through\nadjustments based on the model's representation capability. By meticulously\nanalyzing intra-class dynamics, LumiNet reconstructs more granular inter-class\nrelationships, enabling the student model to learn a richer breadth of\nknowledge. Both teacher and student models are mapped onto this refined matrix,\nwith the student's goal being to minimize representational discrepancies.\nRigorous testing on benchmark datasets (CIFAR-100, ImageNet, and MSCOCO)\nattests to LumiNet's efficacy, revealing its competitive edge over leading\nfeature-based methods. Moreover, in exploring the realm of transfer learning,\nwe assess how effectively the student model, trained using our method, adapts\nto downstream tasks. Notably, when applied to Tiny ImageNet, the transferred\nfeatures exhibit remarkable performance, further underscoring LumiNet's\nversatility and robustness in diverse settings. With LumiNet, we hope to steer\nthe research discourse towards a renewed interest in the latent capabilities of\nlogit-based knowledge distillation.\n","authors":["Md. Ismail Hossain","M M Lutfe Elahi","Sameera Ramasinghe","Ali Cheraghian","Fuad Rahman","Nabeel Mohammed","Shafin Rahman"],"pdf_url":"https://arxiv.org/pdf/2310.03669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03664v1","updated":"2023-10-05T16:40:33Z","published":"2023-10-05T16:40:33Z","title":"Certification of Deep Learning Models for Medical Image Segmentation","summary":"  In medical imaging, segmentation models have known a significant improvement\nin the past decade and are now used daily in clinical practice. However,\nsimilar to classification models, segmentation models are affected by\nadversarial attacks. In a safety-critical field like healthcare, certifying\nmodel predictions is of the utmost importance. Randomized smoothing has been\nintroduced lately and provides a framework to certify models and obtain\ntheoretical guarantees. In this paper, we present for the first time a\ncertified segmentation baseline for medical imaging based on randomized\nsmoothing and diffusion models. Our results show that leveraging the power of\ndenoising diffusion probabilistic models helps us overcome the limits of\nrandomized smoothing. We conduct extensive experiments on five public datasets\nof chest X-rays, skin lesions, and colonoscopies, and empirically show that we\nare able to maintain high certified Dice scores even for highly perturbed\nimages. Our work represents the first attempt to certify medical image\nsegmentation models, and we aspire for it to set a foundation for future\nbenchmarks in this crucial and largely uncharted area.\n","authors":["Othmane Laousy","Alexandre Araujo","Guillaume Chassagnon","Nikos Paragios","Marie-Pierre Revel","Maria Vakalopoulou"],"pdf_url":"https://arxiv.org/pdf/2310.03664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.20062v2","updated":"2023-10-05T16:40:02Z","published":"2023-05-31T17:38:08Z","title":"Chatting Makes Perfect: Chat-based Image Retrieval","summary":"  Chats emerge as an effective user-friendly approach for information\nretrieval, and are successfully employed in many domains, such as customer\nservice, healthcare, and finance. However, existing image retrieval approaches\ntypically address the case of a single query-to-image round, and the use of\nchats for image retrieval has been mostly overlooked. In this work, we\nintroduce ChatIR: a chat-based image retrieval system that engages in a\nconversation with the user to elicit information, in addition to an initial\nquery, in order to clarify the user's search intent. Motivated by the\ncapabilities of today's foundation models, we leverage Large Language Models to\ngenerate follow-up questions to an initial image description. These questions\nform a dialog with the user in order to retrieve the desired image from a large\ncorpus. In this study, we explore the capabilities of such a system tested on a\nlarge dataset and reveal that engaging in a dialog yields significant gains in\nimage retrieval. We start by building an evaluation pipeline from an existing\nmanually generated dataset and explore different modules and training\nstrategies for ChatIR. Our comparison includes strong baselines derived from\nrelated applications trained with Reinforcement Learning. Our system is capable\nof retrieving the target image from a pool of 50K images with over 78% success\nrate after 5 dialogue rounds, compared to 75% when questions are asked by\nhumans, and 64% for a single shot text-to-image retrieval. Extensive\nevaluations reveal the strong capabilities and examine the limitations of\nCharIR under different settings. Project repository is available at\nhttps://github.com/levymsn/ChatIR.\n","authors":["Matan Levy","Rami Ben-Ari","Nir Darshan","Dani Lischinski"],"pdf_url":"https://arxiv.org/pdf/2305.20062v2.pdf","comment":"Camera Ready version for NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.03661v1","updated":"2023-10-05T16:39:14Z","published":"2023-10-05T16:39:14Z","title":"Robustness-Guided Image Synthesis for Data-Free Quantization","summary":"  Quantization has emerged as a promising direction for model compression.\nRecently, data-free quantization has been widely studied as a promising method\nto avoid privacy concerns, which synthesizes images as an alternative to real\ntraining data. Existing methods use classification loss to ensure the\nreliability of the synthesized images. Unfortunately, even if these images are\nwell-classified by the pre-trained model, they still suffer from low semantics\nand homogenization issues. Intuitively, these low-semantic images are sensitive\nto perturbations, and the pre-trained model tends to have inconsistent output\nwhen the generator synthesizes an image with poor semantics. To this end, we\npropose Robustness-Guided Image Synthesis (RIS), a simple but effective method\nto enrich the semantics of synthetic images and improve image diversity,\nfurther boosting the performance of downstream data-free compression tasks.\nConcretely, we first introduce perturbations on input and model weight, then\ndefine the inconsistency metrics at feature and prediction levels before and\nafter perturbations. On the basis of inconsistency on two levels, we design a\nrobustness optimization objective to enhance the semantics of synthetic images.\nMoreover, we also make our approach diversity-aware by forcing the generator to\nsynthesize images with small correlations in the label space. With RIS, we\nachieve state-of-the-art performance for various settings on data-free\nquantization and can be extended to other data-free compression tasks.\n","authors":["Jianhong Bai","Yuchen Yang","Huanpeng Chu","Hualiang Wang","Zuozhu Liu","Ruizhe Chen","Xiaoxuan He","Lianrui Mu","Chengfei Cai","Haoji Hu"],"pdf_url":"https://arxiv.org/pdf/2310.03661v1.pdf","comment":"Submitted to AAAI 2024"},{"id":"http://arxiv.org/abs/2310.03658v1","updated":"2023-10-05T16:35:27Z","published":"2023-10-05T16:35:27Z","title":"Visual inspection for illicit items in X-ray images using Deep Learning","summary":"  Automated detection of contraband items in X-ray images can significantly\nincrease public safety, by enhancing the productivity and alleviating the\nmental load of security officers in airports, subways, customs/post offices,\netc. The large volume and high throughput of passengers, mailed parcels, etc.,\nduring rush hours practically make it a Big Data problem. Modern computer\nvision algorithms relying on Deep Neural Networks (DNNs) have proven capable of\nundertaking this task even under resource-constrained and embedded execution\nscenarios, e.g., as is the case with fast, single-stage object detectors.\nHowever, no comparative experimental assessment of the various relevant DNN\ncomponents/methods has been performed under a common evaluation protocol, which\nmeans that reliable cross-method comparisons are missing. This paper presents\nexactly such a comparative assessment, utilizing a public relevant dataset and\na well-defined methodology for selecting the specific DNN components/modules\nthat are being evaluated. The results indicate the superiority of Transformer\ndetectors, the obsolete nature of auxiliary neural modules that have been\ndeveloped in the past few years for security applications and the efficiency of\nthe CSP-DarkNet backbone CNN.\n","authors":["Ioannis Mademlis","Georgios Batsis","Adamantia Anna Rebolledo Chrysochoou","Georgios Th. Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2310.03658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.12232v2","updated":"2023-10-05T16:30:48Z","published":"2021-11-24T02:22:43Z","title":"PMSSC: Parallelizable multi-subset based self-expressive model for\n  subspace clustering","summary":"  Subspace clustering methods which embrace a self-expressive model that\nrepresents each data point as a linear combination of other data points in the\ndataset provide powerful unsupervised learning techniques. However, when\ndealing with large datasets, representation of each data point by referring to\nall data points via a dictionary suffers from high computational complexity. To\nalleviate this issue, we introduce a parallelizable multi-subset based\nself-expressive model (PMS) which represents each data point by combining\nmultiple subsets, with each consisting of only a small proportion of the\nsamples. The adoption of PMS in subspace clustering (PMSSC) leads to\ncomputational advantages because the optimization problems decomposed over each\nsubset are small, and can be solved efficiently in parallel. Furthermore, PMSSC\nis able to combine multiple self-expressive coefficient vectors obtained from\nsubsets, which contributes to an improvement in self-expressiveness. Extensive\nexperiments on synthetic and real-world datasets show the efficiency and\neffectiveness of our approach in comparison to other methods.\n","authors":["Katsuya Hotta","Takuya Akashi","Shogo Tokai","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2111.12232v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.13933v3","updated":"2023-10-05T16:25:41Z","published":"2023-06-24T10:44:02Z","title":"Boost Video Frame Interpolation via Motion Adaptation","summary":"  Video frame interpolation (VFI) is a challenging task that aims to generate\nintermediate frames between two consecutive frames in a video. Existing\nlearning-based VFI methods have achieved great success, but they still suffer\nfrom limited generalization ability due to the limited motion distribution of\ntraining datasets. In this paper, we propose a novel optimization-based VFI\nmethod that can adapt to unseen motions at test time. Our method is based on a\ncycle-consistency adaptation strategy that leverages the motion characteristics\namong video frames. We also introduce a lightweight adapter that can be\ninserted into the motion estimation module of existing pre-trained VFI models\nto improve the efficiency of adaptation. Extensive experiments on various\nbenchmarks demonstrate that our method can boost the performance of two-frame\nVFI models, outperforming the existing state-of-the-art methods, even those\nthat use extra input.\n","authors":["Haoning Wu","Xiaoyun Zhang","Weidi Xie","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2306.13933v3.pdf","comment":"Accepted by BMVC 2023 (Oral Presentation) Project Page:\n  https://haoningwu3639.github.io/VFI_Adapter_Webpage/"},{"id":"http://arxiv.org/abs/2306.10840v2","updated":"2023-10-05T16:13:17Z","published":"2023-06-19T10:40:09Z","title":"RedMotion: Motion Prediction via Redundancy Reduction","summary":"  Predicting the future motion of traffic agents is vital for self-driving\nvehicles to ensure their safe operation. We introduce RedMotion, a transformer\nmodel for motion prediction that incorporates two types of redundancy\nreduction. The first type of redundancy reduction is induced by an internal\ntransformer decoder and reduces a variable-sized set of road environment\ntokens, such as road graphs with agent data, to a fixed-sized embedding. The\nsecond type of redundancy reduction is a self-supervised learning objective and\napplies the redundancy reduction principle to embeddings generated from\naugmented views of road environments. Our experiments reveal that our\nrepresentation learning approach can outperform PreTraM, Traj-MAE, and\nGraphDINO in a semi-supervised setting. Our RedMotion model achieves results\nthat are competitive with those of Scene Transformer or MTR++. We provide an\nopen source implementation that is accessible via GitHub\n(https://github.com/kit-mrt/red-motion) and Colab\n(https://colab.research.google.com/drive/1Q-Z9VdiqvfPfctNG8oqzPcgm0lP3y1il).\n","authors":["Royden Wagner","Omer Sahin Tas","Marvin Klemp","Carlos Fernandez Lopez"],"pdf_url":"https://arxiv.org/pdf/2306.10840v2.pdf","comment":"Technical report, 13 pages, 8 figures; v2: focus on transformer model"},{"id":"http://arxiv.org/abs/2310.03635v1","updated":"2023-10-05T16:09:48Z","published":"2023-10-05T16:09:48Z","title":"CLEVRER-Humans: Describing Physical and Causal Events the Human Way","summary":"  Building machines that can reason about physical events and their causal\nrelationships is crucial for flexible interaction with the physical world.\nHowever, most existing physical and causal reasoning benchmarks are exclusively\nbased on synthetically generated events and synthetic natural language\ndescriptions of causal relationships. This design brings up two issues. First,\nthere is a lack of diversity in both event types and natural language\ndescriptions; second, causal relationships based on manually-defined heuristics\nare different from human judgments. To address both shortcomings, we present\nthe CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of\nphysical events with human labels. We employ two techniques to improve data\ncollection efficiency: first, a novel iterative event cloze task to elicit a\nnew representation of events in videos, which we term Causal Event Graphs\n(CEGs); second, a data augmentation technique based on neural language\ngenerative models. We convert the collected CEGs into questions and answers to\nbe consistent with prior work. Finally, we study a collection of baseline\napproaches for CLEVRER-Humans question-answering, highlighting the great\nchallenges set forth by our benchmark.\n","authors":["Jiayuan Mao","Xuelin Yang","Xikun Zhang","Noah D. Goodman","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2310.03635v1.pdf","comment":"NeurIPS 2022 (Dataset and Benchmark Track). First two authors\n  contributed equally. Project page:\n  https://sites.google.com/stanford.edu/clevrer-humans/home"},{"id":"http://arxiv.org/abs/2310.03629v1","updated":"2023-10-05T16:03:25Z","published":"2023-10-05T16:03:25Z","title":"Wasserstein Distortion: Unifying Fidelity and Realism","summary":"  We introduce a distortion measure for images, Wasserstein distortion, that\nsimultaneously generalizes pixel-level fidelity on the one hand and realism on\nthe other. We show how Wasserstein distortion reduces mathematically to a pure\nfidelity constraint or a pure realism constraint under different parameter\nchoices. Pairs of images that are close under Wasserstein distortion illustrate\nits utility. In particular, we generate random textures that have high fidelity\nto a reference texture in one location of the image and smoothly transition to\nan independent realization of the texture as one moves away from this point.\nConnections between Wasserstein distortion and models of the human visual\nsystem are noted.\n","authors":["Yang Qiu","Aaron B. Wagner","Johannes Ballé","Lucas Theis"],"pdf_url":"https://arxiv.org/pdf/2310.03629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03624v1","updated":"2023-10-05T16:01:29Z","published":"2023-10-05T16:01:29Z","title":"High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling\n  and Motion Planning","summary":"  A robot self-model is a task-agnostic representation of the robot's physical\nmorphology that can be used for motion planning tasks in absence of classical\ngeometric kinematic models. In particular, when the latter are hard to engineer\nor the robot's kinematics change unexpectedly, human-free self-modeling is a\nnecessary feature of truly autonomous agents. In this work, we leverage neural\nfields to allow a robot to self-model its kinematics as a neural-implicit query\nmodel learned only from 2D images annotated with camera poses and\nconfigurations. This enables significantly greater applicability than existing\napproaches which have been dependent on depth images or geometry knowledge. To\nthis end, alongside a curricular data sampling strategy, we propose a new\nencoder-based neural density field architecture for dynamic object-centric\nscenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF\nrobot test setup, the learned self-model achieves a Chamfer-L2 distance of 2%\nof the robot's workspace dimension. We demonstrate the capabilities of this\nmodel on a motion planning task as an exemplary downstream application.\n","authors":["Lennart Schulze","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2310.03624v1.pdf","comment":"ICCV 2023 Workshop on Neural Fields for Autonomous Driving and\n  Robotics (oral)"},{"id":"http://arxiv.org/abs/2310.03615v1","updated":"2023-10-05T15:49:44Z","published":"2023-10-05T15:49:44Z","title":"Animatable Virtual Humans: Learning pose-dependent human representations\n  in UV space for interactive performance synthesis","summary":"  We propose a novel representation of virtual humans for highly realistic\nreal-time animation and rendering in 3D applications. We learn pose dependent\nappearance and geometry from highly accurate dynamic mesh sequences obtained\nfrom state-of-the-art multiview-video reconstruction. Learning pose-dependent\nappearance and geometry from mesh sequences poses significant challenges, as it\nrequires the network to learn the intricate shape and articulated motion of a\nhuman body. However, statistical body models like SMPL provide valuable\na-priori knowledge which we leverage in order to constrain the dimension of the\nsearch space enabling more efficient and targeted learning and define\npose-dependency. Instead of directly learning absolute pose-dependent geometry,\nwe learn the difference between the observed geometry and the fitted SMPL\nmodel. This allows us to encode both pose-dependent appearance and geometry in\nthe consistent UV space of the SMPL model. This approach not only ensures a\nhigh level of realism but also facilitates streamlined processing and rendering\nof virtual humans in real-time scenarios.\n","authors":["Wieland Morgenstern","Milena T. Bagdasarian","Anna Hilsmann","Peter Eisert"],"pdf_url":"https://arxiv.org/pdf/2310.03615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03608v1","updated":"2023-10-05T15:42:53Z","published":"2023-10-05T15:42:53Z","title":"How Good Are Synthetic Medical Images? An Empirical Study with Lung\n  Ultrasound","summary":"  Acquiring large quantities of data and annotations is known to be effective\nfor developing high-performing deep learning models, but is difficult and\nexpensive to do in the healthcare context. Adding synthetic training data using\ngenerative models offers a low-cost method to deal effectively with the data\nscarcity challenge, and can also address data imbalance and patient privacy\nissues. In this study, we propose a comprehensive framework that fits\nseamlessly into model development workflows for medical image analysis. We\ndemonstrate, with datasets of varying size, (i) the benefits of generative\nmodels as a data augmentation method; (ii) how adversarial methods can protect\npatient privacy via data substitution; (iii) novel performance metrics for\nthese use cases by testing models on real holdout data. We show that training\nwith both synthetic and real data outperforms training with real data alone,\nand that models trained solely with synthetic data approach their real-only\ncounterparts. Code is available at\nhttps://github.com/Global-Health-Labs/US-DCGAN.\n","authors":["Menghan Yu","Sourabh Kulhare","Courosh Mehanian","Charles B Delahunt","Daniel E Shea","Zohreh Laverriere","Ishan Shah","Matthew P Horning"],"pdf_url":"https://arxiv.org/pdf/2310.03608v1.pdf","comment":"accepted in Simulation and Synthesis in Medical Imaging (SASHIMI)"},{"id":"http://arxiv.org/abs/2310.03602v1","updated":"2023-10-05T15:29:52Z","published":"2023-10-05T15:29:52Z","title":"Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout\n  Constraints","summary":"  Text-driven 3D indoor scene generation could be useful for gaming, film\nindustry, and AR/VR applications. However, existing methods cannot faithfully\ncapture the room layout, nor do they allow flexible editing of individual\nobjects in the room. To address these problems, we present Ctrl-Room, which is\nable to generate convincing 3D rooms with designer-style layouts and\nhigh-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables\nversatile interactive editing operations such as resizing or moving individual\nfurniture items. Our key insight is to separate the modeling of layouts and\nappearance. %how to model the room that takes into account both scene texture\nand geometry at the same time. To this end, Our proposed method consists of two\nstages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The\n`Layout Generation Stage' trains a text-conditional diffusion model to learn\nthe layout distribution with our holistic scene code parameterization. Next,\nthe `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a\nvivid panoramic image of the room guided by the 3D scene layout and text\nprompt. In this way, we achieve a high-quality 3D room with convincing layouts\nand lively textures. Benefiting from the scene code parameterization, we can\neasily edit the generated room model through our mask-guided editing module,\nwithout expensive editing-specific training. Extensive experiments on the\nStructured3D dataset demonstrate that our method outperforms existing methods\nin producing more reasonable, view-consistent, and editable 3D rooms from\nnatural language prompts.\n","authors":["Chuan Fang","Xiaotao Hu","Kunming Luo","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2310.03602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.00616v3","updated":"2023-10-05T15:15:58Z","published":"2023-09-01T17:59:56Z","title":"OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation","summary":"  Current 3D open-vocabulary scene understanding methods mostly utilize\nwell-aligned 2D images as the bridge to learn 3D features with language.\nHowever, applying these approaches becomes challenging in scenarios where 2D\nimages are absent. In this work, we introduce a new pipeline, namely,\nOpenIns3D, which requires no 2D image inputs, for 3D open-vocabulary scene\nunderstanding at the instance level. The OpenIns3D framework employs a\n\"Mask-Snap-Lookup\" scheme. The \"Mask\" module learns class-agnostic mask\nproposals in 3D point clouds. The \"Snap\" module generates synthetic scene-level\nimages at multiple scales and leverages 2D vision language models to extract\ninteresting objects. The \"Lookup\" module searches through the outcomes of\n\"Snap\" with the help of Mask2Pixel maps, which contain the precise\ncorrespondence between 3D masks and synthetic images, to assign category names\nto the proposed masks. This 2D input-free and flexible approach achieves\nstate-of-the-art results on a wide range of indoor and outdoor datasets by a\nlarge margin. Moreover, OpenIns3D allows for effortless switching of 2D\ndetectors without re-training. When integrated with powerful 2D open-world\nmodels such as ODISE and GroundingDINO, excellent results were observed on\nopen-vocabulary instance segmentation. When integrated with LLM-powered 2D\nmodels like LISA, it demonstrates a remarkable capacity to process highly\ncomplex text queries which require intricate reasoning and world knowledge.\nProject page: https://zheninghuang.github.io/OpenIns3D/\n","authors":["Zhening Huang","Xiaoyang Wu","Xi Chen","Hengshuang Zhao","Lei Zhu","Joan Lasenby"],"pdf_url":"https://arxiv.org/pdf/2309.00616v3.pdf","comment":"28 pages, 17 figures, 13 tables. Project page:\n  https://zheninghuang.github.io/OpenIns3D/"},{"id":"http://arxiv.org/abs/2302.02394v3","updated":"2023-10-05T14:35:08Z","published":"2023-02-05T14:30:22Z","title":"Eliminating Contextual Prior Bias for Semantic Image Editing via\n  Dual-Cycle Diffusion","summary":"  The recent success of text-to-image generation diffusion models has also\nrevolutionized semantic image editing, enabling the manipulation of images\nbased on query/target texts. Despite these advancements, a significant\nchallenge lies in the potential introduction of contextual prior bias in\npre-trained models during image editing, e.g., making unexpected modifications\nto inappropriate regions. To address this issue, we present a novel approach\ncalled Dual-Cycle Diffusion, which generates an unbiased mask to guide image\nediting. The proposed model incorporates a Bias Elimination Cycle that consists\nof both a forward path and an inverted path, each featuring a Structural\nConsistency Cycle to ensure the preservation of image content during the\nediting process. The forward path utilizes the pre-trained model to produce the\nedited image, while the inverted path converts the result back to the source\nimage. The unbiased mask is generated by comparing differences between the\nprocessed source image and the edited image to ensure that both conform to the\nsame distribution. Our experiments demonstrate the effectiveness of the\nproposed method, as it significantly improves the D-CLIP score from 0.272 to\n0.283. The code will be available at\nhttps://github.com/JohnDreamer/DualCycleDiffsion.\n","authors":["Zuopeng Yang","Tianshu Chu","Xin Lin","Erdun Gao","Daqing Liu","Jie Yang","Chaoyue Wang"],"pdf_url":"https://arxiv.org/pdf/2302.02394v3.pdf","comment":"This paper has been accepted by the IEEE Transactions on Circuits and\n  Systems for Video Technology (TCSVT)"},{"id":"http://arxiv.org/abs/2310.03563v1","updated":"2023-10-05T14:27:06Z","published":"2023-10-05T14:27:06Z","title":"BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance\n  Fields","summary":"  We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which\ndefines the image pose estimation problem as a NeRF based iterative linear\noptimization. NeRFs are novel neural space representation models that can\nsynthesize photorealistic novel views of real-world scenes or objects. Our\ncontributions are as follows: we extend the localization optimization objective\nwith a depth-based loss function, we introduce a multi-image based loss\nfunction where a sequence of images with known relative poses are used without\nincreasing the computational complexity, we omit hierarchical sampling during\nvolumetric rendering, meaning only the coarse model is used for pose\nestimation, and we how that by extending the sampling interval convergence can\nbe achieved even or higher initial pose estimate errors. With the proposed\nmodifications the convergence speed is significantly improved, and the basin of\nconvergence is substantially extended.\n","authors":["Ágoston István Csehi","Csaba Máté Józsa"],"pdf_url":"https://arxiv.org/pdf/2310.03563v1.pdf","comment":"Accepted to Nerf4ADR workshop of ICCV23 conference"},{"id":"http://arxiv.org/abs/2310.03559v1","updated":"2023-10-05T14:16:22Z","published":"2023-10-05T14:16:22Z","title":"MedSynV1: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT\n  Images","summary":"  This paper introduces an innovative methodology for producing high-quality 3D\nlung CT images guided by textual information. While diffusion-based generative\nmodels are increasingly used in medical imaging, current state-of-the-art\napproaches are limited to low-resolution outputs and underutilize radiology\nreports' abundant information. The radiology reports can enhance the generation\nprocess by providing additional guidance and offering fine-grained control over\nthe synthesis of images. Nevertheless, expanding text-guided generation to\nhigh-resolution 3D images poses significant memory and anatomical\ndetail-preserving challenges. Addressing the memory issue, we introduce a\nhierarchical scheme that uses a modified UNet architecture. We start by\nsynthesizing low-resolution images conditioned on the text, serving as a\nfoundation for subsequent generators for complete volumetric data. To ensure\nthe anatomical plausibility of the generated samples, we provide further\nguidance by generating vascular, airway, and lobular segmentation masks in\nconjunction with the CT images. The model demonstrates the capability to use\ntextual input and segmentation tasks to generate synthesized images. The\nresults of comparative assessments indicate that our approach exhibits superior\nperformance compared to the most advanced models based on GAN and diffusion\ntechniques, especially in accurately retaining crucial anatomical features such\nas fissure lines, airways, and vascular structures. This innovation introduces\nnovel possibilities. This study focuses on two main objectives: (1) the\ndevelopment of a method for creating images based on textual prompts and\nanatomical components, and (2) the capability to generate new images\nconditioning on anatomical elements. The advancements in image generation can\nbe applied to enhance numerous downstream tasks.\n","authors":["Yanwu Xu","Li Sun","Wei Peng","Shyam Visweswaran","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2310.03559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15523v4","updated":"2023-10-05T14:13:35Z","published":"2023-09-27T09:41:36Z","title":"Improving Facade Parsing with Vision Transformers and Line Integration","summary":"  Facade parsing stands as a pivotal computer vision task with far-reaching\napplications in areas like architecture, urban planning, and energy efficiency.\nDespite the recent success of deep learning-based methods in yielding\nimpressive results on certain open-source datasets, their viability for\nreal-world applications remains uncertain. Real-world scenarios are\nconsiderably more intricate, demanding greater computational efficiency.\nExisting datasets often fall short in representing these settings, and previous\nmethods frequently rely on extra models to enhance accuracy, which requires\nmuch computation cost. In this paper, we introduce Comprehensive Facade Parsing\n(CFP), a dataset meticulously designed to encompass the intricacies of\nreal-world facade parsing tasks. Comprising a total of 602 high-resolution\nstreet-view images, this dataset captures a diverse array of challenging\nscenarios, including sloping angles and densely clustered buildings, with\npainstakingly curated annotations for each image. We introduce a new pipeline\nknown as Revision-based Transformer Facade Parsing (RTFP). This marks the\npioneering utilization of Vision Transformers (ViT) in facade parsing, and our\nexperimental results definitively substantiate its merit. We also design Line\nAcquisition, Filtering, and Revision (LAFR), an efficient yet accurate revision\nalgorithm that can improve the segment result solely from simple line detection\nusing prior knowledge of the facade. In ECP 2011, RueMonge 2014, and our CFP,\nwe evaluate the superiority of our method. The dataset and code are available\nat https://github.com/wbw520/RTFP.\n","authors":["Bowen Wang","Jiaxing Zhang","Ran Zhang","Yunqin Li","Liangzhi Li","Yuta Nakashima"],"pdf_url":"https://arxiv.org/pdf/2309.15523v4.pdf","comment":"13 pages, 7 figures, 9 tables"},{"id":"http://arxiv.org/abs/2303.09874v3","updated":"2023-10-05T14:06:32Z","published":"2023-03-17T10:38:27Z","title":"Disentangling the Link Between Image Statistics and Human Perception","summary":"  In the 1950s, Barlow and Attneave hypothesised a link between biological\nvision and information maximisation. Following Shannon, information was defined\nusing the probability of natural images. A number of physiological and\npsychophysical phenomena have been derived ever since from principles like\ninfo-max, efficient coding, or optimal denoising. However, it remains unclear\nhow this link is expressed in mathematical terms from image probability. First,\nclassical derivations were subjected to strong assumptions on the probability\nmodels and on the behaviour of the sensors. Moreover, the direct evaluation of\nthe hypothesis was limited by the inability of the classical image models to\ndeliver accurate estimates of the probability. In this work we directly\nevaluate image probabilities using an advanced generative model for natural\nimages, and we analyse how probability-related factors can be combined to\npredict human perception via sensitivity of state-of-the-art subjective image\nquality metrics. We use information theory and regression analysis to find a\ncombination of just two probability-related factors that achieves 0.8\ncorrelation with subjective metrics. This probability-based sensitivity is\npsychophysically validated by reproducing the basic trends of the Contrast\nSensitivity Function, its suprathreshold variation, and trends of the Weber-law\nand masking.\n","authors":["Alexander Hepburn","Valero Laparra","Raúl Santos-Rodriguez","Jesús Malo"],"pdf_url":"https://arxiv.org/pdf/2303.09874v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16206v2","updated":"2023-10-05T14:04:00Z","published":"2023-09-28T07:06:42Z","title":"Alzheimer's Disease Prediction via Brain Structural-Functional Deep\n  Fusing Network","summary":"  Fusing structural-functional images of the brain has shown great potential to\nanalyze the deterioration of Alzheimer's disease (AD). However, it is a big\nchallenge to effectively fuse the correlated and complementary information from\nmultimodal neuroimages. In this paper, a novel model termed cross-modal\ntransformer generative adversarial network (CT-GAN) is proposed to effectively\nfuse the functional and structural information contained in functional magnetic\nresonance imaging (fMRI) and diffusion tensor imaging (DTI). The CT-GAN can\nlearn topological features and generate multimodal connectivity from multimodal\nimaging data in an efficient end-to-end manner. Moreover, the swapping\nbi-attention mechanism is designed to gradually align common features and\neffectively enhance the complementary features between modalities. By analyzing\nthe generated connectivity features, the proposed model can identify AD-related\nbrain connections. Evaluations on the public ADNI dataset show that the\nproposed CT-GAN can dramatically improve prediction performance and detect\nAD-related brain regions effectively. The proposed model also provides new\ninsights for detecting AD-related abnormal neural circuits.\n","authors":["Qiankun Zuo","Junren Pan","Shuqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2309.16206v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2310.03535v1","updated":"2023-10-05T13:35:00Z","published":"2023-10-05T13:35:00Z","title":"Towards Unified Deep Image Deraining: A Survey and A New Benchmark","summary":"  Recent years have witnessed significant advances in image deraining due to\nthe kinds of effective image priors and deep learning models. As each deraining\napproach has individual settings (e.g., training and test datasets, evaluation\ncriteria), how to fairly evaluate existing approaches comprehensively is not a\ntrivial task. Although existing surveys aim to review of image deraining\napproaches comprehensively, few of them focus on providing unify evaluation\nsettings to examine the deraining capability and practicality evaluation. In\nthis paper, we provide a comprehensive review of existing image deraining\nmethod and provide a unify evaluation setting to evaluate the performance of\nimage deraining methods. We construct a new high-quality benchmark named\nHQ-RAIN to further conduct extensive evaluation, consisting of 5,000 paired\nhigh-resolution synthetic images with higher harmony and realism. We also\ndiscuss the existing challenges and highlight several future research\nopportunities worth exploring. To facilitate the reproduction and tracking of\nthe latest deraining technologies for general users, we build an online\nplatform to provide the off-the-shelf toolkit, involving the large-scale\nperformance evaluation. This online platform and the proposed new benchmark are\npublicly available and will be regularly updated at http://www.deraining.tech/.\n","authors":["Xiang Chen","Jinshan Pan","Jiangxin Dong","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2310.03535v1.pdf","comment":"Project website: http://www.deraining.tech/"},{"id":"http://arxiv.org/abs/2310.03534v1","updated":"2023-10-05T13:34:07Z","published":"2023-10-05T13:34:07Z","title":"3D-Aware Hypothesis & Verification for Generalizable Relative Object\n  Pose Estimation","summary":"  Prior methods that tackle the problem of generalizable object pose estimation\nhighly rely on having dense views of the unseen object. By contrast, we address\nthe scenario where only a single reference view of the object is available. Our\ngoal then is to estimate the relative object pose between this reference view\nand a query image that depicts the object in a different pose. In this\nscenario, robust generalization is imperative due to the presence of unseen\nobjects during testing and the large-scale object pose variation between the\nreference and the query. To this end, we present a new\nhypothesis-and-verification framework, in which we generate and evaluate\nmultiple pose hypotheses, ultimately selecting the most reliable one as the\nrelative object pose. To measure reliability, we introduce a 3D-aware\nverification that explicitly applies 3D transformations to the 3D object\nrepresentations learned from the two input images. Our comprehensive\nexperiments on the Objaverse, LINEMOD, and CO3D datasets evidence the superior\naccuracy of our approach in relative pose estimation and its robustness in\nlarge-scale pose variations, when dealing with unseen objects.\n","authors":["Chen Zhao","Tong Zhang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2310.03534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.09573v3","updated":"2023-10-05T13:32:57Z","published":"2022-02-19T10:52:52Z","title":"Diversity in deep generative models and generative AI","summary":"  The decoder-based machine learning generative algorithms such as Generative\nAdversarial Networks (GAN), Variational Auto-Encoders (VAE), Transformers show\nimpressive results when constructing objects similar to those in a training\nensemble. However, the generation of new objects builds mainly on the\nunderstanding of the hidden structure of the training dataset followed by a\nsampling from a multi-dimensional normal variable. In particular each sample is\nindependent from the others and can repeatedly propose same type of objects. To\ncure this drawback we introduce a kernel-based measure quantization method that\ncan produce new objects from a given target measure by approximating it as a\nwhole and even staying away from elements already drawn from that distribution.\nThis ensures a better diversity of the produced objects. The method is tested\non classic machine learning benchmarks.\n","authors":["Gabriel Turinici"],"pdf_url":"https://arxiv.org/pdf/2202.09573v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04554v2","updated":"2023-10-05T13:26:33Z","published":"2023-01-11T16:31:38Z","title":"Universal Detection of Backdoor Attacks via Density-based Clustering and\n  Centroids Analysis","summary":"  We propose a Universal Defence against backdoor attacks based on Clustering\nand Centroids Analysis (CCA-UD). The goal of the defence is to reveal whether a\nDeep Neural Network model is subject to a backdoor attack by inspecting the\ntraining dataset. CCA-UD first clusters the samples of the training set by\nmeans of density-based clustering. Then, it applies a novel strategy to detect\nthe presence of poisoned clusters. The proposed strategy is based on a general\nmisclassification behaviour observed when the features of a representative\nexample of the analysed cluster are added to benign samples. The capability of\ninducing a misclassification error is a general characteristic of poisoned\nsamples, hence the proposed defence is attack-agnostic. This marks a\nsignificant difference with respect to existing defences, that, either can\ndefend against only some types of backdoor attacks, or are effective only when\nsome conditions on the poisoning ratio or the kind of triggering signal used by\nthe attacker are satisfied.\n  Experiments carried out on several classification tasks and network\narchitectures, considering different types of backdoor attacks (with either\nclean or corrupted labels), and triggering signals, including both global and\nlocal triggering signals, as well as sample-specific and source-specific\ntriggers, reveal that the proposed method is very effective to defend against\nbackdoor attacks in all the cases, always outperforming the state of the art\ntechniques.\n","authors":["Wei Guo","Benedetta Tondi","Mauro Barni"],"pdf_url":"https://arxiv.org/pdf/2301.04554v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03525v1","updated":"2023-10-05T13:19:48Z","published":"2023-10-05T13:19:48Z","title":"V2X Cooperative Perception for Autonomous Driving: Recent Advances and\n  Challenges","summary":"  Accurate perception is essential for advancing autonomous driving and\naddressing safety challenges in modern transportation systems. Despite\nsignificant advancements in computer vision for object recognition, current\nperception methods still face difficulties in complex real-world traffic\nenvironments. Challenges such as physical occlusion and limited sensor field of\nview persist for individual vehicle systems. Cooperative Perception (CP) with\nVehicle-to-Everything (V2X) technologies has emerged as a solution to overcome\nthese obstacles and enhance driving automation systems. While some research has\nexplored CP's fundamental architecture and critical components, there remains a\nlack of comprehensive summaries of the latest innovations, particularly in the\ncontext of V2X communication technologies. To address this gap, this paper\nprovides a comprehensive overview of the evolution of CP technologies, spanning\nfrom early explorations to recent developments, including advancements in V2X\ncommunication technologies. Additionally, a contemporary generic framework is\nproposed to illustrate the V2X-based CP workflow, aiding in the structured\nunderstanding of CP system components. Furthermore, this paper categorizes\nprevailing V2X-based CP methodologies based on the critical issues they\naddress. An extensive literature review is conducted within this taxonomy,\nevaluating existing datasets and simulators. Finally, open challenges and\nfuture directions in CP for autonomous driving are discussed by considering\nboth perception and V2X communication advancements.\n","authors":["Tao Huang","Jianan Liu","Xi Zhou","Dinh C. Nguyen","Mostafa Rahimi Azghadi","Yuxuan Xia","Qing-Long Han","Sumei Sun"],"pdf_url":"https://arxiv.org/pdf/2310.03525v1.pdf","comment":"33 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.03517v1","updated":"2023-10-05T12:56:34Z","published":"2023-10-05T12:56:34Z","title":"PrototypeFormer: Learning to Explore Prototype Relationships for\n  Few-shot Image Classification","summary":"  Few-shot image classification has received considerable attention for\naddressing the challenge of poor classification performance with limited\nsamples in novel classes. However, numerous studies have employed sophisticated\nlearning strategies and diversified feature extraction methods to address this\nissue. In this paper, we propose our method called PrototypeFormer, which aims\nto significantly advance traditional few-shot image classification approaches\nby exploring prototype relationships. Specifically, we utilize a transformer\narchitecture to build a prototype extraction module, aiming to extract class\nrepresentations that are more discriminative for few-shot classification.\nAdditionally, during the model training process, we propose a contrastive\nlearning-based optimization approach to optimize prototype features in few-shot\nlearning scenarios. Despite its simplicity, the method performs remarkably\nwell, with no bells and whistles. We have experimented with our approach on\nseveral popular few-shot image classification benchmark datasets, which shows\nthat our method outperforms all current state-of-the-art methods. In\nparticular, our method achieves 97.07% and 90.88% on 5-way 5-shot and 5-way\n1-shot tasks of miniImageNet, which surpasses the state-of-the-art results with\naccuracy of 7.27% and 8.72%, respectively. The code will be released later.\n","authors":["Feihong He","Gang Li","Lingyu Si","Leilei Yan","Fanzhang Li","Fuchun Sun"],"pdf_url":"https://arxiv.org/pdf/2310.03517v1.pdf","comment":"Submitted to AAAI2024"},{"id":"http://arxiv.org/abs/2306.00966v3","updated":"2023-10-05T12:55:12Z","published":"2023-06-01T17:57:08Z","title":"The Hidden Language of Diffusion Models","summary":"  Text-to-image diffusion models have demonstrated an unparalleled ability to\ngenerate high-quality, diverse images from a textual prompt. However, the\ninternal representations learned by these models remain an enigma. In this\nwork, we present Conceptor, a novel method to interpret the internal\nrepresentation of a textual concept by a diffusion model. This interpretation\nis obtained by decomposing the concept into a small set of human-interpretable\ntextual elements. Applied over the state-of-the-art Stable Diffusion model,\nConceptor reveals non-trivial structures in the representations of concepts.\nFor example, we find surprising visual connections between concepts, that\ntranscend their textual semantics. We additionally discover concepts that rely\non mixtures of exemplars, biases, renowned artistic styles, or a simultaneous\nfusion of multiple meanings of the concept. Through a large battery of\nexperiments, we demonstrate Conceptor's ability to provide meaningful, robust,\nand faithful decompositions for a wide variety of abstract, concrete, and\ncomplex textual concepts, while allowing to naturally connect each\ndecomposition element to its corresponding visual impact on the generated\nimages. Our code will be available at: https://hila-chefer.github.io/Conceptor/\n","authors":["Hila Chefer","Oran Lang","Mor Geva","Volodymyr Polosukhin","Assaf Shocher","Michal Irani","Inbar Mosseri","Lior Wolf"],"pdf_url":"https://arxiv.org/pdf/2306.00966v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17329v2","updated":"2023-10-05T12:52:09Z","published":"2023-09-29T15:40:58Z","title":"Efficient Anatomical Labeling of Pulmonary Tree Structures via Implicit\n  Point-Graph Networks","summary":"  Pulmonary diseases rank prominently among the principal causes of death\nworldwide. Curing them will require, among other things, a better understanding\nof the many complex 3D tree-shaped structures within the pulmonary system, such\nas airways, arteries, and veins. In theory, they can be modeled using\nhigh-resolution image stacks. Unfortunately, standard CNN approaches operating\non dense voxel grids are prohibitively expensive. To remedy this, we introduce\na point-based approach that preserves graph connectivity of tree skeleton and\nincorporates an implicit surface representation. It delivers SOTA accuracy at a\nlow computational cost and the resulting models have usable surfaces. Due to\nthe scarcity of publicly accessible data, we have also curated an extensive\ndataset to evaluate our approach and will make it public.\n","authors":["Kangxian Xie","Jiancheng Yang","Donglai Wei","Ziqiao Weng","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2309.17329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03513v1","updated":"2023-10-05T12:48:12Z","published":"2023-10-05T12:48:12Z","title":"Exploring DINO: Emergent Properties and Limitations for Synthetic\n  Aperture Radar Imagery","summary":"  Self-supervised learning (SSL) models have recently demonstrated remarkable\nperformance across various tasks, including image segmentation. This study\ndelves into the emergent characteristics of the Self-Distillation with No\nLabels (DINO) algorithm and its application to Synthetic Aperture Radar (SAR)\nimagery. We pre-train a vision transformer (ViT)-based DINO model using\nunlabeled SAR data, and later fine-tune the model to predict high-resolution\nland cover maps. We rigorously evaluate the utility of attention maps generated\nby the ViT backbone, and compare them with the model's token embedding space.\nWe observe a small improvement in model performance with pre-training compared\nto training from scratch, and discuss the limitations and opportunities of SSL\nfor remote sensing and land cover segmentation. Beyond small performance\nincreases, we show that ViT attention maps hold great intrinsic value for\nremote sensing, and could provide useful inputs to other algorithms. With this,\nour work lays the ground-work for bigger and better SSL models for Earth\nObservation.\n","authors":["Joseph A. Gallego-Mejia","Anna Jungbluth","Laura Martínez-Ferrer","Matt Allen","Francisco Dorr","Freddie Kalaitzis","Raúl Ramos-Pollán"],"pdf_url":"https://arxiv.org/pdf/2310.03513v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.03507v1","updated":"2023-10-05T12:39:27Z","published":"2023-10-05T12:39:27Z","title":"RL-based Stateful Neural Adaptive Sampling and Denoising for Real-Time\n  Path Tracing","summary":"  Monte-Carlo path tracing is a powerful technique for realistic image\nsynthesis but suffers from high levels of noise at low sample counts, limiting\nits use in real-time applications. To address this, we propose a framework with\nend-to-end training of a sampling importance network, a latent space encoder\nnetwork, and a denoiser network. Our approach uses reinforcement learning to\noptimize the sampling importance network, thus avoiding explicit numerically\napproximated gradients. Our method does not aggregate the sampled values per\npixel by averaging but keeps all sampled values which are then fed into the\nlatent space encoder. The encoder replaces handcrafted spatiotemporal\nheuristics by learned representations in a latent space. Finally, a neural\ndenoiser is trained to refine the output image. Our approach increases visual\nquality on several challenging datasets and reduces rendering times for equal\nquality by a factor of 1.6x compared to the previous state-of-the-art, making\nit a promising solution for real-time applications.\n","authors":["Antoine Scardigli","Lukas Cavigelli","Lorenz K. Müller"],"pdf_url":"https://arxiv.org/pdf/2310.03507v1.pdf","comment":"Submitted to NeurIPS. https://openreview.net/forum?id=xNyR7DXUzJ"},{"id":"http://arxiv.org/abs/2309.06262v2","updated":"2023-10-05T12:30:08Z","published":"2023-09-12T14:22:22Z","title":"Modality Unifying Network for Visible-Infrared Person Re-Identification","summary":"  Visible-infrared person re-identification (VI-ReID) is a challenging task due\nto large cross-modality discrepancies and intra-class variations. Existing\nmethods mainly focus on learning modality-shared representations by embedding\ndifferent modalities into the same feature space. As a result, the learned\nfeature emphasizes the common patterns across modalities while suppressing\nmodality-specific and identity-aware information that is valuable for Re-ID. To\naddress these issues, we propose a novel Modality Unifying Network (MUN) to\nexplore a robust auxiliary modality for VI-ReID. First, the auxiliary modality\nis generated by combining the proposed cross-modality learner and\nintra-modality learner, which can dynamically model the modality-specific and\nmodality-shared representations to alleviate both cross-modality and\nintra-modality variations. Second, by aligning identity centres across the\nthree modalities, an identity alignment loss function is proposed to discover\nthe discriminative feature representations. Third, a modality alignment loss is\nintroduced to consistently reduce the distribution distance of visible and\ninfrared images by modality prototype modeling. Extensive experiments on\nmultiple public datasets demonstrate that the proposed method surpasses the\ncurrent state-of-the-art methods by a significant margin.\n","authors":["Hao Yu","Xu Cheng","Wei Peng","Weihao Liu","Guoying Zhao"],"pdf_url":"https://arxiv.org/pdf/2309.06262v2.pdf","comment":"11 pages, 5 figures. Accepted as the poster paper in ICCV2023"},{"id":"http://arxiv.org/abs/2310.03502v1","updated":"2023-10-05T12:29:41Z","published":"2023-10-05T12:29:41Z","title":"Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and\n  Latent Diffusion","summary":"  Text-to-image generation is a significant domain in modern computer vision\nand has achieved substantial improvements through the evolution of generative\narchitectures. Among these, there are diffusion-based models that have\ndemonstrated essential quality enhancements. These models are generally split\ninto two categories: pixel-level and latent-level approaches. We present\nKandinsky1, a novel exploration of latent diffusion architecture, combining the\nprinciples of the image prior models with latent diffusion techniques. The\nimage prior model is trained separately to map text embeddings to image\nembeddings of CLIP. Another distinct feature of the proposed model is the\nmodified MoVQ implementation, which serves as the image autoencoder component.\nOverall, the designed model contains 3.3B parameters. We also deployed a\nuser-friendly demo system that supports diverse generative modes such as\ntext-to-image generation, image fusion, text and image fusion, image variations\ngeneration, and text-guided inpainting/outpainting. Additionally, we released\nthe source code and checkpoints for the Kandinsky models. Experimental\nevaluations demonstrate a FID score of 8.03 on the COCO-30K dataset, marking\nour model as the top open-source performer in terms of measurable image\ngeneration quality.\n","authors":["Anton Razzhigaev","Arseniy Shakhmatov","Anastasia Maltseva","Vladimir Arkhipkin","Igor Pavlov","Ilya Ryabov","Angelina Kuts","Alexander Panchenko","Andrey Kuznetsov","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2310.03502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03499v1","updated":"2023-10-05T12:24:25Z","published":"2023-10-05T12:24:25Z","title":"IceCloudNet: Cirrus and mixed-phase cloud prediction from SEVIRI input\n  learned from sparse supervision","summary":"  Clouds containing ice particles play a crucial role in the climate system.\nYet they remain a source of great uncertainty in climate models and future\nclimate projections. In this work, we create a new observational constraint of\nregime-dependent ice microphysical properties at the spatio-temporal coverage\nof geostationary satellite instruments and the quality of active satellite\nretrievals. We achieve this by training a convolutional neural network on three\nyears of SEVIRI and DARDAR data sets. This work will enable novel research to\nimprove ice cloud process understanding and hence, reduce uncertainties in a\nchanging climate and help assess geoengineering methods for cirrus clouds.\n","authors":["Kai Jeggle","Mikolaj Czerkawski","Federico Serva","Bertrand Le Saux","David Neubauer","Ulrike Lohmann"],"pdf_url":"https://arxiv.org/pdf/2310.03499v1.pdf","comment":"A Preprint. Submitted to Tackling Climate Change with Machine\n  Learning: workshop at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2304.03752v2","updated":"2023-10-05T12:18:14Z","published":"2023-04-07T17:45:35Z","title":"V3Det: Vast Vocabulary Visual Detection Dataset","summary":"  Recent advances in detecting arbitrary objects in the real world are trained\nand evaluated on object detection datasets with a relatively restricted\nvocabulary. To facilitate the development of more general visual object\ndetection, we propose V3Det, a vast vocabulary visual detection dataset with\nprecisely annotated bounding boxes on massive images. V3Det has several\nappealing properties: 1) Vast Vocabulary: It contains bounding boxes of objects\nfrom 13,204 categories on real-world images, which is 10 times larger than the\nexisting large vocabulary object detection dataset, e.g., LVIS. 2) Hierarchical\nCategory Organization: The vast vocabulary of V3Det is organized by a\nhierarchical category tree which annotates the inclusion relationship among\ncategories, encouraging the exploration of category relationships in vast and\nopen vocabulary object detection. 3) Rich Annotations: V3Det comprises\nprecisely annotated objects in 243k images and professional descriptions of\neach category written by human experts and a powerful chatbot. By offering a\nvast exploration space, V3Det enables extensive benchmarks on both vast and\nopen vocabulary object detection, leading to new observations, practices, and\ninsights for future research. It has the potential to serve as a cornerstone\ndataset for developing more general visual perception systems. V3Det is\navailable at https://v3det.openxlab.org.cn/.\n","authors":["Jiaqi Wang","Pan Zhang","Tao Chu","Yuhang Cao","Yujie Zhou","Tong Wu","Bin Wang","Conghui He","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2304.03752v2.pdf","comment":"ICCV 2023 Oral Camera Ready"},{"id":"http://arxiv.org/abs/2310.03485v1","updated":"2023-10-05T11:56:06Z","published":"2023-10-05T11:56:06Z","title":"BTDNet: a Multi-Modal Approach for Brain Tumor Radiogenomic\n  Classification","summary":"  Brain tumors pose significant health challenges worldwide, with glioblastoma\nbeing one of the most aggressive forms. Accurate determination of the\nO6-methylguanine-DNA methyltransferase (MGMT) promoter methylation status is\ncrucial for personalized treatment strategies. However, traditional methods are\nlabor-intensive and time-consuming. This paper proposes a novel multi-modal\napproach, BTDNet, leveraging multi-parametric MRI scans, including FLAIR, T1w,\nT1wCE, and T2 3D volumes, to predict MGMT promoter methylation status. BTDNet\naddresses two main challenges: the variable volume lengths (i.e., each volume\nconsists of a different number of slices) and the volume-level annotations\n(i.e., the whole 3D volume is annotated and not the independent slices that it\nconsists of). BTDNet consists of four components: i) the data augmentation one\n(that performs geometric transformations, convex combinations of data pairs and\ntest-time data augmentation); ii) the 3D analysis one (that performs global\nanalysis through a CNN-RNN); iii) the routing one (that contains a mask layer\nthat handles variable input feature lengths), and iv) the modality fusion one\n(that effectively enhances data representation, reduces ambiguities and\nmitigates data scarcity). The proposed method outperforms by large margins the\nstate-of-the-art methods in the RSNA-ASNR-MICCAI BraTS 2021 Challenge, offering\na promising avenue for enhancing brain tumor diagnosis and treatment.\n","authors":["Dimitrios Kollias","Karanjot Vendal","Priyanka Gadhavi","Solomon Russom"],"pdf_url":"https://arxiv.org/pdf/2310.03485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01338v2","updated":"2023-10-05T11:55:37Z","published":"2023-03-02T15:14:46Z","title":"AdvRain: Adversarial Raindrops to Attack Camera-based Smart Vision\n  Systems","summary":"  Vision-based perception modules are increasingly deployed in many\napplications, especially autonomous vehicles and intelligent robots. These\nmodules are being used to acquire information about the surroundings and\nidentify obstacles. Hence, accurate detection and classification are essential\nto reach appropriate decisions and take appropriate and safe actions at all\ntimes. Current studies have demonstrated that \"printed adversarial attacks\",\nknown as physical adversarial attacks, can successfully mislead perception\nmodels such as object detectors and image classifiers. However, most of these\nphysical attacks are based on noticeable and eye-catching patterns for\ngenerated perturbations making them identifiable/detectable by human eye or in\ntest drives. In this paper, we propose a camera-based inconspicuous adversarial\nattack (\\textbf{AdvRain}) capable of fooling camera-based perception systems\nover all objects of the same class. Unlike mask based fake-weather attacks that\nrequire access to the underlying computing hardware or image memory, our attack\nis based on emulating the effects of a natural weather condition (i.e.,\nRaindrops) that can be printed on a translucent sticker, which is externally\nplaced over the lens of a camera. To accomplish this, we provide an iterative\nprocess based on performing a random search aiming to identify critical\npositions to make sure that the performed transformation is adversarial for a\ntarget classifier. Our transformation is based on blurring predefined parts of\nthe captured image corresponding to the areas covered by the raindrop. We\nachieve a drop in average model accuracy of more than $45\\%$ and $40\\%$ on\nVGG19 for ImageNet and Resnet34 for Caltech-101, respectively, using only $20$\nraindrops.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.01338v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14979v3","updated":"2023-10-05T11:53:31Z","published":"2023-05-24T10:13:32Z","title":"Assessment of the Reliablity of a Model's Decision by Generalizing\n  Attribution to the Wavelet Domain","summary":"  Neural networks have shown remarkable performance in computer vision, but\ntheir deployment in numerous scientific and technical fields is challenging due\nto their black-box nature. Scientists and practitioners need to evaluate the\nreliability of a decision, i.e., to know simultaneously if a model relies on\nthe relevant features and whether these features are robust to image\ncorruptions. Existing attribution methods aim to provide human-understandable\nexplanations by highlighting important regions in the image domain, but fail to\nfully characterize a decision process's reliability. To bridge this gap, we\nintroduce the Wavelet sCale Attribution Method (WCAM), a generalization of\nattribution from the pixel domain to the space-scale domain using wavelet\ntransforms. Attribution in the wavelet domain reveals where {\\it and} on what\nscales the model focuses, thus enabling us to assess whether a decision is\nreliable.\n","authors":["Gabriel Kasmi","Laurent Dubus","Yves-Marie Saint Drenan","Philippe Blanc"],"pdf_url":"https://arxiv.org/pdf/2305.14979v3.pdf","comment":"16 pages, 10 figures, 2 tables. v1 of the manuscript rejected from\n  NeurIPS 2023, mainly due to the lack of quantitative evidence of the\n  relevance of the proposed methodology. In the v2, we propose steps to address\n  this issue and also plan on expanding the insertion and deletion scores for\n  our method"},{"id":"http://arxiv.org/abs/2310.03472v1","updated":"2023-10-05T11:28:32Z","published":"2023-10-05T11:28:32Z","title":"Ammonia-Net: A Multi-task Joint Learning Model for Multi-class\n  Segmentation and Classification in Tooth-marked Tongue Diagnosis","summary":"  In Traditional Chinese Medicine, the tooth marks on the tongue, stemming from\nprolonged dental pressure, serve as a crucial indicator for assessing qi (yang)\ndeficiency, which is intrinsically linked to visceral health. Manual diagnosis\nof tooth-marked tongue solely relies on experience. Nonetheless, the diversity\nin shape, color, and type of tooth marks poses a challenge to diagnostic\naccuracy and consistency. To address these problems, herein we propose a\nmulti-task joint learning model named Ammonia-Net. This model employs a\nconvolutional neural network-based architecture, specifically designed for\nmulti-class segmentation and classification of tongue images. Ammonia-Net\nperforms semantic segmentation of tongue images to identify tongue and tooth\nmarks. With the assistance of segmentation output, it classifies the images\ninto the desired number of classes: healthy tongue, light tongue, moderate\ntongue, and severe tongue. As far as we know, this is the first attempt to\napply the semantic segmentation results of tooth marks for tooth-marked tongue\nclassification. To train Ammonia-Net, we collect 856 tongue images from 856\nsubjects. After a number of extensive experiments, the experimental results\nshow that the proposed model achieves 99.06% accuracy in the two-class\nclassification task of tooth-marked tongue identification and 80.02%. As for\nthe segmentation task, mIoU for tongue and tooth marks amounts to 71.65%.\n","authors":["Shunkai Shi","Yuqi Wang","Qihui Ye","Yanran Wang","Yiming Zhu","Muhammad Hassan","Aikaterini Melliou","Dongmei Yu"],"pdf_url":"https://arxiv.org/pdf/2310.03472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08303v3","updated":"2023-10-05T11:05:42Z","published":"2023-08-16T12:07:02Z","title":"Leveraging Next-Active Objects for Context-Aware Anticipation in\n  Egocentric Videos","summary":"  Objects are crucial for understanding human-object interactions. By\nidentifying the relevant objects, one can also predict potential future\ninteractions or actions that may occur with these objects. In this paper, we\nstudy the problem of Short-Term Object interaction anticipation (STA) and\npropose NAOGAT (Next-Active-Object Guided Anticipation Transformer), a\nmulti-modal end-to-end transformer network, that attends to objects in observed\nframes in order to anticipate the next-active-object (NAO) and, eventually, to\nguide the model to predict context-aware future actions. The task is\nchallenging since it requires anticipating future action along with the object\nwith which the action occurs and the time after which the interaction will\nbegin, a.k.a. the time to contact (TTC). Compared to existing video modeling\narchitectures for action anticipation, NAOGAT captures the relationship between\nobjects and the global scene context in order to predict detections for the\nnext active object and anticipate relevant future actions given these\ndetections, leveraging the objects' dynamics to improve accuracy. One of the\nkey strengths of our approach, in fact, is its ability to exploit the motion\ndynamics of objects within a given clip, which is often ignored by other\nmodels, and separately decoding the object-centric and motion-centric\ninformation. Through our experiments, we show that our model outperforms\nexisting methods on two separate datasets, Ego4D and EpicKitchens-100 (\"Unseen\nSet\"), as measured by several additional metrics, such as time to contact, and\nnext-active-object localization. The code will be available upon acceptance.\n","authors":["Sanket Thakur","Cigdem Beyan","Pietro Morerio","Vittorio Murino","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2308.08303v3.pdf","comment":"Accepted in WACV'24"},{"id":"http://arxiv.org/abs/2310.03456v1","updated":"2023-10-05T10:54:33Z","published":"2023-10-05T10:54:33Z","title":"Multi-Resolution Audio-Visual Feature Fusion for Temporal Action\n  Localization","summary":"  Temporal Action Localization (TAL) aims to identify actions' start, end, and\nclass labels in untrimmed videos. While recent advancements using transformer\nnetworks and Feature Pyramid Networks (FPN) have enhanced visual feature\nrecognition in TAL tasks, less progress has been made in the integration of\naudio features into such frameworks. This paper introduces the Multi-Resolution\nAudio-Visual Feature Fusion (MRAV-FF), an innovative method to merge\naudio-visual data across different temporal resolutions. Central to our\napproach is a hierarchical gated cross-attention mechanism, which discerningly\nweighs the importance of audio information at diverse temporal scales. Such a\ntechnique not only refines the precision of regression boundaries but also\nbolsters classification confidence. Importantly, MRAV-FF is versatile, making\nit compatible with existing FPN TAL architectures and offering a significant\nenhancement in performance when audio data is available.\n","authors":["Edward Fish","Jon Weinbren","Andrew Gilbert"],"pdf_url":"https://arxiv.org/pdf/2310.03456v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2209.12148v2","updated":"2023-10-05T10:37:39Z","published":"2022-09-25T04:56:10Z","title":"Self-Supervised Masked Convolutional Transformer Block for Anomaly\n  Detection","summary":"  Anomaly detection has recently gained increasing attention in the field of\ncomputer vision, likely due to its broad set of applications ranging from\nproduct fault detection on industrial production lines and impending event\ndetection in video surveillance to finding lesions in medical scans. Regardless\nof the domain, anomaly detection is typically framed as a one-class\nclassification task, where the learning is conducted on normal examples only.\nAn entire family of successful anomaly detection methods is based on learning\nto reconstruct masked normal inputs (e.g. patches, future frames, etc.) and\nexerting the magnitude of the reconstruction error as an indicator for the\nabnormality level. Unlike other reconstruction-based methods, we present a\nnovel self-supervised masked convolutional transformer block (SSMCTB) that\ncomprises the reconstruction-based functionality at a core architectural level.\nThe proposed self-supervised block is extremely flexible, enabling information\nmasking at any layer of a neural network and being compatible with a wide range\nof neural architectures. In this work, we extend our previous self-supervised\npredictive convolutional attentive block (SSPCAB) with a 3D masked\nconvolutional layer, a transformer for channel-wise attention, as well as a\nnovel self-supervised objective based on Huber loss. Furthermore, we show that\nour block is applicable to a wider variety of tasks, adding anomaly detection\nin medical images and thermal videos to the previously considered tasks based\non RGB images and surveillance videos. We exhibit the generality and\nflexibility of SSMCTB by integrating it into multiple state-of-the-art neural\nmodels for anomaly detection, bringing forth empirical results that confirm\nconsiderable performance improvements on five benchmarks. We release our code\nand data as open source at: https://github.com/ristea/ssmctb.\n","authors":["Neelu Madan","Nicolae-Catalin Ristea","Radu Tudor Ionescu","Kamal Nasrollahi","Fahad Shahbaz Khan","Thomas B. Moeslund","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2209.12148v2.pdf","comment":"Accepted in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence"},{"id":"http://arxiv.org/abs/2310.03432v1","updated":"2023-10-05T10:17:47Z","published":"2023-10-05T10:17:47Z","title":"Mitigating the Influence of Domain Shift in Skin Lesion Classification:\n  A Benchmark Study of Unsupervised Domain Adaptation Methods on Dermoscopic\n  Images","summary":"  The potential of deep neural networks in skin lesion classification has\nalready been demonstrated to be on-par if not superior to the dermatologists\ndiagnosis. However, the performance of these models usually deteriorates when\nthe test data differs significantly from the training data (i.e. domain shift).\nThis concerning limitation for models intended to be used in real-world skin\nlesion classification tasks poses a risk to patients. For example, different\nimage acquisition systems or previously unseen anatomical sites on the patient\ncan suffice to cause such domain shifts. Mitigating the negative effect of such\nshifts is therefore crucial, but developing effective methods to address domain\nshift has proven to be challenging. In this study, we carry out an in-depth\nanalysis of eight different unsupervised domain adaptation methods to analyze\ntheir effectiveness in improving generalization for dermoscopic datasets. To\nensure robustness of our findings, we test each method on a total of ten\ndistinct datasets, thereby covering a variety of possible domain shifts. In\naddition, we investigated which factors in the domain shifted datasets have an\nimpact on the effectiveness of domain adaptation methods. Our findings show\nthat all of the eight domain adaptation methods result in improved AUPRC for\nthe majority of analyzed datasets. Altogether, these results indicate that\nunsupervised domain adaptations generally lead to performance improvements for\nthe binary melanoma-nevus classification task regardless of the nature of the\ndomain shift. However, small or heavily imbalanced datasets lead to a reduced\nconformity of the results due to the influence of these factors on the methods\nperformance.\n","authors":["Sireesha Chamarthi","Katharina Fogelberg","Roman C. Maron","Titus J. Brinker","Julia Niebling"],"pdf_url":"https://arxiv.org/pdf/2310.03432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03431v1","updated":"2023-10-05T10:17:30Z","published":"2023-10-05T10:17:30Z","title":"Robust Zero Level-Set Extraction from Unsigned Distance Fields Based on\n  Double Covering","summary":"  In this paper, we propose a new method, called DoubleCoverUDF, for extracting\nthe zero level-set from unsigned distance fields (UDFs). DoubleCoverUDF takes a\nlearned UDF and a user-specified parameter $r$ (a small positive real number)\nas input and extracts an iso-surface with an iso-value $r$ using the\nconventional marching cubes algorithm. We show that the computed iso-surface is\nthe boundary of the $r$-offset volume of the target zero level-set $S$, which\nis an orientable manifold, regardless of the topology of $S$. Next, the\nalgorithm computes a covering map to project the boundary mesh onto $S$,\npreserving the mesh's topology and avoiding folding. If $S$ is an orientable\nmanifold surface, our algorithm separates the double-layered mesh into a single\nlayer using a robust minimum-cut post-processing step. Otherwise, it keeps the\ndouble-layered mesh as the output. We validate our algorithm by reconstructing\n3D surfaces of open models and demonstrate its efficacy and effectiveness on\nsynthetic models and benchmark datasets. Our experimental results confirm that\nour method is robust and produces meshes with better quality in terms of both\nvisual evaluation and quantitative measures than existing UDF-based methods.\nThe source code is available at https://github.com/jjjkkyz/DCUDF.\n","authors":["Fei Hou","Xuhui Chen","Wencheng Wang","Hong Qin","Ying He"],"pdf_url":"https://arxiv.org/pdf/2310.03431v1.pdf","comment":"accepted to ACM Transactions on Graphics (SIGGRAPH Asia 2023)"},{"id":"http://arxiv.org/abs/2310.03420v1","updated":"2023-10-05T09:57:23Z","published":"2023-10-05T09:57:23Z","title":"FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained\n  Diffusion Models and Monocular Depth Estimators","summary":"  Matching cross-modality features between images and point clouds is a\nfundamental problem for image-to-point cloud registration. However, due to the\nmodality difference between images and points, it is difficult to learn robust\nand discriminative cross-modality features by existing metric learning methods\nfor feature matching. Instead of applying metric learning on cross-modality\ndata, we propose to unify the modality between images and point clouds by\npretrained large-scale models first, and then establish robust correspondence\nwithin the same modality. We show that the intermediate features, called\ndiffusion features, extracted by depth-to-image diffusion models are\nsemantically consistent between images and point clouds, which enables the\nbuilding of coarse but robust cross-modality correspondences. We further\nextract geometric features on depth maps produced by the monocular depth\nestimator. By matching such geometric features, we significantly improve the\naccuracy of the coarse correspondences produced by diffusion features.\nExtensive experiments demonstrate that without any task-specific training,\ndirect utilization of both features produces accurate image-to-point cloud\nregistration. On three public indoor and outdoor benchmarks, the proposed\nmethod averagely achieves a 20.6 percent improvement in Inlier Ratio, a\nthree-fold higher Inlier Number, and a 48.6 percent improvement in Registration\nRecall than existing state-of-the-arts.\n","authors":["Haiping Wang","Yuan Liu","Bing Wang","Yujing Sun","Zhen Dong","Wenping Wang","Bisheng Yang"],"pdf_url":"https://arxiv.org/pdf/2310.03420v1.pdf","comment":"Project Page: https://whu-usi3dv.github.io/FreeReg/"},{"id":"http://arxiv.org/abs/2303.06088v4","updated":"2023-10-05T09:55:46Z","published":"2023-03-10T17:09:04Z","title":"Towards domain-invariant Self-Supervised Learning with Batch Styles\n  Standardization","summary":"  In Self-Supervised Learning (SSL), models are typically pretrained,\nfine-tuned, and evaluated on the same domains. However, they tend to perform\npoorly when evaluated on unseen domains, a challenge that Unsupervised Domain\nGeneralization (UDG) seeks to address. Current UDG methods rely on domain\nlabels, which are often challenging to collect, and domain-specific\narchitectures that lack scalability when confronted with numerous domains,\nmaking the current methodology impractical and rigid. Inspired by\ncontrastive-based UDG methods that mitigate spurious correlations by\nrestricting comparisons to examples from the same domain, we hypothesize that\neliminating style variability within a batch could provide a more convenient\nand flexible way to reduce spurious correlations without requiring domain\nlabels. To verify this hypothesis, we introduce Batch Styles Standardization\n(BSS), a relatively simple yet powerful Fourier-based method to standardize the\nstyle of images in a batch specifically designed for integration with SSL\nmethods to tackle UDG. Combining BSS with existing SSL methods offers serious\nadvantages over prior UDG methods: (1) It eliminates the need for domain labels\nor domain-specific network components to enhance domain-invariance in SSL\nrepresentations, and (2) offers flexibility as BSS can be seamlessly integrated\nwith diverse contrastive-based but also non-contrastive-based SSL methods.\nExperiments on several UDG datasets demonstrate that it significantly improves\ndownstream task performances on unseen domains, often outperforming or rivaling\nwith UDG methods. Finally, this work clarifies the underlying mechanisms\ncontributing to BSS's effectiveness in improving domain-invariance in SSL\nrepresentations and performances on unseen domain.\n","authors":["Marin Scalbert","Maria Vakalopoulou","Florent Couzinié-Devy"],"pdf_url":"https://arxiv.org/pdf/2303.06088v4.pdf","comment":"Under review as conference paper"},{"id":"http://arxiv.org/abs/2301.04494v2","updated":"2023-10-05T09:28:57Z","published":"2023-01-11T14:42:47Z","title":"Multi-label Image Classification using Adaptive Graph Convolutional\n  Networks: from a Single Domain to Multiple Domains","summary":"  This paper proposes an adaptive graph-based approach for multi-label image\nclassification. Graph-based methods have been largely exploited in the field of\nmulti-label classification, given their ability to model label correlations.\nSpecifically, their effectiveness has been proven not only when considering a\nsingle domain but also when taking into account multiple domains. However, the\ntopology of the used graph is not optimal as it is pre-defined heuristically.\nIn addition, consecutive Graph Convolutional Network (GCN) aggregations tend to\ndestroy the feature similarity. To overcome these issues, an architecture for\nlearning the graph connectivity in an end-to-end fashion is introduced. This is\ndone by integrating an attention-based mechanism and a similarity-preserving\nstrategy. The proposed framework is then extended to multiple domains using an\nadversarial training scheme. Numerous experiments are reported on well-known\nsingle-domain and multi-domain benchmarks. The results demonstrate that our\napproach achieves competitive results in terms of mean Average Precision (mAP)\nand model size as compared to the state-of-the-art. The code will be made\npublicly available.\n","authors":["Indel Pal Singh","Enjie Ghorbel","Oyebade Oyedotun","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2301.04494v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09234v4","updated":"2023-10-05T09:25:26Z","published":"2023-03-16T11:18:04Z","title":"NAISR: A 3D Neural Additive Model for Interpretable Shape Representation","summary":"  Deep implicit functions (DIFs) have emerged as a powerful paradigm for many\ncomputer vision tasks such as 3D shape reconstruction, generation,\nregistration, completion, editing, and understanding. However, given a set of\n3D shapes with associated covariates there is at present no shape\nrepresentation method which allows to precisely represent the shapes while\ncapturing the individual dependencies on each covariate. Such a method would be\nof high utility to researchers to discover knowledge hidden in a population of\nshapes. For scientific shape discovery, we propose a 3D Neural Additive Model\nfor Interpretable Shape Representation ($\\texttt{NAISR}$) which describes\nindividual shapes by deforming a shape atlas in accordance to the effect of\ndisentangled covariates. Our approach captures shape population trends and\nallows for patient-specific predictions through shape transfer.\n$\\texttt{NAISR}$ is the first approach to combine the benefits of deep implicit\nshape representations with an atlas deforming according to specified\ncovariates. We evaluate $\\texttt{NAISR}$ with respect to shape reconstruction,\nshape disentanglement, shape evolution, and shape transfer on three datasets:\n1) $\\textit{Starman}$, a simulated 2D shape dataset; 2) the ADNI hippocampus 3D\nshape dataset; and 3) a pediatric airway 3D shape dataset. Our experiments\ndemonstrate that $\\textit{Starman}$ achieves excellent shape reconstruction\nperformance while retaining interpretability. Our code is available at\n$\\href{https://github.com/uncbiag/NAISR}{https://github.com/uncbiag/NAISR}$.\n","authors":["Yining Jiao","Carlton Zdanski","Julia Kimbell","Andrew Prince","Cameron Worden","Samuel Kirse","Christopher Rutter","Benjamin Shields","William Dunn","Jisan Mahmud","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2303.09234v4.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2306.14840v2","updated":"2023-10-05T09:22:34Z","published":"2023-06-26T16:48:20Z","title":"Building Flyweight FLIM-based CNNs with Adaptive Decoding for Object\n  Detection","summary":"  State-of-the-art (SOTA) object detection methods have succeeded in several\napplications at the price of relying on heavyweight neural networks, which\nmakes them inefficient and inviable for many applications with computational\nresource constraints. This work presents a method to build a Convolutional\nNeural Network (CNN) layer by layer for object detection from user-drawn\nmarkers on discriminative regions of representative images. We address the\ndetection of Schistosomiasis mansoni eggs in microscopy images of fecal\nsamples, and the detection of ships in satellite images as application\nexamples. We could create a flyweight CNN without backpropagation from very few\ninput images. Our method explores a recent methodology, Feature Learning from\nImage Markers (FLIM), to build convolutional feature extractors (encoders) from\nmarker pixels. We extend FLIM to include a single-layer adaptive decoder, whose\nweights vary with the input image -- a concept never explored in CNNs. Our CNN\nweighs thousands of times less than SOTA object detectors, being suitable for\nCPU execution and showing superior or equivalent performance to three methods\nin five measures.\n","authors":["Leonardo de Melo Joao","Azael de Melo e Sousa","Bianca Martins dos Santos","Silvio Jamil Ferzoli Guimaraes","Jancarlo Ferreira Gomes","Ewa Kijak","Alexandre Xavier Falcao"],"pdf_url":"https://arxiv.org/pdf/2306.14840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.05241v2","updated":"2023-10-05T09:20:18Z","published":"2022-08-10T09:49:19Z","title":"CANet: Channel Extending and Axial Attention Catching Network for\n  Multi-structure Kidney Segmentation","summary":"  Renal cancer is one of the most prevalent cancers worldwide. Clinical signs\nof kidney cancer include hematuria and low back discomfort, which are quite\ndistressing to the patient. Some surgery-based renal cancer treatments like\nlaparoscopic partial nephrectomy relys on the 3D kidney parsing on computed\ntomography angiography (CTA) images. Many automatic segmentation techniques\nhave been put forward to make multi-structure segmentation of the kidneys more\naccurate. The 3D visual model of kidney anatomy will help clinicians plan\noperations accurately before surgery. However, due to the diversity of the\ninternal structure of the kidney and the low grey level of the edge. It is\nstill challenging to separate the different parts of the kidney in a clear and\naccurate way. In this paper, we propose a channel extending and axial attention\ncatching Network(CANet) for multi-structure kidney segmentation. Our solution\nis founded based on the thriving nn-UNet architecture. Firstly, by extending\nthe channel size, we propose a larger network, which can provide a broader\nperspective, facilitating the extraction of complex structural information.\nSecondly, we include an axial attention catching(AAC) module in the decoder,\nwhich can obtain detailed information for refining the edges. We evaluate our\nCANet on the KiPA2022 dataset, achieving the dice scores of 95.8%, 89.1%, 87.5%\nand 84.9% for kidney, tumor, artery and vein, respectively, which helps us get\nfourth place in the challenge.\n","authors":["Zhenyu Bu","Kai-Ni Wang","Guang-Quan Zhou"],"pdf_url":"https://arxiv.org/pdf/2208.05241v2.pdf","comment":"KiPA2022 Challenge"},{"id":"http://arxiv.org/abs/2310.03402v1","updated":"2023-10-05T09:12:34Z","published":"2023-10-05T09:12:34Z","title":"A Complementary Global and Local Knowledge Network for Ultrasound\n  denoising with Fine-grained Refinement","summary":"  Ultrasound imaging serves as an effective and non-invasive diagnostic tool\ncommonly employed in clinical examinations. However, the presence of speckle\nnoise in ultrasound images invariably degrades image quality, impeding the\nperformance of subsequent tasks, such as segmentation and classification.\nExisting methods for speckle noise reduction frequently induce excessive image\nsmoothing or fail to preserve detailed information adequately. In this paper,\nwe propose a complementary global and local knowledge network for ultrasound\ndenoising with fine-grained refinement. Initially, the proposed architecture\nemploys the L-CSwinTransformer as encoder to capture global information,\nincorporating CNN as decoder to fuse local features. We expand the resolution\nof the feature at different stages to extract more global information compared\nto the original CSwinTransformer. Subsequently, we integrate Fine-grained\nRefinement Block (FRB) within the skip-connection stage to further augment\nfeatures. We validate our model on two public datasets, HC18 and BUSI.\nExperimental results demonstrate that our model can achieve competitive\nperformance in both quantitative metrics and visual performance. Our code will\nbe available at https://github.com/AAlkaid/USDenoising.\n","authors":["Zhenyu Bu","Kai-Ni Wang","Fuxing Zhao","Shengxiao Li","Guang-Quan Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.03402v1.pdf","comment":"Submitted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2203.06184v4","updated":"2023-10-05T09:04:55Z","published":"2022-03-11T16:52:14Z","title":"GSDA: Generative Adversarial Network-based Semi-Supervised Data\n  Augmentation for Ultrasound Image Classification","summary":"  Medical Ultrasound (US) is one of the most widely used imaging modalities in\nclinical practice, but its usage presents unique challenges such as variable\nimaging quality. Deep Learning (DL) models can serve as advanced medical US\nimage analysis tools, but their performance is greatly limited by the scarcity\nof large datasets. To solve the common data shortage, we develop GSDA, a\nGenerative Adversarial Network (GAN)-based semi-supervised data augmentation\nmethod. GSDA consists of the GAN and Convolutional Neural Network (CNN). The\nGAN synthesizes and pseudo-labels high-resolution, high-quality US images, and\nboth real and synthesized images are then leveraged to train the CNN. To\naddress the training challenges of both GAN and CNN with limited data, we\nemploy transfer learning techniques during their training. We also introduce a\nnovel evaluation standard that balances classification accuracy with\ncomputational time. We evaluate our method on the BUSI dataset and GSDA\noutperforms existing state-of-the-art methods. With the high-resolution and\nhigh-quality images synthesized, GSDA achieves a 97.9% accuracy using merely\n780 images. Given these promising results, we believe that GSDA holds potential\nas an auxiliary tool for medical US analysis.\n","authors":["Zhaoshan Liu","Qiujie Lv","Chau Hung Lee","Lei Shen"],"pdf_url":"https://arxiv.org/pdf/2203.06184v4.pdf","comment":"Heliyon Accepted"},{"id":"http://arxiv.org/abs/2310.03396v1","updated":"2023-10-05T09:03:51Z","published":"2023-10-05T09:03:51Z","title":"Learning to Simplify Spatial-Temporal Graphs in Gait Analysis","summary":"  Gait analysis leverages unique walking patterns for person identification and\nassessment across multiple domains. Among the methods used for gait analysis,\nskeleton-based approaches have shown promise due to their robust and\ninterpretable features. However, these methods often rely on hand-crafted\nspatial-temporal graphs that are based on human anatomy disregarding the\nparticularities of the dataset and task. This paper proposes a novel method to\nsimplify the spatial-temporal graph representation for gait-based gender\nestimation, improving interpretability without losing performance. Our approach\nemploys two models, an upstream and a downstream model, that can adjust the\nadjacency matrix for each walking instance, thereby removing the fixed nature\nof the graph. By employing the Straight-Through Gumbel-Softmax trick, our model\nis trainable end-to-end. We demonstrate the effectiveness of our approach on\nthe CASIA-B dataset for gait-based gender estimation. The resulting graphs are\ninterpretable and differ qualitatively from fixed graphs used in existing\nmodels. Our research contributes to enhancing the explainability and\ntask-specific adaptability of gait recognition, promoting more efficient and\nreliable gait-based biometrics.\n","authors":["Adrian Cosma","Emilian Radoi"],"pdf_url":"https://arxiv.org/pdf/2310.03396v1.pdf","comment":"5 Figures, 1 Table. Short Paper"},{"id":"http://arxiv.org/abs/2211.03660v2","updated":"2023-10-05T08:53:01Z","published":"2022-11-07T16:17:47Z","title":"SC-DepthV3: Robust Self-supervised Monocular Depth Estimation for\n  Dynamic Scenes","summary":"  Self-supervised monocular depth estimation has shown impressive results in\nstatic scenes. It relies on the multi-view consistency assumption for training\nnetworks, however, that is violated in dynamic object regions and occlusions.\nConsequently, existing methods show poor accuracy in dynamic scenes, and the\nestimated depth map is blurred at object boundaries because they are usually\noccluded in other training views. In this paper, we propose SC-DepthV3 for\naddressing the challenges. Specifically, we introduce an external pretrained\nmonocular depth estimation model for generating single-image depth prior,\nnamely pseudo-depth, based on which we propose novel losses to boost\nself-supervised training. As a result, our model can predict sharp and accurate\ndepth maps, even when training from monocular videos of highly-dynamic scenes.\nWe demonstrate the significantly superior performance of our method over\nprevious methods on six challenging datasets, and we provide detailed ablation\nstudies for the proposed terms. Source code and data will be released at\nhttps://github.com/JiawangBian/sc_depth_pl\n","authors":["Libo Sun","Jia-Wang Bian","Huangying Zhan","Wei Yin","Ian Reid","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2211.03660v2.pdf","comment":"Accepted for publication in TPAMI; The code will be available at\n  https://github.com/JiawangBian/sc_depth_pl"},{"id":"http://arxiv.org/abs/2310.03388v1","updated":"2023-10-05T08:49:51Z","published":"2023-10-05T08:49:51Z","title":"OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon","summary":"  Moving deep learning models from the laboratory setting to the open world\nentails preparing them to handle unforeseen conditions. In several applications\nthe occurrence of novel classes during deployment poses a significant threat,\nthus it is crucial to effectively detect them. Ideally, this skill should be\nused when needed without requiring any further computational training effort at\nevery new task. Out-of-distribution detection has attracted significant\nattention in the last years, however the majority of the studies deal with 2D\nimages ignoring the inherent 3D nature of the real-world and often confusing\nbetween domain and semantic novelty. In this work, we focus on the latter,\nconsidering the objects geometric structure captured by 3D point clouds\nregardless of the specific domain. We advance the field by introducing\nOpenPatch that builds on a large pre-trained model and simply extracts from its\nintermediate features a set of patch representations that describe each known\nclass. For any new sample, we obtain a novelty score by evaluating whether it\ncan be recomposed mainly by patches of a single known class or rather via the\ncontribution of multiple classes. We present an extensive experimental\nevaluation of our approach for the task of semantic novelty detection on\nreal-world point cloud samples when the reference known data are synthetic. We\ndemonstrate that OpenPatch excels in both the full and few-shot known sample\nscenarios, showcasing its robustness across varying pre-training objectives and\nnetwork backbones. The inherent training-free nature of our method allows for\nits immediate application to a wide array of real-world tasks, offering a\ncompelling advantage over approaches that need expensive retraining efforts.\n","authors":["Paolo Rabino","Antonio Alliegro","Francesco Cappio Borlino","Tatiana Tommasi"],"pdf_url":"https://arxiv.org/pdf/2310.03388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03377v1","updated":"2023-10-05T08:28:26Z","published":"2023-10-05T08:28:26Z","title":"ACT-Net: Anchor-context Action Detection in Surgery Videos","summary":"  Recognition and localization of surgical detailed actions is an essential\ncomponent of developing a context-aware decision support system. However, most\nexisting detection algorithms fail to provide high-accuracy action classes even\nhaving their locations, as they do not consider the surgery procedure's\nregularity in the whole video. This limitation hinders their application.\nMoreover, implementing the predictions in clinical applications seriously needs\nto convey model confidence to earn entrustment, which is unexplored in surgical\naction prediction. In this paper, to accurately detect fine-grained actions\nthat happen at every moment, we propose an anchor-context action detection\nnetwork (ACTNet), including an anchor-context detection (ACD) module and a\nclass conditional diffusion (CCD) module, to answer the following questions: 1)\nwhere the actions happen; 2) what actions are; 3) how confidence predictions\nare. Specifically, the proposed ACD module spatially and temporally highlights\nthe regions interacting with the extracted anchor in surgery video, which\noutputs action location and its class distribution based on anchor-context\ninteractions. Considering the full distribution of action classes in videos,\nthe CCD module adopts a denoising diffusion-based generative model conditioned\non our ACD estimator to further reconstruct accurately the action predictions.\nMoreover, we utilize the stochastic nature of the diffusion model outputs to\naccess model confidence for each prediction. Our method reports the\nstate-of-the-art performance, with improvements of 4.0% mAP against baseline on\nthe surgical video dataset.\n","authors":["Luoying Hao","Yan Hu","Wenjun Lin","Qun Wang","Heng Li","Huazhu Fu","Jinming Duan","Jiang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.03377v1.pdf","comment":"Accepted early by MICCAI2023 (Oral)"},{"id":"http://arxiv.org/abs/2310.03375v1","updated":"2023-10-05T08:27:33Z","published":"2023-10-05T08:27:33Z","title":"Point-Based Radiance Fields for Controllable Human Motion Synthesis","summary":"  This paper proposes a novel controllable human motion synthesis method for\nfine-level deformation based on static point-based radiance fields. Although\nprevious editable neural radiance field methods can generate impressive results\non novel-view synthesis and allow naive deformation, few algorithms can achieve\ncomplex 3D human editing such as forward kinematics. Our method exploits the\nexplicit point cloud to train the static 3D scene and apply the deformation by\nencoding the point cloud translation using a deformation MLP. To make sure the\nrendering result is consistent with the canonical space training, we estimate\nthe local rotation using SVD and interpolate the per-point rotation to the\nquery view direction of the pre-trained radiance field. Extensive experiments\nshow that our approach can significantly outperform the state-of-the-art on\nfine-level complex deformation which can be generalized to other 3D characters\nbesides humans.\n","authors":["Haitao Yu","Deheng Zhang","Peiyuan Xie","Tianyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.03375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.07091v2","updated":"2023-10-05T07:59:22Z","published":"2022-11-14T03:36:38Z","title":"BiViT: Extremely Compressed Binary Vision Transformer","summary":"  Model binarization can significantly compress model size, reduce energy\nconsumption, and accelerate inference through efficient bit-wise operations.\nAlthough binarizing convolutional neural networks have been extensively\nstudied, there is little work on exploring binarization of vision Transformers\nwhich underpin most recent breakthroughs in visual recognition. To this end, we\npropose to solve two fundamental challenges to push the horizon of Binary\nVision Transformers (BiViT). First, the traditional binary method does not take\nthe long-tailed distribution of softmax attention into consideration, bringing\nlarge binarization errors in the attention module. To solve this, we propose\nSoftmax-aware Binarization, which dynamically adapts to the data distribution\nand reduces the error caused by binarization. Second, to better preserve the\ninformation of the pretrained model and restore accuracy, we propose a\nCross-layer Binarization scheme that decouples the binarization of\nself-attention and multi-layer perceptrons (MLPs), and Parameterized Weight\nScales which introduce learnable scaling factors for weight binarization.\nOverall, our method performs favorably against state-of-the-arts by 19.8% on\nthe TinyImageNet dataset. On ImageNet, our BiViT achieves a competitive 75.6%\nTop-1 accuracy over Swin-S model. Additionally, on COCO object detection, our\nmethod achieves an mAP of 40.8 with a Swin-T backbone over Cascade Mask R-CNN\nframework.\n","authors":["Yefei He","Zhenyu Lou","Luoming Zhang","Jing Liu","Weijia Wu","Hong Zhou","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2211.07091v2.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2310.03365v1","updated":"2023-10-05T07:48:55Z","published":"2023-10-05T07:48:55Z","title":"Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video\n  Sequences Using Swin Transformer-Enhanced UNet","summary":"  Lung cancer is highly lethal, emphasizing the critical need for early\ndetection. However, identifying lung nodules poses significant challenges for\nradiologists, who rely heavily on their expertise and experience for accurate\ndiagnosis. To address this issue, computer-aided diagnosis systems based on\nmachine learning techniques have emerged to assist doctors in identifying lung\nnodules from computed tomography (CT) scans. Unfortunately, existing networks\nin this domain often suffer from computational complexity, leading to high\nrates of false negatives and false positives, limiting their effectiveness. To\naddress these challenges, we present an innovative model that harnesses the\nstrengths of both convolutional neural networks and vision transformers.\nInspired by object detection in videos, we treat each 3D CT image as a video,\nindividual slices as frames, and lung nodules as objects, enabling a\ntime-series application. The primary objective of our work is to overcome\nhardware limitations during model training, allowing for efficient processing\nof 2D data while utilizing inter-slice information for accurate identification\nbased on 3D image context. We validated the proposed network by applying a\n10-fold cross-validation technique to the publicly available Lung Nodule\nAnalysis 2016 dataset. Our proposed architecture achieves an average\nsensitivity criterion of 97.84% and a competition performance metrics (CPM) of\n96.0% with few parameters. Comparative analysis with state-of-the-art\nadvancements in lung nodule identification demonstrates the significant\naccuracy achieved by our proposed model.\n","authors":["Hossein Jafari","Karim Faez","Hamidreza Amindavar"],"pdf_url":"https://arxiv.org/pdf/2310.03365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03363v1","updated":"2023-10-05T07:44:49Z","published":"2023-10-05T07:44:49Z","title":"Realistic Speech-to-Face Generation with Speech-Conditioned Latent\n  Diffusion Model with Face Prior","summary":"  Speech-to-face generation is an intriguing area of research that focuses on\ngenerating realistic facial images based on a speaker's audio speech. However,\nstate-of-the-art methods employing GAN-based architectures lack stability and\ncannot generate realistic face images. To fill this gap, we propose a novel\nspeech-to-face generation framework, which leverages a Speech-Conditioned\nLatent Diffusion Model, called SCLDM. To the best of our knowledge, this is the\nfirst work to harness the exceptional modeling capabilities of diffusion models\nfor speech-to-face generation. Preserving the shared identity information\nbetween speech and face is crucial in generating realistic results. Therefore,\nwe employ contrastive pre-training for both the speech encoder and the face\nencoder. This pre-training strategy facilitates effective alignment between the\nattributes of speech, such as age and gender, and the corresponding facial\ncharacteristics in the face images. Furthermore, we tackle the challenge posed\nby excessive diversity in the synthesis process caused by the diffusion model.\nTo overcome this challenge, we introduce the concept of residuals by\nintegrating a statistical face prior to the diffusion process. This addition\nhelps to eliminate the shared component across the faces and enhances the\nsubtle variations captured by the speech condition. Extensive quantitative,\nqualitative, and user study experiments demonstrate that our method can produce\nmore realistic face images while preserving the identity of the speaker better\nthan state-of-the-art methods. Highlighting the notable enhancements, our\nmethod demonstrates significant gains in all metrics on the AVSpeech dataset\nand Voxceleb dataset, particularly noteworthy are the improvements of 32.17 and\n32.72 on the cosine distance metric for the two datasets, respectively.\n","authors":["Jinting Wang","Li Liu","Jun Wang","Hei Victor Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.03363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10784v3","updated":"2023-10-05T07:43:33Z","published":"2023-07-20T11:33:46Z","title":"SMURF: Spatial Multi-Representation Fusion for 3D Object Detection with\n  4D Imaging Radar","summary":"  The 4D Millimeter wave (mmWave) radar is a promising technology for vehicle\nsensing due to its cost-effectiveness and operability in adverse weather\nconditions. However, the adoption of this technology has been hindered by\nsparsity and noise issues in radar point cloud data. This paper introduces\nspatial multi-representation fusion (SMURF), a novel approach to 3D object\ndetection using a single 4D imaging radar. SMURF leverages multiple\nrepresentations of radar detection points, including pillarization and density\nfeatures of a multi-dimensional Gaussian mixture distribution through kernel\ndensity estimation (KDE). KDE effectively mitigates measurement inaccuracy\ncaused by limited angular resolution and multi-path propagation of radar\nsignals. Additionally, KDE helps alleviate point cloud sparsity by capturing\ndensity features. Experimental evaluations on View-of-Delft (VoD) and\nTJ4DRadSet datasets demonstrate the effectiveness and generalization ability of\nSMURF, outperforming recently proposed 4D imaging radar-based\nsingle-representation models. Moreover, while using 4D imaging radar only,\nSMURF still achieves comparable performance to the state-of-the-art 4D imaging\nradar and camera fusion-based method, with an increase of 1.22% in the mean\naverage precision on bird's-eye view of TJ4DRadSet dataset and 1.32% in the 3D\nmean average precision on the entire annotated area of VoD dataset. Our\nproposed method demonstrates impressive inference time and addresses the\nchallenges of real-time detection, with the inference time no more than 0.05\nseconds for most scans on both datasets. This research highlights the benefits\nof 4D mmWave radar and is a strong benchmark for subsequent works regarding 3D\nobject detection with 4D imaging radar.\n","authors":["Jianan Liu","Qiuchi Zhao","Weiyi Xiong","Tao Huang","Qing-Long Han","Bing Zhu"],"pdf_url":"https://arxiv.org/pdf/2307.10784v3.pdf","comment":"Accepted by IEEE Transactions on Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2310.03360v1","updated":"2023-10-05T07:30:52Z","published":"2023-10-05T07:30:52Z","title":"CSI: Enhancing the Robustness of 3D Point Cloud Recognition against\n  Corruption","summary":"  Despite recent advancements in deep neural networks for point cloud\nrecognition, real-world safety-critical applications present challenges due to\nunavoidable data corruption. Current models often fall short in generalizing to\nunforeseen distribution shifts. In this study, we harness the inherent set\nproperty of point cloud data to introduce a novel critical subset\nidentification (CSI) method, aiming to bolster recognition robustness in the\nface of data corruption. Our CSI framework integrates two pivotal components:\ndensity-aware sampling (DAS) and self-entropy minimization (SEM), which cater\nto static and dynamic CSI, respectively. DAS ensures efficient robust anchor\npoint sampling by factoring in local density, while SEM is employed during\ntraining to accentuate the most salient point-to-point attention. Evaluations\nreveal that our CSI approach yields error rates of 18.4\\% and 16.3\\% on\nModelNet40-C and PointCloud-C, respectively, marking a notable improvement over\nstate-of-the-art methods by margins of 5.2\\% and 4.2\\% on the respective\nbenchmarks. Code is available at\n\\href{https://github.com/masterwu2115/CSI/tree/main}{https://github.com/masterwu2115/CSI/tree/main}\n","authors":["Zhuoyuan Wu","Jiachen Sun","Chaowei Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.03360v1.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.03358v1","updated":"2023-10-05T07:29:29Z","published":"2023-10-05T07:29:29Z","title":"Robust Representation Learning via Asymmetric Negative Contrast and\n  Reverse Attention","summary":"  Deep neural networks are vulnerable to adversarial noise. Adversarial\ntraining (AT) has been demonstrated to be the most effective defense strategy\nto protect neural networks from being fooled. However, we find AT omits to\nlearning robust features, resulting in poor performance of adversarial\nrobustness. To address this issue, we highlight two characteristics of robust\nrepresentation: (1) $\\bf{exclusion}$: the feature of natural examples keeps\naway from that of other classes; (2) $\\bf{alignment}$: the feature of natural\nand corresponding adversarial examples is close to each other. These motivate\nus to propose a generic framework of AT to gain robust representation, by the\nasymmetric negative contrast and reverse attention. Specifically, we design an\nasymmetric negative contrast based on predicted probabilities, to push away\nexamples of different classes in the feature space. Moreover, we propose to\nweight feature by parameters of the linear classifier as the reverse attention,\nto obtain class-aware feature and pull close the feature of the same class.\nEmpirical evaluations on three benchmark datasets show our methods greatly\nadvance the robustness of AT and achieve state-of-the-art performance. Code is\navailable at <https://github.com/changzhang777/ANCRA>.\n","authors":["Nuoyan Zhou","Decheng Liu","Dawei Zhou","Xinbo Gao","Nannan Wang"],"pdf_url":"https://arxiv.org/pdf/2310.03358v1.pdf","comment":"Submitted to ICLR2024"},{"id":"http://arxiv.org/abs/2310.02601v2","updated":"2023-10-05T07:07:38Z","published":"2023-10-04T06:14:06Z","title":"MagicDrive: Street View Generation with Diverse 3D Geometry Control","summary":"  Recent advancements in diffusion models have significantly enhanced the data\nsynthesis with 2D control. Yet, precise 3D control in street view generation,\ncrucial for 3D perception tasks, remains elusive. Specifically, utilizing\nBird's-Eye View (BEV) as the primary condition often leads to challenges in\ngeometry control (e.g., height), affecting the representation of object shapes,\nocclusion patterns, and road surface elevations, all of which are essential to\nperception data synthesis, especially for 3D object detection tasks. In this\npaper, we introduce MagicDrive, a novel street view generation framework\noffering diverse 3D geometry controls, including camera poses, road maps, and\n3D bounding boxes, together with textual descriptions, achieved through\ntailored encoding strategies. Besides, our design incorporates a cross-view\nattention module, ensuring consistency across multiple camera views. With\nMagicDrive, we achieve high-fidelity street-view synthesis that captures\nnuanced 3D geometry and various scene descriptions, enhancing tasks like BEV\nsegmentation and 3D object detection.\n","authors":["Ruiyuan Gao","Kai Chen","Enze Xie","Lanqing Hong","Zhenguo Li","Dit-Yan Yeung","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.02601v2.pdf","comment":"Project Page: https://flymin.github.io/magicdrive"},{"id":"http://arxiv.org/abs/2306.03364v3","updated":"2023-10-05T07:06:27Z","published":"2023-06-06T02:38:01Z","title":"Learning Representations on the Unit Sphere: Investigating Angular\n  Gaussian and von Mises-Fisher Distributions for Online Continual Learning","summary":"  We use the maximum a posteriori estimation principle for learning\nrepresentations distributed on the unit sphere. We propose to use the angular\nGaussian distribution, which corresponds to a Gaussian projected on the\nunit-sphere and derive the associated loss function. We also consider the von\nMises-Fisher distribution, which is the conditional of a Gaussian in the\nunit-sphere. The learned representations are pushed toward fixed directions,\nwhich are the prior means of the Gaussians; allowing for a learning strategy\nthat is resilient to data drift. This makes it suitable for online continual\nlearning, which is the problem of training neural networks on a continuous data\nstream, where multiple classification tasks are presented sequentially so that\ndata from past tasks are no longer accessible, and data from the current task\ncan be seen only once. To address this challenging scenario, we propose a\nmemory-based representation learning technique equipped with our new loss\nfunctions. Our approach does not require negative data or knowledge of task\nboundaries and performs well with smaller batch sizes while being\ncomputationally efficient. We demonstrate with extensive experiments that the\nproposed method outperforms the current state-of-the-art methods on both\nstandard evaluation scenarios and realistic scenarios with blurry task\nboundaries. For reproducibility, we use the same training pipeline for every\ncompared method and share the code at https://t.ly/SQTj.\n","authors":["Nicolas Michel","Giovanni Chierchia","Romain Negrel","Jean-François Bercher"],"pdf_url":"https://arxiv.org/pdf/2306.03364v3.pdf","comment":"17 pages, under review, update title"},{"id":"http://arxiv.org/abs/2310.03346v1","updated":"2023-10-05T06:56:54Z","published":"2023-10-05T06:56:54Z","title":"Combining Datasets with Different Label Sets for Improved Nucleus\n  Segmentation and Classification","summary":"  Segmentation and classification of cell nuclei in histopathology images using\ndeep neural networks (DNNs) can save pathologists' time for diagnosing various\ndiseases, including cancers, by automating cell counting and morphometric\nassessments. It is now well-known that the accuracy of DNNs increases with the\nsizes of annotated datasets available for training. Although multiple datasets\nof histopathology images with nuclear annotations and class labels have been\nmade publicly available, the set of class labels differ across these datasets.\nWe propose a method to train DNNs for instance segmentation and classification\non multiple datasets where the set of classes across the datasets are related\nbut not the same. Specifically, our method is designed to utilize a\ncoarse-to-fine class hierarchy, where the set of classes labeled and annotated\nin a dataset can be at any level of the hierarchy, as long as the classes are\nmutually exclusive. Within a dataset, the set of classes need not even be at\nthe same level of the class hierarchy tree. Our results demonstrate that\nsegmentation and classification metrics for the class set used by the test\nsplit of a dataset can improve by pre-training on another dataset that may even\nhave a different set of classes due to the expansion of the training set\nenabled by our method. Furthermore, generalization to previously unseen\ndatasets also improves by combining multiple other datasets with different sets\nof classes for training. The improvement is both qualitative and quantitative.\nThe proposed method can be adapted for various loss functions, DNN\narchitectures, and application domains.\n","authors":["Amruta Parulekar","Utkarsh Kanwat","Ravi Kant Gupta","Medha Chippa","Thomas Jacob","Tripti Bameta","Swapnil Rane","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2310.03346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14655v2","updated":"2023-10-05T06:55:13Z","published":"2023-03-26T08:43:36Z","title":"GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for\n  Real-time Soccer Commentary Generation","summary":"  Despite the recent emergence of video captioning models, how to generate\nvivid, fine-grained video descriptions based on the background knowledge (i.e.,\nlong and informative commentary about the domain-specific scenes with\nappropriate reasoning) is still far from being solved, which however has great\napplications such as automatic sports narrative. In this paper, we present\nGOAL, a benchmark of over 8.9k soccer video clips, 22k sentences, and 42k\nknowledge triples for proposing a challenging new task setting as\nKnowledge-grounded Video Captioning (KGVC). Moreover, we conduct experimental\nadaption of existing methods to show the difficulty and potential directions\nfor solving this valuable and applicable task. Our data and code are available\nat https://github.com/THU-KEG/goal.\n","authors":["Ji Qi","Jifan Yu","Teng Tu","Kunyu Gao","Yifan Xu","Xinyu Guan","Xiaozhi Wang","Yuxiao Dong","Bin Xu","Lei Hou","Juanzi Li","Jie Tang","Weidong Guo","Hui Liu","Yu Xu"],"pdf_url":"https://arxiv.org/pdf/2303.14655v2.pdf","comment":"Accepted by CIKM 2023"},{"id":"http://arxiv.org/abs/2310.03337v1","updated":"2023-10-05T06:44:13Z","published":"2023-10-05T06:44:13Z","title":"Denoising Diffusion Step-aware Models","summary":"  Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for\ndata generation across various domains. However, a significant bottleneck is\nthe necessity for whole-network computation during every step of the generative\nprocess, leading to high computational overheads. This paper presents a novel\nframework, Denoising Diffusion Step-aware Models (DDSM), to address this\nchallenge. Unlike conventional approaches, DDSM employs a spectrum of neural\nnetworks whose sizes are adapted according to the importance of each generative\nstep, as determined through evolutionary search. This step-wise network\nvariation effectively circumvents redundant computational efforts, particularly\nin less critical steps, thereby enhancing the efficiency of the diffusion\nmodel. Furthermore, the step-aware design can be seamlessly integrated with\nother efficiency-geared diffusion models such as DDIMs and latent diffusion,\nthus broadening the scope of computational savings. Empirical evaluations\ndemonstrate that DDSM achieves computational savings of 49% for CIFAR-10, 61%\nfor CelebA-HQ, 59% for LSUN-bedroom, 71% for AFHQ, and 76% for ImageNet, all\nwithout compromising the generation quality. Our code and models will be\npublicly available.\n","authors":["Shuai Yang","Yukang Chen","Luozhou Wang","Shu Liu","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2310.03337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00944v2","updated":"2023-10-05T06:37:23Z","published":"2023-10-02T07:34:15Z","title":"Towards Robust 3D Object Detection In Rainy Conditions","summary":"  LiDAR sensors are used in autonomous driving applications to accurately\nperceive the environment. However, they are affected by adverse weather\nconditions such as snow, fog, and rain. These everyday phenomena introduce\nunwanted noise into the measurements, severely degrading the performance of\nLiDAR-based perception systems. In this work, we propose a framework for\nimproving the robustness of LiDAR-based 3D object detectors against road spray.\nOur approach uses a state-of-the-art adverse weather detection network to\nfilter out spray from the LiDAR point cloud, which is then used as input for\nthe object detector. In this way, the detected objects are less affected by the\nadverse weather in the scene, resulting in a more accurate perception of the\nenvironment. In addition to adverse weather filtering, we explore the use of\nradar targets to further filter false positive detections. Tests on real-world\ndata show that our approach improves the robustness to road spray of several\npopular 3D object detectors.\n","authors":["Aldi Piroli","Vinzenz Dallabetta","Johannes Kopp","Marc Walessa","Daniel Meissner","Klaus Dietmayer"],"pdf_url":"https://arxiv.org/pdf/2310.00944v2.pdf","comment":"Published at IEEE International Conference on Intelligent\n  Transportation Systems ITSC 2023"},{"id":"http://arxiv.org/abs/2310.03335v1","updated":"2023-10-05T06:35:21Z","published":"2023-10-05T06:35:21Z","title":"Continual Test-time Domain Adaptation via Dynamic Sample Selection","summary":"  The objective of Continual Test-time Domain Adaptation (CTDA) is to gradually\nadapt a pre-trained model to a sequence of target domains without accessing the\nsource data. This paper proposes a Dynamic Sample Selection (DSS) method for\nCTDA. DSS consists of dynamic thresholding, positive learning, and negative\nlearning processes. Traditionally, models learn from unlabeled unknown\nenvironment data and equally rely on all samples' pseudo-labels to update their\nparameters through self-training. However, noisy predictions exist in these\npseudo-labels, so all samples are not equally trustworthy. Therefore, in our\nmethod, a dynamic thresholding module is first designed to select suspected\nlow-quality from high-quality samples. The selected low-quality samples are\nmore likely to be wrongly predicted. Therefore, we apply joint positive and\nnegative learning on both high- and low-quality samples to reduce the risk of\nusing wrong information. We conduct extensive experiments that demonstrate the\neffectiveness of our proposed method for CTDA in the image domain,\noutperforming the state-of-the-art results. Furthermore, our approach is also\nevaluated in the 3D point cloud domain, showcasing its versatility and\npotential for broader applicability.\n","authors":["Yanshuo Wang","Jie Hong","Ali Cheraghian","Shafin Rahman","David Ahmedt-Aristizabal","Lars Petersson","Mehrtash Harandi"],"pdf_url":"https://arxiv.org/pdf/2310.03335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03333v1","updated":"2023-10-05T06:31:38Z","published":"2023-10-05T06:31:38Z","title":"Real-time Multi-modal Object Detection and Tracking on Edge for\n  Regulatory Compliance Monitoring","summary":"  Regulatory compliance auditing across diverse industrial domains requires\nheightened quality assurance and traceability. Present manual and intermittent\napproaches to such auditing yield significant challenges, potentially leading\nto oversights in the monitoring process. To address these issues, we introduce\na real-time, multi-modal sensing system employing 3D time-of-flight and RGB\ncameras, coupled with unsupervised learning techniques on edge AI devices. This\nenables continuous object tracking thereby enhancing efficiency in\nrecord-keeping and minimizing manual interventions. While we validate the\nsystem in a knife sanitization context within agrifood facilities, emphasizing\nits prowess against occlusion and low-light issues with RGB cameras, its\npotential spans various industrial monitoring settings.\n","authors":["Jia Syuen Lim","Ziwei Wang","Jiajun Liu","Abdelwahed Khamis","Reza Arablouei","Robert Barlow","Ryan McAllister"],"pdf_url":"https://arxiv.org/pdf/2310.03333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03006v2","updated":"2023-10-05T05:54:34Z","published":"2023-10-04T17:49:48Z","title":"COOLer: Class-Incremental Learning for Appearance-Based Multiple Object\n  Tracking","summary":"  Continual learning allows a model to learn multiple tasks sequentially while\nretaining the old knowledge without the training data of the preceding tasks.\nThis paper extends the scope of continual learning research to\nclass-incremental learning for multiple object tracking (MOT), which is\ndesirable to accommodate the continuously evolving needs of autonomous systems.\nPrevious solutions for continual learning of object detectors do not address\nthe data association stage of appearance-based trackers, leading to\ncatastrophic forgetting of previous classes' re-identification features. We\nintroduce COOLer, a COntrastive- and cOntinual-Learning-based tracker, which\nincrementally learns to track new categories while preserving past knowledge by\ntraining on a combination of currently available ground truth labels and\npseudo-labels generated by the past tracker. To further exacerbate the\ndisentanglement of instance representations, we introduce a novel contrastive\nclass-incremental instance representation learning technique. Finally, we\npropose a practical evaluation protocol for continual learning for MOT and\nconduct experiments on the BDD100K and SHIFT datasets. Experimental results\ndemonstrate that COOLer continually learns while effectively addressing\ncatastrophic forgetting of both tracking and detection. The code is available\nat https://github.com/BoSmallEar/COOLer.\n","authors":["Zhizheng Liu","Mattia Segu","Fisher Yu"],"pdf_url":"https://arxiv.org/pdf/2310.03006v2.pdf","comment":"GCPR 2023 Oral"},{"id":"http://arxiv.org/abs/2310.03325v1","updated":"2023-10-05T05:41:21Z","published":"2023-10-05T05:41:21Z","title":"Learning Concept-Based Visual Causal Transition and Symbolic Reasoning\n  for Visual Planning","summary":"  Visual planning simulates how humans make decisions to achieve desired goals\nin the form of searching for visual causal transitions between an initial\nvisual state and a final visual goal state. It has become increasingly\nimportant in egocentric vision with its advantages in guiding agents to perform\ndaily tasks in complex environments. In this paper, we propose an interpretable\nand generalizable visual planning framework consisting of i) a novel\nSubstitution-based Concept Learner (SCL) that abstracts visual inputs into\ndisentangled concept representations, ii) symbol abstraction and reasoning that\nperforms task planning via the self-learned symbols, and iii) a Visual Causal\nTransition model (ViCT) that grounds visual causal transitions to semantically\nsimilar real-world actions. Given an initial state, we perform goal-conditioned\nvisual planning with a symbolic reasoning method fueled by the learned\nrepresentations and causal transitions to reach the goal state. To verify the\neffectiveness of the proposed model, we collect a large-scale visual planning\ndataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this\nchallenging dataset demonstrate the superior performance of our method in\nvisual task planning. Empirically, we show that our framework can generalize to\nunseen task trajectories and unseen object categories.\n","authors":["Yilue Qian","Peiyu Yu","Ying Nian Wu","Wei Wang","Lifeng Fan"],"pdf_url":"https://arxiv.org/pdf/2310.03325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03324v1","updated":"2023-10-05T05:37:33Z","published":"2023-10-05T05:37:33Z","title":"Investigating the Limitation of CLIP Models: The Worst-Performing\n  Categories","summary":"  Contrastive Language-Image Pre-training (CLIP) provides a foundation model by\nintegrating natural language into visual concepts, enabling zero-shot\nrecognition on downstream tasks. It is usually expected that satisfactory\noverall accuracy can be achieved across numerous domains through well-designed\ntextual prompts. However, we found that their performance in the worst\ncategories is significantly inferior to the overall performance. For example,\non ImageNet, there are a total of 10 categories with class-wise accuracy as low\nas 0\\%, even though the overall performance has achieved 64.1\\%. This\nphenomenon reveals the potential risks associated with using CLIP models,\nparticularly in risk-sensitive applications where specific categories hold\nsignificant importance. To address this issue, we investigate the alignment\nbetween the two modalities in the CLIP model and propose the Class-wise\nMatching Margin (\\cmm) to measure the inference confusion. \\cmm\\ can\neffectively identify the worst-performing categories and estimate the potential\nperformance of the candidate prompts. We further query large language models to\nenrich descriptions of worst-performing categories and build a weighted\nensemble to highlight the efficient prompts. Experimental results clearly\nverify the effectiveness of our proposal, where the accuracy on the worst-10\ncategories on ImageNet is boosted to 5.2\\%, without manual prompt engineering,\nlaborious optimization, or access to labeled validation data.\n","authors":["Jie-Jing Shao","Jiang-Xin Shi","Xiao-Wen Yang","Lan-Zhe Guo","Yu-Feng Li"],"pdf_url":"https://arxiv.org/pdf/2310.03324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15357v3","updated":"2023-10-05T05:23:07Z","published":"2023-05-24T17:09:54Z","title":"Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image\n  Super-Resolution","summary":"  Diffusion models, as a kind of powerful generative model, have given\nimpressive results on image super-resolution (SR) tasks. However, due to the\nrandomness introduced in the reverse process of diffusion models, the\nperformances of diffusion-based SR models are fluctuating at every time of\nsampling, especially for samplers with few resampled steps. This inherent\nrandomness of diffusion models results in ineffectiveness and instability,\nmaking it challenging for users to guarantee the quality of SR results.\nHowever, our work takes this randomness as an opportunity: fully analyzing and\nleveraging it leads to the construction of an effective plug-and-play sampling\nmethod that owns the potential to benefit a series of diffusion-based SR\nmethods. More in detail, we propose to steadily sample high-quality SR images\nfrom pre-trained diffusion-based SR models by solving diffusion ordinary\ndifferential equations (diffusion ODEs) with optimal boundary conditions (BCs)\nand analyze the characteristics between the choices of BCs and their\ncorresponding SR results. Our analysis shows the route to obtain an\napproximately optimal BC via an efficient exploration in the whole space. The\nquality of SR results sampled by the proposed method with fewer steps\noutperforms the quality of results sampled by current methods with randomness\nfrom the same pre-trained diffusion-based SR model, which means that our\nsampling method \"boosts\" current diffusion-based SR models without any\nadditional training.\n","authors":["Yiyang Ma","Huan Yang","Wenhan Yang","Jianlong Fu","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2305.15357v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03314v1","updated":"2023-10-05T05:12:14Z","published":"2023-10-05T05:12:14Z","title":"Enhanced Human-Robot Collaboration using Constrained Probabilistic\n  Human-Motion Prediction","summary":"  Human motion prediction is an essential step for efficient and safe\nhuman-robot collaboration. Current methods either purely rely on representing\nthe human joints in some form of neural network-based architecture or use\nregression models offline to fit hyper-parameters in the hope of capturing a\nmodel encompassing human motion. While these methods provide good initial\nresults, they are missing out on leveraging well-studied human body kinematic\nmodels as well as body and scene constraints which can help boost the efficacy\nof these prediction frameworks while also explicitly avoiding implausible human\njoint configurations. We propose a novel human motion prediction framework that\nincorporates human joint constraints and scene constraints in a Gaussian\nProcess Regression (GPR) model to predict human motion over a set time horizon.\nThis formulation is combined with an online context-aware constraints model to\nleverage task-dependent motions. It is tested on a human arm kinematic model\nand implemented on a human-robot collaborative setup with a UR5 robot arm to\ndemonstrate the real-time capability of our approach. Simulations were also\nperformed on datasets like HA4M and ANDY. The simulation and experimental\nresults demonstrate considerable improvements in a Gaussian Process framework\nwhen these constraints are explicitly considered.\n","authors":["Aadi Kothari","Tony Tohme","Xiaotong Zhang","Kamal Youcef-Toumi"],"pdf_url":"https://arxiv.org/pdf/2310.03314v1.pdf","comment":"7 pages, 5 figures. Associated video demonstration can be found at\n  https://www.youtube.com/@MITMechatronics"},{"id":"http://arxiv.org/abs/2305.15086v2","updated":"2023-10-05T05:12:09Z","published":"2023-05-24T12:05:24Z","title":"Unpaired Image-to-Image Translation via Neural Schrödinger Bridge","summary":"  Diffusion models are a powerful class of generative models which simulate\nstochastic differential equations (SDEs) to generate data from noise. Although\ndiffusion models have achieved remarkable progress in recent years, they have\nlimitations in the unpaired image-to-image translation tasks due to the\nGaussian prior assumption. Schr\\\"odinger Bridge (SB), which learns an SDE to\ntranslate between two arbitrary distributions, have risen as an attractive\nsolution to this problem. However, none of SB models so far have been\nsuccessful at unpaired translation between high-resolution images. In this\nwork, we propose the Unpaired Neural Schr\\\"odinger Bridge (UNSB), which\nexpresses SB problem as a sequence of adversarial learning problems. This\nallows us to incorporate advanced discriminators and regularization to learn a\nSB between unpaired data. We demonstrate that UNSB is scalable and successfully\nsolves various unpaired image-to-image translation tasks. Code:\n\\url{https://github.com/cyclomon/UNSB}\n","authors":["Beomsu Kim","Gihyun Kwon","Kwanyoung Kim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2305.15086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.02936v2","updated":"2023-10-05T04:47:52Z","published":"2023-02-06T17:11:09Z","title":"Private GANs, Revisited","summary":"  We show that the canonical approach for training differentially private GANs\n-- updating the discriminator with differentially private stochastic gradient\ndescent (DPSGD) -- can yield significantly improved results after modifications\nto training. Specifically, we propose that existing instantiations of this\napproach neglect to consider how adding noise only to discriminator updates\ninhibits discriminator training, disrupting the balance between the generator\nand discriminator necessary for successful GAN training. We show that a simple\nfix -- taking more discriminator steps between generator steps -- restores\nparity between the generator and discriminator and improves results.\n  Additionally, with the goal of restoring parity, we experiment with other\nmodifications -- namely, large batch sizes and adaptive discriminator update\nfrequency -- to improve discriminator training and see further improvements in\ngeneration quality. Our results demonstrate that on standard image synthesis\nbenchmarks, DPSGD outperforms all alternative GAN privatization schemes. Code:\nhttps://github.com/alexbie98/dpgan-revisit.\n","authors":["Alex Bie","Gautam Kamath","Guojun Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.02936v2.pdf","comment":"28 pages; revisions and new experiments from TMLR camera-ready + code\n  release at https://github.com/alexbie98/dpgan-revisit"},{"id":"http://arxiv.org/abs/2309.11745v2","updated":"2023-10-05T04:45:21Z","published":"2023-09-21T02:46:32Z","title":"PIE: Simulating Disease Progression via Progressive Image Editing","summary":"  Disease progression simulation is a crucial area of research that has\nsignificant implications for clinical diagnosis, prognosis, and treatment. One\nmajor challenge in this field is the lack of continuous medical imaging\nmonitoring of individual patients over time. To address this issue, we develop\na novel framework termed Progressive Image Editing (PIE) that enables\ncontrolled manipulation of disease-related image features, facilitating precise\nand realistic disease progression simulation. Specifically, we leverage recent\nadvancements in text-to-image generative models to simulate disease progression\naccurately and personalize it for each patient. We theoretically analyze the\niterative refining process in our framework as a gradient descent with an\nexponentially decayed learning rate. To validate our framework, we conduct\nexperiments in three medical imaging domains. Our results demonstrate the\nsuperiority of PIE over existing methods such as Stable Diffusion Walk and\nStyle-Based Manifold Extrapolation based on CLIP score (Realism) and Disease\nClassification Confidence (Alignment). Our user study collected feedback from\n35 veteran physicians to assess the generated progressions. Remarkably, 76.2%\nof the feedback agrees with the fidelity of the generated progressions. To our\nbest knowledge, PIE is the first of its kind to generate disease progression\nimages meeting real-world standards. It is a promising tool for medical\nresearch and clinical practice, potentially allowing healthcare providers to\nmodel disease trajectories over time, predict future treatment responses, and\nimprove patient outcomes.\n","authors":["Kaizhao Liang","Xu Cao","Kuei-Da Liao","Tianren Gao","Wenqian Ye","Zhengyu Chen","Jianguo Cao","Tejas Nama","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2309.11745v2.pdf","comment":"Code and checkpoints for replicating our results can be found at\n  https://github.com/IrohXu/PIE and\n  https://huggingface.co/IrohXu/stable-diffusion-mimic-cxr-v0.1"},{"id":"http://arxiv.org/abs/2306.01879v2","updated":"2023-10-05T04:12:28Z","published":"2023-06-02T19:19:43Z","title":"Revisiting the Role of Language Priors in Vision-Language Models","summary":"  Vision-language models (VLMs) are impactful in part because they can be\napplied to a variety of visual understanding tasks in a zero-shot fashion,\nwithout any fine-tuning. We study $\\textit{generative VLMs}$ that are trained\nfor next-word generation given an image. We explore their zero-shot performance\non the illustrative task of image-text retrieval across 8 popular\nvision-language benchmarks. Our first observation is that they can be\nrepurposed for discriminative tasks (such as image-text retrieval) by simply\ncomputing the match score of generating a particular text string given an\nimage. We call this probabilistic score the $\\textit{Visual Generative\nPre-Training Score}$ (VisualGPTScore). While the VisualGPTScore produces\nnear-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on\nothers. We analyze this behavior through a probabilistic lens, pointing out\nthat some benchmarks inadvertently capture unnatural language distributions by\ncreating adversarial but unlikely text captions. In fact, we demonstrate that\neven a \"blind\" language model that ignores any image evidence can sometimes\noutperform all prior art, reminiscent of similar challenges faced by the\nvisual-question answering (VQA) community many years ago. We derive a\nprobabilistic post-processing scheme that controls for the amount of linguistic\nbias in generative VLMs at test time without having to retrain or fine-tune the\nmodel. We show that the VisualGPTScore, when appropriately debiased, is a\nstrong zero-shot baseline for vision-language understanding, oftentimes\nproducing state-of-the-art accuracy.\n","authors":["Zhiqiu Lin","Xinyue Chen","Deepak Pathak","Pengchuan Zhang","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2306.01879v2.pdf","comment":"Website: https://linzhiqiu.github.io/papers/visual_gpt_score/ Code:\n  https://github.com/linzhiqiu/visual_gpt_score/"},{"id":"http://arxiv.org/abs/2110.14883v3","updated":"2023-10-05T04:09:09Z","published":"2021-10-28T04:45:55Z","title":"Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel\n  Training","summary":"  The success of Transformer models has pushed the deep learning model scale to\nbillions of parameters. Due to the limited memory resource of a single GPU,\nHowever, the best practice for choosing the optimal parallel strategy is still\nlacking, since it requires domain expertise in both deep learning and parallel\ncomputing.\n  The Colossal-AI system addressed the above challenge by introducing a unified\ninterface to scale your sequential code of model training to distributed\nenvironments. It supports parallel training methods such as data, pipeline,\ntensor, and sequence parallelism, as well as heterogeneous training methods\nintegrated with zero redundancy optimizer. Compared to the baseline system,\nColossal-AI can achieve up to 2.76 times training speedup on large-scale\nmodels.\n","authors":["Shenggui Li","Hongxin Liu","Zhengda Bian","Jiarui Fang","Haichen Huang","Yuliang Liu","Boxiang Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2110.14883v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12439v2","updated":"2023-10-05T04:08:47Z","published":"2023-08-23T21:47:06Z","title":"BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input\n  Detection","summary":"  We present a novel defense, against backdoor attacks on Deep Neural Networks\n(DNNs), wherein adversaries covertly implant malicious behaviors (backdoors)\ninto DNNs. Our defense falls within the category of post-development defenses\nthat operate independently of how the model was generated. The proposed defense\nis built upon a novel reverse engineering approach that can directly extract\nbackdoor functionality of a given backdoored model to a backdoor expert model.\nThe approach is straightforward -- finetuning the backdoored model over a small\nset of intentionally mislabeled clean samples, such that it unlearns the normal\nfunctionality while still preserving the backdoor functionality, and thus\nresulting in a model (dubbed a backdoor expert model) that can only recognize\nbackdoor inputs. Based on the extracted backdoor expert model, we show the\nfeasibility of devising highly accurate backdoor input detectors that filter\nout the backdoor inputs during model inference. Further augmented by an\nensemble strategy with a finetuned auxiliary model, our defense, BaDExpert\n(Backdoor Input Detection with Backdoor Expert), effectively mitigates 17 SOTA\nbackdoor attacks while minimally impacting clean utility. The effectiveness of\nBaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet)\nacross various model architectures (ResNet, VGG, MobileNetV2 and Vision\nTransformer).\n","authors":["Tinghao Xie","Xiangyu Qi","Ping He","Yiming Li","Jiachen T. Wang","Prateek Mittal"],"pdf_url":"https://arxiv.org/pdf/2308.12439v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03295v1","updated":"2023-10-05T03:51:21Z","published":"2023-10-05T03:51:21Z","title":"Can pre-trained models assist in dataset distillation?","summary":"  Dataset Distillation (DD) is a prominent technique that encapsulates\nknowledge from a large-scale original dataset into a small synthetic dataset\nfor efficient training. Meanwhile, Pre-trained Models (PTMs) function as\nknowledge repositories, containing extensive information from the original\ndataset. This naturally raises a question: Can PTMs effectively transfer\nknowledge to synthetic datasets, guiding DD accurately? To this end, we conduct\npreliminary experiments, confirming the contribution of PTMs to DD. Afterwards,\nwe systematically study different options in PTMs, including initialization\nparameters, model architecture, training epoch and domain knowledge, revealing\nthat: 1) Increasing model diversity enhances the performance of synthetic\ndatasets; 2) Sub-optimal models can also assist in DD and outperform\nwell-trained ones in certain cases; 3) Domain-specific PTMs are not mandatory\nfor DD, but a reasonable domain match is crucial. Finally, by selecting optimal\noptions, we significantly improve the cross-architecture generalization over\nbaseline DD methods. We hope our work will facilitate researchers to develop\nbetter DD techniques. Our code is available at\nhttps://github.com/yaolu-zjut/DDInterpreter.\n","authors":["Yao Lu","Xuguang Chen","Yuchen Zhang","Jianyang Gu","Tianle Zhang","Yifan Zhang","Xiaoniu Yang","Qi Xuan","Kai Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2310.03295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12214v2","updated":"2023-10-05T03:50:19Z","published":"2023-03-21T22:24:27Z","title":"Prompt-MIL: Boosting Multi-Instance Learning Schemes via Task-specific\n  Prompt Tuning","summary":"  Whole slide image (WSI) classification is a critical task in computational\npathology, requiring the processing of gigapixel-sized images, which is\nchallenging for current deep-learning methods. Current state of the art methods\nare based on multi-instance learning schemes (MIL), which usually rely on\npretrained features to represent the instances. Due to the lack of\ntask-specific annotated data, these features are either obtained from\nwell-established backbones on natural images, or, more recently from\nself-supervised models pretrained on histopathology. However, both approaches\nyield task-agnostic features, resulting in performance loss compared to the\nappropriate task-related supervision, if available. In this paper, we show that\nwhen task-specific annotations are limited, we can inject such supervision into\ndownstream task training, to reduce the gap between fully task-tuned and task\nagnostic features. We propose Prompt-MIL, an MIL framework that integrates\nprompts into WSI classification. Prompt-MIL adopts a prompt tuning mechanism,\nwhere only a small fraction of parameters calibrates the pretrained features to\nencode task-specific information, rather than the conventional full fine-tuning\napproaches. Extensive experiments on three WSI datasets, TCGA-BRCA, TCGA-CRC,\nand BRIGHT, demonstrate the superiority of Prompt-MIL over conventional MIL\nmethods, achieving a relative improvement of 1.49%-4.03% in accuracy and\n0.25%-8.97% in AUROC while using fewer than 0.3% additional parameters.\nCompared to conventional full fine-tuning approaches, we fine-tune less than\n1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in\naccuracy and 3.22%-27.18% in AUROC and reduce GPU memory consumption by 38%-45%\nwhile training 21%-27% faster. Our code is available at\nhttps://github.com/cvlab-stonybrook/PromptMIL.\n","authors":["Jingwei Zhang","Saarthak Kapse","Ke Ma","Prateek Prasanna","Joel Saltz","Maria Vakalopoulou","Dimitris Samaras"],"pdf_url":"https://arxiv.org/pdf/2303.12214v2.pdf","comment":"Accepted to MICCAI 2023 (Oral)"},{"id":"http://arxiv.org/abs/2310.03291v1","updated":"2023-10-05T03:40:06Z","published":"2023-10-05T03:40:06Z","title":"SimVLG: Simple and Efficient Pretraining of Visual Language Generative\n  Models","summary":"  In this paper, we propose ``SimVLG'', a streamlined framework for the\npre-training of computationally intensive vision-language generative models,\nleveraging frozen pre-trained large language models (LLMs). The prevailing\nparadigm in vision-language pre-training (VLP) typically involves a two-stage\noptimization process: an initial resource-intensive phase dedicated to\ngeneral-purpose vision-language representation learning, aimed at extracting\nand consolidating pertinent visual features, followed by a subsequent phase\nfocusing on end-to-end alignment between visual and linguistic modalities. Our\none-stage, single-loss framework circumvents the aforementioned computationally\ndemanding first stage of training by gradually merging similar visual tokens\nduring training. This gradual merging process effectively compacts the visual\ninformation while preserving the richness of semantic content, leading to fast\nconvergence without sacrificing performance. Our experiments show that our\napproach can speed up the training of vision-language models by a factor\n$\\times 5$ without noticeable impact on the overall performance. Additionally,\nwe show that our models can achieve comparable performance to current\nvision-language models with only $1/10$ of the data. Finally, we demonstrate\nhow our image-text models can be easily adapted to video-language generative\ntasks through a novel soft attentive temporal token merging modules.\n","authors":["Yiren Jian","Tingkai Liu","Yunzhe Tao","Soroush Vosoughi","HX Yang"],"pdf_url":"https://arxiv.org/pdf/2310.03291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03288v1","updated":"2023-10-05T03:33:35Z","published":"2023-10-05T03:33:35Z","title":"PoseAction: Action Recognition for Patients in the Ward using Deep\n  Learning Approaches","summary":"  Real-time intelligent detection and prediction of subjects' behavior\nparticularly their movements or actions is critical in the ward. This approach\noffers the advantage of reducing in-hospital care costs and improving the\nefficiency of healthcare workers, which is especially true for scenarios at\nnight or during peak admission periods. Therefore, in this work, we propose\nusing computer vision (CV) and deep learning (DL) methods for detecting\nsubjects and recognizing their actions. We utilize OpenPose as an accurate\nsubject detector for recognizing the positions of human subjects in the video\nstream. Additionally, we employ AlphAction's Asynchronous Interaction\nAggregation (AIA) network to predict the actions of detected subjects. This\nintegrated model, referred to as PoseAction, is proposed. At the same time, the\nproposed model is further trained to predict 12 common actions in ward areas,\nsuch as staggering, chest pain, and falling down, using medical-related video\nclips from the NTU RGB+D and NTU RGB+D 120 datasets. The results demonstrate\nthat PoseAction achieves the highest classification mAP of 98.72% (IoU@0.5).\nAdditionally, this study develops an online real-time mode for action\nrecognition, which strongly supports the clinical translation of PoseAction.\nFurthermore, using OpenPose's function for recognizing face key points, we also\nimplement face blurring, which is a practical solution to address the privacy\nprotection concerns of patients and healthcare workers. Nevertheless, the\ntraining data for PoseAction is currently limited, particularly in terms of\nlabel diversity. Consequently, the subsequent step involves utilizing a more\ndiverse dataset (including general actions) to train the model's parameters for\nimproved generalization.\n","authors":["Zherui Li","Raye Chen-Hua Yeow"],"pdf_url":"https://arxiv.org/pdf/2310.03288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03279v1","updated":"2023-10-05T03:11:54Z","published":"2023-10-05T03:11:54Z","title":"Classifying Whole Slide Images: What Matters?","summary":"  Recently there have been many algorithms proposed for the classification of\nvery high resolution whole slide images (WSIs). These new algorithms are mostly\nfocused on finding novel ways to combine the information from small local\npatches extracted from the slide, with an emphasis on effectively aggregating\nmore global information for the final predictor. In this paper we thoroughly\nexplore different key design choices for WSI classification algorithms to\ninvestigate what matters most for achieving high accuracy. Surprisingly, we\nfound that capturing global context information does not necessarily mean\nbetter performance. A model that captures the most global information\nconsistently performs worse than a model that captures less global information.\nIn addition, a very simple multi-instance learning method that captures no\nglobal information performs almost as well as models that capture a lot of\nglobal information. These results suggest that the most important features for\neffective WSI classification are captured at the local small patch level, where\ncell and tissue micro-environment detail is most pronounced. Another surprising\nfinding was that unsupervised pre-training on a larger set of 33 cancers gives\nsignificantly worse performance compared to pre-training on a smaller dataset\nof 7 cancers (including the target cancer). We posit that pre-training on a\nsmaller, more focused dataset allows the feature extractor to make better use\nof the limited feature space to better discriminate between subtle differences\nin the input patch.\n","authors":["Long Nguyen","Aiden Nibali","Joshua Millward","Zhen He"],"pdf_url":"https://arxiv.org/pdf/2310.03279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03273v1","updated":"2023-10-05T02:59:48Z","published":"2023-10-05T02:59:48Z","title":"Ablation Study to Clarify the Mechanism of Object Segmentation in\n  Multi-Object Representation Learning","summary":"  Multi-object representation learning aims to represent complex real-world\nvisual input using the composition of multiple objects. Representation learning\nmethods have often used unsupervised learning to segment an input image into\nindividual objects and encode these objects into each latent vector. However,\nit is not clear how previous methods have achieved the appropriate segmentation\nof individual objects. Additionally, most of the previous methods regularize\nthe latent vectors using a Variational Autoencoder (VAE). Therefore, it is not\nclear whether VAE regularization contributes to appropriate object\nsegmentation. To elucidate the mechanism of object segmentation in multi-object\nrepresentation learning, we conducted an ablation study on MONet, which is a\ntypical method. MONet represents multiple objects using pairs that consist of\nan attention mask and the latent vector corresponding to the attention mask.\nEach latent vector is encoded from the input image and attention mask. Then,\nthe component image and attention mask are decoded from each latent vector. The\nloss function of MONet consists of 1) the sum of reconstruction losses between\nthe input image and decoded component image, 2) the VAE regularization loss of\nthe latent vector, and 3) the reconstruction loss of the attention mask to\nexplicitly encode shape information. We conducted an ablation study on these\nthree loss functions to investigate the effect on segmentation performance. Our\nresults showed that the VAE regularization loss did not affect segmentation\nperformance and the others losses did affect it. Based on this result, we\nhypothesize that it is important to maximize the attention mask of the image\nregion best represented by a single latent vector corresponding to the\nattention mask. We confirmed this hypothesis by evaluating a new loss function\nwith the same mechanism as the hypothesis.\n","authors":["Takayuki Komatsu","Yoshiyuki Ohmura","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2310.03273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03270v1","updated":"2023-10-05T02:51:53Z","published":"2023-10-05T02:51:53Z","title":"EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit\n  Diffusion Models","summary":"  Diffusion models have demonstrated remarkable capabilities in image synthesis\nand related generative tasks. Nevertheless, their practicality for low-latency\nreal-world applications is constrained by substantial computational costs and\nlatency issues. Quantization is a dominant way to compress and accelerate\ndiffusion models, where post-training quantization (PTQ) and quantization-aware\ntraining (QAT) are two main approaches, each bearing its own properties. While\nPTQ exhibits efficiency in terms of both time and data usage, it may lead to\ndiminished performance in low bit-width. On the other hand, QAT can alleviate\nperformance degradation but comes with substantial demands on computational and\ndata resources. To capitalize on the advantages while avoiding their respective\ndrawbacks, we introduce a data-free and parameter-efficient fine-tuning\nframework for low-bit diffusion models, dubbed EfficientDM, to achieve\nQAT-level performance with PTQ-like efficiency. Specifically, we propose a\nquantization-aware variant of the low-rank adapter (QALoRA) that can be merged\nwith model weights and jointly quantized to low bit-width. The fine-tuning\nprocess distills the denoising capabilities of the full-precision model into\nits quantized counterpart, eliminating the requirement for training data. We\nalso introduce scale-aware optimization and employ temporal learned step-size\nquantization to further enhance performance. Extensive experimental results\ndemonstrate that our method significantly outperforms previous PTQ-based\ndiffusion models while maintaining similar time and data efficiency.\nSpecifically, there is only a marginal 0.05 sFID increase when quantizing both\nweights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to\nQAT-based methods, our EfficientDM also boasts a 16.2x faster quantization\nspeed with comparable generation quality.\n","authors":["Yefei He","Jing Liu","Weijia Wu","Hong Zhou","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2310.03270v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2310.02676v2","updated":"2023-10-05T02:49:36Z","published":"2023-10-04T09:27:39Z","title":"PostRainBench: A comprehensive benchmark and a new model for\n  precipitation forecasting","summary":"  Accurate precipitation forecasting is a vital challenge of both scientific\nand societal importance. Data-driven approaches have emerged as a widely used\nsolution for addressing this challenge. However, solely relying on data-driven\napproaches has limitations in modeling the underlying physics, making accurate\npredictions difficult. Coupling AI-based post-processing techniques with\ntraditional Numerical Weather Prediction (NWP) methods offers a more effective\nsolution for improving forecasting accuracy. Despite previous post-processing\nefforts, accurately predicting heavy rainfall remains challenging due to the\nimbalanced precipitation data across locations and complex relationships\nbetween multiple meteorological variables. To address these limitations, we\nintroduce the PostRainBench, a comprehensive multi-variable NWP post-processing\nbenchmark consisting of three datasets for NWP post-processing-based\nprecipitation forecasting. We propose CAMT, a simple yet effective Channel\nAttention Enhanced Multi-task Learning framework with a specially designed\nweighted loss function. Its flexible design allows for easy plug-and-play\nintegration with various backbones. Extensive experimental results on the\nproposed benchmark show that our method outperforms state-of-the-art methods by\n6.3%, 4.7%, and 26.8% in rain CSI on the three datasets respectively. Most\nnotably, our model is the first deep learning-based method to outperform\ntraditional Numerical Weather Prediction (NWP) approaches in extreme\nprecipitation conditions. It shows improvements of 15.6%, 17.4%, and 31.8% over\nNWP predictions in heavy rain CSI on respective datasets. These results\nhighlight the potential impact of our model in reducing the severe consequences\nof extreme weather events.\n","authors":["Yujin Tang","Jiaming Zhou","Xiang Pan","Zeying Gong","Junwei Liang"],"pdf_url":"https://arxiv.org/pdf/2310.02676v2.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2308.16738v2","updated":"2023-10-05T02:06:24Z","published":"2023-08-31T13:54:57Z","title":"SFUSNet: A Spatial-Frequency domain-based Multi-branch Network for\n  diagnosis of Cervical Lymph Node Lesions in Ultrasound Images","summary":"  Booming deep learning has substantially improved the diagnosis for diverse\nlesions in ultrasound images, but a conspicuous research gap concerning\ncervical lymph node lesions still remains. The objective of this work is to\ndiagnose cervical lymph node lesions in ultrasound images by leveraging a deep\nlearning model. To this end, we first collected 3392 cervical ultrasound images\ncontaining normal lymph nodes, benign lymph node lesions, malignant primary\nlymph node lesions, and malignant metastatic lymph node lesions. Given that\nultrasound images are generated by the reflection and scattering of sound waves\nacross varied bodily tissues, we proposed the Conv-FFT Block. It integrates\nconvolutional operations with the fast Fourier transform to more astutely model\nthe images. Building upon this foundation, we designed a novel architecture,\nnamed SFUSNet. SFUSNet not only discerns variances in ultrasound images from\nthe spatial domain but also adeptly captures micro-structural alterations\nacross various lesions in the frequency domain. To ascertain the potential of\nSFUSNet, we benchmarked it against 12 popular architectures through five-fold\ncross-validation. The results show that SFUSNet is the state-of-the-art model\nand can achieve 92.89% accuracy. Moreover, its average precision, average\nsensitivity and average specificity for four types of lesions achieve 90.46%,\n89.95% and 97.49%, respectively.\n","authors":["Yubiao Yue","Jun Xue","Haihua Liang","Bingchun Luo","Zhenzhang Li"],"pdf_url":"https://arxiv.org/pdf/2308.16738v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09472v3","updated":"2023-10-05T01:45:37Z","published":"2023-09-18T04:10:27Z","title":"Reconstructing Existing Levels through Level Inpainting","summary":"  Procedural Content Generation (PCG) and Procedural Content Generation via\nMachine Learning (PCGML) have been used in prior work for generating levels in\nvarious games. This paper introduces Content Augmentation and focuses on the\nsubproblem of level inpainting, which involves reconstructing and extending\nvideo game levels. Drawing inspiration from image inpainting, we adapt two\ntechniques from this domain to address our specific use case. We present two\napproaches for level inpainting: an Autoencoder and a U-net. Through a\ncomprehensive case study, we demonstrate their superior performance compared to\na baseline method and discuss their relative merits. Furthermore, we provide a\npractical demonstration of both approaches for the level inpainting task and\noffer insights into potential directions for future research.\n","authors":["Johor Jara Gonzalez","Matthew Guzdial"],"pdf_url":"https://arxiv.org/pdf/2309.09472v3.pdf","comment":"8 pages, 5 figures, Artificial Intelligence and Interactive Digital\n  Entertainment"},{"id":"http://arxiv.org/abs/2205.03519v3","updated":"2023-10-05T00:15:00Z","published":"2022-05-07T01:49:31Z","title":"Self-supervised Deep Unrolled Reconstruction Using Regularization by\n  Denoising","summary":"  Deep learning methods have been successfully used in various computer vision\ntasks. Inspired by that success, deep learning has been explored in magnetic\nresonance imaging (MRI) reconstruction. In particular, integrating deep\nlearning and model-based optimization methods has shown considerable\nadvantages. However, a large amount of labeled training data is typically\nneeded for high reconstruction quality, which is challenging for some MRI\napplications. In this paper, we propose a novel reconstruction method, named\nDURED-Net, that enables interpretable self-supervised learning for MR image\nreconstruction by combining a self-supervised denoising network and a\nplug-and-play method. We aim to boost the reconstruction performance of\nNoise2Noise in MR reconstruction by adding an explicit prior that utilizes\nimaging physics. Specifically, the leverage of a denoising network for MRI\nreconstruction is achieved using Regularization by Denoising (RED). Experiment\nresults demonstrate that the proposed method requires a reduced amount of\ntraining data to achieve high reconstruction quality among the state-of-art of\nMR reconstruction utilizing the Noise2Noise method.\n","authors":["Peizhou Huang","Chaoyi Zhang","Xiaoliang Zhang","Xiaojuan Li","Liang Dong","Leslie Ying"],"pdf_url":"https://arxiv.org/pdf/2205.03519v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03940v1","updated":"2023-10-05T23:09:19Z","published":"2023-10-05T23:09:19Z","title":"Hard View Selection for Contrastive Learning","summary":"  Many Contrastive Learning (CL) methods train their models to be invariant to\ndifferent \"views\" of an image input for which a good data augmentation pipeline\nis crucial. While considerable efforts were directed towards improving pre-text\ntasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax\ncentering), the majority of these methods remain strongly reliant on the random\nsampling of operations within the image augmentation pipeline, such as the\nrandom resized crop or color distortion operation. In this paper, we argue that\nthe role of the view generation and its effect on performance has so far\nreceived insufficient attention. To address this, we propose an easy,\nlearning-free, yet powerful Hard View Selection (HVS) strategy designed to\nextend the random view generation to expose the pretrained model to harder\nsamples during CL training. It encompasses the following iterative steps: 1)\nrandomly sample multiple views and create pairs of two views, 2) run forward\npasses for each view pair on the currently trained model, 3) adversarially\nselect the pair yielding the worst loss, and 4) run the backward pass with the\nselected pair. In our empirical analysis we show that under the hood, HVS\nincreases task difficulty by controlling the Intersection over Union of views\nduring pretraining. With only 300-epoch pretraining, HVS is able to closely\nrival the 800-epoch DINO baseline which remains very favorable even when\nfactoring in the slowdown induced by the additional forwards of HVS.\nAdditionally, HVS consistently achieves accuracy improvements on ImageNet\nbetween 0.55% and 1.9% on linear evaluation and similar improvements on\ntransfer tasks across multiple CL methods, such as DINO, SimSiam, and SimCLR.\n","authors":["Fabio Ferreira","Ivo Rapant","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2310.03940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03937v1","updated":"2023-10-05T23:00:27Z","published":"2023-10-05T23:00:27Z","title":"Diffusion Models as Masked Audio-Video Learners","summary":"  Over the past several years, the synchronization between audio and visual\nsignals has been leveraged to learn richer audio-visual representations. Aided\nby the large availability of unlabeled videos, many unsupervised training\nframeworks have demonstrated impressive results in various downstream audio and\nvideo tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a\nstate-of-the-art audio-video pre-training framework. MAViL couples contrastive\nlearning with masked autoencoding to jointly reconstruct audio spectrograms and\nvideo frames by fusing information from both modalities. In this paper, we\nstudy the potential synergy between diffusion models and MAViL, seeking to\nderive mutual benefits from these two frameworks. The incorporation of\ndiffusion into MAViL, combined with various training efficiency methodologies\nthat include the utilization of a masking ratio curriculum and adaptive batch\nsizing, results in a notable 32% reduction in pre-training Floating-Point\nOperations (FLOPS) and an 18% decrease in pre-training wall clock time.\nCrucially, this enhanced efficiency does not compromise the model's performance\nin downstream audio-classification tasks when compared to MAViL's performance.\n","authors":["Elvis Nunez","Yanzi Jin","Mohammad Rastegari","Sachin Mehta","Maxwell Horton"],"pdf_url":"https://arxiv.org/pdf/2310.03937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03887v2","updated":"2023-10-05T22:53:15Z","published":"2023-07-08T03:42:54Z","title":"Improving Prototypical Part Networks with Reward Reweighing,\n  Reselection, and Retraining","summary":"  In recent years, work has gone into developing deep interpretable methods for\nimage classification that clearly attributes a model's output to specific\nfeatures of the data. One such of these methods is the Prototypical Part\nNetwork (ProtoPNet), which attempts to classify images based on meaningful\nparts of the input. While this method results in interpretable classifications,\nit often learns to classify from spurious or inconsistent parts of the image.\nHoping to remedy this, we take inspiration from the recent developments in\nReinforcement Learning with Human Feedback (RLHF) to fine-tune these\nprototypes. By collecting human annotations of prototypes quality via a 1-5\nscale on the CUB-200-2011 dataset, we construct a reward model that learns\nhuman preferences and identify non-spurious prototypes. In place of a full RL\nupdate, we propose the Reweighed, Reselected, and Retrained Prototypical Part\nNetwork (R3-ProtoPNet), which adds an additional three steps to the ProtoPNet\ntraining loop. The first two steps are reward-based reweighting and\nreselection, which align prototypes with human feedback. The final step is\nretraining to realign the model's features with the updated prototypes. We find\nthat R3-ProtoPNet improves the overall meaningfulness of the prototypes, and\nmaintains or improves individual model performance. When multiple trained\nR3-ProtoPNets are incorporated into an ensemble, we find increases in both\ninterpretability and predictive performance.\n","authors":["Robin Netzorg","Jiaxun Li","Bin Yu"],"pdf_url":"https://arxiv.org/pdf/2307.03887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18030v3","updated":"2023-10-05T22:41:01Z","published":"2023-05-25T19:41:40Z","title":"Automated Search-Space Generation Neural Architecture Search","summary":"  To search an optimal sub-network within a general deep neural network (DNN),\nexisting neural architecture search (NAS) methods typically rely on\nhandcrafting a search space beforehand. Such requirements make it challenging\nto extend them onto general scenarios without significant human expertise and\nmanual intervention. To overcome the limitations, we propose Automated\nSearch-Space Generation Neural Architecture Search (ASGNAS), perhaps the first\nautomated system to train general DNNs that cover all candidate connections and\noperations and produce high-performing sub-networks in the one shot manner.\nTechnologically, ASGNAS delivers three noticeable contributions to minimize\nhuman efforts: (i) automated search space generation for general DNNs; (ii) a\nHierarchical Half-Space Projected Gradient (H2SPG) that leverages the hierarchy\nand dependency within generated search space to ensure the network validity\nduring optimization, and reliably produces a solution with both high\nperformance and hierarchical group sparsity; and (iii) automated sub-network\nconstruction upon the H2SPG solution. Numerically, we demonstrate the\neffectiveness of ASGNAS on a variety of general DNNs, including RegNet,\nStackedUnets, SuperResNet, and DARTS, over benchmark datasets such as CIFAR10,\nFashion-MNIST, ImageNet, STL-10 , and SVNH. The sub-networks computed by ASGNAS\nachieve competitive even superior performance compared to the starting full\nDNNs and other state-of-the-arts. The library will be released at\nhttps://github.com/tianyic/only_train_once.\n","authors":["Tianyi Chen","Luming Liang","Tianyu Ding","Ilya Zharkov"],"pdf_url":"https://arxiv.org/pdf/2305.18030v3.pdf","comment":"Graph visualization for DARTS, SuperResNet are omitted for arXiv\n  version due to exceeding page dimension limit. Please refer to the\n  open-review version for taking the visualizations"},{"id":"http://arxiv.org/abs/2310.02687v2","updated":"2023-10-05T22:35:19Z","published":"2023-10-04T09:51:58Z","title":"USB-NeRF: Unrolling Shutter Bundle Adjusted Neural Radiance Fields","summary":"  Neural Radiance Fields (NeRF) has received much attention recently due to its\nimpressive capability to represent 3D scene and synthesize novel view images.\nExisting works usually assume that the input images are captured by a global\nshutter camera. Thus, rolling shutter (RS) images cannot be trivially applied\nto an off-the-shelf NeRF algorithm for novel view synthesis. Rolling shutter\neffect would also affect the accuracy of the camera pose estimation (e.g. via\nCOLMAP), which further prevents the success of NeRF algorithm with RS images.\nIn this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance\nFields (USB-NeRF). USB-NeRF is able to correct rolling shutter distortions and\nrecover accurate camera motion trajectory simultaneously under the framework of\nNeRF, by modeling the physical image formation process of a RS camera.\nExperimental results demonstrate that USB-NeRF achieves better performance\ncompared to prior works, in terms of RS effect removal, novel view image\nsynthesis as well as camera motion estimation. Furthermore, our algorithm can\nalso be used to recover high-fidelity high frame-rate global shutter video from\na sequence of RS images.\n","authors":["Moyang Li","Peng Wang","Lingzhe Zhao","Bangyan Liao","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2310.02687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03923v1","updated":"2023-10-05T21:57:36Z","published":"2023-10-05T21:57:36Z","title":"Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene\n  Representation","summary":"  Precise 3D environmental mapping is pivotal in robotics. Existing methods\noften rely on predefined concepts during training or are time-intensive when\ngenerating semantic maps. This paper presents Open-Fusion, a groundbreaking\napproach for real-time open-vocabulary 3D mapping and queryable scene\nrepresentation using RGB-D data. Open-Fusion harnesses the power of a\npre-trained vision-language foundation model (VLFM) for open-set semantic\ncomprehension and employs the Truncated Signed Distance Function (TSDF) for\nswift 3D scene reconstruction. By leveraging the VLFM, we extract region-based\nembeddings and their associated confidence maps. These are then integrated with\n3D knowledge from TSDF using an enhanced Hungarian-based feature-matching\nmechanism. Notably, Open-Fusion delivers outstanding annotation-free 3D\nsegmentation for open-vocabulary without necessitating additional 3D training.\nBenchmark tests on the ScanNet dataset against leading zero-shot methods\nhighlight Open-Fusion's superiority. Furthermore, it seamlessly combines the\nstrengths of region-based VLFM and TSDF, facilitating real-time 3D scene\ncomprehension that includes object concepts and open-world semantics. We\nencourage the readers to view the demos on our project page:\nhttps://uark-aicv.github.io/OpenFusion\n","authors":["Kashu Yamazaki","Taisei Hanyu","Khoa Vo","Thang Pham","Minh Tran","Gianfranco Doretto","Anh Nguyen","Ngan Le"],"pdf_url":"https://arxiv.org/pdf/2310.03923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03911v1","updated":"2023-10-05T21:30:37Z","published":"2023-10-05T21:30:37Z","title":"Coloring Deep CNN Layers with Activation Hue Loss","summary":"  This paper proposes a novel hue-like angular parameter to model the structure\nof deep convolutional neural network (CNN) activation space, referred to as the\n{\\em activation hue}, for the purpose of regularizing models for more effective\nlearning. The activation hue generalizes the notion of color hue angle in\nstandard 3-channel RGB intensity space to $N$-channel activation space. A\nseries of observations based on nearest neighbor indexing of activation vectors\nwith pre-trained networks indicate that class-informative activations are\nconcentrated about an angle $\\theta$ in both the $(x,y)$ image plane and in\nmulti-channel activation space. A regularization term in the form of hue-like\nangular $\\theta$ labels is proposed to complement standard one-hot loss.\nTraining from scratch using combined one-hot + activation hue loss improves\nclassification performance modestly for a wide variety of classification tasks,\nincluding ImageNet.\n","authors":["Louis-François Bouchard","Mohsen Ben Lazreg","Matthew Toews"],"pdf_url":"https://arxiv.org/pdf/2310.03911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.03893v4","updated":"2023-10-05T21:26:48Z","published":"2021-04-08T17:01:19Z","title":"Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in\n  Prosthetic Hand Control","summary":"  Objective: For lower arm amputees, robotic prosthetic hands promise to regain\nthe capability to perform daily living activities. Current control methods\nbased on physiological signals such as electromyography (EMG) are prone to\nyielding poor inference outcomes due to motion artifacts, muscle fatigue, and\nmany more. Vision sensors are a major source of information about the\nenvironment state and can play a vital role in inferring feasible and intended\ngestures. However, visual evidence is also susceptible to its own artifacts,\nmost often due to object occlusion, lighting changes, etc. Multimodal evidence\nfusion using physiological and vision sensor measurements is a natural approach\ndue to the complementary strengths of these modalities. Methods: In this paper,\nwe present a Bayesian evidence fusion framework for grasp intent inference\nusing eye-view video, eye-gaze, and EMG from the forearm processed by neural\nnetwork models. We analyze individual and fused performance as a function of\ntime as the hand approaches the object to grasp it. For this purpose, we have\nalso developed novel data processing and augmentation techniques to train\nneural network components. Results: Our results indicate that, on average,\nfusion improves the instantaneous upcoming grasp type classification accuracy\nwhile in the reaching phase by 13.66% and 14.8%, relative to EMG and visual\nevidence individually, resulting in an overall fusion accuracy of 95.3%.\nConclusion: Our experimental data analyses demonstrate that EMG and visual\nevidence show complementary strengths, and as a consequence, fusion of\nmultimodal evidence can outperform each individual evidence modality at any\ngiven time.\n","authors":["Mehrshad Zandigohar","Mo Han","Mohammadreza Sharif","Sezen Yagmur Gunay","Mariusz P. Furmanek","Mathew Yarossi","Paolo Bonato","Cagdas Onal","Taskin Padir","Deniz Erdogmus","Gunar Schirner"],"pdf_url":"https://arxiv.org/pdf/2104.03893v4.pdf","comment":"This work has been submitted to Frontiers for possible publication"},{"id":"http://arxiv.org/abs/2212.06096v2","updated":"2023-10-05T21:01:15Z","published":"2022-12-12T18:10:33Z","title":"Implicit Convolutional Kernels for Steerable CNNs","summary":"  Steerable convolutional neural networks (CNNs) provide a general framework\nfor building neural networks equivariant to translations and other\ntransformations belonging to an origin-preserving group $G$, such as\nreflections and rotations. They rely on standard convolutions with\n$G$-steerable kernels obtained by analytically solving the group-specific\nequivariance constraint imposed onto the kernel space. As the solution is\ntailored to a particular group $G$, the implementation of a kernel basis does\nnot generalize to other symmetry transformations, which complicates the\ndevelopment of general group equivariant models. We propose using implicit\nneural representation via multi-layer perceptrons (MLPs) to parameterize\n$G$-steerable kernels. The resulting framework offers a simple and flexible way\nto implement Steerable CNNs and generalizes to any group $G$ for which a\n$G$-equivariant MLP can be built. We prove the effectiveness of our method on\nmultiple tasks, including N-body simulations, point cloud classification and\nmolecular property prediction.\n","authors":["Maksim Zhdanov","Nico Hoffmann","Gabriele Cesa"],"pdf_url":"https://arxiv.org/pdf/2212.06096v2.pdf","comment":"Accepted to 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2310.03895v1","updated":"2023-10-05T21:01:04Z","published":"2023-10-05T21:01:04Z","title":"TWICE Dataset: Digital Twin of Test Scenarios in a Controlled\n  Environment","summary":"  Ensuring the safe and reliable operation of autonomous vehicles under adverse\nweather remains a significant challenge. To address this, we have developed a\ncomprehensive dataset composed of sensor data acquired in a real test track and\nreproduced in the laboratory for the same test scenarios. The provided dataset\nincludes camera, radar, LiDAR, inertial measurement unit (IMU), and GPS data\nrecorded under adverse weather conditions (rainy, night-time, and snowy\nconditions). We recorded test scenarios using objects of interest such as car,\ncyclist, truck and pedestrian -- some of which are inspired by EURONCAP\n(European New Car Assessment Programme). The sensor data generated in the\nlaboratory is acquired by the execution of simulation-based tests in\nhardware-in-the-loop environment with the digital twin of each real test\nscenario. The dataset contains more than 2 hours of recording, which totals\nmore than 280GB of data. Therefore, it is a valuable resource for researchers\nin the field of autonomous vehicles to test and improve their algorithms in\nadverse weather conditions, as well as explore the simulation-to-reality gap.\nThe dataset is available for download at: https://twicedataset.github.io/site/\n","authors":["Leonardo Novicki Neto","Fabio Reway","Yuri Poledna","Maikol Funk Drechsler","Eduardo Parente Ribeiro","Werner Huber","Christian Icking"],"pdf_url":"https://arxiv.org/pdf/2310.03895v1.pdf","comment":"8 pages, 13 figures, submitted to IEEE Sensors Journal"},{"id":"http://arxiv.org/abs/2310.03893v1","updated":"2023-10-05T20:54:05Z","published":"2023-10-05T20:54:05Z","title":"Characterizing the Features of Mitotic Figures Using a Conditional\n  Diffusion Probabilistic Model","summary":"  Mitotic figure detection in histology images is a hard-to-define, yet\nclinically significant task, where labels are generated with pathologist\ninterpretations and where there is no ``gold-standard'' independent\nground-truth. However, it is well-established that these interpretation based\nlabels are often unreliable, in part, due to differences in expertise levels\nand human subjectivity. In this paper, our goal is to shed light on the\ninherent uncertainty of mitosis labels and characterize the mitotic figure\nclassification task in a human interpretable manner. We train a probabilistic\ndiffusion model to synthesize patches of cell nuclei for a given mitosis label\ncondition. Using this model, we can then generate a sequence of synthetic\nimages that correspond to the same nucleus transitioning into the mitotic\nstate. This allows us to identify different image features associated with\nmitosis, such as cytoplasm granularity, nuclear density, nuclear irregularity\nand high contrast between the nucleus and the cell body. Our approach offers a\nnew tool for pathologists to interpret and communicate the features driving the\ndecision to recognize a mitotic figure.\n","authors":["Cagla Deniz Bahadir","Benjamin Liechty","David J. Pisapia","Mert R. Sabuncu"],"pdf_url":"https://arxiv.org/pdf/2310.03893v1.pdf","comment":"Accepted for Deep Generative Models Workshop at Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2023"},{"id":"http://arxiv.org/abs/2310.03890v1","updated":"2023-10-05T20:49:48Z","published":"2023-10-05T20:49:48Z","title":"Accelerated Neural Network Training with Rooted Logistic Objectives","summary":"  Many neural networks deployed in the real world scenarios are trained using\ncross entropy based loss functions. From the optimization perspective, it is\nknown that the behavior of first order methods such as gradient descent\ncrucially depend on the separability of datasets. In fact, even in the most\nsimplest case of binary classification, the rate of convergence depends on two\nfactors: (1) condition number of data matrix, and (2) separability of the\ndataset. With no further pre-processing techniques such as\nover-parametrization, data augmentation etc., separability is an intrinsic\nquantity of the data distribution under consideration. We focus on the\nlandscape design of the logistic function and derive a novel sequence of {\\em\nstrictly} convex functions that are at least as strict as logistic loss. The\nminimizers of these functions coincide with those of the minimum norm solution\nwherever possible. The strict convexity of the derived function can be extended\nto finetune state-of-the-art models and applications. In empirical experimental\nanalysis, we apply our proposed rooted logistic objective to multiple deep\nmodels, e.g., fully-connected neural networks and transformers, on various of\nclassification benchmarks. Our results illustrate that training with rooted\nloss function is converged faster and gains performance improvements.\nFurthermore, we illustrate applications of our novel rooted loss function in\ngenerative modeling based downstream applications, such as finetuning StyleGAN\nmodel with the rooted loss. The code implementing our losses and models can be\nfound here for open source software development purposes:\nhttps://anonymous.4open.science/r/rooted_loss.\n","authors":["Zhu Wang","Praveen Raj Veluswami","Harsh Mishra","Sathya N. Ravi"],"pdf_url":"https://arxiv.org/pdf/2310.03890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03872v1","updated":"2023-10-05T19:58:36Z","published":"2023-10-05T19:58:36Z","title":"FNOSeg3D: Resolution-Robust 3D Image Segmentation with Fourier Neural\n  Operator","summary":"  Due to the computational complexity of 3D medical image segmentation,\ntraining with downsampled images is a common remedy for out-of-memory errors in\ndeep learning. Nevertheless, as standard spatial convolution is sensitive to\nvariations in image resolution, the accuracy of a convolutional neural network\ntrained with downsampled images can be suboptimal when applied on the original\nresolution. To address this limitation, we introduce FNOSeg3D, a 3D\nsegmentation model robust to training image resolution based on the Fourier\nneural operator (FNO). The FNO is a deep learning framework for learning\nmappings between functions in partial differential equations, which has the\nappealing properties of zero-shot super-resolution and global receptive field.\nWe improve the FNO by reducing its parameter requirement and enhancing its\nlearning capability through residual connections and deep supervision, and\nthese result in our FNOSeg3D model which is parameter efficient and resolution\nrobust. When tested on the BraTS'19 dataset, it achieved superior robustness to\ntraining image resolution than other tested models with less than 1% of their\nmodel parameters.\n","authors":["Ken C. L. Wong","Hongzhi Wang","Tanveer Syeda-Mahmood"],"pdf_url":"https://arxiv.org/pdf/2310.03872v1.pdf","comment":"This paper was accepted by the IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2023"},{"id":"http://arxiv.org/abs/2310.03870v1","updated":"2023-10-05T19:58:13Z","published":"2023-10-05T19:58:13Z","title":"Consistency Regularization Improves Placenta Segmentation in Fetal EPI\n  MRI Time Series","summary":"  The placenta plays a crucial role in fetal development. Automated 3D placenta\nsegmentation from fetal EPI MRI holds promise for advancing prenatal care. This\npaper proposes an effective semi-supervised learning method for improving\nplacenta segmentation in fetal EPI MRI time series. We employ consistency\nregularization loss that promotes consistency under spatial transformation of\nthe same image and temporal consistency across nearby images in a time series.\nThe experimental results show that the method improves the overall segmentation\naccuracy and provides better performance for outliers and hard samples. The\nevaluation also indicates that our method improves the temporal coherency of\nthe prediction, which could lead to more accurate computation of temporal\nplacental biomarkers. This work contributes to the study of the placenta and\nprenatal clinical decision-making. Code is available at\nhttps://github.com/firstmover/cr-seg.\n","authors":["Yingcheng Liu","Neerav Karani","Neel Dey","S. Mazdak Abulnaga","Junshen Xu","P. Ellen Grant","Esra Abaci Turk","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2310.03870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02239v2","updated":"2023-10-05T19:33:29Z","published":"2023-10-03T17:49:04Z","title":"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative\n  Vokens","summary":"  Large Language Models (LLMs) have garnered significant attention for their\nadvancements in natural language processing, demonstrating unparalleled prowess\nin text comprehension and generation. Yet, the simultaneous generation of\nimages with coherent textual narratives remains an evolving frontier. In\nresponse, we introduce an innovative interleaved vision-and-language generation\ntechnique anchored by the concept of \"generative vokens,\" acting as the bridge\nfor harmonized image-text outputs. Our approach is characterized by a\ndistinctive two-staged training strategy focusing on description-free\nmultimodal generation, where the training requires no comprehensive\ndescriptions of images. To bolster model integrity, classifier-free guidance is\nincorporated, enhancing the effectiveness of vokens on image generation. Our\nmodel, MiniGPT-5, exhibits substantial improvement over the baseline Divter\nmodel on the MMDialog dataset and consistently delivers superior or comparable\nmultimodal outputs in human evaluations on the VIST dataset, highlighting its\nefficacy across diverse benchmarks.\n","authors":["Kaizhi Zheng","Xuehai He","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02239v2.pdf","comment":"20 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.03848v1","updated":"2023-10-05T19:08:08Z","published":"2023-10-05T19:08:08Z","title":"OpenIncrement: A Unified Framework for Open Set Recognition and Deep\n  Class-Incremental Learning","summary":"  In most works on deep incremental learning research, it is assumed that novel\nsamples are pre-identified for neural network retraining. However, practical\ndeep classifiers often misidentify these samples, leading to erroneous\npredictions. Such misclassifications can degrade model performance. Techniques\nlike open set recognition offer a means to detect these novel samples,\nrepresenting a significant area in the machine learning domain.\n  In this paper, we introduce a deep class-incremental learning framework\nintegrated with open set recognition. Our approach refines class-incrementally\nlearned features to adapt them for distance-based open set recognition.\nExperimental results validate that our method outperforms state-of-the-art\nincremental learning techniques and exhibits superior performance in open set\nrecognition compared to baseline methods.\n","authors":["Jiawen Xu","Claas Grohnfeldt","Odej Kao"],"pdf_url":"https://arxiv.org/pdf/2310.03848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02492v2","updated":"2023-10-05T19:02:36Z","published":"2023-10-03T23:44:35Z","title":"Harvard Eye Fairness: A Large-Scale 3D Imaging Dataset for Equitable Eye\n  Diseases Screening and Fair Identity Scaling","summary":"  Fairness or equity in machine learning is profoundly important for societal\nwell-being, but limited public datasets hinder its progress, especially in the\narea of medicine. It is undeniable that fairness in medicine is one of the most\nimportant areas for fairness learning's applications. Currently, no large-scale\npublic medical datasets with 3D imaging data for fairness learning are\navailable, while 3D imaging data in modern clinics are standard tests for\ndisease diagnosis. In addition, existing medical fairness datasets are actually\nrepurposed datasets, and therefore they typically have limited demographic\nidentity attributes with at most three identity attributes of age, gender, and\nrace for fairness modeling. To address this gap, we introduce our Eye Fairness\ndataset with 30,000 subjects (Harvard-EF) covering three major eye diseases\nincluding age-related macular degeneration, diabetic retinopathy, and glaucoma\naffecting 380 million patients globally. Our Harvard-EF dataset includes both\n2D fundus photos and 3D optical coherence tomography scans with six demographic\nidentity attributes including age, gender, race, ethnicity, preferred language,\nand marital status. We also propose a fair identity scaling (FIS) approach\ncombining group and individual scaling together to improve model fairness. Our\nFIS approach is compared with various state-of-the-art fairness learning\nmethods with superior performance in the racial, gender, and ethnicity fairness\ntasks with 2D and 3D imaging data, which demonstrate the utilities of our\nHarvard-EF dataset for fairness learning. To facilitate fairness comparisons\nbetween different models, we propose performance-scaled disparity measures,\nwhich can be used to compare model fairness accounting for overall performance\nlevels. The dataset and code are publicly accessible via\nhttps://ophai.hms.harvard.edu/datasets/harvard-ef30k.\n","authors":["Yan Luo","Yu Tian","Min Shi","Tobias Elze","Mengyu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03843v1","updated":"2023-10-05T19:00:49Z","published":"2023-10-05T19:00:49Z","title":"Less is More: On the Feature Redundancy of Pretrained Models When\n  Transferring to Few-shot Tasks","summary":"  Transferring a pretrained model to a downstream task can be as easy as\nconducting linear probing with target data, that is, training a linear\nclassifier upon frozen features extracted from the pretrained model. As there\nmay exist significant gaps between pretraining and downstream datasets, one may\nask whether all dimensions of the pretrained features are useful for a given\ndownstream task. We show that, for linear probing, the pretrained features can\nbe extremely redundant when the downstream data is scarce, or few-shot. For\nsome cases such as 5-way 1-shot tasks, using only 1\\% of the most important\nfeature dimensions is able to recover the performance achieved by using the\nfull representation. Interestingly, most dimensions are redundant only under\nfew-shot settings and gradually become useful when the number of shots\nincreases, suggesting that feature redundancy may be the key to characterizing\nthe \"few-shot\" nature of few-shot transfer problems. We give a theoretical\nunderstanding of this phenomenon and show how dimensions with high variance and\nsmall distance between class centroids can serve as confounding factors that\nseverely disturb classification results under few-shot settings. As an attempt\nat solving this problem, we find that the redundant features are difficult to\nidentify accurately with a small number of training samples, but we can instead\nadjust feature magnitude with a soft mask based on estimated feature\nimportance. We show that this method can generally improve few-shot transfer\nperformance across various pretrained models and downstream datasets.\n","authors":["Xu Luo","Difan Zou","Lianli Gao","Zenglin Xu","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2310.03843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03827v1","updated":"2023-10-05T18:19:56Z","published":"2023-10-05T18:19:56Z","title":"Integrating Audio-Visual Features for Multimodal Deepfake Detection","summary":"  Deepfakes are AI-generated media in which an image or video has been\ndigitally modified. The advancements made in deepfake technology have led to\nprivacy and security issues. Most deepfake detection techniques rely on the\ndetection of a single modality. Existing methods for audio-visual detection do\nnot always surpass that of the analysis based on single modalities. Therefore,\nthis paper proposes an audio-visual-based method for deepfake detection, which\nintegrates fine-grained deepfake identification with binary classification. We\ncategorize the samples into four types by combining labels specific to each\nsingle modality. This method enhances the detection under intra-domain and\ncross-domain testing.\n","authors":["Sneha Muppalla","Shan Jia","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2310.03827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03821v1","updated":"2023-10-05T18:17:07Z","published":"2023-10-05T18:17:07Z","title":"WLST: Weak Labels Guided Self-training for Weakly-supervised Domain\n  Adaptation on 3D Object Detection","summary":"  In the field of domain adaptation (DA) on 3D object detection, most of the\nwork is dedicated to unsupervised domain adaptation (UDA). Yet, without any\ntarget annotations, the performance gap between the UDA approaches and the\nfully-supervised approach is still noticeable, which is impractical for\nreal-world applications. On the other hand, weakly-supervised domain adaptation\n(WDA) is an underexplored yet practical task that only requires few labeling\neffort on the target domain. To improve the DA performance in a cost-effective\nway, we propose a general weak labels guided self-training framework, WLST,\ndesigned for WDA on 3D object detection. By incorporating autolabeler, which\ncan generate 3D pseudo labels from 2D bounding boxes, into the existing\nself-training pipeline, our method is able to generate more robust and\nconsistent pseudo labels that would benefit the training process on the target\ndomain. Extensive experiments demonstrate the effectiveness, robustness, and\ndetector-agnosticism of our WLST framework. Notably, it outperforms previous\nstate-of-the-art methods on all evaluation tasks.\n","authors":["Tsung-Lin Tsou","Tsung-Han Wu","Winston H. Hsu"],"pdf_url":"https://arxiv.org/pdf/2310.03821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03773v1","updated":"2023-10-05T04:46:52Z","published":"2023-10-05T04:46:52Z","title":"Functional data learning using convolutional neural networks","summary":"  In this paper, we show how convolutional neural networks (CNN) can be used in\nregression and classification learning problems of noisy and non-noisy\nfunctional data. The main idea is to transform the functional data into a 28 by\n28 image. We use a specific but typical architecture of a convolutional neural\nnetwork to perform all the regression exercises of parameter estimation and\nfunctional form classification. First, we use some functional case studies of\nfunctional data with and without random noise to showcase the strength of the\nnew method. In particular, we use it to estimate exponential growth and decay\nrates, the bandwidths of sine and cosine functions, and the magnitudes and\nwidths of curve peaks. We also use it to classify the monotonicity and\ncurvatures of functional data, algebraic versus exponential growth, and the\nnumber of peaks of functional data. Second, we apply the same convolutional\nneural networks to Lyapunov exponent estimation in noisy and non-noisy chaotic\ndata, in estimating rates of disease transmission from epidemic curves, and in\ndetecting the similarity of drug dissolution profiles. Finally, we apply the\nmethod to real-life data to detect Parkinson's disease patients in a\nclassification problem. The method, although simple, shows high accuracy and is\npromising for future use in engineering and medical applications.\n","authors":["Jose Galarza","Tamer Oraby"],"pdf_url":"https://arxiv.org/pdf/2310.03773v1.pdf","comment":"38 pages, 23 figures"}]},"2023-10-06T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2310.04349v1","updated":"2023-10-06T16:16:00Z","published":"2023-10-06T16:16:00Z","title":"Learning to Grasp: from Somewhere to Anywhere","summary":"  Robotic grasping is still a partially solved, multidisciplinary problem where\ndata-driven techniques play an increasing role. The sparse nature of rewards\nmake the automatic generation of grasping datasets challenging, especially for\nunconventional morphologies or highly actuated end-effectors. Most approaches\nfor obtaining large-scale datasets rely on numerous human-provided\ndemonstrations or heavily engineered solutions that do not scale well. Recent\nadvances in Quality-Diversity (QD) methods have investigated how to learn\nobject grasping at a specific pose with different robot morphologies. The\npresent work introduces a pipeline for adapting QD-generated trajectories to\nnew object poses. Using an RGB-D data stream, the vision pipeline first detects\nthe targeted object, predicts its 6-DOF pose, and finally tracks it. An\nautomatically generated reach-and-grasp trajectory can then be adapted by\nprojecting it relatively to the object frame. Hundreds of trajectories have\nbeen deployed into the real world on several objects and with different robotic\nsetups: a Franka Research 3 with a parallel gripper and a UR5 with a dexterous\nSIH Schunk hand. The transfer ratio obtained when applying transformation to\nthe object pose matches the one obtained when the object pose matches the\nsimulation, demonstrating the efficiency of the proposed approach.\n","authors":["François Hélénon","Johann Huber","Faïz Ben Amar","Stéphane Doncieux"],"pdf_url":"https://arxiv.org/pdf/2310.04349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11865v2","updated":"2023-10-06T15:41:54Z","published":"2023-07-21T19:09:37Z","title":"CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction\n  Execution for Robots","summary":"  This work explores the capacity of large language models (LLMs) to address\nproblems at the intersection of spatial planning and natural language\ninterfaces for navigation.Our focus is on following relatively complex\ninstructions that are more akin to natural conversation than traditional\nexplicit procedural directives seen in robotics. Unlike most prior work, where\nnavigation directives are provided as imperative commands (e.g., go to the\nfridge), we examine implicit directives within conversational interactions. We\nleverage the 3D simulator AI2Thor to create complex and repeatable scenarios at\nscale, and augment it by adding complex language queries for 40 object types.\nWe demonstrate that a robot can better parse descriptive language queries than\nexisting methods by using an LLM to interpret the user interaction in the\ncontext of a list of the objects in the scene.\n","authors":["Dmitriy Rivkin","Nikhil Kakodkar","Francois Hogan","Bobak H. Baghi","Gregory Dudek"],"pdf_url":"https://arxiv.org/pdf/2307.11865v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04294v1","updated":"2023-10-06T14:52:25Z","published":"2023-10-06T14:52:25Z","title":"Graph learning in robotics: a survey","summary":"  Deep neural networks for graphs have emerged as a powerful tool for learning\non complex non-euclidean data, which is becoming increasingly common for a\nvariety of different applications. Yet, although their potential has been\nwidely recognised in the machine learning community, graph learning is largely\nunexplored for downstream tasks such as robotics applications. To fully unlock\ntheir potential, hence, we propose a review of graph neural architectures from\na robotics perspective. The paper covers the fundamentals of graph-based\nmodels, including their architecture, training procedures, and applications. It\nalso discusses recent advancements and challenges that arise in applied\nsettings, related for example to the integration of perception,\ndecision-making, and control. Finally, the paper provides an extensive review\nof various robotic applications that benefit from learning on graph structures,\nsuch as bodies and contacts modelling, robotic manipulation, action\nrecognition, fleet motion planning, and many more. This survey aims to provide\nreaders with a thorough understanding of the capabilities and limitations of\ngraph neural architectures in robotics, and to highlight potential avenues for\nfuture research.\n","authors":["Francesca Pistilli","Giuseppe Averta"],"pdf_url":"https://arxiv.org/pdf/2310.04294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04289v1","updated":"2023-10-06T14:49:34Z","published":"2023-10-06T14:49:34Z","title":"A Dataset of Anatomical Environments for Medical Robots: Modeling\n  Respiratory Deformation","summary":"  Anatomical models of a medical robot's environment can significantly help\nguide design and development of a new robotic system. These models can be used\nfor benchmarking motion planning algorithms, evaluating controllers, optimizing\nmechanical design choices, simulating procedures, and even as resources for\ndata generation. Currently, the time-consuming task of generating these\nenvironments is repeatedly performed by individual research groups and rarely\nshared broadly. This not only leads to redundant efforts, but also makes it\nchallenging to compare systems and algorithms accurately. In this work, we\npresent a collection of clinically-relevant anatomical environments for medical\nrobots operating in the lungs. Since anatomical deformation is a fundamental\nchallenge for medical robots operating in the lungs, we describe a way to model\nrespiratory deformation in these environments using patient-derived data. We\nshare the environments and deformation data publicly by adding them to the\nMedical Robotics Anatomical Dataset (Med-RAD), our public dataset of anatomical\nenvironments for medical robots.\n","authors":["Inbar Fried","Janine Hoelscher","Jason A. Akulian","Ron Alterovitz"],"pdf_url":"https://arxiv.org/pdf/2310.04289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.10888v3","updated":"2023-10-06T14:17:13Z","published":"2023-04-21T11:09:23Z","title":"Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild","summary":"  Recently, reinforcement learning has become a promising and polular solution\nfor robot legged locomotion. Compared to model-based control, reinforcement\nlearning based controllers can achieve better robustness against uncertainties\nof environments through sim-to-real learning. However, the corresponding\nlearned gaits are in general overly conservative and unatural. In this paper,\nwe propose a new framework for learning robust, agile and natural legged\nlocomotion skills over challenging terrain. We incorporate an adversarial\ntraining branch based on real animal locomotion data upon a teacher-student\ntraining pipeline for robust sim-to-real transfer. Empirical results on both\nsimulation and real world of a quadruped robot demonstrate that our proposed\nalgorithm enables robustly traversing challenging terrains such as stairs,\nrocky ground and slippery floor with only proprioceptive perception. Meanwhile,\nthe gaits are more agile, natural, and energy efficient compared to the\nbaselines. Both qualitative and quantitative results are presented in this\npaper.\n","authors":["Yikai Wang","Zheyuan Jiang","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2304.10888v3.pdf","comment":"Project page and videos:\n  https://sites.google.com/view/adaptive-multiskill-locomotion"},{"id":"http://arxiv.org/abs/2310.04271v1","updated":"2023-10-06T14:16:49Z","published":"2023-10-06T14:16:49Z","title":"Compositional Servoing by Recombining Demonstrations","summary":"  Learning-based manipulation policies from image inputs often show weak task\ntransfer capabilities. In contrast, visual servoing methods allow efficient\ntask transfer in high-precision scenarios while requiring only a few\ndemonstrations. In this work, we present a framework that formulates the visual\nservoing task as graph traversal. Our method not only extends the robustness of\nvisual servoing, but also enables multitask capability based on a few\ntask-specific demonstrations. We construct demonstration graphs by splitting\nexisting demonstrations and recombining them. In order to traverse the\ndemonstration graph in the inference case, we utilize a similarity function\nthat helps select the best demonstration for a specific task. This enables us\nto compute the shortest path through the graph. Ultimately, we show that\nrecombining demonstrations leads to higher task-respective success. We present\nextensive simulation and real-world experimental results that demonstrate the\nefficacy of our approach.\n","authors":["Max Argus","Abhijeet Nayak","Martin Büchner","Silvio Galesso","Abhinav Valada","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2310.04271v1.pdf","comment":"http://compservo.cs.uni-freiburg.de"},{"id":"http://arxiv.org/abs/2310.04266v1","updated":"2023-10-06T14:11:35Z","published":"2023-10-06T14:11:35Z","title":"DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms\n  Trajectories","summary":"  This investigation introduces a novel deep reinforcement learning-based suite\nto control floating platforms in both simulated and real-world environments.\nFloating platforms serve as versatile test-beds to emulate microgravity\nenvironments on Earth. Our approach addresses the system and environmental\nuncertainties in controlling such platforms by training policies capable of\nprecise maneuvers amid dynamic and unpredictable conditions. Leveraging\nstate-of-the-art deep reinforcement learning techniques, our suite achieves\nrobustness, adaptability, and good transferability from simulation to reality.\nOur Deep Reinforcement Learning (DRL) framework provides advantages such as\nfast training times, large-scale testing capabilities, rich visualization\noptions, and ROS bindings for integration with real-world robotic systems.\nBeyond policy development, our suite provides a comprehensive platform for\nresearchers, offering open-access at\nhttps://github.com/elharirymatteo/RANS/tree/ICRA24.\n","authors":["Matteo El-Hariry","Antoine Richard","Vivek Muralidharan","Baris Can Yalcin","Matthieu Geist","Miguel Olivares-Mendez"],"pdf_url":"https://arxiv.org/pdf/2310.04266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04259v1","updated":"2023-10-06T14:04:31Z","published":"2023-10-06T14:04:31Z","title":"A New Safety Objective for the Calibration of the Intelligent Driver\n  Model","summary":"  The intelligent driver model (IDM) is one of the most widely used\ncar-following (CF) models in recent years. The parameters of this model have\nbeen calibrated using real trajectories obtained from naturalistic driving\n,driving simulator experiment and drone data. An important aspect of the model\ncalibration process is defining the main objective of the calibration. This\nobjective, influences the objective function and the performance measure for\nthe calibration. For example, to calibrate CF models, the objective is usually\nto minimize the error in measured spacing or speed while important safety\naspects of the models such as the collision avoidance mechanisms are ignored.\nFor such models, there is no guarantee that the calibrated parameters will\npreserve the safety properties of the model since they are not explicitly taken\ninto account. To explicitly account for the safety properties during\ncalibration, this paper proposes a simple objective function which minimizes\nboth the error in the actual measured spacing (as it is currently done) and the\nerror in the dynamic safety spacing (desired minimum gap) derived from the\ncollision free property of the IDM model. The proposed objective function is\nused to calibrate two variants of the IDM using vehicle trajectories obtained\nwith drone from a Dutch highway. The calibration performance is then compared\nin terms of the error in actual spacing and time gap. The results show that the\nproposed safety objective 15 function leads to lower errors in spacing and time\ngap compared to when minimizing for only spacing and preserves collision\nproperty of the IDM.\n","authors":["Kingsley Adjenughwure","Arturo Tejada","Pedro F. V. Oliveira","Jeroen Hogema","Gerdien Klunder"],"pdf_url":"https://arxiv.org/pdf/2310.04259v1.pdf","comment":"To be submitted to the Transportation Research Records Journal"},{"id":"http://arxiv.org/abs/2310.04257v1","updated":"2023-10-06T14:02:34Z","published":"2023-10-06T14:02:34Z","title":"On Solving Close Enough Orienteering Problem with Overlapped\n  Neighborhoods","summary":"  The Close Enough Traveling Salesman Problem (CETSP) is a well-known variant\nof the classic Traveling Salesman Problem whereby the agent may complete its\nmission at any point within a target neighborhood. Heuristics based on\noverlapped neighborhoods, known as Steiner Zones (SZ), have gained attention in\naddressing CETSPs. While SZs offer effective approximations to the original\ngraph, their inherent overlap imposes constraints on the search space,\npotentially conflicting with global optimization objectives. Here we present\nthe Close Enough Orienteering Problem with Non-uniform Neighborhoods (CEOP-N),\nwhich extends CETSP by introducing variable prize attributes and non-uniform\ncost considerations for prize collection. To tackle CEOP-N, we develop a new\napproach featuring a Randomized Steiner Zone Discretization (RSZD) scheme\ncoupled with a hybrid algorithm based on Particle Swarm Optimization (PSO) and\nAnt Colony System (ACS) - CRaSZe-AntS. The RSZD scheme identifies sub-regions\nfor PSO exploration, and ACS determines the discrete visiting sequence. We\nevaluate the RSZD's discretization performance on CEOP instances derived from\nestablished CETSP instances, and compare CRaSZe-AntS against the most relevant\nstate-of-the-art heuristic focused on single-neighborhood optimization for\nCEOP. We also compare the performance of the interior search within SZs and the\nboundary search on individual neighborhoods in the context of CEOP-N. Our\nresults show CRaSZe-AntS can yield comparable solution quality with\nsignificantly reduced computation time compared to the single-neighborhood\nstrategy, where we observe an averaged 140.44% increase in prize collection and\n55.18% reduction of execution time. CRaSZe-AntS is thus highly effective in\nsolving emerging CEOP-N, examples of which include truck-and-drone delivery\nscenarios.\n","authors":["Qiuchen Qian","Yanran Wang","David Boyle"],"pdf_url":"https://arxiv.org/pdf/2310.04257v1.pdf","comment":"26 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.04232v1","updated":"2023-10-06T13:17:46Z","published":"2023-10-06T13:17:46Z","title":"The WayHome: Long-term Motion Prediction on Dynamically Scaled","summary":"  One of the key challenges for autonomous vehicles is the ability to\naccurately predict the motion of other objects in the surrounding environment,\nsuch as pedestrians or other vehicles. In this contribution, a novel motion\nforecasting approach for autonomous vehicles is developed, inspired by the work\nof Gilles et al. [1]. We predict multiple heatmaps with a neuralnetwork-based\nmodel for every traffic participant in the vicinity of the autonomous vehicle;\nwith one heatmap per timestep. The heatmaps are used as input to a novel\nsampling algorithm that extracts coordinates corresponding to the most likely\nfuture positions. We experiment with different encoders and decoders, as well\nas a comparison of two loss functions. Additionally, a new grid-scaling\ntechnique is introduced, showing further improved performance. Overall, our\napproach improves stateof-the-art miss rate performance for the\nfunction-relevant prediction interval of 3 seconds while being competitive in\nlonger prediction intervals (up to eight seconds). The evaluation is done on\nthe public 2022 Waymo motion challenge.\n","authors":["Kay Scheerer","Thomas Michalke","Juergen Mathes"],"pdf_url":"https://arxiv.org/pdf/2310.04232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04231v1","updated":"2023-10-06T13:17:38Z","published":"2023-10-06T13:17:38Z","title":"Indoor Positioning based on Active Radar Sensing and Passive Reflectors:\n  Concepts & Initial Results","summary":"  To navigate reliably in indoor environments, an industrial autonomous vehicle\nmust know its position. However, current indoor vehicle positioning\ntechnologies either lack accuracy, usability or are too expensive. Thus, we\npropose a novel concept called local reference point assisted active radar\npositioning, which is able to overcome these drawbacks. It is based on\ndistributing passive retroreflectors in the indoor environment such that each\nposition of the vehicle can be identified by a unique reflection characteristic\nregarding the reflectors. To observe these characteristics, the autonomous\nvehicle is equipped with an active radar system. On one hand, this paper\npresents the basic idea and concept of our new approach towards indoor vehicle\npositioning and especially focuses on the crucial placement of the reflectors.\nOn the other hand, it also provides a proof of concept by conducting a full\nsystem simulation including the placement of the local reference points, the\nradar-based distance estimation and the comparison of two different positioning\nmethods. It successfully demonstrates the feasibility of our proposed approach.\n","authors":["Pascal Schlachter","Zhibin Yu","Naveed Iqbal","Xiaofeng Wu","Sven Hinderer","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04181v1","updated":"2023-10-06T11:53:04Z","published":"2023-10-06T11:53:04Z","title":"DiffPrompter: Differentiable Implicit Visual Prompts for\n  Semantic-Segmentation in Adverse Conditions","summary":"  Semantic segmentation in adverse weather scenarios is a critical task for\nautonomous driving systems. While foundation models have shown promise, the\nneed for specialized adaptors becomes evident for handling more challenging\nscenarios. We introduce DiffPrompter, a novel differentiable visual and latent\nprompting mechanism aimed at expanding the learning capabilities of existing\nadaptors in foundation models. Our proposed $\\nabla$HFC image processing block\nexcels particularly in adverse weather conditions, where conventional methods\noften fall short. Furthermore, we investigate the advantages of jointly\ntraining visual and latent prompts, demonstrating that this combined approach\nsignificantly enhances performance in out-of-distribution scenarios. Our\ndifferentiable visual prompts leverage parallel and series architectures to\ngenerate prompts, effectively improving object segmentation tasks in adverse\nconditions. Through a comprehensive series of experiments and evaluations, we\nprovide empirical evidence to support the efficacy of our approach. Project\npage at https://diffprompter.github.io.\n","authors":["Sanket Kalwar","Mihir Ungarala","Shruti Jain","Aaron Monis","Krishna Reddy Konda","Sourav Garg","K Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2310.04181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04172v1","updated":"2023-10-06T11:42:07Z","published":"2023-10-06T11:42:07Z","title":"Towards 6D MCL for LiDARs in 3D TSDF Maps on Embedded Systems with GPUs","summary":"  Monte Carlo Localization is a widely used approach in the field of mobile\nrobotics. While this problem has been well studied in the 2D case, global\nlocalization in 3D maps with six degrees of freedom has so far been too\ncomputationally demanding. Hence, no mobile robot system has yet been presented\nin literature that is able to solve it in real-time. The computationally most\nintensive step is the evaluation of the sensor model, but it also offers high\nparallelization potential. This work investigates the massive parallelization\nof the evaluation of particles in truncated signed distance fields for\nthree-dimensional laser scanners on embedded GPUs. The implementation on the\nGPU is 30 times as fast and more than 50 times more energy efficient compared\nto a CPU implementation.\n","authors":["Marc Eisoldt","Alexander Mock","Mario Porrmann","Thomas Wiemann"],"pdf_url":"https://arxiv.org/pdf/2310.04172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04162v1","updated":"2023-10-06T11:21:31Z","published":"2023-10-06T11:21:31Z","title":"Light-LOAM: A Lightweight LiDAR Odometry and Mapping based on\n  Graph-Matching","summary":"  Simultaneous Localization and Mapping (SLAM) plays an important role in robot\nautonomy. Reliability and efficiency are the two most valued features for\napplying SLAM in robot applications. In this paper, we consider achieving a\nreliable LiDAR-based SLAM function in computation-limited platforms, such as\nquadrotor UAVs based on graph-based point cloud association. First, contrary to\nmost works selecting salient features for point cloud registration, we propose\na non-conspicuous feature selection strategy for reliability and robustness\npurposes. Then a two-stage correspondence selection method is used to register\nthe point cloud, which includes a KD-tree-based coarse matching followed by a\ngraph-based matching method that uses geometric consistency to vote out\nincorrect correspondences. Additionally, we propose an odometry approach where\nthe weight optimizations are guided by vote results from the aforementioned\ngeometric consistency graph. In this way, the optimization of LiDAR odometry\nrapidly converges and evaluates a fairly accurate transformation resulting in\nthe back-end module efficiently finishing the mapping task. Finally, we\nevaluate our proposed framework on the KITTI odometry dataset and real-world\nenvironments. Experiments show that our SLAM system achieves a comparative\nlevel or higher level of accuracy with more balanced computation efficiency\ncompared with the mainstream LiDAR-based SLAM solutions.\n","authors":["Shiquan Yi","Yang Lyu","Lin Hua","Quan Pan","Chunhui Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.04162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16008v2","updated":"2023-10-06T10:50:19Z","published":"2023-05-25T12:48:55Z","title":"Vision-based Safe Autonomous UAV Docking with Panoramic Sensors","summary":"  The remarkable growth of unmanned aerial vehicles (UAVs) has also sparked\nconcerns about safety measures during their missions. To advance towards safer\nautonomous aerial robots, this work presents a vision-based solution to\nensuring safe autonomous UAV landings with minimal infrastructure. During\ndocking maneuvers, UAVs pose a hazard to people in the vicinity. In this paper,\nwe propose the use of a single omnidirectional panoramic camera pointing\nupwards from a landing pad to detect and estimate the position of people around\nthe landing area. The images are processed in real-time in an embedded\ncomputer, which communicates with the onboard computer of approaching UAVs to\ntransition between landing, hovering or emergency landing states. While\nlanding, the ground camera also aids in finding an optimal position, which can\nbe required in case of low-battery or when hovering is no longer possible. We\nuse a YOLOv7-based object detection model and a XGBooxt model for localizing\nnearby people, and the open-source ROS and PX4 frameworks for communication,\ninterfacing, and control of the UAV. We present both simulation and real-world\nindoor experimental results to show the efficiency of our methods.\n","authors":["Phuoc Nguyen Thuan","Tomi Westerlund","Jorge Peña Queralta"],"pdf_url":"https://arxiv.org/pdf/2305.16008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04117v1","updated":"2023-10-06T09:32:34Z","published":"2023-10-06T09:32:34Z","title":"Maximizing Performance with Minimal Resources for Real-Time Transition\n  Detection","summary":"  Assistive devices, such as exoskeletons and prostheses, have revolutionized\nthe field of rehabilitation and mobility assistance. Efficiently detecting\ntransitions between different activities, such as walking, stair ascending and\ndescending, and sitting, is crucial for ensuring adaptive control and enhancing\nuser experience. We here present an approach for real-time transition\ndetection, aimed at optimizing the processing-time performance. By establishing\nactivity-specific threshold values through trained machine learning models, we\neffectively distinguish motion patterns and we identify transition moments\nbetween locomotion modes. This threshold-based method improves real-time\nembedded processing time performance by up to 11 times compared to machine\nlearning approaches. The efficacy of the developed finite-state machine is\nvalidated using data collected from three different measurement systems.\nMoreover, experiments with healthy participants were conducted on an active\npelvis orthosis to validate the robustness and reliability of our approach. The\nproposed algorithm achieved high accuracy in detecting transitions between\nactivities. These promising results show the robustness and reliability of the\nmethod, reinforcing its potential for integration into practical applications.\n","authors":["Zeynep Ozge Orhan","Andrea Dal Prete","Anastasia Bolotnikova","Marta Gandolla","Auke Ijspeert","Mohamed Bouri"],"pdf_url":"https://arxiv.org/pdf/2310.04117v1.pdf","comment":"Submitted for a conference. 7 pages including references, 8 figures,\n  3 tables"},{"id":"http://arxiv.org/abs/2310.04113v1","updated":"2023-10-06T09:23:25Z","published":"2023-10-06T09:23:25Z","title":"Doppler-only Single-scan 3D Vehicle Odometry","summary":"  We present a novel 3D odometry method that recovers the full motion of a\nvehicle only from a Doppler-capable range sensor. It leverages the radial\nvelocities measured from the scene, estimating the sensor's velocity from a\nsingle scan. The vehicle's 3D motion, defined by its linear and angular\nvelocities, is calculated taking into consideration its kinematic model which\nprovides a constraint between the velocity measured at the sensor frame and the\nvehicle frame.\n  Experiments carried out prove the viability of our single-sensor method\ncompared to mounting an additional IMU. Our method provides the translation of\nthe sensor, which cannot be reliably determined from an IMU, as well as its\nrotation. Its short-term accuracy and fast operation (~5ms) make it a proper\ncandidate to supply the initialization to more complex localization algorithms\nor mapping pipelines. Not only does it reduce the error of the mapper, but it\ndoes so at a comparable level of accuracy as an IMU would. All without the need\nto mount and calibrate an extra sensor on the vehicle.\n","authors":["Andres Galeote-Luque","Vladimír Kubelka","Martin Magnusson","Jose-Raul Ruiz-Sarmiento","Javier Gonzalez-Jimenez"],"pdf_url":"https://arxiv.org/pdf/2310.04113v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2202.04835v3","updated":"2023-10-06T08:29:42Z","published":"2022-02-10T05:04:25Z","title":"A robophysical model of spacetime dynamics","summary":"  Systems consisting of spheres rolling on elastic membranes have been used to\nintroduce a core conceptual idea of General Relativity (GR): how curvature\nguides the movement of matter. However, such schemes cannot accurately\nrepresent relativistic dynamics in the laboratory because of the dominance of\ndissipation and external gravitational fields. Here we demonstrate that an\n``active\" object (a wheeled robot), which moves in a straight line on level\nground and can alter its speed depending on the curvature of the deformable\nterrain it moves on, can exactly capture dynamics in curved relativistic\nspacetimes. Via the systematic study of the robot's dynamics in the radial and\norbital directions, we develop a mapping of the emergent trajectories of a\nwheeled vehicle on a spandex membrane to the motion in a curved spacetime. Our\nmapping demonstrates how the driven robot's dynamics mix space and time in a\nmetric, and shows how active particles do not necessarily follow geodesics in\nthe real space but instead follow geodesics in a fiducial spacetime. The\nmapping further reveals how parameters such as the membrane elasticity and\ninstantaneous speed allow the programming of a desired spacetime, such as the\nSchwarzschild metric near a non-rotating blackhole. Our mapping and framework\nfacilitate creation of a robophysical analog to a general relativistic system\nin the laboratory at low cost that can provide insights into active matter in\ndeformable environments and robot exploration in complex landscapes.\n","authors":["Shengkai Li","Hussain N. Gynai","Steven Tarr","Emily Alicea-Muñoz","Pablo Laguna","Gongjie Li","Daniel I. Goldman"],"pdf_url":"https://arxiv.org/pdf/2202.04835v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04044v1","updated":"2023-10-06T06:39:58Z","published":"2023-10-06T06:39:58Z","title":"Graph-based 3D Collision-distance Estimation Network with Probabilistic\n  Graph Rewiring","summary":"  We aim to solve the problem of data-driven collision-distance estimation\ngiven 3-dimensional (3D) geometries. Conventional algorithms suffer from low\naccuracy due to their reliance on limited representations, such as point\nclouds. In contrast, our previous graph-based model, GraphDistNet, achieves\nhigh accuracy using edge information but incurs higher message-passing costs\nwith growing graph size, limiting its applicability to 3D geometries. To\novercome these challenges, we propose GDN-R, a novel 3D graph-based estimation\nnetwork.GDN-R employs a layer-wise probabilistic graph-rewiring algorithm\nleveraging the differentiable Gumbel-top-K relaxation. Our method accurately\ninfers minimum distances through iterative graph rewiring and updating relevant\nembeddings. The probabilistic rewiring enables fast and robust embedding with\nrespect to unforeseen categories of geometries. Through 41,412 random benchmark\ntasks with 150 pairs of 3D objects, we show GDN-R outperforms state-of-the-art\nbaseline methods in terms of accuracy and generalizability. We also show that\nthe proposed rewiring improves the update performance reducing the size of the\nestimation model. We finally show its batch prediction and auto-differentiation\ncapabilities for trajectory optimization in both simulated and real-world\nscenarios.\n","authors":["Minjae Song","Yeseung Kim","Daehyung Park"],"pdf_url":"https://arxiv.org/pdf/2310.04044v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2302.01665v2","updated":"2023-10-06T06:26:34Z","published":"2023-02-03T11:37:20Z","title":"CVTNet: A Cross-View Transformer Network for Place Recognition Using\n  LiDAR Data","summary":"  LiDAR-based place recognition (LPR) is one of the most crucial components of\nautonomous vehicles to identify previously visited places in GPS-denied\nenvironments. Most existing LPR methods use mundane representations of the\ninput point cloud without considering different views, which may not fully\nexploit the information from LiDAR sensors. In this paper, we propose a\ncross-view transformer-based network, dubbed CVTNet, to fuse the range image\nviews (RIVs) and bird's eye views (BEVs) generated from the LiDAR data. It\nextracts correlations within the views themselves using intra-transformers and\nbetween the two different views using inter-transformers. Based on that, our\nproposed CVTNet generates a yaw-angle-invariant global descriptor for each\nlaser scan end-to-end online and retrieves previously seen places by descriptor\nmatching between the current query scan and the pre-built database. We evaluate\nour approach on three datasets collected with different sensor setups and\nenvironmental conditions. The experimental results show that our method\noutperforms the state-of-the-art LPR methods with strong robustness to\nviewpoint changes and long-time spans. Furthermore, our approach has a good\nreal-time performance that can run faster than the typical LiDAR frame rate.\nThe implementation of our method is released as open source at:\nhttps://github.com/BIT-MJY/CVTNet.\n","authors":["Junyi Ma","Guangming Xiong","Jingyi Xu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2302.01665v2.pdf","comment":"accepted by IEEE Transactions on Industrial Informatics 2023"},{"id":"http://arxiv.org/abs/2310.03953v1","updated":"2023-10-06T00:32:48Z","published":"2023-10-06T00:32:48Z","title":"CineTransfer: Controlling a Robot to Imitate Cinematographic Style from\n  a Single Example","summary":"  This work presents CineTransfer, an algorithmic framework that drives a robot\nto record a video sequence that mimics the cinematographic style of an input\nvideo. We propose features that abstract the aesthetic style of the input\nvideo, so the robot can transfer this style to a scene with visual details that\nare significantly different from the input video. The framework builds upon\nCineMPC, a tool that allows users to control cinematographic features, like\nsubjects' position on the image and the depth of field, by manipulating the\nintrinsics and extrinsics of a cinematographic camera. However, CineMPC\nrequires a human expert to specify the desired style of the shot (composition,\ncamera motion, zoom, focus, etc). CineTransfer bridges this gap, aiming a fully\nautonomous cinematographic platform. The user chooses a single input video as a\nstyle guide. CineTransfer extracts and optimizes two important style features,\nthe composition of the subject in the image and the scene depth of field, and\nprovides instructions for CineMPC to control the robot to record an output\nsequence that matches these features as closely as possible. In contrast with\nother style transfer methods, our approach is a lightweight and portable\nframework which does not require deep network training or extensive datasets.\nExperiments with real and simulated videos demonstrate the system's ability to\nanalyze and transfer style between recordings, and are available in the\nsupplementary video.\n","authors":["Pablo Pueyo","Eduardo Montijano","Ana C. Murillo","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2310.03953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10865v3","updated":"2023-10-06T00:03:17Z","published":"2023-03-20T04:55:56Z","title":"Rotating Objects via In-Hand Pivoting using Vision, Force and Touch","summary":"  We propose a robotic manipulation system that can pivot objects on a surface\nusing vision, wrist force and tactile sensing. We aim to control the rotation\nof an object around the grip point of a parallel gripper by allowing rotational\nslip, while maintaining a desired wrist force profile. Our approach runs an\nend-effector position controller and a gripper width controller concurrently in\na closed loop. The position controller maintains a desired force using vision\nand wrist force. The gripper controller uses tactile sensing to keep the grip\nfirm enough to prevent translational slip, but loose enough to induce\nrotational slip. Our sensor-based control approach relies on matching a desired\nforce profile derived from object dimensions and weight and vision-based\nmonitoring of the object pose. The gripper controller uses tactile sensors to\ndetect and prevent translational slip by tightening the grip when needed.\nExperimental results where the robot was tasked with rotating cuboid objects 90\ndegrees show that the multi-modal pivoting approach was able to rotate the\nobjects without causing lift or slip, and was more energy-efficient compared to\nusing a single sensor modality and to pick-and-place. While our work\ndemonstrated the benefit of multi-modal sensing for the pivoting task, further\nwork is needed to generalize our approach to any given object.\n","authors":["Shiyu Xu","Tianyuan Liu","Michael Wong","Dana Kulić","Akansel Cosgun"],"pdf_url":"https://arxiv.org/pdf/2303.10865v3.pdf","comment":"8 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2310.04617v1","updated":"2023-10-06T22:37:34Z","published":"2023-10-06T22:37:34Z","title":"SlotGNN: Unsupervised Discovery of Multi-Object Representations and\n  Visual Dynamics","summary":"  Learning multi-object dynamics from visual data using unsupervised techniques\nis challenging due to the need for robust, object representations that can be\nlearned through robot interactions. This paper presents a novel framework with\ntwo new architectures: SlotTransport for discovering object representations\nfrom RGB images and SlotGNN for predicting their collective dynamics from RGB\nimages and robot interactions. Our SlotTransport architecture is based on slot\nattention for unsupervised object discovery and uses a feature transport\nmechanism to maintain temporal alignment in object-centric representations.\nThis enables the discovery of slots that consistently reflect the composition\nof multi-object scenes. These slots robustly bind to distinct objects, even\nunder heavy occlusion or absence. Our SlotGNN, a novel unsupervised graph-based\ndynamics model, predicts the future state of multi-object scenes. SlotGNN\nlearns a graph representation of the scene using the discovered slots from\nSlotTransport and performs relational and spatial reasoning to predict the\nfuture appearance of each slot conditioned on robot actions. We demonstrate the\neffectiveness of SlotTransport in learning object-centric features that\naccurately encode both visual and positional information. Further, we highlight\nthe accuracy of SlotGNN in downstream robotic tasks, including challenging\nmulti-object rearrangement and long-horizon prediction. Finally, our\nunsupervised approach proves effective in the real world. With only minimal\nadditional data, our framework robustly predicts slots and their corresponding\ndynamics in real-world control tasks.\n","authors":["Alireza Rezazadeh","Athreyi Badithela","Karthik Desingh","Changhyun Choi"],"pdf_url":"https://arxiv.org/pdf/2310.04617v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15620v2","updated":"2023-10-06T21:39:24Z","published":"2023-03-27T22:29:27Z","title":"Optimizing Lead Time in Fall Detection for a Planar Bipedal Robot","summary":"  For legged robots to operate in complex terrains, they must be robust to the\ndisturbances and uncertainties they encounter. This paper contributes to\nenhancing robustness through the design of fall detection/prediction algorithms\nthat will provide sufficient lead time for corrective motions to be taken.\nFalls can be caused by abrupt (fast-acting), incipient (slow-acting), or\nintermittent (non-continuous) faults. Early fall detection is a challenging\ntask due to the masking effects of controllers (through their disturbance\nattenuation actions), the inverse relationship between lead time and false\npositive rates, and the temporal behavior of the faults/underlying factors. In\nthis paper, we propose a fall detection algorithm that is capable of detecting\nboth incipient and abrupt faults while maximizing lead time and meeting desired\nthresholds on the false positive and negative rates.\n","authors":["M. Eva Mungai","Jessy Grizzle"],"pdf_url":"https://arxiv.org/pdf/2303.15620v2.pdf","comment":"\\c{opyright} 2023 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2310.04590v1","updated":"2023-10-06T21:11:52Z","published":"2023-10-06T21:11:52Z","title":"Deep Model Predictive Optimization","summary":"  A major challenge in robotics is to design robust policies which enable\ncomplex and agile behaviors in the real world. On one end of the spectrum, we\nhave model-free reinforcement learning (MFRL), which is incredibly flexible and\ngeneral but often results in brittle policies. In contrast, model predictive\ncontrol (MPC) continually re-plans at each time step to remain robust to\nperturbations and model inaccuracies. However, despite its real-world\nsuccesses, MPC often under-performs the optimal strategy. This is due to model\nquality, myopic behavior from short planning horizons, and approximations due\nto computational constraints. And even with a perfect model and enough compute,\nMPC can get stuck in bad local optima, depending heavily on the quality of the\noptimization algorithm. To this end, we propose Deep Model Predictive\nOptimization (DMPO), which learns the inner-loop of an MPC optimization\nalgorithm directly via experience, specifically tailored to the needs of the\ncontrol problem. We evaluate DMPO on a real quadrotor agile trajectory tracking\ntask, on which it improves performance over a baseline MPC algorithm for a\ngiven computational budget. It can outperform the best MPC algorithm by up to\n27% with fewer samples and an end-to-end policy trained with MFRL by 19%.\nMoreover, because DMPO requires fewer samples, it can also achieve these\nbenefits with 4.3X less memory. When we subject the quadrotor to turbulent wind\nfields with an attached drag plate, DMPO can adapt zero-shot while still\noutperforming all baselines. Additional results can be found at\nhttps://tinyurl.com/mr2ywmnw.\n","authors":["Jacob Sacks","Rwik Rana","Kevin Huang","Alex Spitzer","Guanya Shi","Byron Boots"],"pdf_url":"https://arxiv.org/pdf/2310.04590v1.pdf","comment":"Main paper is 6 pages with 4 figures and 1 table. Code available at:\n  https://github.com/jisacks/dmpo"},{"id":"http://arxiv.org/abs/2310.04582v1","updated":"2023-10-06T20:48:43Z","published":"2023-10-06T20:48:43Z","title":"Universal Humanoid Motion Representations for Physics-Based Control","summary":"  We present a universal motion representation that encompasses a comprehensive\nrange of motor skills for physics-based humanoid control. Due to the\nhigh-dimensionality of humanoid control as well as the inherent difficulties in\nreinforcement learning, prior methods have focused on learning skill embeddings\nfor a narrow range of movement styles (e.g. locomotion, game characters) from\nspecialized motion datasets. This limited scope hampers its applicability in\ncomplex tasks. Our work closes this gap, significantly increasing the coverage\nof motion representation space. To achieve this, we first learn a motion\nimitator that can imitate all of human motion from a large, unstructured motion\ndataset. We then create our motion representation by distilling skills directly\nfrom the imitator. This is achieved using an encoder-decoder structure with a\nvariational information bottleneck. Additionally, we jointly learn a prior\nconditioned on proprioception (humanoid's own pose and velocities) to improve\nmodel expressiveness and sampling efficiency for downstream tasks. Sampling\nfrom the prior, we can generate long, stable, and diverse human motions. Using\nthis latent space for hierarchical RL, we show that our policies solve tasks\nusing natural and realistic human behavior. We demonstrate the effectiveness of\nour motion representation by solving generative tasks (e.g. strike, terrain\ntraversal) and motion tracking using VR controllers.\n","authors":["Zhengyi Luo","Jinkun Cao","Josh Merel","Alexander Winkler","Jing Huang","Kris Kitani","Weipeng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.04582v1.pdf","comment":"Project page: https://zhengyiluo.github.io/PULSE/"},{"id":"http://arxiv.org/abs/2310.04572v1","updated":"2023-10-06T20:27:30Z","published":"2023-10-06T20:27:30Z","title":"LIVE: Lidar Informed Visual Search for Multiple Objects with Multiple\n  Robots","summary":"  This paper introduces LIVE: Lidar Informed Visual Search focused on the\nproblem of multi-robot (MR) planning and execution for robust visual detection\nof multiple objects. We perform extensive real-world experiments with a\ntwo-robot team in an indoor apartment setting. LIVE acts as a perception module\nthat detects unmapped obstacles, or Short Term Features (STFs), in Lidar\nobservations. STFs are filtered, resulting in regions to be visually inspected\nby modifying plans online. Lidar Coverage Path Planning (CPP) is employed for\ngenerating highly efficient global plans for heterogeneous robot teams.\nFinally, we present a data model and a demonstration dataset, which can be\nfound by visiting our project website\nhttps://sites.google.com/view/live-iros2023/home.\n","authors":["Ryan Gupta","Minkyu Kim","Juliana T Rodriguez","Kyle Morgenstein","Luis Sentis"],"pdf_url":"https://arxiv.org/pdf/2310.04572v1.pdf","comment":"4 pages + references; 6 figures"},{"id":"http://arxiv.org/abs/2310.04566v1","updated":"2023-10-06T20:13:07Z","published":"2023-10-06T20:13:07Z","title":"Knolling bot: A Transformer-based Approach to Organizing a Messy Table","summary":"  In this study, we propose an approach to equip domestic robots with the\nability to perform simple household tidying tasks. We focus specifically on\n'knolling,' an activity related to organizing scattered items into neat and\nspace-efficient arrangements. Unlike the uniformity of industrial environments,\nhousehold settings present unique challenges due to their diverse array of\nitems and the subjectivity of tidiness. Here, we draw inspiration from natural\nlanguage processing (NLP) and utilize a transformer-based approach that\npredicts the next position of an item in a sequence of neatly positioned items.\nWe integrate the knolling model with a visual perception model and a physical\nrobot arm to demonstrate a machine that declutters and organizes a dozen\nfreeform items of various shapes and sizes.\n","authors":["Yuhang Hu","Zhizhuo Zhang","Ruibo Liu","Philippe Wyder","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2310.04566v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2305.08643v2","updated":"2023-10-06T19:26:02Z","published":"2023-05-15T13:39:40Z","title":"Quadratic Programming-based Reference Spreading Control for Dual-Arm\n  Robotic Manipulation with Planned Simultaneous Impacts","summary":"  With the aim of further enabling the exploitation of intentional impacts in\nrobotic manipulation, a control framework is presented that directly tackles\nthe challenges posed by tracking control of robotic manipulators that are\ntasked to perform nominally simultaneous impacts. This framework is an\nextension of the reference spreading control framework, in which overlapping\nante- and post-impact references that are consistent with impact dynamics are\ndefined. In this work, such a reference is constructed starting from a\nteleoperation-based approach. By using the corresponding ante- and post-impact\ncontrol modes in the scope of a quadratic programming control approach, peaking\nof the velocity error and control inputs due to impacts is avoided while\nmaintaining high tracking performance. With the inclusion of a novel interim\nmode, we aim to also avoid input peaks and steps when uncertainty in the\nenvironment causes a series of unplanned single impacts to occur rather than\nthe planned simultaneous impact. This work in particular presents for the first\ntime an experimental evaluation of reference spreading control on a robotic\nsetup, showcasing its robustness against uncertainty in the environment\ncompared to three baseline control approaches.\n","authors":["Jari van Steen","Gijs van den Brandt","Nathan van de Wouw","Jens Kober","Alessandro Saccon"],"pdf_url":"https://arxiv.org/pdf/2305.08643v2.pdf","comment":"14 pages, 10 figures. Submitted for publication to IEEE Transactions\n  on Robotics (T-RO) in September, 2023"},{"id":"http://arxiv.org/abs/2310.04538v1","updated":"2023-10-06T19:06:07Z","published":"2023-10-06T19:06:07Z","title":"mCLARI: a shape-morphing insect-scale robot capable of omnidirectional\n  terrain-adaptive locomotion in laterally confined spaces","summary":"  Soft compliant microrobots have the potential to deliver significant societal\nimpact when deployed in applications such as search and rescue. In this\nresearch we present mCLARI, a body compliant quadrupedal microrobot of 20mm\nneutral body length and 0.97g, improving on its larger predecessor, CLARI. This\nrobot has four independently actuated leg modules with 2 degrees of freedom,\neach driven by piezoelectric actuators. The legs are interconnected in a closed\nkinematic chain via passive body joints, enabling passive body compliance for\nshape adaptation to external constraints. Despite scaling its larger\npredecessor down to 60 percent in length and 38 percent in mass, mCLARI\nmaintains 80 percent of the actuation power to achieve high agility.\nAdditionally, we demonstrate the new capability of passively shape-morphing\nmCLARI - omnidirectional laterally confined locomotion - and experimentally\nquantify its running performance achieving a new unconstrained top speed of 3\nbodylengths/s (60 mms-1). Leveraging passive body compliance, mCLARI can\nnavigate through narrow spaces with a body compression ratio of up to 1.5x the\nneutral body shape.\n","authors":["Heiko Kabutz","Alexander Hedrick","Parker McDonnell","Kaushik Jayaram"],"pdf_url":"https://arxiv.org/pdf/2310.04538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04517v1","updated":"2023-10-06T18:26:09Z","published":"2023-10-06T18:26:09Z","title":"Domain Randomization for Sim2real Transfer of Automatically Generated\n  Grasping Datasets","summary":"  Robotic grasping refers to making a robotic system pick an object by applying\nforces and torques on its surface. Many recent studies use data-driven\napproaches to address grasping, but the sparse reward nature of this task made\nthe learning process challenging to bootstrap. To avoid constraining the\noperational space, an increasing number of works propose grasping datasets to\nlearn from. But most of them are limited to simulations. The present paper\ninvestigates how automatically generated grasps can be exploited in the real\nworld. More than 7000 reach-and-grasp trajectories have been generated with\nQuality-Diversity (QD) methods on 3 different arms and grippers, including\nparallel fingers and a dexterous hand, and tested in the real world. Conducted\nanalysis on the collected measure shows correlations between several Domain\nRandomization-based quality criteria and sim-to-real transferability. Key\nchallenges regarding the reality gap for grasping have been identified,\nstressing matters on which researchers on grasping should focus in the future.\nA QD approach has finally been proposed for making grasps more robust to domain\nrandomization, resulting in a transfer ratio of 84% on the Franka Research 3\narm.\n","authors":["Johann Huber","François Hélénon","Hippolyte Watrelot","Faiz Ben Amar","Stéphane Doncieux"],"pdf_url":"https://arxiv.org/pdf/2310.04517v1.pdf","comment":"6 pages, 7 figures, draft version"},{"id":"http://arxiv.org/abs/2310.00156v2","updated":"2023-10-06T18:09:26Z","published":"2023-09-29T21:32:42Z","title":"Learning Generalizable Tool-use Skills through Trajectory Generation","summary":"  Autonomous systems that efficiently utilize tools can assist humans in\ncompleting many common tasks such as cooking and cleaning. However, current\nsystems fall short of matching human-level of intelligence in terms of adapting\nto novel tools. Prior works based on affordance often make strong assumptions\nabout the environments and cannot scale to more complex, contact-rich tasks. In\nthis work, we tackle this challenge and explore how agents can learn to use\npreviously unseen tools to manipulate deformable objects. We propose to learn a\ngenerative model of the tool-use trajectories as a sequence of point clouds,\nwhich generalizes to different tool shapes. Given any novel tool, we first\ngenerate a tool-use trajectory and then optimize the sequence of tool poses to\nalign with the generated trajectory. We train a single model for four different\nchallenging deformable object manipulation tasks. Our model is trained with\ndemonstration data from just a single tool for each task and is able to\ngeneralize to various novel tools, significantly outperforming baselines.\nAdditional materials can be found on our project website:\nhttps://sites.google.com/view/toolgen.\n","authors":["Carl Qi","Sarthak Shetty","Xingyu Lin","David Held"],"pdf_url":"https://arxiv.org/pdf/2310.00156v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.04416v1","updated":"2023-10-06T17:58:26Z","published":"2023-10-06T17:58:26Z","title":"Alice Benchmarks: Connecting Real World Object Re-Identification with\n  the Synthetic","summary":"  For object re-identification (re-ID), learning from synthetic data has become\na promising strategy to cheaply acquire large-scale annotated datasets and\neffective models, with few privacy concerns. Many interesting research problems\narise from this strategy, e.g., how to reduce the domain gap between synthetic\nsource and real-world target. To facilitate developing more new approaches in\nlearning from synthetic data, we introduce the Alice benchmarks, large-scale\ndatasets providing benchmarks as well as evaluation protocols to the research\ncommunity. Within the Alice benchmarks, two object re-ID tasks are offered:\nperson and vehicle re-ID. We collected and annotated two challenging real-world\ntarget datasets: AlicePerson and AliceVehicle, captured under various\nilluminations, image resolutions, etc. As an important feature of our real\ntarget, the clusterability of its training set is not manually guaranteed to\nmake it closer to a real domain adaptation test scenario. Correspondingly, we\nreuse existing PersonX and VehicleX as synthetic source domains. The primary\ngoal is to train models from synthetic data that can work effectively in the\nreal world. In this paper, we detail the settings of Alice benchmarks, provide\nan analysis of existing commonly-used domain adaptation methods, and discuss\nsome interesting future directions. An online server will be set up for the\ncommunity to evaluate methods conveniently and fairly.\n","authors":["Xiaoxiao Sun","Yue Yao","Shengjin Wang","Hongdong Li","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.04416v1.pdf","comment":"9 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2310.04414v1","updated":"2023-10-06T17:58:20Z","published":"2023-10-06T17:58:20Z","title":"CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model\n  Generalization Analysis","summary":"  Analyzing model performance in various unseen environments is a critical\nresearch problem in the machine learning community. To study this problem, it\nis important to construct a testbed with out-of-distribution test sets that\nhave broad coverage of environmental discrepancies. However, existing testbeds\ntypically either have a small number of domains or are synthesized by image\ncorruptions, hindering algorithm design that demonstrates real-world\neffectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of\n180 datasets collected by prompting image search engines and diffusion models\nin various ways. Generally sized between 300 and 8,000 images, the datasets\ncontain natural images, cartoons, certain colors, or objects that do not\nnaturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen\nthe understanding of two generalization tasks: domain generalization and model\naccuracy prediction in various out-of-distribution environments. We conduct\nextensive benchmarking and comparison experiments and show that CIFAR-10-W\noffers new and interesting insights inherent to these tasks. We also discuss\nother fields that would benefit from CIFAR-10-W.\n","authors":["Xiaoxiao Sun","Xingjian Leng","Zijian Wang","Yang Yang","Zi Huang","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.04414v1.pdf","comment":"9 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.04412v1","updated":"2023-10-06T17:57:50Z","published":"2023-10-06T17:57:50Z","title":"FedConv: Enhancing Convolutional Neural Networks for Handling Data\n  Heterogeneity in Federated Learning","summary":"  Federated learning (FL) is an emerging paradigm in machine learning, where a\nshared model is collaboratively learned using data from multiple devices to\nmitigate the risk of data leakage. While recent studies posit that Vision\nTransformer (ViT) outperforms Convolutional Neural Networks (CNNs) in\naddressing data heterogeneity in FL, the specific architectural components that\nunderpin this advantage have yet to be elucidated. In this paper, we\nsystematically investigate the impact of different architectural elements, such\nas activation functions and normalization layers, on the performance within\nheterogeneous FL. Through rigorous empirical analyses, we are able to offer the\nfirst-of-its-kind general guidance on micro-architecture design principles for\nheterogeneous FL.\n  Intriguingly, our findings indicate that with strategic architectural\nmodifications, pure CNNs can achieve a level of robustness that either matches\nor even exceeds that of ViTs when handling heterogeneous data clients in FL.\nAdditionally, our approach is compatible with existing FL techniques and\ndelivers state-of-the-art solutions across a broad spectrum of FL benchmarks.\nThe code is publicly available at https://github.com/UCSC-VLAA/FedConv\n","authors":["Peiran Xu","Zeyu Wang","Jieru Mei","Liangqiong Qu","Alan Yuille","Cihang Xie","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.04412v1.pdf","comment":"9 pages, 6 figures. Equal contribution by P. Xu and Z. Wang"},{"id":"http://arxiv.org/abs/2310.04406v1","updated":"2023-10-06T17:55:11Z","published":"2023-10-06T17:55:11Z","title":"Language Agent Tree Search Unifies Reasoning Acting and Planning in\n  Language Models","summary":"  While large language models (LLMs) have demonstrated impressive performance\non a range of decision-making tasks, they rely on simple acting processes and\nfall short of broad deployment as autonomous agents. We introduce LATS\n(Language Agent Tree Search), a general framework that synergizes the\ncapabilities of LLMs in planning, acting, and reasoning. Drawing inspiration\nfrom Monte Carlo tree search in model-based reinforcement learning, LATS\nemploys LLMs as agents, value functions, and optimizers, repurposing their\nlatent strengths for enhanced decision-making. What is crucial in this method\nis the use of an environment for external feedback, which offers a more\ndeliberate and adaptive problem-solving mechanism that moves beyond the\nlimitations of existing techniques. Our experimental evaluation across diverse\ndomains, such as programming, HotPotQA, and WebShop, illustrates the\napplicability of LATS for both reasoning and acting. In particular, LATS\nachieves 94.4\\% for programming on HumanEval with GPT-4 and an average score of\n75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness\nand generality of our method.\n","authors":["Andy Zhou","Kai Yan","Michal Shlapentokh-Rothman","Haohan Wang","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04406v1.pdf","comment":"Website and code can be found at\n  https://andyz245.github.io/LanguageAgentTreeSearch"},{"id":"http://arxiv.org/abs/2309.16108v2","updated":"2023-10-06T17:52:34Z","published":"2023-09-28T02:20:59Z","title":"Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words","summary":"  Vision Transformer (ViT) has emerged as a powerful architecture in the realm\nof modern computer vision. However, its application in certain imaging fields,\nsuch as microscopy and satellite imaging, presents unique challenges. In these\ndomains, images often contain multiple channels, each carrying semantically\ndistinct and independent information. Furthermore, the model must demonstrate\nrobustness to sparsity in input channels, as they may not be densely available\nduring training or testing. In this paper, we propose a modification to the ViT\narchitecture that enhances reasoning across the input channels and introduce\nHierarchical Channel Sampling (HCS) as an additional regularization technique\nto ensure robustness when only partial channels are presented during test time.\nOur proposed model, ChannelViT, constructs patch tokens independently from each\ninput channel and utilizes a learnable channel embedding that is added to the\npatch tokens, similar to positional embeddings. We evaluate the performance of\nChannelViT on ImageNet, JUMP-CP (microscopy cell imaging), and So2Sat\n(satellite imaging). Our results show that ChannelViT outperforms ViT on\nclassification tasks and generalizes well, even when a subset of input channels\nis used during testing. Across our experiments, HCS proves to be a powerful\nregularizer, independent of the architecture employed, suggesting itself as a\nstraightforward technique for robust ViT training. Lastly, we find that\nChannelViT generalizes effectively even when there is limited access to all\nchannels during training, highlighting its potential for multi-channel imaging\nunder real-world conditions with sparse sensors.\n","authors":["Yujia Bao","Srinivasan Sivanandan","Theofanis Karaletsos"],"pdf_url":"https://arxiv.org/pdf/2309.16108v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.14358v3","updated":"2023-10-06T17:34:25Z","published":"2022-10-25T21:54:26Z","title":"Multi-Domain Long-Tailed Learning by Augmenting Disentangled\n  Representations","summary":"  There is an inescapable long-tailed class-imbalance issue in many real-world\nclassification problems. Current methods for addressing this problem only\nconsider scenarios where all examples come from the same distribution. However,\nin many cases, there are multiple domains with distinct class imbalance. We\nstudy this multi-domain long-tailed learning problem and aim to produce a model\nthat generalizes well across all classes and domains. Towards that goal, we\nintroduce TALLY, a method that addresses this multi-domain long-tailed learning\nproblem. Built upon a proposed selective balanced sampling strategy, TALLY\nachieves this by mixing the semantic representation of one example with the\ndomain-associated nuisances of another, producing a new representation for use\nas data augmentation. To improve the disentanglement of semantic\nrepresentations, TALLY further utilizes a domain-invariant class prototype that\naverages out domain-specific effects. We evaluate TALLY on several benchmarks\nand real-world datasets and find that it consistently outperforms other\nstate-of-the-art methods in both subpopulation and domain shift. Our code and\ndata have been released at https://github.com/huaxiuyao/TALLY.\n","authors":["Xinyu Yang","Huaxiu Yao","Allan Zhou","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2210.14358v3.pdf","comment":"Accepted by TMLR"},{"id":"http://arxiv.org/abs/2310.04378v1","updated":"2023-10-06T17:11:58Z","published":"2023-10-06T17:11:58Z","title":"Latent Consistency Models: Synthesizing High-Resolution Images with\n  Few-Step Inference","summary":"  Latent Diffusion models (LDMs) have achieved remarkable results in\nsynthesizing high-resolution images. However, the iterative sampling process is\ncomputationally intensive and leads to slow generation. Inspired by Consistency\nModels (song et al.), we propose Latent Consistency Models (LCMs), enabling\nswift inference with minimal steps on any pre-trained LDMs, including Stable\nDiffusion (rombach et al). Viewing the guided reverse diffusion process as\nsolving an augmented probability flow ODE (PF-ODE), LCMs are designed to\ndirectly predict the solution of such ODE in latent space, mitigating the need\nfor numerous iterations and allowing rapid, high-fidelity sampling. Efficiently\ndistilled from pre-trained classifier-free guided diffusion models, a\nhigh-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training.\nFurthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method\nthat is tailored for fine-tuning LCMs on customized image datasets. Evaluation\non the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve\nstate-of-the-art text-to-image generation performance with few-step inference.\nProject Page: https://latent-consistency-models.github.io/\n","authors":["Simian Luo","Yiqin Tan","Longbo Huang","Jian Li","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.04378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04360v1","updated":"2023-10-06T16:33:24Z","published":"2023-10-06T16:33:24Z","title":"SwimXYZ: A large-scale dataset of synthetic swimming motions and videos","summary":"  Technologies play an increasingly important role in sports and become a real\ncompetitive advantage for the athletes who benefit from it. Among them, the use\nof motion capture is developing in various sports to optimize sporting\ngestures. Unfortunately, traditional motion capture systems are expensive and\nconstraining. Recently developed computer vision-based approaches also struggle\nin certain sports, like swimming, due to the aquatic environment. One of the\nreasons for the gap in performance is the lack of labeled datasets with\nswimming videos. In an attempt to address this issue, we introduce SwimXYZ, a\nsynthetic dataset of swimming motions and videos. SwimXYZ contains 3.4 million\nframes annotated with ground truth 2D and 3D joints, as well as 240 sequences\nof swimming motions in the SMPL parameters format. In addition to making this\ndataset publicly available, we present use cases for SwimXYZ in swimming stroke\nclustering and 2D pose estimation.\n","authors":["Fiche Guénolé","Sevestre Vincent","Gonzalez-Barral Camila","Leglaive Simon","Séguier Renaud"],"pdf_url":"https://arxiv.org/pdf/2310.04360v1.pdf","comment":"ACM MIG 2023"},{"id":"http://arxiv.org/abs/2304.00570v3","updated":"2023-10-06T16:20:41Z","published":"2023-04-02T16:39:59Z","title":"FedFTN: Personalized Federated Learning with Deep Feature Transformation\n  Network for Multi-institutional Low-count PET Denoising","summary":"  Low-count PET is an efficient way to reduce radiation exposure and\nacquisition time, but the reconstructed images often suffer from low\nsignal-to-noise ratio (SNR), thus affecting diagnosis and other downstream\ntasks. Recent advances in deep learning have shown great potential in improving\nlow-count PET image quality, but acquiring a large, centralized, and diverse\ndataset from multiple institutions for training a robust model is difficult due\nto privacy and security concerns of patient data. Moreover, low-count PET data\nat different institutions may have different data distribution, thus requiring\npersonalized models. While previous federated learning (FL) algorithms enable\nmulti-institution collaborative training without the need of aggregating local\ndata, addressing the large domain shift in the application of\nmulti-institutional low-count PET denoising remains a challenge and is still\nhighly under-explored. In this work, we propose FedFTN, a personalized\nfederated learning strategy that addresses these challenges. FedFTN uses a\nlocal deep feature transformation network (FTN) to modulate the feature outputs\nof a globally shared denoising network, enabling personalized low-count PET\ndenoising for each institution. During the federated learning process, only the\ndenoising network's weights are communicated and aggregated, while the FTN\nremains at the local institutions for feature transformation. We evaluated our\nmethod using a large-scale dataset of multi-institutional low-count PET imaging\ndata from three medical centers located across three continents, and showed\nthat FedFTN provides high-quality low-count PET images, outperforming previous\nbaseline FL reconstruction methods across all low-count levels at all three\ninstitutions.\n","authors":["Bo Zhou","Huidong Xie","Qiong Liu","Xiongchao Chen","Xueqi Guo","Zhicheng Feng","Jun Hou","S. Kevin Zhou","Biao Li","Axel Rominger","Kuangyu Shi","James S. Duncan","Chi Liu"],"pdf_url":"https://arxiv.org/pdf/2304.00570v3.pdf","comment":"13 pages, 6 figures, Accepted at Medical Image Analysis Journal\n  (MedIA)"},{"id":"http://arxiv.org/abs/2306.02099v2","updated":"2023-10-06T15:46:49Z","published":"2023-06-03T12:23:17Z","title":"NeuroSURF: Neural Uncertainty-aware Robust Surface Reconstruction","summary":"  Neural implicit functions have become popular for representing surfaces\nbecause they offer an adaptive resolution and support arbitrary topologies.\nWhile previous works rely on ground truth point clouds, they often ignore the\neffect of input quality and sampling methods on the reconstruction. In this\npaper, we introduce NeuroSURF, which generates significantly improved\nqualitative and quantitative reconstructions driven by a novel sampling and\ninterpolation technique. We show that employing a sampling technique that\nconsiders the geometric characteristics of inputs can enhance the training\nprocess. To this end, we introduce a strategy that efficiently computes\ndifferentiable geometric features, namely, mean curvatures, to augment the\nsampling phase during the training period. Moreover, we augment the neural\nimplicit surface representation with uncertainty, which offers insights into\nthe occupancy and reliability of the output signed distance value, thereby\nexpanding representation capabilities into open surfaces. Finally, we\ndemonstrate that NeuroSURF leads to state-of-the-art reconstructions on both\nsynthetic and real-world data.\n","authors":["Lu Sang","Abhishek Saroha","Maolin Gao","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2306.02099v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2210.11717v2","updated":"2023-10-06T15:17:59Z","published":"2022-10-21T03:58:43Z","title":"A Survey of Dataset Refinement for Problems in Computer Vision Datasets","summary":"  Large-scale datasets have played a crucial role in the advancement of\ncomputer vision. However, they often suffer from problems such as class\nimbalance, noisy labels, dataset bias, or high resource costs, which can\ninhibit model performance and reduce trustworthiness. With the advocacy of\ndata-centric research, various data-centric solutions have been proposed to\nsolve the dataset problems mentioned above. They improve the quality of\ndatasets by re-organizing them, which we call dataset refinement. In this\nsurvey, we provide a comprehensive and structured overview of recent advances\nin dataset refinement for problematic computer vision datasets. Firstly, we\nsummarize and analyze the various problems encountered in large-scale computer\nvision datasets. Then, we classify the dataset refinement algorithms into three\ncategories based on the refinement process: data sampling, data subset\nselection, and active learning. In addition, we organize these dataset\nrefinement methods according to the addressed data problems and provide a\nsystematic comparative description. We point out that these three types of\ndataset refinement have distinct advantages and disadvantages for dataset\nproblems, which informs the choice of the data-centric method appropriate to a\nparticular research objective. Finally, we summarize the current literature and\npropose potential future research topics.\n","authors":["Zhijing Wan","Zhixiang Wang","CheukTing Chung","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2210.11717v2.pdf","comment":"33 pages, 10 figures, to be published in ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2310.04311v1","updated":"2023-10-06T15:17:45Z","published":"2023-10-06T15:17:45Z","title":"Distributed Deep Joint Source-Channel Coding with Decoder-Only Side\n  Information","summary":"  We consider low-latency image transmission over a noisy wireless channel when\ncorrelated side information is present only at the receiver side (the Wyner-Ziv\nscenario). In particular, we are interested in developing practical schemes\nusing a data-driven joint source-channel coding (JSCC) approach, which has been\npreviously shown to outperform conventional separation-based approaches in the\npractical finite blocklength regimes, and to provide graceful degradation with\nchannel quality. We propose a novel neural network architecture that\nincorporates the decoder-only side information at multiple stages at the\nreceiver side. Our results demonstrate that the proposed method succeeds in\nintegrating the side information, yielding improved performance at all channel\nnoise levels in terms of the various distortion criteria considered here,\nespecially at low channel signal-to-noise ratios (SNRs) and small bandwidth\nratios (BRs). We also provide the source code of the proposed method to enable\nfurther research and reproducibility of the results.\n","authors":["Selim F. Yilmaz","Ezgi Ozyilkan","Deniz Gunduz","Elza Erkip"],"pdf_url":"https://arxiv.org/pdf/2310.04311v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.04306v1","updated":"2023-10-06T15:05:41Z","published":"2023-10-06T15:05:41Z","title":"Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware\n  Learning","summary":"  Group-level emotion recognition (GER) is an inseparable part of human\nbehavior analysis, aiming to recognize an overall emotion in a multi-person\nscene. However, the existing methods are devoted to combing diverse emotion\ncues while ignoring the inherent uncertainties under unconstrained\nenvironments, such as congestion and occlusion occurring within a group.\nAdditionally, since only group-level labels are available, inconsistent emotion\npredictions among individuals in one group can confuse the network. In this\npaper, we propose an uncertainty-aware learning (UAL) method to extract more\nrobust representations for GER. By explicitly modeling the uncertainty of each\nindividual, we utilize stochastic embedding drawn from a Gaussian distribution\ninstead of deterministic point embedding. This representation captures the\nprobabilities of different emotions and generates diverse predictions through\nthis stochasticity during the inference stage. Furthermore,\nuncertainty-sensitive scores are adaptively assigned as the fusion weights of\nindividuals' face within each group. Moreover, we develop an image enhancement\nmodule to enhance the model's robustness against severe noise. The overall\nthree-branch model, encompassing face, object, and scene component, is guided\nby a proportional-weighted fusion strategy and integrates the proposed\nuncertainty-aware method to produce the final group-level output. Experimental\nresults demonstrate the effectiveness and generalization ability of our method\nacross three widely used databases.\n","authors":["Qing Zhu","Qirong Mao","Jialin Zhang","Xiaohua Huang","Wenming Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.04306v1.pdf","comment":"11 pages,3 figures"},{"id":"http://arxiv.org/abs/2310.04299v1","updated":"2023-10-06T15:01:32Z","published":"2023-10-06T15:01:32Z","title":"Convergent ADMM Plug and Play PET Image Reconstruction","summary":"  In this work, we investigate hybrid PET reconstruction algorithms based on\ncoupling a model-based variational reconstruction and the application of a\nseparately learnt Deep Neural Network operator (DNN) in an ADMM Plug and Play\nframework. Following recent results in optimization, fixed point convergence of\nthe scheme can be achieved by enforcing an additional constraint on network\nparameters during learning. We propose such an ADMM algorithm and show in a\nrealistic [18F]-FDG synthetic brain exam that the proposed scheme indeed lead\nexperimentally to convergence to a meaningful fixed point. When the proposed\nconstraint is not enforced during learning of the DNN, the proposed ADMM\nalgorithm was observed experimentally not to converge.\n","authors":["Florent Sureau","Mahdi Latreche","Marion Savanier","Claude Comtat"],"pdf_url":"https://arxiv.org/pdf/2310.04299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04294v1","updated":"2023-10-06T14:52:25Z","published":"2023-10-06T14:52:25Z","title":"Graph learning in robotics: a survey","summary":"  Deep neural networks for graphs have emerged as a powerful tool for learning\non complex non-euclidean data, which is becoming increasingly common for a\nvariety of different applications. Yet, although their potential has been\nwidely recognised in the machine learning community, graph learning is largely\nunexplored for downstream tasks such as robotics applications. To fully unlock\ntheir potential, hence, we propose a review of graph neural architectures from\na robotics perspective. The paper covers the fundamentals of graph-based\nmodels, including their architecture, training procedures, and applications. It\nalso discusses recent advancements and challenges that arise in applied\nsettings, related for example to the integration of perception,\ndecision-making, and control. Finally, the paper provides an extensive review\nof various robotic applications that benefit from learning on graph structures,\nsuch as bodies and contacts modelling, robotic manipulation, action\nrecognition, fleet motion planning, and many more. This survey aims to provide\nreaders with a thorough understanding of the capabilities and limitations of\ngraph neural architectures in robotics, and to highlight potential avenues for\nfuture research.\n","authors":["Francesca Pistilli","Giuseppe Averta"],"pdf_url":"https://arxiv.org/pdf/2310.04294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03205v2","updated":"2023-10-06T14:37:58Z","published":"2023-10-04T23:24:22Z","title":"A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized\n  Optimization","summary":"  We propose NeuFace, a 3D face mesh pseudo annotation method on videos via\nneural re-parameterized optimization. Despite the huge progress in 3D face\nreconstruction methods, generating reliable 3D face labels for in-the-wild\ndynamic videos remains challenging. Using NeuFace optimization, we annotate the\nper-view/-frame accurate and consistent face meshes on large-scale face videos,\ncalled the NeuFace-dataset. We investigate how neural re-parameterization helps\nto reconstruct image-aligned facial details on 3D meshes via gradient analysis.\nBy exploiting the naturalness and diversity of 3D faces in our dataset, we\ndemonstrate the usefulness of our dataset for 3D face-related tasks: improving\nthe reconstruction accuracy of an existing 3D face reconstruction model and\nlearning 3D facial motion prior. Code and datasets will be available at\nhttps://neuface-dataset.github.io.\n","authors":["Kim Youwang","Lee Hyun","Kim Sung-Bin","Suekyeong Nam","Janghoon Ju","Tae-Hyun Oh"],"pdf_url":"https://arxiv.org/pdf/2310.03205v2.pdf","comment":"9 pages, 7 figures, and 3 tables for the main paper. 8 pages, 6\n  figures and 3 tables for the appendix"},{"id":"http://arxiv.org/abs/2310.04285v1","updated":"2023-10-06T14:37:22Z","published":"2023-10-06T14:37:22Z","title":"Assessing Robustness via Score-Based Adversarial Image Generation","summary":"  Most adversarial attacks and defenses focus on perturbations within small\n$\\ell_p$-norm constraints. However, $\\ell_p$ threat models cannot capture all\nrelevant semantic-preserving perturbations, and hence, the scope of robustness\nevaluations is limited. In this work, we introduce Score-Based Adversarial\nGeneration (ScoreAG), a novel framework that leverages the advancements in\nscore-based generative models to generate adversarial examples beyond\n$\\ell_p$-norm constraints, so-called unrestricted adversarial examples,\novercoming their limitations. Unlike traditional methods, ScoreAG maintains the\ncore semantics of images while generating realistic adversarial examples,\neither by transforming existing images or synthesizing new ones entirely from\nscratch. We further exploit the generative capability of ScoreAG to purify\nimages, empirically enhancing the robustness of classifiers. Our extensive\nempirical evaluation demonstrates that ScoreAG matches the performance of\nstate-of-the-art attacks and defenses across multiple benchmarks. This work\nhighlights the importance of investigating adversarial examples bounded by\nsemantics rather than $\\ell_p$-norm constraints. ScoreAG represents an\nimportant step towards more encompassing robustness assessments.\n","authors":["Marcel Kollovieh","Lukas Gosch","Yan Scholten","Marten Lienen","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2310.04285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04271v1","updated":"2023-10-06T14:16:49Z","published":"2023-10-06T14:16:49Z","title":"Compositional Servoing by Recombining Demonstrations","summary":"  Learning-based manipulation policies from image inputs often show weak task\ntransfer capabilities. In contrast, visual servoing methods allow efficient\ntask transfer in high-precision scenarios while requiring only a few\ndemonstrations. In this work, we present a framework that formulates the visual\nservoing task as graph traversal. Our method not only extends the robustness of\nvisual servoing, but also enables multitask capability based on a few\ntask-specific demonstrations. We construct demonstration graphs by splitting\nexisting demonstrations and recombining them. In order to traverse the\ndemonstration graph in the inference case, we utilize a similarity function\nthat helps select the best demonstration for a specific task. This enables us\nto compute the shortest path through the graph. Ultimately, we show that\nrecombining demonstrations leads to higher task-respective success. We present\nextensive simulation and real-world experimental results that demonstrate the\nefficacy of our approach.\n","authors":["Max Argus","Abhijeet Nayak","Martin Büchner","Silvio Galesso","Abhinav Valada","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2310.04271v1.pdf","comment":"http://compservo.cs.uni-freiburg.de"},{"id":"http://arxiv.org/abs/2301.12291v2","updated":"2023-10-06T14:14:10Z","published":"2023-01-28T20:09:34Z","title":"CancerUniT: Towards a Single Unified Model for Effective Detection,\n  Segmentation, and Diagnosis of Eight Major Cancers Using a Large Collection\n  of CT Scans","summary":"  Human readers or radiologists routinely perform full-body multi-organ\nmulti-disease detection and diagnosis in clinical practice, while most medical\nAI systems are built to focus on single organs with a narrow list of a few\ndiseases. This might severely limit AI's clinical adoption. A certain number of\nAI models need to be assembled non-trivially to match the diagnostic process of\na human reading a CT scan. In this paper, we construct a Unified Tumor\nTransformer (CancerUniT) model to jointly detect tumor existence & location and\ndiagnose tumor characteristics for eight major cancers in CT scans. CancerUniT\nis a query-based Mask Transformer model with the output of multi-tumor\nprediction. We decouple the object queries into organ queries, tumor detection\nqueries and tumor diagnosis queries, and further establish hierarchical\nrelationships among the three groups. This clinically-inspired architecture\neffectively assists inter- and intra-organ representation learning of tumors\nand facilitates the resolution of these complex, anatomically related\nmulti-organ cancer image reading tasks. CancerUniT is trained end-to-end using\na curated large-scale CT images of 10,042 patients including eight major types\nof cancers and occurring non-cancer tumors (all are pathology-confirmed with 3D\ntumor masks annotated by radiologists). On the test set of 631 patients,\nCancerUniT has demonstrated strong performance under a set of clinically\nrelevant evaluation metrics, substantially outperforming both multi-disease\nmethods and an assembly of eight single-organ expert models in tumor detection,\nsegmentation, and diagnosis. This moves one step closer towards a universal\nhigh performance cancer screening tool.\n","authors":["Jieneng Chen","Yingda Xia","Jiawen Yao","Ke Yan","Jianpeng Zhang","Le Lu","Fakai Wang","Bo Zhou","Mingyan Qiu","Qihang Yu","Mingze Yuan","Wei Fang","Yuxing Tang","Minfeng Xu","Jian Zhou","Yuqian Zhao","Qifeng Wang","Xianghua Ye","Xiaoli Yin","Yu Shi","Xin Chen","Jingren Zhou","Alan Yuille","Zaiyi Liu","Ling Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.12291v2.pdf","comment":"ICCV 2023 Camera Ready Version"},{"id":"http://arxiv.org/abs/2310.04253v1","updated":"2023-10-06T13:51:46Z","published":"2023-10-06T13:51:46Z","title":"Collaborative Camouflaged Object Detection: A Large-Scale Dataset and\n  Benchmark","summary":"  In this paper, we provide a comprehensive study on a new task called\ncollaborative camouflaged object detection (CoCOD), which aims to\nsimultaneously detect camouflaged objects with the same properties from a group\nof relevant images. To this end, we meticulously construct the first\nlarge-scale dataset, termed CoCOD8K, which consists of 8,528 high-quality and\nelaborately selected images with object mask annotations, covering 5\nsuperclasses and 70 subclasses. The dataset spans a wide range of natural and\nartificial camouflage scenes with diverse object appearances and backgrounds,\nmaking it a very challenging dataset for CoCOD. Besides, we propose the first\nbaseline model for CoCOD, named bilateral-branch network (BBNet), which\nexplores and aggregates co-camouflaged cues within a single image and between\nimages within a group, respectively, for accurate camouflaged object detection\nin given images. This is implemented by an inter-image collaborative feature\nexploration (CFE) module, an intra-image object feature search (OFS) module,\nand a local-global refinement (LGR) module. We benchmark 18 state-of-the-art\nmodels, including 12 COD algorithms and 6 CoSOD algorithms, on the proposed\nCoCOD8K dataset under 5 widely used evaluation metrics. Extensive experiments\ndemonstrate the effectiveness of the proposed method and the significantly\nsuperior performance compared to other competitors. We hope that our proposed\ndataset and model will boost growth in the COD community. The dataset, model,\nand results will be available at: https://github.com/zc199823/BBNet--CoCOD.\n","authors":["Cong Zhang","Hongbo Bi","Tian-Zhu Xiang","Ranwan Wu","Jinghui Tong","Xiufang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04253v1.pdf","comment":"Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (TNNLS)"},{"id":"http://arxiv.org/abs/2310.04247v1","updated":"2023-10-06T13:41:39Z","published":"2023-10-06T13:41:39Z","title":"Semantic segmentation of longitudinal thermal images for identification\n  of hot and cool spots in urban areas","summary":"  This work presents the analysis of semantically segmented, longitudinally,\nand spatially rich thermal images collected at the neighborhood scale to\nidentify hot and cool spots in urban areas. An infrared observatory was\noperated over a few months to collect thermal images of different types of\nbuildings on the educational campus of the National University of Singapore. A\nsubset of the thermal image dataset was used to train state-of-the-art deep\nlearning models to segment various urban features such as buildings,\nvegetation, sky, and roads. It was observed that the U-Net segmentation model\nwith `resnet34' CNN backbone has the highest mIoU score of 0.99 on the test\ndataset, compared to other models such as DeepLabV3, DeeplabV3+, FPN, and\nPSPnet. The masks generated using the segmentation models were then used to\nextract the temperature from thermal images and correct for differences in the\nemissivity of various urban features. Further, various statistical measure of\nthe temperature extracted using the predicted segmentation masks is shown to\nclosely match the temperature extracted using the ground truth masks. Finally,\nthe masks were used to identify hot and cool spots in the urban feature at\nvarious instances of time. This forms one of the very few studies demonstrating\nthe automated analysis of thermal images, which can be of potential use to\nurban planners for devising mitigation strategies for reducing the urban heat\nisland (UHI) effect, improving building energy efficiency, and maximizing\noutdoor thermal comfort.\n","authors":["Vasantha Ramani","Pandarasamy Arjunan","Kameshwar Poolla","Clayton Miller"],"pdf_url":"https://arxiv.org/pdf/2310.04247v1.pdf","comment":"14 pages, 13 figures"},{"id":"http://arxiv.org/abs/2306.15350v2","updated":"2023-10-06T13:22:14Z","published":"2023-06-27T10:03:15Z","title":"CellViT: Vision Transformers for Precise Cell Segmentation and\n  Classification","summary":"  Nuclei detection and segmentation in hematoxylin and eosin-stained (H&E)\ntissue images are important clinical tasks and crucial for a wide range of\napplications. However, it is a challenging task due to nuclei variances in\nstaining and size, overlapping boundaries, and nuclei clustering. While\nconvolutional neural networks have been extensively used for this task, we\nexplore the potential of Transformer-based networks in this domain. Therefore,\nwe introduce a new method for automated instance segmentation of cell nuclei in\ndigitized tissue samples using a deep learning architecture based on Vision\nTransformer called CellViT. CellViT is trained and evaluated on the PanNuke\ndataset, which is one of the most challenging nuclei instance segmentation\ndatasets, consisting of nearly 200,000 annotated Nuclei into 5 clinically\nimportant classes in 19 tissue types. We demonstrate the superiority of\nlarge-scale in-domain and out-of-domain pre-trained Vision Transformers by\nleveraging the recently published Segment Anything Model and a ViT-encoder\npre-trained on 104 million histological image patches - achieving\nstate-of-the-art nuclei detection and instance segmentation performance on the\nPanNuke dataset with a mean panoptic quality of 0.50 and an F1-detection score\nof 0.83. The code is publicly available at https://github.com/TIO-IKIM/CellViT\n","authors":["Fabian Hörst","Moritz Rempe","Lukas Heine","Constantin Seibold","Julius Keyl","Giulia Baldini","Selma Ugurel","Jens Siveke","Barbara Grünwald","Jan Egger","Jens Kleesiek"],"pdf_url":"https://arxiv.org/pdf/2306.15350v2.pdf","comment":"18 pages, 5 figures, appendix included"},{"id":"http://arxiv.org/abs/2304.01814v2","updated":"2023-10-06T12:57:53Z","published":"2023-04-04T14:13:13Z","title":"CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for\n  Low-Dose CT Denoising and Generalization","summary":"  Low-dose computed tomography (CT) images suffer from noise and artifacts due\nto photon starvation and electronic noise. Recently, some works have attempted\nto use diffusion models to address the over-smoothness and training instability\nencountered by previous deep-learning-based denoising models. However,\ndiffusion models suffer from long inference times due to the large number of\nsampling steps involved. Very recently, cold diffusion model generalizes\nclassical diffusion models and has greater flexibility. Inspired by the cold\ndiffusion, this paper presents a novel COntextual eRror-modulated gEneralized\nDiffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First,\nCoreDiff utilizes LDCT images to displace the random Gaussian noise and employs\na novel mean-preserving degradation operator to mimic the physical process of\nCT degradation, significantly reducing sampling steps thanks to the informative\nLDCT images as the starting point of the sampling process. Second, to alleviate\nthe error accumulation problem caused by the imperfect restoration operator in\nthe sampling process, we propose a novel ContextuaL Error-modulAted Restoration\nNetwork (CLEAR-Net), which can leverage contextual information to constrain the\nsampling process from structural distortion and modulate time step embedding\nfeatures for better alignment with the input at the next time step. Third, to\nrapidly generalize to a new, unseen dose level with as few resources as\npossible, we devise a one-shot learning framework to make CoreDiff generalize\nfaster and better using only a single LDCT image (un)paired with NDCT.\nExtensive experimental results on two datasets demonstrate that our CoreDiff\noutperforms competing methods in denoising and generalization performance, with\na clinically acceptable inference time. Source code is made available at\nhttps://github.com/qgao21/CoreDiff.\n","authors":["Qi Gao","Zilong Li","Junping Zhang","Yi Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2304.01814v2.pdf","comment":"IEEE Transactions on Medical Imaging, 2023"},{"id":"http://arxiv.org/abs/2303.05686v2","updated":"2023-10-06T12:35:11Z","published":"2023-03-10T03:39:23Z","title":"Generative AI for Rapid Diffusion MRI with Improved Image Quality,\n  Reliability and Generalizability","summary":"  Diffusion MRI is a non-invasive, in-vivo biomedical imaging method for\nmapping tissue microstructure. Applications include structural connectivity\nimaging of the human brain and detecting microstructural neural changes.\nHowever, acquiring high signal-to-noise ratio dMRI datasets with high angular\nand spatial resolution requires prohibitively long scan times, limiting usage\nin many important clinical settings, especially for children, the elderly, and\nin acute neurological disorders that may require conscious sedation or general\nanesthesia. We employ a Swin UNEt Transformers model, trained on augmented\nHuman Connectome Project data and conditioned on registered T1 scans, to\nperform generalized denoising of dMRI. We also qualitatively demonstrate\nsuper-resolution with artificially downsampled HCP data in normal adult\nvolunteers. Remarkably, Swin UNETR can be fine-tuned for an out-of-domain\ndataset with a single example scan, as we demonstrate on dMRI of children with\nneurodevelopmental disorders and of adults with acute evolving traumatic brain\ninjury, each cohort scanned on different models of scanners with different\nimaging protocols at different sites. We exceed current state-of-the-art\ndenoising methods in accuracy and test-retest reliability of rapid diffusion\ntensor imaging requiring only 90 seconds of scan time. Applied to tissue\nmicrostructural modeling of dMRI, Swin UNETR denoising achieves dramatic\nimprovements over the state-of-the-art for test-retest reliability of\nintracellular volume fraction and free water fraction measurements and can\nremove heavy-tail noise, improving biophysical modeling fidelity. Swin UNeTR\nenables rapid diffusion MRI with unprecedented accuracy and reliability,\nespecially for probing biological tissues for scientific and clinical\napplications. The code and model are publicly available at\nhttps://github.com/ucsfncl/dmri-swin.\n","authors":["Amir Sadikov","Xinlei Pan","Hannah Choi","Lanya T. Cai","Pratik Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2303.05686v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04194v1","updated":"2023-10-06T12:20:40Z","published":"2023-10-06T12:20:40Z","title":"Enhancing the Authenticity of Rendered Portraits with\n  Identity-Consistent Transfer Learning","summary":"  Despite rapid advances in computer graphics, creating high-quality\nphoto-realistic virtual portraits is prohibitively expensive. Furthermore, the\nwell-know ''uncanny valley'' effect in rendered portraits has a significant\nimpact on the user experience, especially when the depiction closely resembles\na human likeness, where any minor artifacts can evoke feelings of eeriness and\nrepulsiveness. In this paper, we present a novel photo-realistic portrait\ngeneration framework that can effectively mitigate the ''uncanny valley''\neffect and improve the overall authenticity of rendered portraits. Our key idea\nis to employ transfer learning to learn an identity-consistent mapping from the\nlatent space of rendered portraits to that of real portraits. During the\ninference stage, the input portrait of an avatar can be directly transferred to\na realistic portrait by changing its appearance style while maintaining the\nfacial identity. To this end, we collect a new dataset, Daz-Rendered-Faces-HQ\n(DRFHQ), that is specifically designed for rendering-style portraits. We\nleverage this dataset to fine-tune the StyleGAN2 generator, using our carefully\ncrafted framework, which helps to preserve the geometric and color features\nrelevant to facial identity. We evaluate our framework using portraits with\ndiverse gender, age, and race variations. Qualitative and quantitative\nevaluations and ablation studies show the advantages of our method compared to\nstate-of-the-art approaches.\n","authors":["Luyuan Wang","Yiqian Wu","Yongliang Yang","Chen Liu","Xiaogang Jin"],"pdf_url":"https://arxiv.org/pdf/2310.04194v1.pdf","comment":"10 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2310.04189v1","updated":"2023-10-06T12:08:15Z","published":"2023-10-06T12:08:15Z","title":"Bridging the Gap between Human Motion and Action Semantics via Kinematic\n  Phrases","summary":"  The goal of motion understanding is to establish a reliable mapping between\nmotion and action semantics, while it is a challenging many-to-many problem. An\nabstract action semantic (i.e., walk forwards) could be conveyed by\nperceptually diverse motions (walk with arms up or swinging), while a motion\ncould carry different semantics w.r.t. its context and intention. This makes an\nelegant mapping between them difficult. Previous attempts adopted\ndirect-mapping paradigms with limited reliability. Also, current automatic\nmetrics fail to provide reliable assessments of the consistency between motions\nand action semantics. We identify the source of these problems as the\nsignificant gap between the two modalities. To alleviate this gap, we propose\nKinematic Phrases (KP) that take the objective kinematic facts of human motion\nwith proper abstraction, interpretability, and generality characteristics.\nBased on KP as a mediator, we can unify a motion knowledge base and build a\nmotion understanding system. Meanwhile, KP can be automatically converted from\nmotions and to text descriptions with no subjective bias, inspiring Kinematic\nPrompt Generation (KPG) as a novel automatic motion generation benchmark. In\nextensive experiments, our approach shows superiority over other methods. Our\ncode and data would be made publicly available at https://foruck.github.io/KP.\n","authors":["Xinpeng Liu","Yong-Lu Li","Ailing Zeng","Zizheng Zhou","Yang You","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2310.04189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04187v1","updated":"2023-10-06T12:01:55Z","published":"2023-10-06T12:01:55Z","title":"Whole Slide Multiple Instance Learning for Predicting Axillary Lymph\n  Node Metastasis","summary":"  Breast cancer is a major concern for women's health globally, with axillary\nlymph node (ALN) metastasis identification being critical for prognosis\nevaluation and treatment guidance. This paper presents a deep learning (DL)\nclassification pipeline for quantifying clinical information from digital\ncore-needle biopsy (CNB) images, with one step less than existing methods. A\npublicly available dataset of 1058 patients was used to evaluate the\nperformance of different baseline state-of-the-art (SOTA) DL models in\nclassifying ALN metastatic status based on CNB images. An extensive ablation\nstudy of various data augmentation techniques was also conducted. Finally, the\nmanual tumor segmentation and annotation step performed by the pathologists was\nassessed.\n","authors":["Glejdis Shkëmbi","Johanna P. Müller","Zhe Li","Katharina Breininger","Peter Schüffler","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2310.04187v1.pdf","comment":"Accepted for MICCAI DEMI Workshop 2023"},{"id":"http://arxiv.org/abs/2310.04181v1","updated":"2023-10-06T11:53:04Z","published":"2023-10-06T11:53:04Z","title":"DiffPrompter: Differentiable Implicit Visual Prompts for\n  Semantic-Segmentation in Adverse Conditions","summary":"  Semantic segmentation in adverse weather scenarios is a critical task for\nautonomous driving systems. While foundation models have shown promise, the\nneed for specialized adaptors becomes evident for handling more challenging\nscenarios. We introduce DiffPrompter, a novel differentiable visual and latent\nprompting mechanism aimed at expanding the learning capabilities of existing\nadaptors in foundation models. Our proposed $\\nabla$HFC image processing block\nexcels particularly in adverse weather conditions, where conventional methods\noften fall short. Furthermore, we investigate the advantages of jointly\ntraining visual and latent prompts, demonstrating that this combined approach\nsignificantly enhances performance in out-of-distribution scenarios. Our\ndifferentiable visual prompts leverage parallel and series architectures to\ngenerate prompts, effectively improving object segmentation tasks in adverse\nconditions. Through a comprehensive series of experiments and evaluations, we\nprovide empirical evidence to support the efficacy of our approach. Project\npage at https://diffprompter.github.io.\n","authors":["Sanket Kalwar","Mihir Ungarala","Shruti Jain","Aaron Monis","Krishna Reddy Konda","Sourav Garg","K Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2310.04181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04180v1","updated":"2023-10-06T11:52:31Z","published":"2023-10-06T11:52:31Z","title":"Degradation-Aware Self-Attention Based Transformer for Blind Image\n  Super-Resolution","summary":"  Compared to CNN-based methods, Transformer-based methods achieve impressive\nimage restoration outcomes due to their abilities to model remote dependencies.\nHowever, how to apply Transformer-based methods to the field of blind\nsuper-resolution (SR) and further make an SR network adaptive to degradation\ninformation is still an open problem. In this paper, we propose a new\ndegradation-aware self-attention-based Transformer model, where we incorporate\ncontrastive learning into the Transformer network for learning the degradation\nrepresentations of input images with unknown noise. In particular, we integrate\nboth CNN and Transformer components into the SR network, where we first use the\nCNN modulated by the degradation information to extract local features, and\nthen employ the degradation-aware Transformer to extract global semantic\nfeatures. We apply our proposed model to several popular large-scale benchmark\ndatasets for testing, and achieve the state-of-the-art performance compared to\nexisting methods. In particular, our method yields a PSNR of 32.43 dB on the\nUrban100 dataset at $\\times$2 scale, 0.94 dB higher than DASR, and 26.62 dB on\nthe Urban100 dataset at $\\times$4 scale, 0.26 dB improvement over KDSR, setting\na new benchmark in this area. Source code is available at:\nhttps://github.com/I2-Multimedia-Lab/DSAT/tree/main.\n","authors":["Qingguo Liu","Pan Gao","Kang Han","Ningzhong Liu","Wei Xiang"],"pdf_url":"https://arxiv.org/pdf/2310.04180v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2310.04179v1","updated":"2023-10-06T11:49:21Z","published":"2023-10-06T11:49:21Z","title":"Entropic Score metric: Decoupling Topology and Size in Training-free NAS","summary":"  Neural Networks design is a complex and often daunting task, particularly for\nresource-constrained scenarios typical of mobile-sized models. Neural\nArchitecture Search is a promising approach to automate this process, but\nexisting competitive methods require large training time and computational\nresources to generate accurate models. To overcome these limits, this paper\ncontributes with: i) a novel training-free metric, named Entropic Score, to\nestimate model expressivity through the aggregated element-wise entropy of its\nactivations; ii) a cyclic search algorithm to separately yet synergistically\nsearch model size and topology. Entropic Score shows remarkable ability in\nsearching for the topology of the network, and a proper combination with\nLogSynflow, to search for model size, yields superior capability to completely\ndesign high-performance Hybrid Transformers for edge applications in less than\n1 GPU hour, resulting in the fastest and most accurate NAS method for ImageNet\nclassification.\n","authors":["Niccolò Cavagnero","Luca Robbiano","Francesca Pistilli","Barbara Caputo","Giuseppe Averta"],"pdf_url":"https://arxiv.org/pdf/2310.04179v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.04152v1","updated":"2023-10-06T10:55:34Z","published":"2023-10-06T10:55:34Z","title":"Improving Neural Radiance Field using Near-Surface Sampling with Point\n  Cloud Generation","summary":"  Neural radiance field (NeRF) is an emerging view synthesis method that\nsamples points in a three-dimensional (3D) space and estimates their existence\nand color probabilities. The disadvantage of NeRF is that it requires a long\ntraining time since it samples many 3D points. In addition, if one samples\npoints from occluded regions or in the space where an object is unlikely to\nexist, the rendering quality of NeRF can be degraded. These issues can be\nsolved by estimating the geometry of 3D scene. This paper proposes a\nnear-surface sampling framework to improve the rendering quality of NeRF. To\nthis end, the proposed method estimates the surface of a 3D object using depth\nimages of the training set and sampling is performed around there only. To\nobtain depth information on a novel view, the paper proposes a 3D point cloud\ngeneration method and a simple refining method for projected depth from a point\ncloud. Experimental results show that the proposed near-surface sampling NeRF\nframework can significantly improve the rendering quality, compared to the\noriginal NeRF and a state-of-the-art depth-based NeRF method. In addition, one\ncan significantly accelerate the training time of a NeRF model with the\nproposed near-surface sampling framework.\n","authors":["Hye Bin Yoo","Hyun Min Han","Sung Soo Hwang","Il Yong Chun"],"pdf_url":"https://arxiv.org/pdf/2310.04152v1.pdf","comment":"13 figures, 2 tables"},{"id":"http://arxiv.org/abs/2310.04148v1","updated":"2023-10-06T10:40:46Z","published":"2023-10-06T10:40:46Z","title":"Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement\n  Learning","summary":"  The performance of existing supervised neuron segmentation methods is highly\ndependent on the number of accurate annotations, especially when applied to\nlarge scale electron microscopy (EM) data. By extracting semantic information\nfrom unlabeled data, self-supervised methods can improve the performance of\ndownstream tasks, among which the mask image model (MIM) has been widely used\ndue to its simplicity and effectiveness in recovering original information from\nmasked images. However, due to the high degree of structural locality in EM\nimages, as well as the existence of considerable noise, many voxels contain\nlittle discriminative information, making MIM pretraining inefficient on the\nneuron segmentation task. To overcome this challenge, we propose a\ndecision-based MIM that utilizes reinforcement learning (RL) to automatically\nsearch for optimal image masking ratio and masking strategy. Due to the vast\nexploration space, using single-agent RL for voxel prediction is impractical.\nTherefore, we treat each input patch as an agent with a shared behavior policy,\nallowing for multi-agent collaboration. Furthermore, this multi-agent model can\ncapture dependencies between voxels, which is beneficial for the downstream\nsegmentation task. Experiments conducted on representative EM datasets\ndemonstrate that our approach has a significant advantage over alternative\nself-supervised methods on the task of neuron segmentation. Code is available\nat \\url{https://github.com/ydchen0806/dbMiM}.\n","authors":["Yinda Chen","Wei Huang","Shenglong Zhou","Qi Chen","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.04148v1.pdf","comment":"IJCAI 23 main track paper"},{"id":"http://arxiv.org/abs/2310.04134v1","updated":"2023-10-06T10:16:26Z","published":"2023-10-06T10:16:26Z","title":"TiC: Exploring Vision Transformer in Convolution","summary":"  While models derived from Vision Transformers (ViTs) have been phonemically\nsurging, pre-trained models cannot seamlessly adapt to arbitrary resolution\nimages without altering the architecture and configuration, such as sampling\nthe positional encoding, limiting their flexibility for various vision tasks.\nFor instance, the Segment Anything Model (SAM) based on ViT-Huge requires all\ninput images to be resized to 1024$\\times$1024. To overcome this limitation, we\npropose the Multi-Head Self-Attention Convolution (MSA-Conv) that incorporates\nSelf-Attention within generalized convolutions, including standard, dilated,\nand depthwise ones. Enabling transformers to handle images of varying sizes\nwithout retraining or rescaling, the use of MSA-Conv further reduces\ncomputational costs compared to global attention in ViT, which grows costly as\nimage size increases. Later, we present the Vision Transformer in Convolution\n(TiC) as a proof of concept for image classification with MSA-Conv, where two\ncapacity enhancing strategies, namely Multi-Directional Cyclic Shifted\nMechanism and Inter-Pooling Mechanism, have been proposed, through establishing\nlong-distance connections between tokens and enlarging the effective receptive\nfield. Extensive experiments have been carried out to validate the overall\neffectiveness of TiC. Additionally, ablation studies confirm the performance\nimprovement made by MSA-Conv and the two capacity enhancing strategies\nseparately. Note that our proposal aims at studying an alternative to the\nglobal attention used in ViT, while MSA-Conv meets our goal by making TiC\ncomparable to state-of-the-art on ImageNet-1K. Code will be released at\nhttps://github.com/zs670980918/MSA-Conv.\n","authors":["Song Zhang","Qingzhong Wang","Jiang Bian","Haoyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.04134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00558v2","updated":"2023-10-06T10:01:53Z","published":"2023-10-01T03:27:41Z","title":"Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes","summary":"  When used in a real-world noisy environment, the capacity to generalize to\nmultiple domains is essential for any autonomous scene text spotting system.\nHowever, existing state-of-the-art methods employ pretraining and fine-tuning\nstrategies on natural scene datasets, which do not exploit the feature\ninteraction across other complex domains. In this work, we explore and\ninvestigate the problem of domain-agnostic scene text spotting, i.e., training\na model on multi-domain source data such that it can directly generalize to\ntarget domains rather than being specialized for a specific domain or scenario.\nIn this regard, we present the community a text spotting validation benchmark\ncalled Under-Water Text (UWT) for noisy underwater scenes to establish an\nimportant case study. Moreover, we also design an efficient super-resolution\nbased end-to-end transformer baseline called DA-TextSpotter which achieves\ncomparable or superior performance over existing text spotting architectures\nfor both regular and arbitrary-shaped scene text spotting benchmarks in terms\nof both accuracy and model efficiency. The dataset, code and pre-trained models\nwill be released upon acceptance.\n","authors":["Alloy Das","Sanket Biswas","Umapada Pal","Josep Lladós"],"pdf_url":"https://arxiv.org/pdf/2310.00558v2.pdf","comment":"10 images"},{"id":"http://arxiv.org/abs/2307.13717v3","updated":"2023-10-06T09:57:01Z","published":"2023-07-25T17:29:32Z","title":"On the Leakage of Fuzzy Matchers","summary":"  In a biometric recognition system, the matcher compares an old and a fresh\ntemplate to decide if it is a match or not. Beyond the binary output (`yes' or\n`no'), more information is computed. This paper provides an in-depth analysis\nof information leakage during distance evaluation, with an emphasis on\nthreshold-based obfuscated distance (\\textit{i.e.}, Fuzzy Matcher). Leakage can\noccur due to a malware infection or the use of a weakly privacy-preserving\nmatcher, exemplified by side channel attacks or partially obfuscated designs.\nWe provide an exhaustive catalog of information leakage scenarios as well as\ntheir impacts on the security concerning data privacy. Each of the scenarios\nleads to generic attacks whose impacts are expressed in terms of computational\ncosts, hence allowing the establishment of upper bounds on the security level.\n","authors":["Axel Durbet","Kevin Thiry-Atighehchi","Dorine Chagnon","Paul-Marie Grollemund"],"pdf_url":"https://arxiv.org/pdf/2307.13717v3.pdf","comment":"Minor corrections"},{"id":"http://arxiv.org/abs/2310.00917v2","updated":"2023-10-06T09:50:50Z","published":"2023-10-02T06:08:01Z","title":"Harnessing the Power of Multi-Lingual Datasets for Pre-training: Towards\n  Enhancing Text Spotting Performance","summary":"  The adaptation capability to a wide range of domains is crucial for scene\ntext spotting models when deployed to real-world conditions. However, existing\nstate-of-the-art (SOTA) approaches usually incorporate scene text detection and\nrecognition simply by pretraining on natural scene text datasets, which do not\ndirectly exploit the intermediate feature representations between multiple\ndomains. Here, we investigate the problem of domain-adaptive scene text\nspotting, i.e., training a model on multi-domain source data such that it can\ndirectly adapt to target domains rather than being specialized for a specific\ndomain or scenario. Further, we investigate a transformer baseline called\nSwin-TESTR to focus on solving scene-text spotting for both regular and\narbitrary-shaped scene text along with an exhaustive evaluation. The results\nclearly demonstrate the potential of intermediate representations to achieve\nsignificant performance on text spotting benchmarks across multiple domains\n(e.g. language, synth-to-real, and documents). both in terms of accuracy and\nefficiency.\n","authors":["Alloy Das","Sanket Biswas","Ayan Banerjee","Saumik Bhattacharya","Josep Lladós","Umapada Pal"],"pdf_url":"https://arxiv.org/pdf/2310.00917v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2309.05257v2","updated":"2023-10-06T09:46:05Z","published":"2023-09-11T06:27:25Z","title":"FusionFormer: A Multi-sensory Fusion in Bird's-Eye-View and Temporal\n  Consistent Transformer for 3D Object Detection","summary":"  Multi-sensor modal fusion has demonstrated strong advantages in 3D object\ndetection tasks. However, existing methods that fuse multi-modal features\nrequire transforming features into the bird's eye view space and may lose\ncertain information on Z-axis, thus leading to inferior performance. To this\nend, we propose a novel end-to-end multi-modal fusion transformer-based\nframework, dubbed FusionFormer, that incorporates deformable attention and\nresidual structures within the fusion encoding module. Specifically, by\ndeveloping a uniform sampling strategy, our method can easily sample from 2D\nimage and 3D voxel features spontaneously, thus exploiting flexible\nadaptability and avoiding explicit transformation to the bird's eye view space\nduring the feature concatenation process. We further implement a residual\nstructure in our feature encoder to ensure the model's robustness in case of\nmissing an input modality. Through extensive experiments on a popular\nautonomous driving benchmark dataset, nuScenes, our method achieves\nstate-of-the-art single model performance of 72.6% mAP and 75.1% NDS in the 3D\nobject detection task without test time augmentation.\n","authors":["Chunyong Hu","Hang Zheng","Kun Li","Jianyun Xu","Weibo Mao","Maochun Luo","Lingxuan Wang","Mingxia Chen","Qihao Peng","Kaixuan Liu","Yiru Zhao","Peihan Hao","Minzhe Liu","Kaicheng Yu"],"pdf_url":"https://arxiv.org/pdf/2309.05257v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04122v1","updated":"2023-10-06T09:42:12Z","published":"2023-10-06T09:42:12Z","title":"VI-Diff: Unpaired Visible-Infrared Translation Diffusion Model for\n  Single Modality Labeled Visible-Infrared Person Re-identification","summary":"  Visible-Infrared person re-identification (VI-ReID) in real-world scenarios\nposes a significant challenge due to the high cost of cross-modality data\nannotation. Different sensing cameras, such as RGB/IR cameras for good/poor\nlighting conditions, make it costly and error-prone to identify the same person\nacross modalities. To overcome this, we explore the use of single-modality\nlabeled data for the VI-ReID task, which is more cost-effective and practical.\nBy labeling pedestrians in only one modality (e.g., visible images) and\nretrieving in another modality (e.g., infrared images), we aim to create a\ntraining set containing both originally labeled and modality-translated data\nusing unpaired image-to-image translation techniques. In this paper, we propose\nVI-Diff, a diffusion model that effectively addresses the task of\nVisible-Infrared person image translation. Through comprehensive experiments,\nwe demonstrate that VI-Diff outperforms existing diffusion and GAN models,\nmaking it a promising solution for VI-ReID with single-modality labeled data.\nOur approach can be a promising solution to the VI-ReID task with\nsingle-modality labeled data and serves as a good starting point for future\nstudy. Code will be available.\n","authors":["Han Huang","Yan Huang","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.04122v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.04114v1","updated":"2023-10-06T09:26:09Z","published":"2023-10-06T09:26:09Z","title":"Aorta Segmentation from 3D CT in MICCAI SEG.A. 2023 Challenge","summary":"  Aorta provides the main blood supply of the body. Screening of aorta with\nimaging helps for early aortic disease detection and monitoring. In this work,\nwe describe our solution to the Segmentation of the Aorta (SEG.A.231) from 3D\nCT challenge. We use automated segmentation method Auto3DSeg available in\nMONAI. Our solution achieves an average Dice score of 0.920 and 95th percentile\nof the Hausdorff Distance (HD95) of 6.013, which ranks first and wins the\nSEG.A. 2023 challenge.\n","authors":["Andriy Myronenko","Dong Yang","Yufan He","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.04114v1.pdf","comment":"MICCAI 2023, SEG.A. 2023 challenge 1st place"},{"id":"http://arxiv.org/abs/2310.04111v1","updated":"2023-10-06T09:20:58Z","published":"2023-10-06T09:20:58Z","title":"Dense Random Texture Detection using Beta Distribution Statistics","summary":"  This note describes a method for detecting dense random texture using fully\nconnected points sampled on image edges. An edge image is randomly sampled with\npoints, the standard L2 distance is calculated between all connected points in\na neighbourhood. For each point, a check is made if the point intersects with\nan image edge. If this is the case, a unity value is added to the distance,\notherwise zero. From this an edge excess index is calculated for the fully\nconnected edge graph in the range [1.0..2.0], where 1.0 indicate no edges. The\nratio can be interpreted as a sampled Bernoulli process with unknown\nprobability. The Bayesian posterior estimate of the probability can be\nassociated with its conjugate prior which is a Beta($\\alpha$, $\\beta$)\ndistribution, with hyper parameters $\\alpha$ and $\\beta$ related to the number\nof edge crossings. Low values of $\\beta$ indicate a texture rich area, higher\nvalues less rich. The method has been applied to real-time SLAM-based moving\nobject detection, where points are confined to tracked boxes (rois).\n","authors":["Soeren Molander"],"pdf_url":"https://arxiv.org/pdf/2310.04111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04110v1","updated":"2023-10-06T09:20:22Z","published":"2023-10-06T09:20:22Z","title":"Automated 3D Segmentation of Kidneys and Tumors in MICCAI KiTS 2023\n  Challenge","summary":"  Kidney and Kidney Tumor Segmentation Challenge (KiTS) 2023 offers a platform\nfor researchers to compare their solutions to segmentation from 3D CT. In this\nwork, we describe our submission to the challenge using automated segmentation\nof Auto3DSeg available in MONAI. Our solution achieves the average dice of\n0.835 and surface dice of 0.723, which ranks first and wins the KiTS 2023\nchallenge.\n","authors":["Andriy Myronenko","Dong Yang","Yufan He","Daguang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.04110v1.pdf","comment":"MICCAI 2023, KITS 2023 challenge 1st place"},{"id":"http://arxiv.org/abs/2310.04099v1","updated":"2023-10-06T09:01:15Z","published":"2023-10-06T09:01:15Z","title":"ClusVPR: Efficient Visual Place Recognition with Clustering-based\n  Weighted Transformer","summary":"  Visual place recognition (VPR) is a highly challenging task that has a wide\nrange of applications, including robot navigation and self-driving vehicles.\nVPR is particularly difficult due to the presence of duplicate regions and the\nlack of attention to small objects in complex scenes, resulting in recognition\ndeviations. In this paper, we present ClusVPR, a novel approach that tackles\nthe specific issues of redundant information in duplicate regions and\nrepresentations of small objects. Different from existing methods that rely on\nConvolutional Neural Networks (CNNs) for feature map generation, ClusVPR\nintroduces a unique paradigm called Clustering-based Weighted Transformer\nNetwork (CWTNet). CWTNet leverages the power of clustering-based weighted\nfeature maps and integrates global dependencies to effectively address visual\ndeviations encountered in large-scale VPR problems. We also introduce the\noptimized-VLAD (OptLAD) layer that significantly reduces the number of\nparameters and enhances model efficiency. This layer is specifically designed\nto aggregate the information obtained from scale-wise image patches.\nAdditionally, our pyramid self-supervised strategy focuses on extracting\nrepresentative and diverse information from scale-wise image patches instead of\nentire images, which is crucial for capturing representative and diverse\ninformation in VPR. Extensive experiments on four VPR datasets show our model's\nsuperior performance compared to existing models while being less complex.\n","authors":["Yifan Xu","Pourya Shamsolmoali","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1806.06298v4","updated":"2023-10-06T08:41:14Z","published":"2018-06-16T21:17:02Z","title":"Deformable Generator Networks: Unsupervised Disentanglement of\n  Appearance and Geometry","summary":"  We present a deformable generator model to disentangle the appearance and\ngeometric information for both image and video data in a purely unsupervised\nmanner. The appearance generator network models the information related to\nappearance, including color, illumination, identity or category, while the\ngeometric generator performs geometric warping, such as rotation and\nstretching, through generating deformation field which is used to warp the\ngenerated appearance to obtain the final image or video sequences. Two\ngenerators take independent latent vectors as input to disentangle the\nappearance and geometric information from image or video sequences. For video\ndata, a nonlinear transition model is introduced to both the appearance and\ngeometric generators to capture the dynamics over time. The proposed scheme is\ngeneral and can be easily integrated into different generative models. An\nextensive set of qualitative and quantitative experiments shows that the\nappearance and geometric information can be well disentangled, and the learned\ngeometric generator can be conveniently transferred to other image datasets to\nfacilitate knowledge transfer tasks.\n","authors":["Xianglei Xing","Ruiqi Gao","Tian Han","Song-Chun Zhu","Ying Nian Wu"],"pdf_url":"https://arxiv.org/pdf/1806.06298v4.pdf","comment":"The version of IEEE Transactions on Pattern Analysis and Machine\n  Intelligence"},{"id":"http://arxiv.org/abs/2310.04086v1","updated":"2023-10-06T08:30:20Z","published":"2023-10-06T08:30:20Z","title":"End-to-End Chess Recognition","summary":"  Chess recognition refers to the task of identifying the chess pieces\nconfiguration from a chessboard image. Contrary to the predominant approach\nthat aims to solve this task through the pipeline of chessboard detection,\nsquare localization, and piece classification, we rely on the power of deep\nlearning models and introduce two novel methodologies to circumvent this\npipeline and directly predict the chessboard configuration from the entire\nimage. In doing so, we avoid the inherent error accumulation of the sequential\napproaches and the need for intermediate annotations. Furthermore, we introduce\na new dataset, Chess Recognition Dataset (ChessReD), specifically designed for\nchess recognition that consists of 10,800 images and their corresponding\nannotations. In contrast to existing synthetic datasets with limited angles,\nthis dataset comprises a diverse collection of real images of chess formations\ncaptured from various angles using smartphone cameras; a sensor choice made to\nensure real-world applicability. We use this dataset to both train our model\nand evaluate and compare its performance to that of the current\nstate-of-the-art. Our approach in chess recognition on this new benchmark\ndataset outperforms related approaches, achieving a board recognition accuracy\nof 15.26% ($\\approx$7x better than the current state-of-the-art).\n","authors":["Athanasios Masouris","Jan van Gemert"],"pdf_url":"https://arxiv.org/pdf/2310.04086v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2310.04081v1","updated":"2023-10-06T08:22:24Z","published":"2023-10-06T08:22:24Z","title":"A Deeply Supervised Semantic Segmentation Method Based on GAN","summary":"  In recent years, the field of intelligent transportation has witnessed rapid\nadvancements, driven by the increasing demand for automation and efficiency in\ntransportation systems. Traffic safety, one of the tasks integral to\nintelligent transport systems, requires accurately identifying and locating\nvarious road elements, such as road cracks, lanes, and traffic signs. Semantic\nsegmentation plays a pivotal role in achieving this task, as it enables the\npartition of images into meaningful regions with accurate boundaries. In this\nstudy, we propose an improved semantic segmentation model that combines the\nstrengths of adversarial learning with state-of-the-art semantic segmentation\ntechniques. The proposed model integrates a generative adversarial network\n(GAN) framework into the traditional semantic segmentation model, enhancing the\nmodel's performance in capturing complex and subtle features in transportation\nimages. The effectiveness of our approach is demonstrated by a significant\nboost in performance on the road crack dataset compared to the existing\nmethods, \\textit{i.e.,} SEGAN. This improvement can be attributed to the\nsynergistic effect of adversarial learning and semantic segmentation, which\nleads to a more refined and accurate representation of road structures and\nconditions. The enhanced model not only contributes to better detection of road\ncracks but also to a wide range of applications in intelligent transportation,\nsuch as traffic sign recognition, vehicle detection, and lane segmentation.\n","authors":["Wei Zhao","Qiyu Wei","Zeng Zeng"],"pdf_url":"https://arxiv.org/pdf/2310.04081v1.pdf","comment":"6 pages, 2 figures, ITSC conference"},{"id":"http://arxiv.org/abs/2306.14435v4","updated":"2023-10-06T08:15:42Z","published":"2023-06-26T06:04:09Z","title":"DragDiffusion: Harnessing Diffusion Models for Interactive Point-based\n  Image Editing","summary":"  Accurate and controllable image editing is a challenging task that has\nattracted significant attention recently. Notably, DragGAN is an interactive\npoint-based image editing framework that achieves impressive editing results\nwith pixel-level precision. However, due to its reliance on generative\nadversarial networks (GANs), its generality is limited by the capacity of\npretrained GAN models. In this work, we extend this editing framework to\ndiffusion models and propose a novel approach DragDiffusion. By harnessing\nlarge-scale pretrained diffusion models, we greatly enhance the applicability\nof interactive point-based editing on both real and diffusion-generated images.\nOur approach involves optimizing the diffusion latents to achieve precise\nspatial control. The supervision signal of this optimization process is from\nthe diffusion model's UNet features, which are known to contain rich semantic\nand geometric information. Moreover, we introduce two additional techniques,\nnamely LoRA fine-tuning and latent-MasaCtrl, to further preserve the identity\nof the original image. Lastly, we present a challenging benchmark dataset\ncalled DragBench -- the first benchmark to evaluate the performance of\ninteractive point-based image editing methods. Experiments across a wide range\nof challenging cases (e.g., images with multiple objects, diverse object\ncategories, various styles, etc.) demonstrate the versatility and generality of\nDragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion.\n","authors":["Yujun Shi","Chuhui Xue","Jun Hao Liew","Jiachun Pan","Hanshu Yan","Wenqing Zhang","Vincent Y. F. Tan","Song Bai"],"pdf_url":"https://arxiv.org/pdf/2306.14435v4.pdf","comment":"Code is released at https://github.com/Yujun-Shi/DragDiffusion"},{"id":"http://arxiv.org/abs/2306.11363v3","updated":"2023-10-06T08:06:15Z","published":"2023-06-20T08:02:59Z","title":"Masked Diffusion Models Are Fast Distribution Learners","summary":"  Diffusion models have emerged as the \\emph{de-facto} generative model for\nimage synthesis, yet they entail significant training overhead, hindering the\ntechnique's broader adoption in the research community. We observe that these\nmodels are commonly trained to learn all fine-grained visual information from\nscratch, thus motivating our investigation on its necessity. In this work, we\nshow that it suffices to set up pre-training stage to initialize a diffusion\nmodel by encouraging it to learn some primer distribution of the unknown real\nimage distribution. Then the pre-trained model can be fine-tuned for specific\ngeneration tasks efficiently. To approximate the primer distribution, our\napproach centers on masking a high proportion (e.g., up to 90\\%) of an input\nimage and employing masked denoising score matching to denoise visible areas.\nUtilizing the learned primer distribution in subsequent fine-tuning, we\nefficiently train a ViT-based diffusion model on CelebA-HQ $256 \\times 256$ in\nthe raw pixel space, achieving superior training acceleration compared to\ndenoising diffusion probabilistic model (DDPM) counterpart and a new FID score\nrecord of 6.73 for ViT-based diffusion models. Moreover, our masked\npre-training technique can be universally applied to various diffusion models\nthat directly generate images in the pixel space, aiding in the learning of\npre-trained models with superior generalizability. For instance, a diffusion\nmodel pre-trained on VGGFace2 attains a 46\\% quality improvement through\nfine-tuning on only 10\\% data from a different dataset. Our code is available\nat \\url{https://github.com/jiachenlei/maskdm}.\n","authors":["Jiachen Lei","Qinglong Wang","Peng Cheng","Zhongjie Ba","Zhan Qin","Zhibo Wang","Zhenguang Liu","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2306.11363v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09375v2","updated":"2023-10-06T07:58:27Z","published":"2023-08-18T08:10:41Z","title":"Image Processing and Machine Learning for Hyperspectral Unmixing: An\n  Overview and the HySUPP Python Package","summary":"  Spectral pixels are often a mixture of the pure spectra of the materials,\ncalled endmembers, due to the low spatial resolution of hyperspectral sensors,\ndouble scattering, and intimate mixtures of materials in the scenes. Unmixing\nestimates the fractional abundances of the endmembers within the pixel.\nDepending on the prior knowledge of endmembers, linear unmixing can be divided\ninto three main groups: supervised, semi-supervised, and unsupervised (blind)\nlinear unmixing. Advances in Image processing and machine learning\nsubstantially affected unmixing. This paper provides an overview of advanced\nand conventional unmixing approaches. Additionally, we draw a critical\ncomparison between advanced and conventional techniques from the three\ncategories. We compare the performance of the unmixing techniques on three\nsimulated and two real datasets. The experimental results reveal the advantages\nof different unmixing categories for different unmixing scenarios. Moreover, we\nprovide an open-source Python-based package available at\nhttps://github.com/BehnoodRasti/HySUPP to reproduce the results.\n","authors":["Behnood Rasti","Alexandre Zouaoui","Julien Mairal","Jocelyn Chanussot"],"pdf_url":"https://arxiv.org/pdf/2308.09375v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04043v1","updated":"2023-10-06T06:33:20Z","published":"2023-10-06T06:33:20Z","title":"In the Blink of an Eye: Event-based Emotion Recognition","summary":"  We introduce a wearable single-eye emotion recognition device and a real-time\napproach to recognizing emotions from partial observations of an emotion that\nis robust to changes in lighting conditions. At the heart of our method is a\nbio-inspired event-based camera setup and a newly designed lightweight Spiking\nEye Emotion Network (SEEN). Compared to conventional cameras, event-based\ncameras offer a higher dynamic range (up to 140 dB vs. 80 dB) and a higher\ntemporal resolution. Thus, the captured events can encode rich temporal cues\nunder challenging lighting conditions. However, these events lack texture\ninformation, posing problems in decoding temporal information effectively. SEEN\ntackles this issue from two different perspectives. First, we adopt\nconvolutional spiking layers to take advantage of the spiking neural network's\nability to decode pertinent temporal information. Second, SEEN learns to\nextract essential spatial cues from corresponding intensity frames and\nleverages a novel weight-copy scheme to convey spatial attention to the\nconvolutional spiking layers during training and inference. We extensively\nvalidate and demonstrate the effectiveness of our approach on a specially\ncollected Single-eye Event-based Emotion (SEE) dataset. To the best of our\nknowledge, our method is the first eye-based emotion recognition method that\nleverages event-based cameras and spiking neural network.\n","authors":["Haiwei Zhang","Jiqing Zhang","Bo Dong","Pieter Peers","Wenwei Wu","Xiaopeng Wei","Felix Heide","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01665v2","updated":"2023-10-06T06:26:34Z","published":"2023-02-03T11:37:20Z","title":"CVTNet: A Cross-View Transformer Network for Place Recognition Using\n  LiDAR Data","summary":"  LiDAR-based place recognition (LPR) is one of the most crucial components of\nautonomous vehicles to identify previously visited places in GPS-denied\nenvironments. Most existing LPR methods use mundane representations of the\ninput point cloud without considering different views, which may not fully\nexploit the information from LiDAR sensors. In this paper, we propose a\ncross-view transformer-based network, dubbed CVTNet, to fuse the range image\nviews (RIVs) and bird's eye views (BEVs) generated from the LiDAR data. It\nextracts correlations within the views themselves using intra-transformers and\nbetween the two different views using inter-transformers. Based on that, our\nproposed CVTNet generates a yaw-angle-invariant global descriptor for each\nlaser scan end-to-end online and retrieves previously seen places by descriptor\nmatching between the current query scan and the pre-built database. We evaluate\nour approach on three datasets collected with different sensor setups and\nenvironmental conditions. The experimental results show that our method\noutperforms the state-of-the-art LPR methods with strong robustness to\nviewpoint changes and long-time spans. Furthermore, our approach has a good\nreal-time performance that can run faster than the typical LiDAR frame rate.\nThe implementation of our method is released as open source at:\nhttps://github.com/BIT-MJY/CVTNet.\n","authors":["Junyi Ma","Guangming Xiong","Jingyi Xu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2302.01665v2.pdf","comment":"accepted by IEEE Transactions on Industrial Informatics 2023"},{"id":"http://arxiv.org/abs/2310.04010v1","updated":"2023-10-06T04:40:48Z","published":"2023-10-06T04:40:48Z","title":"Excision and Recovery: Enhancing Surface Anomaly Detection with\n  Attention-based Single Deterministic Masking","summary":"  Anomaly detection (AD) in surface inspection is an essential yet challenging\ntask in manufacturing due to the quantity imbalance problem of scarce abnormal\ndata. To overcome the above, a reconstruction encoder-decoder (ED) such as\nautoencoder or U-Net which is trained with only anomaly-free samples is widely\nadopted, in the hope that unseen abnormals should yield a larger reconstruction\nerror than normal. Over the past years, researches on self-supervised\nreconstruction-by-inpainting have been reported. They mask out suspected\ndefective regions for inpainting in order to make them invisible to the\nreconstruction ED to deliberately cause inaccurate reconstruction for\nabnormals. However, their limitation is multiple random masking to cover the\nwhole input image due to defective regions not being known in advance. We\npropose a novel reconstruction-by-inpainting method dubbed Excision and\nRecovery (EAR) that features single deterministic masking. For this, we exploit\na pre-trained spatial attention model to predict potential suspected defective\nregions that should be masked out. We also employ a variant of U-Net as our ED\nto further limit the reconstruction ability of the U-Net model for abnormals,\nin which skip connections of different layers can be selectively disabled. In\nthe training phase, all the skip connections are switched on to fully take the\nbenefits from the U-Net architecture. In contrast, for inferencing, we only\nkeep deeper skip connections with shallower connections off. We validate the\neffectiveness of EAR using an MNIST pre-trained attention for a commonly used\nsurface AD dataset, KolektorSDD2. The experimental results show that EAR\nachieves both better AD performance and higher throughput than state-of-the-art\nmethods. We expect that the proposed EAR model can be widely adopted as\ntraining and inference strategies for AD purposes.\n","authors":["YeongHyeon Park","Sungho Kang","Myung Jin Kim","Yeonho Lee","Juneho Yi"],"pdf_url":"https://arxiv.org/pdf/2310.04010v1.pdf","comment":"5 pages, 3 figures, 4 tables"},{"id":"http://arxiv.org/abs/2211.16785v2","updated":"2023-10-06T04:36:27Z","published":"2022-11-30T06:56:39Z","title":"SafeSpace MFNet: Precise and Efficient MultiFeature Drone Detection\n  Network","summary":"  The increasing prevalence of unmanned aerial vehicles (UAVs), commonly known\nas drones, has generated a demand for reliable detection systems. The\ninappropriate use of drones presents potential security and privacy hazards,\nparticularly concerning sensitive facilities. To overcome those obstacles, we\nproposed the concept of MultiFeatureNet (MFNet), a solution that enhances\nfeature representation by capturing the most concentrated feature maps.\nAdditionally, we present MultiFeatureNet-Feature Attention (MFNet-FA), a\ntechnique that adaptively weights different channels of the input feature maps.\nTo meet the requirements of multi-scale detection, we presented the versions of\nMFNet and MFNet-FA, namely the small (S), medium (M), and large (L). The\noutcomes reveal notable performance enhancements. For optimal bird detection,\nMFNet-M (Ablation study 2) achieves an impressive precision of 99.8\\%, while\nfor UAV detection, MFNet-L (Ablation study 2) achieves a precision score of\n97.2\\%. Among the options, MFNet-FA-S (Ablation study 3) emerges as the most\nresource-efficient alternative, considering its small feature map size,\ncomputational demands (GFLOPs), and operational efficiency (in frame per\nsecond). This makes it particularly suitable for deployment on hardware with\nlimited capabilities. Additionally, MFNet-FA-S (Ablation study 3) stands out\nfor its swift real-time inference and multiple-object detection due to the\nincorporation of the FA module. The proposed MFNet-L with the focus module\n(Ablation study 2) demonstrates the most remarkable classification outcomes,\nboasting an average precision of 98.4\\%, average recall of 96.6\\%, average mean\naverage precision (mAP) of 98.3\\%, and average intersection over union (IoU) of\n72.8\\%. To encourage reproducible research, the dataset, and code for MFNet are\nfreely available as an open-source project:\ngithub.com/ZeeshanKaleem/MultiFeatureNet.\n","authors":["Misha Urooj Khan","Mahnoor Dil","Muhammad Zeshan Alam","Farooq Alam Orakazi","Abdullah M. Almasoud","Zeeshan Kaleem","Chau Yuen"],"pdf_url":"https://arxiv.org/pdf/2211.16785v2.pdf","comment":"Paper accepted in IEEE TVT"},{"id":"http://arxiv.org/abs/2305.03040v3","updated":"2023-10-06T04:33:33Z","published":"2023-05-04T17:58:05Z","title":"TUVF: Learning Generalizable Texture UV Radiance Fields","summary":"  Textures are a vital aspect of creating visually appealing and realistic 3D\nmodels. In this paper, we study the problem of generating high-fidelity texture\ngiven shapes of 3D assets, which has been relatively less explored compared\nwith generic 3D shape modeling. Our goal is to facilitate a controllable\ntexture generation process, such that one texture code can correspond to a\nparticular appearance style independent of any input shapes from a category. We\nintroduce Texture UV Radiance Fields (TUVF) that generate textures in a\nlearnable UV sphere space rather than directly on the 3D shape. This allows the\ntexture to be disentangled from the underlying shape and transferable to other\nshapes that share the same UV space, i.e., from the same category. We integrate\nthe UV sphere space with the radiance field, which provides a more efficient\nand accurate representation of textures than traditional texture maps. We\nperform our experiments on synthetic and real-world object datasets where we\nachieve not only realistic synthesis but also substantial improvements over\nstate-of-the-arts on texture controlling and editing. Project Page:\nhttps://www.anjiecheng.me/TUVF\n","authors":["An-Chieh Cheng","Xueting Li","Sifei Liu","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2305.03040v3.pdf","comment":"Project Page: https://www.anjiecheng.me/TUVF"},{"id":"http://arxiv.org/abs/2308.01251v2","updated":"2023-10-06T04:15:56Z","published":"2023-08-02T16:11:51Z","title":"Hyper-pixel-wise Contrastive Learning Augmented Segmentation Network for\n  Old Landslide Detection through Fusing High-Resolution Remote Sensing Images\n  and Digital Elevation Model Data","summary":"  As a natural disaster, landslide often brings tremendous losses to human\nlives, so it urgently demands reliable detection of landslide risks. When\ndetecting old landslides that present important information for landslide risk\nwarning, problems such as visual blur and small-sized dataset cause great\nchallenges when using remote sensing data. To extract accurate semantic\nfeatures, a hyper-pixel-wise contrastive learning augmented segmentation\nnetwork (HPCL-Net) is proposed, which augments the local salient feature\nextraction from boundaries of landslides through HPCL-Net and fuses\nheterogeneous infromation in the semantic space from high-resolution remote\nsensing images and digital elevation model data. For full utilization of\nprecious samples, a global hyper-pixel-wise sample pair queues-based\ncontrastive learning method is developed, which includes the construction of\nglobal queues that store hyper-pixel-wise samples and the updating scheme of a\nmomentum encoder, reliably enhancing the extraction ability of semantic\nfeatures. The proposed HPCL-Net is evaluated on the Loess Plateau old landslide\ndataset and experimental results verify that the proposed HPCL-Net greatly\noutperforms existing models, where the mIoU is increased from 0.620 to 0.651,\nthe Landslide IoU is improved from 0.334 to 0.394 and the F1score is enhanced\nfrom 0.501 to 0.565.\n","authors":["Yiming Zhou","Yuexing Peng","Wei Li","Junchuan Yu","Daqing Ge","Wei Xiang"],"pdf_url":"https://arxiv.org/pdf/2308.01251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03149v2","updated":"2023-10-06T03:39:49Z","published":"2023-10-04T20:26:59Z","title":"Attributing Learned Concepts in Neural Networks to Training Data","summary":"  By now there is substantial evidence that deep learning models learn certain\nhuman-interpretable features as part of their internal representations of data.\nAs having the right (or wrong) concepts is critical to trustworthy machine\nlearning systems, it is natural to ask which inputs from the model's original\ntraining set were most important for learning a concept at a given layer. To\nanswer this, we combine data attribution methods with methods for probing the\nconcepts learned by a model. Training network and probe ensembles for two\nconcept datasets on a range of network layers, we use the recently developed\nTRAK method for large-scale data attribution. We find some evidence for\nconvergence, where removing the 10,000 top attributing images for a concept and\nretraining the model does not change the location of the concept in the network\nnor the probing sparsity of the concept. This suggests that rather than being\nhighly dependent on a few specific examples, the features that inform the\ndevelopment of a concept are spread in a more diffuse manner across its\nexemplars, implying robustness in concept formation.\n","authors":["Nicholas Konz","Charles Godfrey","Madelyn Shapiro","Jonathan Tu","Henry Kvinge","Davis Brown"],"pdf_url":"https://arxiv.org/pdf/2310.03149v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14293v2","updated":"2023-10-06T03:18:36Z","published":"2023-09-25T17:04:30Z","title":"NAS-NeRF: Generative Neural Architecture Search for Neural Radiance\n  Fields","summary":"  Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but\ntheir high computational complexity limits deployability. While existing\nneural-based solutions strive for efficiency, they use one-size-fits-all\narchitectures regardless of scene complexity. The same architecture may be\nunnecessarily large for simple scenes but insufficient for complex ones. Thus,\nthere is a need to dynamically optimize the neural network component of NeRFs\nto achieve a balance between computational complexity and specific targets for\nsynthesis quality. We introduce NAS-NeRF, a generative neural architecture\nsearch strategy that generates compact, scene-specialized NeRF architectures by\nbalancing architecture complexity and target synthesis quality metrics. Our\nmethod incorporates constraints on target metrics and budgets to guide the\nsearch towards architectures tailored for each scene. Experiments on the\nBlender synthetic dataset show the proposed NAS-NeRF can generate architectures\nup to 5.74$\\times$ smaller, with 4.19$\\times$ fewer FLOPs, and 1.93$\\times$\nfaster on a GPU than baseline NeRFs, without suffering a drop in SSIM.\nFurthermore, we illustrate that NAS-NeRF can also achieve architectures up to\n23$\\times$ smaller, with 22$\\times$ fewer FLOPs, and 4.7$\\times$ faster than\nbaseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made\npublicly available at https://saeejithnair.github.io/NAS-NeRF.\n","authors":["Saeejith Nair","Yuhao Chen","Mohammad Javad Shafiee","Alexander Wong"],"pdf_url":"https://arxiv.org/pdf/2309.14293v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2310.03986v1","updated":"2023-10-06T03:04:21Z","published":"2023-10-06T03:04:21Z","title":"Robust Multimodal Learning with Missing Modalities via\n  Parameter-Efficient Adaptation","summary":"  Multimodal learning seeks to utilize data from multiple sources to improve\nthe overall performance of downstream tasks. It is desirable for redundancies\nin the data to make multimodal systems robust to missing or corrupted\nobservations in some correlated modalities. However, we observe that the\nperformance of several existing multimodal networks significantly deteriorates\nif one or multiple modalities are absent at test time. To enable robustness to\nmissing modalities, we propose simple and parameter-efficient adaptation\nprocedures for pretrained multimodal networks. In particular, we exploit\nlow-rank adaptation and modulation of intermediate features to compensate for\nthe missing modalities. We demonstrate that such adaptation can partially\nbridge performance drop due to missing modalities and outperform independent,\ndedicated networks trained for the available modality combinations in some\ncases. The proposed adaptation requires extremely small number of parameters\n(e.g., fewer than 0.7% of the total parameters in most experiments). We conduct\na series of experiments to highlight the robustness of our proposed method\nusing diverse datasets for RGB-thermal and RGB-Depth semantic segmentation,\nmultimodal material segmentation, and multimodal sentiment analysis tasks. Our\nproposed method demonstrates versatility across various tasks and datasets, and\noutperforms existing methods for robust multimodal learning with missing\nmodalities.\n","authors":["Md Kaykobad Reza","Ashley Prater-Bennette","M. Salman Asif"],"pdf_url":"https://arxiv.org/pdf/2310.03986v1.pdf","comment":"18 pages, 3 figures, 11 tables"},{"id":"http://arxiv.org/abs/2310.03981v1","updated":"2023-10-06T02:35:31Z","published":"2023-10-06T02:35:31Z","title":"CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell\n  Segmentation","summary":"  While pre-training on object detection tasks, such as Common Objects in\nContexts (COCO) [1], could significantly boost the performance of cell\nsegmentation, it still consumes on massive fine-annotated cell images [2] with\nbounding boxes, masks, and cell types for every cell in every image, to\nfine-tune the pre-trained model. To lower the cost of annotation, this work\nconsiders the problem of pre-training DNN models for few-shot cell\nsegmentation, where massive unlabeled cell images are available but only a\nsmall proportion is annotated. Hereby, we propose Cross-domain Unsupervised\nPre-training, namely CUPre, transferring the capability of object detection and\ninstance segmentation for common visual objects (learned from COCO) to the\nvisual domain of cells using unlabeled images. Given a standard COCO\npre-trained network with backbone, neck, and head modules, CUPre adopts an\nalternate multi-task pre-training (AMT2) procedure with two sub-tasks -- in\nevery iteration of pre-training, AMT2 first trains the backbone with cell\nimages from multiple cell datasets via unsupervised momentum contrastive\nlearning (MoCo) [3], and then trains the whole model with vanilla COCO datasets\nvia instance segmentation. After pre-training, CUPre fine-tunes the whole model\non the cell segmentation task using a few annotated images. We carry out\nextensive experiments to evaluate CUPre using LIVECell [2] and BBBC038 [4]\ndatasets in few-shot instance segmentation settings. The experiment shows that\nCUPre can outperform existing pre-training methods, achieving the highest\naverage precision (AP) for few-shot cell segmentation and detection.\n","authors":["Weibin Liao","Xuhong Li","Qingzhong Wang","Yanwu Xu","Zhaozheng Yin","Haoyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.03981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08851v2","updated":"2023-10-06T02:31:51Z","published":"2023-05-15T17:59:15Z","title":"MV-Map: Offboard HD-Map Generation with Multi-view Consistency","summary":"  While bird's-eye-view (BEV) perception models can be useful for building\nhigh-definition maps (HD-Maps) with less human labor, their results are often\nunreliable and demonstrate noticeable inconsistencies in the predicted HD-Maps\nfrom different viewpoints. This is because BEV perception is typically set up\nin an 'onboard' manner, which restricts the computation and consequently\nprevents algorithms from reasoning multiple views simultaneously. This paper\novercomes these limitations and advocates a more practical 'offboard' HD-Map\ngeneration setup that removes the computation constraints, based on the fact\nthat HD-Maps are commonly reusable infrastructures built offline in data\ncenters. To this end, we propose a novel offboard pipeline called MV-Map that\ncapitalizes multi-view consistency and can handle an arbitrary number of frames\nwith the key design of a 'region-centric' framework. In MV-Map, the target\nHD-Maps are created by aggregating all the frames of onboard predictions,\nweighted by the confidence scores assigned by an 'uncertainty network'. To\nfurther enhance multi-view consistency, we augment the uncertainty network with\nthe global 3D structure optimized by a voxelized neural radiance field\n(Voxel-NeRF). Extensive experiments on nuScenes show that our MV-Map\nsignificantly improves the quality of HD-Maps, further highlighting the\nimportance of offboard methods for HD-Map generation.\n","authors":["Ziyang Xie","Ziqi Pang","Yuxiong Wang"],"pdf_url":"https://arxiv.org/pdf/2305.08851v2.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2310.03967v1","updated":"2023-10-06T01:53:27Z","published":"2023-10-06T01:53:27Z","title":"Sub-token ViT Embedding via Stochastic Resonance Transformers","summary":"  We discover the presence of quantization artifacts in Vision Transformers\n(ViTs), which arise due to the image tokenization step inherent in these\narchitectures. These artifacts result in coarsely quantized features, which\nnegatively impact performance, especially on downstream dense prediction tasks.\nWe present a zero-shot method to improve how pre-trained ViTs handle spatial\nquantization. In particular, we propose to ensemble the features obtained from\nperturbing input images via sub-token spatial translations, inspired by\nStochastic Resonance, a method traditionally applied to climate dynamics and\nsignal processing. We term our method ``Stochastic Resonance Transformer\"\n(SRT), which we show can effectively super-resolve features of pre-trained\nViTs, capturing more of the local fine-grained structures that might otherwise\nbe neglected as a result of tokenization. SRT can be applied at any layer, on\nany task, and does not require any fine-tuning. The advantage of the former is\nevident when applied to monocular depth prediction, where we show that\nensembling model outputs are detrimental while applying SRT on intermediate ViT\nfeatures outperforms the baseline models by an average of 4.7% and 14.9% on the\nRMSE and RMSE-log metrics across three different architectures. When applied to\nsemi-supervised video object segmentation, SRT also improves over the baseline\nmodels uniformly across all metrics, and by an average of 2.4% in F&J score. We\nfurther show that these quantization artifacts can be attenuated to some extent\nvia self-distillation. On the unsupervised salient region segmentation, SRT\nimproves upon the base model by an average of 2.1% on the maxF metric. Finally,\ndespite operating purely on pixel-level features, SRT generalizes to non-dense\nprediction tasks such as image retrieval and object discovery, yielding\nconsistent improvements of up to 2.6% and 1.0% respectively.\n","authors":["Dong Lao","Yangchao Wu","Tian Yu Liu","Alex Wong","Stefano Soatto"],"pdf_url":"https://arxiv.org/pdf/2310.03967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09464v2","updated":"2023-10-06T01:10:33Z","published":"2023-09-18T03:55:41Z","title":"Reducing Adversarial Training Cost with Gradient Approximation","summary":"  Deep learning models have achieved state-of-the-art performances in various\ndomains, while they are vulnerable to the inputs with well-crafted but small\nperturbations, which are named after adversarial examples (AEs). Among many\nstrategies to improve the model robustness against AEs, Projected Gradient\nDescent (PGD) based adversarial training is one of the most effective methods.\nUnfortunately, the prohibitive computational overhead of generating strong\nenough AEs, due to the maximization of the loss function, sometimes makes the\nregular PGD adversarial training impractical when using larger and more\ncomplicated models. In this paper, we propose that the adversarial loss can be\napproximated by the partial sum of Taylor series. Furthermore, we approximate\nthe gradient of adversarial loss and propose a new and efficient adversarial\ntraining method, adversarial training with gradient approximation (GAAT), to\nreduce the cost of building up robust models. Additionally, extensive\nexperiments demonstrate that this efficiency improvement can be achieved\nwithout any or with very little loss in accuracy on natural and adversarial\nexamples, which show that our proposed method saves up to 60\\% of the training\ntime with comparable model test accuracy on MNIST, CIFAR-10 and CIFAR-100\ndatasets.\n","authors":["Huihui Gong","Shuo Yang","Siqi Ma","Seyit Camtepe","Surya Nepal","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2309.09464v2.pdf","comment":"There are some issues of the experiments. Withraw this manuscript"},{"id":"http://arxiv.org/abs/2310.03959v1","updated":"2023-10-06T00:58:52Z","published":"2023-10-06T00:58:52Z","title":"Towards Increasing the Robustness of Predictive Steering-Control\n  Autonomous Navigation Systems Against Dash Cam Image Angle Perturbations Due\n  to Pothole Encounters","summary":"  Vehicle manufacturers are racing to create autonomous navigation and steering\ncontrol algorithms for their vehicles. These software are made to handle\nvarious real-life scenarios such as obstacle avoidance and lane maneuvering.\nThere is some ongoing research to incorporate pothole avoidance into these\nautonomous systems. However, there is very little research on the effect of\nhitting a pothole on the autonomous navigation software that uses cameras to\nmake driving decisions. Perturbations in the camera angle when hitting a\npothole can cause errors in the predicted steering angle. In this paper, we\npresent a new model to compensate for such angle perturbations and reduce any\nerrors in steering control prediction algorithms. We evaluate our model on\nperturbations of publicly available datasets and show our model can reduce the\nerrors in the estimated steering angle from perturbed images to 2.3%, making\nautonomous steering control robust against the dash cam image angle\nperturbations induced when one wheel of a car goes over a pothole.\n","authors":["Shivam Aarya"],"pdf_url":"https://arxiv.org/pdf/2310.03959v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.03957v1","updated":"2023-10-06T00:52:48Z","published":"2023-10-06T00:52:48Z","title":"Understanding prompt engineering may not require rethinking\n  generalization","summary":"  Zero-shot learning in prompted vision-language models, the practice of\ncrafting prompts to build classifiers without an explicit training process, has\nachieved impressive performance in many settings. This success presents a\nseemingly surprising observation: these methods suffer relatively little from\noverfitting, i.e., when a prompt is manually engineered to achieve low error on\na given training set (thus rendering the method no longer actually zero-shot),\nthe approach still performs well on held-out test data. In this paper, we show\nthat we can explain such performance well via recourse to classical PAC-Bayes\nbounds. Specifically, we show that the discrete nature of prompts, combined\nwith a PAC-Bayes prior given by a language model, results in generalization\nbounds that are remarkably tight by the standards of the literature: for\ninstance, the generalization bound of an ImageNet classifier is often within a\nfew percentage points of the true test error. We demonstrate empirically that\nthis holds for existing handcrafted prompts and prompts generated through\nsimple greedy search. Furthermore, the resulting bound is well-suited for model\nselection: the models with the best bound typically also have the best test\nperformance. This work thus provides a possible justification for the\nwidespread practice of prompt engineering, even if it seems that such methods\ncould potentially overfit the training data.\n","authors":["Victor Akinwande","Yiding Jiang","Dylan Sam","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2310.03957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03956v1","updated":"2023-10-06T00:47:57Z","published":"2023-10-06T00:47:57Z","title":"Gradient Descent Provably Solves Nonlinear Tomographic Reconstruction","summary":"  In computed tomography (CT), the forward model consists of a linear Radon\ntransform followed by an exponential nonlinearity based on the attenuation of\nlight according to the Beer-Lambert Law. Conventional reconstruction often\ninvolves inverting this nonlinearity as a preprocessing step and then solving a\nconvex inverse problem. However, this nonlinear measurement preprocessing\nrequired to use the Radon transform is poorly conditioned in the vicinity of\nhigh-density materials, such as metal. This preprocessing makes CT\nreconstruction methods numerically sensitive and susceptible to artifacts near\nhigh-density regions. In this paper, we study a technique where the signal is\ndirectly reconstructed from raw measurements through the nonlinear forward\nmodel. Though this optimization is nonconvex, we show that gradient descent\nprovably converges to the global optimum at a geometric rate, perfectly\nreconstructing the underlying signal with a near minimal number of random\nmeasurements. We also prove similar results in the under-determined setting\nwhere the number of measurements is significantly smaller than the dimension of\nthe signal. This is achieved by enforcing prior structural information about\nthe signal through constraints on the optimization variables. We illustrate the\nbenefits of direct nonlinear CT reconstruction with cone-beam CT experiments on\nsynthetic and real 3D volumes. We show that this approach reduces metal\nartifacts compared to a commercial reconstruction of a human skull with metal\ndental crowns.\n","authors":["Sara Fridovich-Keil","Fabrizio Valdivia","Gordon Wetzstein","Benjamin Recht","Mahdi Soltanolkotabi"],"pdf_url":"https://arxiv.org/pdf/2310.03956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12245v2","updated":"2023-10-06T00:45:00Z","published":"2023-09-21T16:43:29Z","title":"Adaptive Input-image Normalization for Solving the Mode Collapse Problem\n  in GAN-based X-ray Images","summary":"  Biomedical image datasets can be imbalanced due to the rarity of targeted\ndiseases. Generative Adversarial Networks play a key role in addressing this\nimbalance by enabling the generation of synthetic images to augment datasets.\nIt is important to generate synthetic images that incorporate a diverse range\nof features to accurately represent the distribution of features present in the\ntraining imagery. Furthermore, the absence of diverse features in synthetic\nimages can degrade the performance of machine learning classifiers. The mode\ncollapse problem impacts Generative Adversarial Networks' capacity to generate\ndiversified images. Mode collapse comes in two varieties: intra-class and\ninter-class. In this paper, both varieties of the mode collapse problem are\ninvestigated, and their subsequent impact on the diversity of synthetic X-ray\nimages is evaluated. This work contributes an empirical demonstration of the\nbenefits of integrating the adaptive input-image normalization with the Deep\nConvolutional GAN and Auxiliary Classifier GAN to alleviate the mode collapse\nproblems. Synthetically generated images are utilized for data augmentation and\ntraining a Vision Transformer model. The classification performance of the\nmodel is evaluated using accuracy, recall, and precision scores. Results\ndemonstrate that the DCGAN and the ACGAN with adaptive input-image\nnormalization outperform the DCGAN and ACGAN with un-normalized X-ray images as\nevidenced by the superior diversity scores and classification scores.\n","authors":["Muhammad Muneeb Saad","Mubashir Husain Rehmani","Ruairi O'Reilly"],"pdf_url":"https://arxiv.org/pdf/2309.12245v2.pdf","comment":"Submitted to the Elsevier Journal"},{"id":"http://arxiv.org/abs/2112.03379v2","updated":"2023-10-06T00:44:19Z","published":"2021-12-03T01:38:38Z","title":"Deep Efficient Continuous Manifold Learning for Time Series Modeling","summary":"  Modeling non-Euclidean data is drawing extensive attention along with the\nunprecedented successes of deep neural networks in diverse fields.\nParticularly, a symmetric positive definite matrix is being actively studied in\ncomputer vision, signal processing, and medical image analysis, due to its\nability to learn beneficial statistical representations. However, owing to its\nrigid constraints, it remains challenging to optimization problems and\ninefficient computational costs, especially, when incorporating it with a deep\nlearning framework. In this paper, we propose a framework to exploit a\ndiffeomorphism mapping between Riemannian manifolds and a Cholesky space, by\nwhich it becomes feasible not only to efficiently solve optimization problems\nbut also to greatly reduce computation costs. Further, for dynamic modeling of\ntime-series data, we devise a continuous manifold learning method by\nsystematically integrating a manifold ordinary differential equation and a\ngated recurrent neural network. It is worth noting that due to the nice\nparameterization of matrices in a Cholesky space, training our proposed network\nequipped with Riemannian geometric metrics is straightforward. We demonstrate\nthrough experiments over regular and irregular time-series datasets that our\nproposed model can be efficiently and reliably trained and outperforms existing\nmanifold methods and state-of-the-art methods in various time-series tasks.\n","authors":["Seungwoo Jeong","Wonjun Ko","Ahmad Wisnu Mulyadi","Heung-Il Suk"],"pdf_url":"https://arxiv.org/pdf/2112.03379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03559v2","updated":"2023-10-06T00:42:58Z","published":"2023-10-05T14:16:22Z","title":"MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT\n  Images","summary":"  This paper introduces an innovative methodology for producing high-quality 3D\nlung CT images guided by textual information. While diffusion-based generative\nmodels are increasingly used in medical imaging, current state-of-the-art\napproaches are limited to low-resolution outputs and underutilize radiology\nreports' abundant information. The radiology reports can enhance the generation\nprocess by providing additional guidance and offering fine-grained control over\nthe synthesis of images. Nevertheless, expanding text-guided generation to\nhigh-resolution 3D images poses significant memory and anatomical\ndetail-preserving challenges. Addressing the memory issue, we introduce a\nhierarchical scheme that uses a modified UNet architecture. We start by\nsynthesizing low-resolution images conditioned on the text, serving as a\nfoundation for subsequent generators for complete volumetric data. To ensure\nthe anatomical plausibility of the generated samples, we provide further\nguidance by generating vascular, airway, and lobular segmentation masks in\nconjunction with the CT images. The model demonstrates the capability to use\ntextual input and segmentation tasks to generate synthesized images. The\nresults of comparative assessments indicate that our approach exhibits superior\nperformance compared to the most advanced models based on GAN and diffusion\ntechniques, especially in accurately retaining crucial anatomical features such\nas fissure lines, airways, and vascular structures. This innovation introduces\nnovel possibilities. This study focuses on two main objectives: (1) the\ndevelopment of a method for creating images based on textual prompts and\nanatomical components, and (2) the capability to generate new images\nconditioning on anatomical elements. The advancements in image generation can\nbe applied to enhance numerous downstream tasks.\n","authors":["Yanwu Xu","Li Sun","Wei Peng","Shyam Visweswaran","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2310.03559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13196v3","updated":"2023-10-06T00:38:16Z","published":"2023-09-22T22:12:30Z","title":"ClusterFormer: Clustering As A Universal Visual Learner","summary":"  This paper presents CLUSTERFORMER, a universal vision model that is based on\nthe CLUSTERing paradigm with TransFORMER. It comprises two novel designs: 1.\nrecurrent cross-attention clustering, which reformulates the cross-attention\nmechanism in Transformer and enables recursive updates of cluster centers to\nfacilitate strong representation learning; and 2. feature dispatching, which\nuses the updated cluster centers to redistribute image features through\nsimilarity-based metrics, resulting in a transparent pipeline. This elegant\ndesign streamlines an explainable and transferable workflow, capable of\ntackling heterogeneous vision tasks (i.e., image classification, object\ndetection, and image segmentation) with varying levels of clustering\ngranularity (i.e., image-, box-, and pixel-level). Empirical results\ndemonstrate that CLUSTERFORMER outperforms various well-known specialized\narchitectures, achieving 83.41% top-1 acc. over ImageNet-1K for image\nclassification, 54.2% and 47.0% mAP over MS COCO for object detection and\ninstance segmentation, 52.4% mIoU over ADE20K for semantic segmentation, and\n55.8% PQ over COCO Panoptic for panoptic segmentation. For its efficacy, we\nhope our work can catalyze a paradigm shift in universal models in computer\nvision.\n","authors":["James C. Liang","Yiming Cui","Qifan Wang","Tong Geng","Wenguan Wang","Dongfang Liu"],"pdf_url":"https://arxiv.org/pdf/2309.13196v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03952v1","updated":"2023-10-06T00:32:36Z","published":"2023-10-06T00:32:36Z","title":"ILSH: The Imperial Light-Stage Head Dataset for Human Head View\n  Synthesis","summary":"  This paper introduces the Imperial Light-Stage Head (ILSH) dataset, a novel\nlight-stage-captured human head dataset designed to support view synthesis\nacademic challenges for human heads. The ILSH dataset is intended to facilitate\ndiverse approaches, such as scene-specific or generic neural rendering,\nmultiple-view geometry, 3D vision, and computer graphics, to further advance\nthe development of photo-realistic human avatars. This paper details the setup\nof a light-stage specifically designed to capture high-resolution (4K) human\nhead images and describes the process of addressing challenges (preprocessing,\nethical issues) in collecting high-quality data. In addition to the data\ncollection, we address the split of the dataset into train, validation, and\ntest sets. Our goal is to design and support a fair view synthesis challenge\ntask for this novel dataset, such that a similar level of performance can be\nmaintained and expected when using the test set, as when using the validation\nset. The ILSH dataset consists of 52 subjects captured using 24 cameras with\nall 82 lighting sources turned on, resulting in a total of 1,248 close-up head\nimages, border masks, and camera pose pairs.\n","authors":["Jiali Zheng","Youngkyoon Jang","Athanasios Papaioannou","Christos Kampouris","Rolandos Alexandros Potamias","Foivos Paraperas Papantoniou","Efstathios Galanakis","Ales Leonardis","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2310.03952v1.pdf","comment":"ICCV 2023 Workshop, 9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2308.12462v2","updated":"2023-10-06T00:32:26Z","published":"2023-08-23T22:55:45Z","title":"Overcoming General Knowledge Loss with Selective Parameter Update","summary":"  Foundation models encompass an extensive knowledge base and offer remarkable\ntransferability. However, this knowledge becomes outdated or insufficient over\ntime. The challenge lies in continuously updating foundation models to\naccommodate novel information while retaining their original capabilities.\nLeveraging the fact that foundation models have initial knowledge on various\ntasks and domains, we propose a novel approach that, instead of updating all\nparameters equally, localizes the updates to a sparse set of parameters\nrelevant to the task being learned. We strike a balance between efficiency and\nnew tasks performance, while maintaining the transferability and\ngeneralizability of foundation models. We extensively evaluate our method on\nfoundational vision-language models with a diverse spectrum of continual\nlearning tasks. Our method achieves improvements on the newly learned tasks\naccuracy up to 7% while preserving the pretraining knowledge with a negligible\ndecrease of 0.9% on a representative control set accuracy.\n","authors":["Wenxuan Zhang","Paul Janson","Rahaf Aljundi","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2308.12462v2.pdf","comment":null}]},"2023-10-09T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2310.05921v1","updated":"2023-10-09T17:59:30Z","published":"2023-10-09T17:59:30Z","title":"Conformal Decision Theory: Safe Autonomous Decisions from Imperfect\n  Predictions","summary":"  We introduce Conformal Decision Theory, a framework for producing safe\nautonomous decisions despite imperfect machine learning predictions. Examples\nof such decisions are ubiquitous, from robot planning algorithms that rely on\npedestrian predictions, to calibrating autonomous manufacturing to exhibit high\nthroughput and low error, to the choice of trusting a nominal policy versus\nswitching to a safe backup policy at run-time. The decisions produced by our\nalgorithms are safe in the sense that they come with provable statistical\nguarantees of having low risk without any assumptions on the world model\nwhatsoever; the observations need not be I.I.D. and can even be adversarial.\nThe theory extends results from conformal prediction to calibrate decisions\ndirectly, without requiring the construction of prediction sets. Experiments\ndemonstrate the utility of our approach in robot motion planning around humans,\nautomated stock trading, and robot manufacturin\n","authors":["Jordan Lekeufack","Anastasios A. Angelopoulos","Andrea Bajcsy","Michael I. Jordan","Jitendra Malik"],"pdf_url":"https://arxiv.org/pdf/2310.05921v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.05905v1","updated":"2023-10-09T17:49:50Z","published":"2023-10-09T17:49:50Z","title":"TAIL: Task-specific Adapters for Imitation Learning with Large\n  Pretrained Models","summary":"  The full potential of large pretrained models remains largely untapped in\ncontrol domains like robotics. This is mainly because of the scarcity of data\nand the computational challenges associated with training or fine-tuning these\nlarge models for such applications. Prior work mainly emphasizes effective\npretraining of large models for decision-making, with little exploration into\nhow to perform data-efficient continual adaptation of these models for new\ntasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters\nfor Imitation Learning), a framework for efficient adaptation to new control\ntasks. Inspired by recent advancements in parameter-efficient fine-tuning in\nlanguage domains, we explore efficient fine-tuning techniques -- e.g.,\nBottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) -- in TAIL to\nadapt large pretrained models for new tasks with limited demonstration data.\nOur extensive experiments in large-scale language-conditioned manipulation\ntasks comparing prevalent parameter-efficient fine-tuning techniques and\nadaptation baselines suggest that TAIL with LoRA can achieve the best\npost-adaptation performance with only 1\\% of the trainable parameters of full\nfine-tuning, while avoiding catastrophic forgetting and preserving adaptation\nplasticity in continual learning settings.\n","authors":["Zuxin Liu","Jesse Zhang","Kavosh Asadi","Yao Liu","Ding Zhao","Shoham Sabach","Rasool Fakoor"],"pdf_url":"https://arxiv.org/pdf/2310.05905v1.pdf","comment":"21 pages, 8 figures, 8 tables"},{"id":"http://arxiv.org/abs/2310.05904v1","updated":"2023-10-09T17:47:09Z","published":"2023-10-09T17:47:09Z","title":"On Multi-Fidelity Impedance Tuning for Human-Robot Cooperative\n  Manipulation","summary":"  We examine how a human-robot interaction (HRI) system may be designed when\ninput-output data from previous experiments are available. In particular, we\nconsider how to select an optimal impedance in the assistance design for a\ncooperative manipulation task with a new operator. Due to the variability\nbetween individuals, the design parameters that best suit one operator of the\nrobot may not be the best parameters for another one. However, by incorporating\nhistorical data using a linear auto-regressive (AR-1) Gaussian process, the\nsearch for a new operator's optimal parameters can be accelerated. We lay out a\nframework for optimizing the human-robot cooperative manipulation that only\nrequires input-output data. We establish how the AR-1 model improves the bound\non the regret and numerically simulate a human-robot cooperative manipulation\ntask to show the regret improvement. Further, we show how our approach's\ninput-output nature provides robustness against modeling error through an\nadditional numerical study.\n","authors":["Ethan Lau","Vaibhav Srivastava","Shaunak D. Bopardikar"],"pdf_url":"https://arxiv.org/pdf/2310.05904v1.pdf","comment":"7 pages, 3 figures. Submitted to the 2024 ACC on September 29, 2023"},{"id":"http://arxiv.org/abs/2310.05885v1","updated":"2023-10-09T17:28:05Z","published":"2023-10-09T17:28:05Z","title":"DTPP: Differentiable Joint Conditional Prediction and Cost Evaluation\n  for Tree Policy Planning in Autonomous Driving","summary":"  Motion prediction and cost evaluation are vital components in the\ndecision-making system of autonomous vehicles. However, existing methods often\nignore the importance of cost learning and treat them as separate modules. In\nthis study, we employ a tree-structured policy planner and propose a\ndifferentiable joint training framework for both ego-conditioned prediction and\ncost models, resulting in a direct improvement of the final planning\nperformance. For conditional prediction, we introduce a query-centric\nTransformer model that performs efficient ego-conditioned motion prediction.\nFor planning cost, we propose a learnable context-aware cost function with\nlatent interaction features, facilitating differentiable joint learning. We\nvalidate our proposed approach using the real-world nuPlan dataset and its\nassociated planning test platform. Our framework not only matches\nstate-of-the-art planning methods but outperforms other learning-based methods\nin planning quality, while operating more efficiently in terms of runtime. We\nshow that joint training delivers significantly better performance than\nseparate training of the two modules. Additionally, we find that\ntree-structured policy planning outperforms the conventional single-stage\nplanning approach.\n","authors":["Zhiyu Huang","Peter Karkus","Boris Ivanovic","Yuxiao Chen","Marco Pavone","Chen Lv"],"pdf_url":"https://arxiv.org/pdf/2310.05885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14970v2","updated":"2023-10-09T17:25:52Z","published":"2023-09-26T14:42:28Z","title":"Recurrent Hypernetworks are Surprisingly Strong in Meta-RL","summary":"  Deep reinforcement learning (RL) is notoriously impractical to deploy due to\nsample inefficiency. Meta-RL directly addresses this sample inefficiency by\nlearning to perform few-shot learning when a distribution of related tasks is\navailable for meta-training. While many specialized meta-RL methods have been\nproposed, recent work suggests that end-to-end learning in conjunction with an\noff-the-shelf sequential model, such as a recurrent network, is a surprisingly\nstrong baseline. However, such claims have been controversial due to limited\nsupporting evidence, particularly in the face of prior work establishing\nprecisely the opposite. In this paper, we conduct an empirical investigation.\nWhile we likewise find that a recurrent network can achieve strong performance,\nwe demonstrate that the use of hypernetworks is crucial to maximizing their\npotential. Surprisingly, when combined with hypernetworks, the recurrent\nbaselines that are far simpler than existing specialized methods actually\nachieve the strongest performance of all methods evaluated.\n","authors":["Jacob Beck","Risto Vuorio","Zheng Xiong","Shimon Whiteson"],"pdf_url":"https://arxiv.org/pdf/2309.14970v2.pdf","comment":"Published at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2208.10765v2","updated":"2023-10-09T17:03:26Z","published":"2022-08-23T06:55:53Z","title":"A Low-Cost Lane-Following Algorithm for Cyber-Physical Robots","summary":"  Duckiebots are low-cost mobile robots that are widely used in the fields of\nresearch and education. Although there are existing self-driving algorithms for\nthe Duckietown platform, they are either too complex or perform too poorly to\nnavigate a multi-lane track. Moreover, it is essential to give memory and\ncomputational resources to a Duckiebot so it can perform additional tasks such\nas out-of-distribution input detection. In order to satisfy these constraints,\nwe built a low-cost autonomous driving algorithm capable of driving on a\ntwo-lane track. The algorithm uses traditional computer vision techniques to\nidentify the central lane on the track and obtain the relevant steering angle.\nThe steering is then controlled by a PID controller that smoothens the movement\nof the Duckiebot. The performance of the algorithm was compared to that of the\nNeurIPS 2018 AI Driving Olympics (AIDO) finalists, and it outperformed all but\none finalists. The two main contributions of our algorithm are its low\ncomputational requirements and very quick set-up, with ongoing efforts to make\nit more reliable.\n","authors":["Archit Gupta","Arvind Easwaran"],"pdf_url":"https://arxiv.org/pdf/2208.10765v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05865v1","updated":"2023-10-09T17:02:50Z","published":"2023-10-09T17:02:50Z","title":"A Learning-Based Framework for Safe Human-Robot Collaboration with\n  Multiple Backup Control Barrier Functions","summary":"  Ensuring robot safety in complex environments is a difficult task due to\nactuation limits, such as torque bounds. This paper presents a safety-critical\ncontrol framework that leverages learning-based switching between multiple\nbackup controllers to formally guarantee safety under bounded control inputs\nwhile satisfying driver intention. By leveraging backup controllers designed to\nuphold safety and input constraints, backup control barrier functions (BCBFs)\nconstruct implicitly defined control invariance sets via a feasible quadratic\nprogram (QP). However, BCBF performance largely depends on the design and\nconservativeness of the chosen backup controller, especially in our setting of\nhuman-driven vehicles in complex, e.g, off-road, conditions. While\nconservativeness can be reduced by using multiple backup controllers,\ndetermining when to switch is an open problem. Consequently, we develop a\nbroadcast scheme that estimates driver intention and integrates BCBFs with\nmultiple backup strategies for human-robot interaction. An LSTM classifier uses\ndata inputs from the robot, human, and safety algorithms to continually choose\na backup controller in real-time. We demonstrate our method's efficacy on a\ndual-track robot in obstacle avoidance scenarios. Our framework guarantees\nrobot safety while adhering to driver intention.\n","authors":["Neil C. Janwani","Ersin Daş","Thomas Touma","Skylar X. Wei","Tamas G. Molnar","Joel W. Burdick"],"pdf_url":"https://arxiv.org/pdf/2310.05865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17207v2","updated":"2023-10-09T16:06:52Z","published":"2023-03-30T07:59:30Z","title":"Exploiting Redundancy for UWB Anomaly Detection in Infrastructure-Free\n  Multi-Robot Relative Localization","summary":"  Ultra-wideband (UWB) localization methods have emerged as a cost-effective\nand accurate solution for GNSS-denied environments. There is a significant\namount of previous research in terms of resilience of UWB ranging, with\nnon-line-of-sight and multipath detection methods. However, little attention\nhas been paid to resilience against disturbances in relative localization\nsystems involving multiple nodes. This paper presents an approach to detecting\nrange anomalies in UWB ranging measurements from the perspective of multi-robot\ncooperative localization. We introduce an approach to exploiting redundancy for\nrelative localization in multi-robot systems, where the position of each node\nis calculated using different subsets of available data. This enables us to\neffectively identify nodes that present ranging anomalies and eliminate their\neffect within the cooperative localization scheme. We analyze anomalies created\nby timing errors in the ranging process, e.g., owing to malfunctioning\nhardware. However, our method is generic and can be extended to other types of\nranging anomalies. Our approach results in a more resilient cooperative\nlocalization framework with a negligible impact in terms of the computational\nworkload.\n","authors":["Sahar Salimpour","Paola Torrico Morón","Xianjia Yu","Tomi Westerlund","Jorge Peña Queralta"],"pdf_url":"https://arxiv.org/pdf/2303.17207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05808v1","updated":"2023-10-09T15:45:08Z","published":"2023-10-09T15:45:08Z","title":"A Simple Open-Loop Baseline for Reinforcement Learning Locomotion Tasks","summary":"  In search of the simplest baseline capable of competing with Deep\nReinforcement Learning on locomotion tasks, we propose a biologically inspired\nmodel-free open-loop strategy. Drawing upon prior knowledge and harnessing the\nelegance of simple oscillators to generate periodic joint motions, it achieves\nrespectable performance in five different locomotion environments, with a\nnumber of tunable parameters that is a tiny fraction of the thousands typically\nrequired by RL algorithms. Unlike RL methods, which are prone to performance\ndegradation when exposed to sensor noise or failure, our open-loop oscillators\nexhibit remarkable robustness due to their lack of reliance on sensors.\nFurthermore, we showcase a successful transfer from simulation to reality using\nan elastic quadruped, all without the need for randomization or reward\nengineering.\n","authors":["Antonin Raffin","Olivier Sigaud","Jens Kober","Alin Albu-Schäffer","João Silvério","Freek Stulp"],"pdf_url":"https://arxiv.org/pdf/2310.05808v1.pdf","comment":"video: https://b2drop.eudat.eu/s/ykDPMM7F9KFyLgi"},{"id":"http://arxiv.org/abs/2310.05766v1","updated":"2023-10-09T14:49:16Z","published":"2023-10-09T14:49:16Z","title":"FeatSense -- A Feature-based Registration Algorithm with GPU-accelerated\n  TSDF-Mapping Backend for NVIDIA Jetson Boards","summary":"  This paper presents FeatSense, a feature-based GPU-accelerated SLAM system\nfor high resolution LiDARs, combined with a map generation algorithm for\nreal-time generation of large Truncated Signed Distance Fields (TSDFs) on\nembedded hardware. FeatSense uses LiDAR point cloud features for odometry\nestimation and point cloud registration. The registered point clouds are\nintegrated into a global Truncated Signed Distance Field (TSDF) representation.\nFeatSense is intended to run on embedded systems with integrated\nGPU-accelerator like NVIDIA Jetson boards. In this paper, we present a\nreal-time capable TSDF-SLAM system specially tailored for close coupled CPU/GPU\nsystems. The implementation is evaluated in various structured and unstructured\nenvironments and benchmarked against existing reference datasets. The main\ncontribution of this paper is the ability to register up to 128 scan lines of\nan Ouster OS1-128 LiDAR at 10Hz on a NVIDIA AGX Xavier while achieving a TSDF\nmap generation speedup by a factor of 100 compared to previous work on the same\npower budget.\n","authors":["Julian Gaal","Thomas Wiemann","Alexander Mock","Mario Porrmann"],"pdf_url":"https://arxiv.org/pdf/2310.05766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05765v1","updated":"2023-10-09T14:46:21Z","published":"2023-10-09T14:46:21Z","title":"Examining the simulation-to-reality gap of a wheel loader digging in\n  deformable terrain","summary":"  We investigate how well a wheel loader simulator can replicate a real one\nwhen performing bucket filling in a pile of gravel. The comparisons are made\nusing field test time series of the vehicle motion and actuation forces, loaded\nmass, and total work. The vehicle was modeled as a rigid multibody system with\nfrictional contacts, driveline, and linear actuators. For the soil, we tested\ndiscrete element models of different resolutions, with and without multiscale\nacceleration. The spatio-temporal resolution ranged between 50-400 mm and 2-500\nms, and the computational speed was between 1/10,000 to 5 times faster than\nreal-time. The simulation-to-reality gap was found to be around 10% and\nexhibited a weak dependence on the level of fidelity, i.e. accessible with\nreal-time simulation and faster. Furthermore, the sensitivity of an optimized\nforce feedback controller under transfer between different simulation domains\nwas investigated. The domain bias was observed to cause a performance reduction\nof 5% despite the domain gap being about 15%.\n","authors":["Koji Aoshima","Martin Servin"],"pdf_url":"https://arxiv.org/pdf/2310.05765v1.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2310.05762v1","updated":"2023-10-09T14:44:01Z","published":"2023-10-09T14:44:01Z","title":"3D tomatoes' localisation with monocular cameras using histogram filters","summary":"  Performing tasks in agriculture, such as fruit monitoring or harvesting,\nrequires perceiving the objects' spatial position. RGB-D cameras are limited\nunder open-field environments due to lightning interferences. Therefore, in\nthis study, we approach the use of Histogram Filters (Bayesian Discrete\nFilters) to estimate the position of tomatoes in the tomato plant. Two kernel\nfilters were studied: the square kernel and the Gaussian kernel. The\nimplemented algorithm was essayed in simulation, with and without Gaussian\nnoise and random noise, and in a testbed at laboratory conditions. The\nalgorithm reported a mean absolute error lower than 10 mm in simulation and 20\nmm in the testbed at laboratory conditions with an assessing distance of about\n0.5 m. So, the results are viable for real environments and should be improved\nat closer distances.\n","authors":["Sandro Costa Magalhães","Filipe Neves dos Santos","António Paulo Moreira","Jorge Dias"],"pdf_url":"https://arxiv.org/pdf/2310.05762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05717v1","updated":"2023-10-09T13:39:06Z","published":"2023-10-09T13:39:06Z","title":"STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects\n  on Production Lines","summary":"  In this work, we present STOPNet, a framework for 6-DoF object suction\ndetection on production lines, with a focus on but not limited to transparent\nobjects, which is an important and challenging problem in robotic systems and\nmodern industry. Current methods requiring depth input fail on transparent\nobjects due to depth cameras' deficiency in sensing their geometry, while we\nproposed a novel framework to reconstruct the scene on the production line\ndepending only on RGB input, based on multiview stereo. Compared to existing\nworks, our method not only reconstructs the whole 3D scene in order to obtain\nhigh-quality 6-DoF suction poses in real time but also generalizes to novel\nenvironments, novel arrangements and novel objects, including challenging\ntransparent objects, both in simulation and the real world. Extensive\nexperiments in simulation and the real world show that our method significantly\nsurpasses the baselines and has better generalizability, which caters to\npractical industrial needs.\n","authors":["Yuxuan Kuang","Qin Han","Danshi Li","Qiyu Dai","Lian Ding","Dong Sun","Hanlin Zhao","He Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05717v1.pdf","comment":"Under Review. ICRA 2024 submission"},{"id":"http://arxiv.org/abs/2310.05714v1","updated":"2023-10-09T13:38:03Z","published":"2023-10-09T13:38:03Z","title":"DecAP: Decaying Action Priors for Accelerated Learning of Torque-Based\n  Legged Locomotion Policies","summary":"  Optimal Control for legged robots has gone through a paradigm shift from\nposition-based to torque-based control, owing to the latter's compliant and\nrobust nature. In parallel to this shift, the community has also turned to Deep\nReinforcement Learning (DRL) as a promising approach to directly learn\nlocomotion policies for complex real-life tasks. However, most end-to-end DRL\napproaches still operate in position space, mainly because learning in torque\nspace is often sample-inefficient and does not consistently converge to natural\ngaits. To address these challenges, we introduce Decaying Action Priors\n(DecAP), a novel three-stage framework to learn and deploy torque policies for\nlegged locomotion. In the first stage, we generate our own imitation data by\ntraining a position policy, eliminating the need for expert knowledge in\ndesigning optimal controllers. The second stage incorporates decaying action\npriors to enhance the exploration of torque-based policies aided by imitation\nrewards. We show that our approach consistently outperforms imitation learning\nalone and is significantly robust to the scaling of these rewards. Finally, our\nthird stage facilitates safe sim-to-real transfer by directly deploying our\nlearned torques, alongside low-gain PID control from our trained position\npolicy. We demonstrate the generality of our approach by training torque-based\nlocomotion policies for a biped, a quadruped, and a hexapod robot in\nsimulation, and experimentally demonstrate our learned policies on a quadruped\n(Unitree Go1).\n","authors":["Shivam Sood","Ge Sun","Peizhuo Li","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2310.05714v1.pdf","comment":"Submitted to the IEEE International Conference on Robotics and\n  Automation (ICRA 2024)"},{"id":"http://arxiv.org/abs/2310.05670v1","updated":"2023-10-09T12:39:44Z","published":"2023-10-09T12:39:44Z","title":"Reinforcement learning for freeform robot design","summary":"  Inspired by the necessity of morphological adaptation in animals, a growing\nbody of work has attempted to expand robot training to encompass physical\naspects of a robot's design. However, reinforcement learning methods capable of\noptimizing the 3D morphology of a robot have been restricted to reorienting or\nresizing the limbs of a predetermined and static topological genus. Here we\nshow policy gradients for designing freeform robots with arbitrary external and\ninternal structure. This is achieved through actions that deposit or remove\nbundles of atomic building blocks to form higher-level nonparametric\nmacrostructures such as appendages, organs and cavities. Although results are\nprovided for open loop control only, we discuss how this method could be\nadapted for closed loop control and sim2real transfer to physical machines in\nfuture.\n","authors":["Muhan Li","David Matthews","Sam Kriegman"],"pdf_url":"https://arxiv.org/pdf/2310.05670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18829v3","updated":"2023-10-09T11:59:31Z","published":"2023-05-30T08:23:06Z","title":"UniScene: Multi-Camera Unified Pre-training via 3D Scene Reconstruction","summary":"  Multi-camera 3D perception has emerged as a prominent research field in\nautonomous driving, offering a viable and cost-effective alternative to\nLiDAR-based solutions. The existing multi-camera algorithms primarily rely on\nmonocular 2D pre-training. However, the monocular 2D pre-training overlooks the\nspatial and temporal correlations among the multi-camera system. To address\nthis limitation, we propose the first multi-camera unified pre-training\nframework, called UniScene, which involves initially reconstructing the 3D\nscene as the foundational stage and subsequently fine-tuning the model on\ndownstream tasks. Specifically, we employ Occupancy as the general\nrepresentation for the 3D scene, enabling the model to grasp geometric priors\nof the surrounding world through pre-training. A significant benefit of\nUniScene is its capability to utilize a considerable volume of unlabeled\nimage-LiDAR pairs for pre-training purposes. The proposed multi-camera unified\npre-training framework demonstrates promising results in key tasks such as\nmulti-camera 3D object detection and surrounding semantic scene completion.\nWhen compared to monocular pre-training methods on the nuScenes dataset,\nUniScene shows a significant improvement of about 2.0% in mAP and 2.0% in NDS\nfor multi-camera 3D object detection, as well as a 3% increase in mIoU for\nsurrounding semantic scene completion. By adopting our unified pre-training\nmethod, a 25% reduction in 3D training annotation costs can be achieved,\noffering significant practical value for the implementation of real-world\nautonomous driving. Codes are publicly available at\nhttps://github.com/chaytonmin/UniScene.\n","authors":["Chen Min","Liang Xiao","Dawei Zhao","Yiming Nie","Bin Dai"],"pdf_url":"https://arxiv.org/pdf/2305.18829v3.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.05600v1","updated":"2023-10-09T10:35:37Z","published":"2023-10-09T10:35:37Z","title":"Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care\n  Environments","summary":"  As labor shortage increases in the health sector, the demand for assistive\nrobotics grows. However, the needed test data to develop those robots is\nscarce, especially for the application of active 3D object detection, where no\nreal data exists at all. This short paper counters this by introducing such an\nannotated dataset of real environments. The captured environments represent\nareas which are already in use in the field of robotic health care research. We\nfurther provide ground truth data within one room, for assessing SLAM\nalgorithms running directly on a health care robot.\n","authors":["Michael G. Adam","Sebastian Eger","Martin Piccolrovazzi","Maged Iskandar","Joern Vogel","Alexander Dietrich","Seongjien Bien","Jon Skerlj","Abdeldjallil Naceri","Eckehard Steinbach","Alin Albu-Schaeffer","Sami Haddadin","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2310.05600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05567v1","updated":"2023-10-09T09:43:36Z","published":"2023-10-09T09:43:36Z","title":"Collision Avoidance for Autonomous Surface Vessels using Novel\n  Artificial Potential Fields","summary":"  As the demand for transportation through waterways continues to rise, the\nnumber of vessels plying the waters has correspondingly increased. This has\nresulted in a greater number of accidents and collisions between ships, some of\nwhich lead to significant loss of life and financial losses. Research has shown\nthat human error is a major factor responsible for such incidents. The maritime\nindustry is constantly exploring newer approaches to autonomy to mitigate this\nissue. This study presents the use of novel Artificial Potential Fields (APFs)\nto perform obstacle and collision avoidance in marine environments. This study\nhighlights the advantage of harmonic functions over traditional functions in\nmodeling potential fields. With a modification, the method is extended to\neffectively avoid dynamic obstacles while adhering to COLREGs. Improved\nperformance is observed as compared to the traditional potential fields and\nalso against the popular velocity obstacle approach. A comprehensive\nstatistical analysis is also performed through Monte Carlo simulations in\ndifferent congested environments that emulate real traffic conditions to\ndemonstrate robustness of the approach.\n","authors":["Aditya Kailas Jadhav","Anantha Raj Pandi","Abhilash Somayajula"],"pdf_url":"https://arxiv.org/pdf/2310.05567v1.pdf","comment":"28 pages, 30 figures"},{"id":"http://arxiv.org/abs/2310.05547v1","updated":"2023-10-09T09:16:31Z","published":"2023-10-09T09:16:31Z","title":"Geometry-Aware Safety-Critical Local Reactive Controller for Robot\n  Navigation in Unknown and Cluttered Environments","summary":"  This work proposes a safety-critical local reactive controller that enables\nthe robot to navigate in unknown and cluttered environments. In particular, the\ntrajectory tracking task is formulated as a constrained polynomial optimization\nproblem. Then, safety constraints are imposed on the control variables invoking\nthe notion of polynomial positivity certificates in conjunction with their\nSum-of-Squares (SOS) approximation, thereby confining the robot motion inside\nthe locally extracted convex free region. It is noteworthy that, in the process\nof devising the proposed safety constraints, the geometry of the robot can be\napproximated using any shape that can be characterized with a set of polynomial\nfunctions. The optimization problem is further convexified into a semidefinite\nprogram (SDP) leveraging truncated multi-sequences (tms) and moment relaxation,\nwhich favorably facilitates the effective use of off-the-shelf conic\nprogramming solvers, such that real-time performance is attainable. Various\nrobot navigation tasks are investigated to demonstrate the effectiveness of the\nproposed approach in terms of safety and tracking performance.\n","authors":["Yulin Li","Xindong Tang","Kai Chen","Chunxin Zheng","Haichao Liu","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2310.05547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05541v1","updated":"2023-10-09T09:05:18Z","published":"2023-10-09T09:05:18Z","title":"Collaborative Visual Place Recognition","summary":"  Visual place recognition (VPR) capabilities enable autonomous robots to\nnavigate complex environments by discovering the environment's topology based\non visual input. Most research efforts focus on enhancing the accuracy and\nrobustness of single-robot VPR but often encounter issues such as occlusion due\nto individual viewpoints. Despite a number of research on multi-robot\nmetric-based localization, there is a notable gap in research concerning more\nrobust and efficient place-based localization with a multi-robot system. This\nwork proposes collaborative VPR, where multiple robots share abstracted visual\nfeatures to enhance place recognition capabilities. We also introduce a novel\ncollaborative VPR framework based on similarity-regularized information fusion,\nreducing irrelevant noise while harnessing valuable data from collaborators.\nThis framework seamlessly integrates with well-established single-robot VPR\ntechniques and supports end-to-end training with a weakly-supervised\ncontrastive loss. We conduct experiments in urban, rural, and indoor scenes,\nachieving a notable improvement over single-agent VPR in urban environments\n(~12\\%), along with consistent enhancements in rural (~3\\%) and indoor (~1\\%)\nscenarios. Our work presents a promising solution to the pressing challenges of\nVPR, representing a substantial step towards safe and robust autonomous\nsystems.\n","authors":["Yiming Li","Zonglin Lyu","Mingxuan Lu","Chao Chen","Michael Milford","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2310.05541v1.pdf","comment":"https://ai4ce.github.io/CoVPR/"},{"id":"http://arxiv.org/abs/2310.05520v1","updated":"2023-10-09T08:35:46Z","published":"2023-10-09T08:35:46Z","title":"One Problem, One Solution: Unifying Robot and Environment Design\n  Optimization","summary":"  The task-specific optimization of robotic systems has long been divided into\nthe optimization of the robot and the optimization of the environment. In this\nletter, we argue that these two problems are interdependent and should be\ntreated as such. To this end, we present a unified problem formulation that\nenables for the simultaneous optimization of both the robot kinematics and the\nenvironment. We demonstrate the effectiveness of our approach by jointly\noptimizing a robotic milling system. To compare our approach to the state of\nthe art we also optimize the robot kinematics and environment separately. The\nresults show that our approach outperforms the state of the art and that\nsimultaneous optimization leads to a much better solution.\n","authors":["Jan Baumgärtner","Gajanan Kanagalingam","Alexander Puchtaand Jürgen Fleischer"],"pdf_url":"https://arxiv.org/pdf/2310.05520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05504v1","updated":"2023-10-09T08:09:15Z","published":"2023-10-09T08:09:15Z","title":"Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud\n  Registration","summary":"  State-of-the-art techniques for monocular camera reconstruction predominantly\nrely on the Structure from Motion (SfM) pipeline. However, such methods often\nyield reconstruction outcomes that lack crucial scale information, and over\ntime, accumulation of images leads to inevitable drift issues. In contrast,\nmapping methods based on LiDAR scans are popular in large-scale urban scene\nreconstruction due to their precise distance measurements, a capability\nfundamentally absent in visual-based approaches. Researchers have made attempts\nto utilize concurrent LiDAR and camera measurements in pursuit of precise\nscaling and color details within mapping outcomes. However, the outcomes are\nsubject to extrinsic calibration and time synchronization precision. In this\npaper, we propose a novel cost-effective reconstruction pipeline that utilizes\na pre-established LiDAR map as a fixed constraint to effectively address the\ninherent scale challenges present in monocular camera reconstruction. To our\nknowledge, our method is the first to register images onto the point cloud map\nwithout requiring synchronous capture of camera and LiDAR data, granting us the\nflexibility to manage reconstruction detail levels across various areas of\ninterest. To facilitate further research in this domain, we have released\nColmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that\nenables precise fine-scale registration of images to the point cloud map.\n","authors":["Chunge Bai","Ruijie Fu","Xiang Gao"],"pdf_url":"https://arxiv.org/pdf/2310.05504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05430v1","updated":"2023-10-09T06:06:34Z","published":"2023-10-09T06:06:34Z","title":"Replication of Multi-agent Reinforcement Learning for the \"Hide and\n  Seek\" Problem","summary":"  Reinforcement learning generates policies based on reward functions and\nhyperparameters. Slight changes in these can significantly affect results. The\nlack of documentation and reproducibility in Reinforcement learning research\nmakes it difficult to replicate once-deduced strategies. While previous\nresearch has identified strategies using grounded maneuvers, there is limited\nwork in more complex environments. The agents in this study are simulated\nsimilarly to Open Al's hider and seek agents, in addition to a flying\nmechanism, enhancing their mobility, and expanding their range of possible\nactions and strategies. This added functionality improves the Hider agents to\ndevelop a chasing strategy from approximately 2 million steps to 1.6 million\nsteps and hiders\n","authors":["Haider Kamal","Muaz A. Niazi","Hammad Afzal"],"pdf_url":"https://arxiv.org/pdf/2310.05430v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2303.04516v3","updated":"2023-10-09T05:39:07Z","published":"2023-03-08T11:13:49Z","title":"Time-Optimal Control via Heaviside Step-Function Approximation","summary":"  Least-squares programming is a popular tool in robotics due to its simplicity\nand availability of open-source solvers. However, certain problems like sparse\nprogramming in the $\\ell_0$- or $\\ell_1$-norm for time-optimal control are not\nequivalently solvable. In this work, we propose a non-linear hierarchical\nleast-squares programming (NL-HLSP) for time-optimal control of non-linear\ndiscrete dynamic systems. We use a continuous approximation of the heaviside\nstep function with an additional term that avoids vanishing gradients. We use a\nsimple discretization method by keeping states and controls piece-wise constant\nbetween discretization steps. This way, we obtain a comparatively easily\nimplementable NL-HLSP in contrast to direct transcription approaches of optimal\ncontrol. We show that the NL-HLSP indeed recovers the discrete time-optimal\ncontrol in the limit for resting goal points. We confirm the results in\nsimulation for linear and non-linear control scenarios.\n","authors":["Kai Pfeiffer","Quang-Cuong Pham"],"pdf_url":"https://arxiv.org/pdf/2303.04516v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05414v1","updated":"2023-10-09T05:17:14Z","published":"2023-10-09T05:17:14Z","title":"Ethics of Artificial Intelligence and Robotics in the Architecture,\n  Engineering, and Construction Industry","summary":"  Artificial intelligence (AI) and robotics research and implementation emerged\nin the architecture, engineering, and construction (AEC) industry to positively\nimpact project efficiency and effectiveness concerns such as safety,\nproductivity, and quality. This shift, however, warrants the need for ethical\nconsiderations of AI and robotics adoption due to its potential negative\nimpacts on aspects such as job security, safety, and privacy. Nevertheless,\nthis did not receive sufficient attention, particularly within the academic\ncommunity. This research systematically reviews AI and robotics research\nthrough the lens of ethics in the AEC community for the past five years. It\nidentifies nine key ethical issues namely job loss, data privacy, data\nsecurity, data transparency, decision-making conflict, acceptance and trust,\nreliability and safety, fear of surveillance, and liability, by summarizing\nexisting literature and filtering it further based on its AEC relevance.\nFurthermore, thirteen research topics along the process were identified based\non existing AEC studies that had direct relevance to the theme of ethics in\ngeneral and their parallels are further discussed. Finally, the current\nchallenges and knowledge gaps are discussed and seven specific future research\ndirections are recommended. This study not only signifies more stakeholder\nawareness of this important topic but also provides imminent steps towards\nsafer and more efficient realization.\n","authors":["Ci-Jyun Liang","Thai-Hoa Le","Youngjib Ham","Bharadwaj R. K. Mantha","Marvin H. Cheng","Jacob J. Lin"],"pdf_url":"https://arxiv.org/pdf/2310.05414v1.pdf","comment":"109 pages, 5 figures, submitted to Automation in Construction"},{"id":"http://arxiv.org/abs/2310.05407v1","updated":"2023-10-09T04:56:30Z","published":"2023-10-09T04:56:30Z","title":"GPS Attack Detection and Mitigation for Safe Autonomous Driving using\n  Image and Map based Lateral Direction Localization","summary":"  The accuracy and robustness of vehicle localization are critical for\nachieving safe and reliable high-level autonomy. Recent results show that GPS\nis vulnerable to spoofing attacks, which is one major threat to autonomous\ndriving. In this paper, a novel anomaly detection and mitigation method against\nGPS attacks that utilizes onboard camera and high-precision maps is proposed to\nensure accurate vehicle localization. First, lateral direction localization in\ndriving lanes is calculated by camera-based lane detection and map matching\nrespectively. Then, a real-time detector for GPS spoofing attack is developed\nto evaluate the localization data. When the attack is detected, a multi-source\nfusion-based localization method using Unscented Kalman filter is derived to\nmitigate GPS attack and improve the localization accuracy. The proposed method\nis validated in various scenarios in Carla simulator and open-source public\ndataset to demonstrate its effectiveness in timely GPS attack detection and\ndata recovery.\n","authors":["Qingming Chen","Peng Liu","Guoqiang Li","Zhenpo Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03773v2","updated":"2023-10-09T04:25:33Z","published":"2023-04-04T21:49:02Z","title":"Safe Explicable Planning","summary":"  Human expectations stem from their knowledge about the others and the world.\nWhere human-AI interaction is concerned, such knowledge may be inconsistent\nwith the ground truth, resulting in the AI agent not meeting its expectations\nand degraded team performance. Explicable planning was previously introduced as\na novel planning approach to reconciling human expectations and the agent's\noptimal behavior for more interpretable decision-making. One critical issue\nthat remains unaddressed is safety in explicable planning since it can lead to\nexplicable behaviors that are unsafe. We propose Safe Explicable Planning (SEP)\nto extend the prior work to support the specification of a safety bound. The\nobjective of SEP is to search for behaviors that are close to the human's\nexpectations while satisfying the bound on the agent's return, the safety\ncriterion chosen in this work. We show that the problem generalizes\nmulti-objective optimization and our formulation introduces a Pareto set. Under\nsuch a formulation, we propose a novel exact method that returns the Pareto set\nof safe explicable policies, a more efficient greedy method that returns one of\nthe Pareto optimal policies, and approximate solutions for them based on the\naggregation of states to further scalability. Formal proofs are provided to\nvalidate the desired theoretical properties of the exact and greedy methods. We\nevaluate our methods both in simulation and with physical robot experiments.\nResults confirm the validity and efficacy of our methods for safe explicable\nplanning.\n","authors":["Akkamahadevi Hanni","Andrew Boateng","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.03773v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16940v2","updated":"2023-10-09T03:46:41Z","published":"2023-09-29T02:45:56Z","title":"Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow","summary":"  Collaborative perception can substantially boost each agent's perception\nability by facilitating communication among multiple agents. However, temporal\nasynchrony among agents is inevitable in the real world due to communication\ndelays, interruptions, and clock misalignments. This issue causes information\nmismatch during multi-agent fusion, seriously shaking the foundation of\ncollaboration. To address this issue, we propose CoBEVFlow, an\nasynchrony-robust collaborative perception system based on bird's eye view\n(BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align\nasynchronous collaboration messages sent by multiple agents. To model the\nmotion in a scene, we propose BEV flow, which is a collection of the motion\nvector corresponding to each spatial location. Based on BEV flow, asynchronous\nperceptual features can be reassigned to appropriate positions, mitigating the\nimpact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle\nasynchronous collaboration messages sent at irregular, continuous time stamps\nwithout discretization; and (ii) with BEV flow, CoBEVFlow only transports the\noriginal perceptual features, instead of generating new perceptual features,\navoiding additional noises. To validate CoBEVFlow's efficacy, we create\nIRregular V2V(IRV2V), the first synthetic collaborative perception dataset with\nvarious temporal asynchronies that simulate different real-world scenarios.\nExtensive experiments conducted on both IRV2V and the real-world dataset\nDAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is\nrobust in extremely asynchronous settings. The code is available at\nhttps://github.com/MediaBrain-SJTU/CoBEVFlow.\n","authors":["Sizhe Wei","Yuxi Wei","Yue Hu","Yifan Lu","Yiqi Zhong","Siheng Chen","Ya Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.16940v2.pdf","comment":"16 pages, 9 figures. Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05346v1","updated":"2023-10-09T02:15:45Z","published":"2023-10-09T02:15:45Z","title":"Anyview: Generalizable Indoor 3D Object Detection with Variable Frames","summary":"  In this paper, we propose a novel network framework for indoor 3D object\ndetection to handle variable input frame numbers in practical scenarios.\nExisting methods only consider fixed frames of input data for a single\ndetector, such as monocular RGB-D images or point clouds reconstructed from\ndense multi-view RGB-D images. While in practical application scenes such as\nrobot navigation and manipulation, the raw input to the 3D detectors is the\nRGB-D images with variable frame numbers instead of the reconstructed scene\npoint cloud. However, the previous approaches can only handle fixed frame input\ndata and have poor performance with variable frame input. In order to\nfacilitate 3D object detection methods suitable for practical tasks, we present\na novel 3D detection framework named AnyView for our practical applications,\nwhich generalizes well across different numbers of input frames with a single\nmodel. To be specific, we propose a geometric learner to mine the local\ngeometric features of each input RGB-D image frame and implement local-global\nfeature interaction through a designed spatial mixture module. Meanwhile, we\nfurther utilize a dynamic token strategy to adaptively adjust the number of\nextracted features for each frame, which ensures consistent global feature\ndensity and further enhances the generalization after fusion. Extensive\nexperiments on the ScanNet dataset show our method achieves both great\ngeneralizability and high detection accuracy with a simple and clean\narchitecture containing a similar amount of parameters with the baselines.\n","authors":["Zhenyu Wu","Xiuwei Xu","Ziwei Wang","Chong Xia","Linqing Zhao","Jiwen Lu","Haibin Yan"],"pdf_url":"https://arxiv.org/pdf/2310.05346v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2211.13175v2","updated":"2023-10-09T01:36:02Z","published":"2022-11-23T17:52:27Z","title":"Coordination of multiple mobile manipulators for ordered sorting of\n  cluttered objects","summary":"  We present a coordination method for multiple mobile manipulators to sort\nobjects in clutter. We consider the object rearrangement problem in which the\nobjects must be sorted into different groups in a particular order. In clutter,\nthe order constraints could not be easily satisfied since some objects occlude\nother objects so the occluded ones are not directly accessible to the robots.\nThose objects occluding others need to be moved more than once to make the\noccluded objects accessible. Such rearrangement problems fall into the class of\nnonmonotone rearrangement problems which are computationally intractable. While\nthe nonmonotone problems with order constraints are harder, involving with\nmultiple robots requires another computation for task allocation. The proposed\nmethod first finds a sequence of objects to be sorted using a search such that\nthe order constraint in each group is satisfied. The search can solve\nnonmonotone instances that require temporal relocation of some objects to\naccess the next object to be sorted. Once a complete sorting sequence is found,\nthe objects in the sequence are assigned to multiple mobile manipulators using\na greedy allocation method. We develop four versions of the method with\ndifferent search strategies. In the experiments, we show that our method can\nfind a sorting sequence quickly (e.g., 4.6 sec with 20 objects sorted into five\ngroups) even though the solved instances include hard nonmonotone ones. The\nextensive tests and the experiments in simulation show the ability of the\nmethod to solve the real-world sorting problem using multiple mobile\nmanipulators.\n","authors":["Jeeho Ahn","Seabin Lee","Changjoo Nam"],"pdf_url":"https://arxiv.org/pdf/2211.13175v2.pdf","comment":"Presented at iROS 2023"},{"id":"http://arxiv.org/abs/2310.06210v1","updated":"2023-10-09T23:42:33Z","published":"2023-10-09T23:42:33Z","title":"CAT-RRT: Motion Planning that Admits Contact One Link at a Time","summary":"  Current motion planning approaches rely on binary collision checking to\nevaluate the validity of a state and thereby dictate where the robot is allowed\nto move. This approach leaves little room for robots to engage in contact with\nan object, as is often necessary when operating in densely cluttered spaces. In\nthis work, we propose an alternative method that considers contact states as\nhigh-cost states that the robot should avoid but can traverse if necessary to\ncomplete a task. More specifically, we introduce Contact Admissible\nTransition-based Rapidly exploring Random Trees (CAT-RRT), a planner that uses\na novel per-link cost heuristic to find a path by traversing high-cost obstacle\nregions. Through extensive testing, we find that state-of-the-art optimization\nplanners tend to over-explore low-cost states, which leads to slow and\ninefficient convergence to contact regions. Conversely, CAT-RRT searches both\nlow and high-cost regions simultaneously with an adaptive thresholding\nmechanism carried out at each robot link. This leads to paths with a balance\nbetween efficiency, path length, and contact cost.\n","authors":["Nataliya Nechyporenko","Caleb Escobedo","Shreyas Kadekodi","Alessandro Roncone"],"pdf_url":"https://arxiv.org/pdf/2310.06210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06208v1","updated":"2023-10-09T23:34:09Z","published":"2023-10-09T23:34:09Z","title":"Human-Robot Gym: Benchmarking Reinforcement Learning in Human-Robot\n  Collaboration","summary":"  Deep reinforcement learning (RL) has shown promising results in robot motion\nplanning with first attempts in human-robot collaboration (HRC). However, a\nfair comparison of RL approaches in HRC under the constraint of guaranteed\nsafety is yet to be made. We, therefore, present human-robot gym, a benchmark\nfor safe RL in HRC. Our benchmark provides eight challenging, realistic HRC\ntasks in a modular simulation framework. Most importantly, human-robot gym\nincludes a safety shield that provably guarantees human safety. We are,\nthereby, the first to provide a benchmark to train RL agents that adhere to the\nsafety specifications of real-world HRC. This bridges a critical gap between\ntheoretic RL research and its real-world deployment. Our evaluation of six\nenvironments led to three key results: (a) the diverse nature of the tasks\noffered by human-robot gym creates a challenging benchmark for state-of-the-art\nRL methods, (b) incorporating expert knowledge in the RL training in the form\nof an action-based reward can outperform the expert, and (c) our agents\nnegligibly overfit to training data.\n","authors":["Jakob Thumm","Felix Trost","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2310.06208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06198v1","updated":"2023-10-09T23:01:32Z","published":"2023-10-09T23:01:32Z","title":"Motion Memory: Leveraging Past Experiences to Accelerate Future Motion\n  Planning","summary":"  When facing a new motion-planning problem, most motion planners solve it from\nscratch, e.g., via sampling and exploration or starting optimization from a\nstraight-line path. However, most motion planners have to experience a variety\nof planning problems throughout their lifetimes, which are yet to be leveraged\nfor future planning. In this paper, we present a simple but efficient method\ncalled Motion Memory, which allows different motion planners to accelerate\nfuture planning using past experiences. Treating existing motion planners as\neither a closed or open box, we present a variety of ways that Motion Memory\ncan contribute to reduce the planning time when facing a new planning problem.\nWe provide extensive experiment results with three different motion planners on\nthree classes of planning problems with over 30,000 problem instances and show\nthat planning speed can be significantly reduced by up to 89% with the proposed\nMotion Memory technique and with increasing past planning experiences.\n","authors":["Dibyendu Das","Yuanjie Lu","Erion Plaku","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.06198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13679v2","updated":"2023-10-09T22:13:10Z","published":"2023-09-24T16:05:31Z","title":"Neural Network-PSO-based Velocity Control Algorithm for Landing UAVs on\n  a Boat","summary":"  Precise landing of Unmanned Aerial Vehicles (UAVs) onto moving platforms like\nAutonomous Surface Vehicles (ASVs) is both important and challenging,\nespecially in GPS-denied environments, for collaborative navigation of\nheterogeneous vehicles. UAVs need to land within a confined space onboard ASV\nto get energy replenishment, while ASV is subject to translational and\nrotational disturbances due to wind and water flow. Current solutions either\nrely on high-level waypoint navigation, which struggles to robustly land on\nvaried-speed targets, or necessitate laborious manual tuning of controller\nparameters, and expensive sensors for target localization. Therefore, we\npropose an adaptive velocity control algorithm that leverages Particle Swarm\nOptimization (PSO) and Neural Network (NN) to optimize PID parameters across\nvarying flight altitudes and distinct speeds of a moving boat. The cost\nfunction of PSO includes the status change rates of UAV and proximity to the\ntarget. The NN further interpolates the PSO-founded PID parameters. The\nproposed method implemented on a water strider hexacopter design, not only\nensures accuracy but also increases robustness. Moreover, this NN-PSO can be\nreadily adapted to suit various mission requirements. Its ability to achieve\nprecise landings extends its applicability to scenarios, including but not\nlimited to rescue missions, package deliveries, and workspace inspections.\n","authors":["Li-Fan Wu","Zihan Wang","Mo Rastgaar","Nina Mahmoudian"],"pdf_url":"https://arxiv.org/pdf/2309.13679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06171v1","updated":"2023-10-09T21:49:48Z","published":"2023-10-09T21:49:48Z","title":"Memory-Consistent Neural Networks for Imitation Learning","summary":"  Imitation learning considerably simplifies policy synthesis compared to\nalternative approaches by exploiting access to expert demonstrations. For such\nimitation policies, errors away from the training samples are particularly\ncritical. Even rare slip-ups in the policy action outputs can compound quickly\nover time, since they lead to unfamiliar future states where the policy is\nstill more likely to err, eventually causing task failures. We revisit simple\nsupervised ``behavior cloning'' for conveniently training the policy from\nnothing more than pre-recorded demonstrations, but carefully design the model\nclass to counter the compounding error phenomenon. Our ``memory-consistent\nneural network'' (MCNN) outputs are hard-constrained to stay within clearly\nspecified permissible regions anchored to prototypical ``memory'' training\nsamples. We provide a guaranteed upper bound for the sub-optimality gap induced\nby MCNN policies. Using MCNNs on 9 imitation learning tasks, with MLP,\nTransformer, and Diffusion backbones, spanning dexterous robotic manipulation\nand driving, proprioceptive inputs and visual inputs, and varying sizes and\ntypes of demonstration data, we find large and consistent gains in performance,\nvalidating that MCNNs are better-suited than vanilla deep neural networks for\nimitation learning applications. Website:\nhttps://sites.google.com/view/mcnn-imitation\n","authors":["Kaustubh Sridhar","Souradeep Dutta","Dinesh Jayaraman","James Weimer","Insup Lee"],"pdf_url":"https://arxiv.org/pdf/2310.06171v1.pdf","comment":"22 pages (9 main pages)"},{"id":"http://arxiv.org/abs/2310.06169v1","updated":"2023-10-09T21:47:15Z","published":"2023-10-09T21:47:15Z","title":"Synthesizing Robust Walking Gaits via Discrete-Time Barrier Functions\n  with Application to Multi-Contact Exoskeleton Locomotion","summary":"  Successfully achieving bipedal locomotion remains challenging due to\nreal-world factors such as model uncertainty, random disturbances, and\nimperfect state estimation. In this work, we propose the use of discrete-time\nbarrier functions to certify hybrid forward invariance of reduced step-to-step\ndynamics. The size of these invariant sets can then be used as a metric for\nlocomotive robustness. We demonstrate an application of this metric towards\nsynthesizing robust nominal walking gaits using a simulation-in-the-loop\napproach. This procedure produces reference motions with step-to-step dynamics\nthat are maximally forward-invariant with respect to the reduced representation\nof choice. The results demonstrate robust locomotion for both flat-foot walking\nand multi-contact walking on the Atalante lower-body exoskeleton.\n","authors":["Maegan Tucker","Kejun Li","Aaron D. Ames"],"pdf_url":"https://arxiv.org/pdf/2310.06169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06160v1","updated":"2023-10-09T21:18:14Z","published":"2023-10-09T21:18:14Z","title":"Entropy Based Multi-robot Active SLAM","summary":"  In this article, we present an efficient multi-robot active SLAM framework\nthat involves a frontier-sharing method for maximum exploration of an unknown\nenvironment. It encourages the robots to spread into the environment while\nweighting the goal frontiers with the pose graph SLAM uncertainly and path\nentropy. Our approach works on a limited number of frontier points and weights\nthe goal frontiers with a utility function that encapsulates both the SLAM and\nmap uncertainties, thus providing an efficient and not computationally\nexpensive solution. Our approach has been tested on publicly available\nsimulation environments and on real robots. An accumulative 31% more coverage\nthan similar state-of-the-art approaches has been obtained, proving the\ncapability of our approach for efficient environment exploration.\n","authors":["Muhammad Farhan Ahmed","Matteo Maragliano","Vincent Frémont","Carmine Tommaso Recchiuto"],"pdf_url":"https://arxiv.org/pdf/2310.06160v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.06153v1","updated":"2023-10-09T21:00:43Z","published":"2023-10-09T21:00:43Z","title":"Multi-Robot Task Assignment and Path Finding for Time-Sensitive Missions\n  with Online Task Generation","summary":"  Executing time-sensitive multi-robot missions involves two distinct problems:\nMulti-Robot Task Assignment (MRTA) and Multi-Agent Path Finding (MAPF).\nComputing safe paths that complete every task and minimize the time to mission\ncompletion, or makespan, is a significant computational challenge even for\nsmall teams. In many missions, tasks can be generated during execution which is\ntypically handled by either recomputing task assignments and paths from\nscratch, or by modifying existing plans using approximate approaches. While\nperforming task reassignment and path finding from scratch produces\ntheoretically optimal results, the computational load makes it too expensive\nfor online implementation. In this work, we present Time-Sensitive Online Task\nAssignment and Navigation (TSOTAN), a framework which can quickly incorporate\nonline generated tasks while guaranteeing bounded suboptimal task assignment\nmakespans. It does this by assessing the quality of partial task reassignments\nand only performing a complete reoptimization when the makespan exceeds a user\nspecified suboptimality bound. Through experiments in 2D environments we\ndemonstrate TSOTAN's ability to produce quality solutions with computation\ntimes suitable for online implementation.\n","authors":["David Thorne","Brett T. Lopez"],"pdf_url":"https://arxiv.org/pdf/2310.06153v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.06138v1","updated":"2023-10-09T20:32:49Z","published":"2023-10-09T20:32:49Z","title":"Layout Sequence Prediction From Noisy Mobile Modality","summary":"  Trajectory prediction plays a vital role in understanding pedestrian movement\nfor applications such as autonomous driving and robotics. Current trajectory\nprediction models depend on long, complete, and accurately observed sequences\nfrom visual modalities. Nevertheless, real-world situations often involve\nobstructed cameras, missed objects, or objects out of sight due to\nenvironmental factors, leading to incomplete or noisy trajectories. To overcome\nthese limitations, we propose LTrajDiff, a novel approach that treats objects\nobstructed or out of sight as equally important as those with fully visible\ntrajectories. LTrajDiff utilizes sensor data from mobile phones to surmount\nout-of-sight constraints, albeit introducing new challenges such as modality\nfusion, noisy data, and the absence of spatial layout and object size\ninformation. We employ a denoising diffusion model to predict precise layout\nsequences from noisy mobile data using a coarse-to-fine diffusion strategy,\nincorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model\npredicts layout sequences by implicitly inferring object size and projection\nstatus from a single reference timestamp or significantly obstructed sequences.\nAchieving SOTA results in randomly obstructed experiments and extremely short\ninput experiments, our model illustrates the effectiveness of leveraging noisy\nmobile data. In summary, our approach offers a promising solution to the\nchallenges faced by layout sequence and trajectory prediction models in\nreal-world settings, paving the way for utilizing sensor data from mobile\nphones to accurately predict pedestrian bounding box trajectories. To the best\nof our knowledge, this is the first work that addresses severely obstructed and\nextremely short layout sequences by combining vision with noisy mobile\nmodality, making it the pioneering work in the field of layout sequence\ntrajectory prediction.\n","authors":["Haichao Zhang","Yi Xu","Hongsheng Lu","Takayuki Shimizu","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2310.06138v1.pdf","comment":"In Proceedings of the 31st ACM International Conference on Multimedia\n  2023 (MM 23)"},{"id":"http://arxiv.org/abs/2310.06084v1","updated":"2023-10-09T18:46:57Z","published":"2023-10-09T18:46:57Z","title":"Exoskeleton-Mediated Physical Human-Human Interaction for a Sit-to-Stand\n  Rehabilitation Task","summary":"  Sit-to-Stand (StS) is a fundamental daily activity that can be challenging\nfor stroke survivors due to strength, motor control, and proprioception\ndeficits in their lower limbs. Existing therapies involve repetitive StS\nexercises, but these can be physically demanding for therapists while assistive\ndevices may limit patient participation and hinder motor learning. To address\nthese challenges, this work proposes the use of two lower-limb exoskeletons to\nmediate physical interaction between therapists and patients during a StS\nrehabilitative task. This approach offers several advantages, including\nimproved therapist-patient interaction, safety enforcement, and performance\nquantification. The whole body control of the two exoskeletons transmits online\nfeedback between the two users, but at the same time assists in movement and\nensures balance, and thus helping subjects with greater difficulty. In this\nstudy we present the architecture of the framework, presenting and discussing\nsome technical choices made in the design.\n","authors":["Lorenzo Vianello","Emek Barış Küçüktabak","Matthew Short","Clément Lhoste","Lorenzo Amato","Kevin Lynch","Jose Pons"],"pdf_url":"https://arxiv.org/pdf/2310.06084v1.pdf","comment":"7 pages, 6 figures, submitted to 2024 IEEE The International\n  Conference on Robotics and Automation (ICRA)"},{"id":"http://arxiv.org/abs/2310.06074v1","updated":"2023-10-09T18:30:36Z","published":"2023-10-09T18:30:36Z","title":"Towards Agility: A Momentum Aware Trajectory Optimisation Framework\n  using Full-Centroidal Dynamics & Implicit Inverse Kinematics","summary":"  Online planning and execution of acrobatic maneuvers pose significant\nchallenges in legged locomotion. Their underlying combinatorial nature, along\nwith the current hardware's limitations constitute the main obstacles in\nunlocking the true potential of legged-robots. This letter tries to expose the\nintricacies of these optimal control problems in a tangible way, directly\napplicable to the creation of more efficient online trajectory optimisation\nframeworks. By analysing the fundamental principles that shape the behaviour of\nthe system, the dynamics themselves can be exploited to surpass its hardware\nlimitations. More specifically, a trajectory optimisation formulation is\nproposed that exploits the system's high-order nonlinearities, such as the\nnonholonomy of the angular momentum, and phase-space symmetries in order to\nproduce feasible high-acceleration maneuvers. By leveraging the full-centroidal\ndynamics of the quadruped ANYmal C and directly optimising its footholds and\ncontact forces, the framework is capable of producing efficient motion plans\nwith low computational overhead. The feasibility of the produced trajectories\nis ensured by taking into account the configuration-dependent inertial\nproperties of the robot during the planning process, while its robustness is\nincreased by supplying the full analytic derivatives & hessians to the solver.\nFinally, a significant portion of the discussion is centred around the\ndeployment of the proposed framework on the ANYmal C platform, while its true\ncapabilities are demonstrated through real-world experiments, with the\nsuccessful execution of high-acceleration motion scenarios like the squat-jump.\n","authors":["Aristotelis Papatheodorou","Wolfgang Merkt","Alexander L. Mitchell","Ioannis Havoutis"],"pdf_url":"https://arxiv.org/pdf/2310.06074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.03729v2","updated":"2023-10-09T18:17:46Z","published":"2022-10-07T17:56:57Z","title":"Flexible Attention-Based Multi-Policy Fusion for Efficient Deep\n  Reinforcement Learning","summary":"  Reinforcement learning (RL) agents have long sought to approach the\nefficiency of human learning. Humans are great observers who can learn by\naggregating external knowledge from various sources, including observations\nfrom others' policies of attempting a task. Prior studies in RL have\nincorporated external knowledge policies to help agents improve sample\nefficiency. However, it remains non-trivial to perform arbitrary combinations\nand replacements of those policies, an essential feature for generalization and\ntransferability. In this work, we present Knowledge-Grounded RL (KGRL), an RL\nparadigm fusing multiple knowledge policies and aiming for human-like\nefficiency and flexibility. We propose a new actor architecture for KGRL,\nKnowledge-Inclusive Attention Network (KIAN), which allows free knowledge\nrearrangement due to embedding-based attentive action prediction. KIAN also\naddresses entropy imbalance, a problem arising in maximum entropy KGRL that\nhinders an agent from efficiently exploring the environment, through a new\ndesign of policy distributions. The experimental results demonstrate that KIAN\noutperforms alternative methods incorporating external knowledge policies and\nachieves efficient and flexible learning. Our implementation is available at\nhttps://github.com/Pascalson/KGRL.git\n","authors":["Zih-Yun Chiu","Yi-Lin Tuan","William Yang Wang","Michael C. Yip"],"pdf_url":"https://arxiv.org/pdf/2210.03729v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06020v1","updated":"2023-10-09T18:00:01Z","published":"2023-10-09T18:00:01Z","title":"DyST: Towards Dynamic Neural Scene Representations on Real-World Videos","summary":"  Visual understanding of the world goes beyond the semantics and flat\nstructure of individual images. In this work, we aim to capture both the 3D\nstructure and dynamics of real-world scenes from monocular real-world videos.\nOur Dynamic Scene Transformer (DyST) model leverages recent work in neural\nscene representation to learn a latent decomposition of monocular real-world\nvideos into scene content, per-view scene dynamics, and camera pose. This\nseparation is achieved through a novel co-training scheme on monocular videos\nand our new synthetic dataset DySO. DyST learns tangible latent representations\nfor dynamic scenes that enable view generation with separate control over the\ncamera and the content of the scene.\n","authors":["Maximilian Seitzer","Sjoerd van Steenkiste","Thomas Kipf","Klaus Greff","Mehdi S. M. Sajjadi"],"pdf_url":"https://arxiv.org/pdf/2310.06020v1.pdf","comment":"Project website: https://dyst-paper.github.io/"},{"id":"http://arxiv.org/abs/2310.06006v1","updated":"2023-10-09T16:47:20Z","published":"2023-10-09T16:47:20Z","title":"Review of control algorithms for mobile robotics","summary":"  This article presents a comprehensive review of control algorithms used in\nmobile robotics, a field in constant evolution. Mobile robotics has seen\nsignificant advances in recent years, driven by the demand for applications in\nvarious sectors, such as industrial automation, space exploration, and medical\ncare. The review focuses on control algorithms that address specific challenges\nin navigation, localization, mapping, and path planning in changing and unknown\nenvironments. Classical approaches, such as PID control and methods based on\nclassical control theory, as well as modern techniques, including deep learning\nand model-based planning, are discussed in detail. In addition, practical\napplications and remaining challenges in implementing these algorithms in\nreal-world mobile robots are highlighted. Ultimately, this review provides a\ncomprehensive overview of the diversity and complexity of control algorithms in\nmobile robotics, helping researchers and practitioners to better understand the\noptions available to address specific problems in this exciting area of study.\n","authors":["Andres-David Suarez-Gomez","Andres A. Hernandez Ortega"],"pdf_url":"https://arxiv.org/pdf/2310.06006v1.pdf","comment":"8 pages, in Spanish"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.05922v1","updated":"2023-10-09T17:59:53Z","published":"2023-10-09T17:59:53Z","title":"FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video\n  editing","summary":"  Text-to-video editing aims to edit the visual appearance of a source video\nconditional on textual prompts. A major challenge in this task is to ensure\nthat all frames in the edited video are visually consistent. Most recent works\napply advanced text-to-image diffusion models to this task by inflating 2D\nspatial attention in the U-Net into spatio-temporal attention. Although\ntemporal context can be added through spatio-temporal attention, it may\nintroduce some irrelevant information for each patch and therefore cause\ninconsistency in the edited video. In this paper, for the first time, we\nintroduce optical flow into the attention module in the diffusion model's U-Net\nto address the inconsistency issue for text-to-video editing. Our method,\nFLATTEN, enforces the patches on the same flow path across different frames to\nattend to each other in the attention module, thus improving the visual\nconsistency in the edited videos. Additionally, our method is training-free and\ncan be seamlessly integrated into any diffusion-based text-to-video editing\nmethods and improve their visual consistency. Experiment results on existing\ntext-to-video editing benchmarks show that our proposed method achieves the new\nstate-of-the-art performance. In particular, our method excels in maintaining\nthe visual consistency in the edited videos.\n","authors":["Yuren Cong","Mengmeng Xu","Christian Simon","Shoufa Chen","Jiawei Ren","Yanping Xie","Juan-Manuel Perez-Rua","Bodo Rosenhahn","Tao Xiang","Sen He"],"pdf_url":"https://arxiv.org/pdf/2310.05922v1.pdf","comment":"Project page: https://flatten-video-editing.github.io/"},{"id":"http://arxiv.org/abs/2310.05920v1","updated":"2023-10-09T17:59:26Z","published":"2023-10-09T17:59:26Z","title":"SimPLR: A Simple and Plain Transformer for Object Detection and\n  Segmentation","summary":"  The ability to detect objects in images at varying scales has played a\npivotal role in the design of modern object detectors. Despite considerable\nprogress in removing handcrafted components using transformers, multi-scale\nfeature maps remain a key factor for their empirical success, even with a plain\nbackbone like the Vision Transformer (ViT). In this paper, we show that this\nreliance on feature pyramids is unnecessary and a transformer-based detector\nwith scale-aware attention enables the plain detector `SimPLR' whose backbone\nand detection head both operate on single-scale features. The plain\narchitecture allows SimPLR to effectively take advantages of self-supervised\nlearning and scaling approaches with ViTs, yielding strong performance compared\nto multi-scale counterparts. We demonstrate through our experiments that when\nscaling to larger backbones, SimPLR indicates better performance than\nend-to-end detectors (Mask2Former) and plain-backbone detectors (ViTDet), while\nconsistently being faster. The code will be released.\n","authors":["Duy-Kien Nguyen","Martin R. Oswald","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2310.05920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05917v1","updated":"2023-10-09T17:59:12Z","published":"2023-10-09T17:59:12Z","title":"Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic\n  Clothing Driven by Sparse RGB-D Input","summary":"  Clothing is an important part of human appearance but challenging to model in\nphotorealistic avatars. In this work we present avatars with dynamically moving\nloose clothing that can be faithfully driven by sparse RGB-D inputs as well as\nbody and face motion. We propose a Neural Iterative Closest Point (N-ICP)\nalgorithm that can efficiently track the coarse garment shape given sparse\ndepth input. Given the coarse tracking results, the input RGB-D images are then\nremapped to texel-aligned features, which are fed into the drivable avatar\nmodels to faithfully reconstruct appearance details. We evaluate our method\nagainst recent image-driven synthesis baselines, and conduct a comprehensive\nanalysis of the N-ICP algorithm. We demonstrate that our method can generalize\nto a novel testing environment, while preserving the ability to produce\nhigh-fidelity and faithful clothing dynamics and appearance.\n","authors":["Donglai Xiang","Fabian Prada","Zhe Cao","Kaiwen Guo","Chenglei Wu","Jessica Hodgins","Timur Bagautdinov"],"pdf_url":"https://arxiv.org/pdf/2310.05917v1.pdf","comment":"SIGGRAPH Asia 2023 Conference Paper. Project website:\n  https://xiangdonglai.github.io/www-sa23-drivable-clothing/"},{"id":"http://arxiv.org/abs/2310.05916v1","updated":"2023-10-09T17:59:04Z","published":"2023-10-09T17:59:04Z","title":"Interpreting CLIP's Image Representation via Text-Based Decomposition","summary":"  We investigate the CLIP image encoder by analyzing how individual model\ncomponents affect the final representation. We decompose the image\nrepresentation as a sum across individual image patches, model layers, and\nattention heads, and use CLIP's text representation to interpret the summands.\nInterpreting the attention heads, we characterize each head's role by\nautomatically finding text representations that span its output space, which\nreveals property-specific roles for many heads (e.g. location or shape). Next,\ninterpreting the image patches, we uncover an emergent spatial localization\nwithin CLIP. Finally, we use this understanding to remove spurious features\nfrom CLIP and to create a strong zero-shot image segmenter. Our results\nindicate that a scalable understanding of transformer models is attainable and\ncan be used to repair and improve models.\n","authors":["Yossi Gandelsman","Alexei A. Efros","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2310.05916v1.pdf","comment":"Project page and code:\n  https://yossigandelsman.github.io/clip_decomposition/"},{"id":"http://arxiv.org/abs/2301.08245v2","updated":"2023-10-09T17:58:14Z","published":"2023-01-19T18:59:28Z","title":"Booster: a Benchmark for Depth from Images of Specular and Transparent\n  Surfaces","summary":"  Estimating depth from images nowadays yields outstanding results, both in\nterms of in-domain accuracy and generalization. However, we identify two main\nchallenges that remain open in this field: dealing with non-Lambertian\nmaterials and effectively processing high-resolution images. Purposely, we\npropose a novel dataset that includes accurate and dense ground-truth labels at\nhigh resolution, featuring scenes containing several specular and transparent\nsurfaces. Our acquisition pipeline leverages a novel deep space-time stereo\nframework, enabling easy and accurate labeling with sub-pixel precision. The\ndataset is composed of 606 samples collected in 85 different scenes, each\nsample includes both a high-resolution pair (12 Mpx) as well as an unbalanced\nstereo pair (Left: 12 Mpx, Right: 1.1 Mpx), typical of modern mobile devices\nthat mount sensors with different resolutions. Additionally, we provide\nmanually annotated material segmentation masks and 15K unlabeled samples. The\ndataset is composed of a train set and two test sets, the latter devoted to the\nevaluation of stereo and monocular depth estimation networks. Our experiments\nhighlight the open challenges and future research directions in this field.\n","authors":["Pierluigi Zama Ramirez","Alex Costanzino","Fabio Tosi","Matteo Poggi","Samuele Salti","Stefano Mattoccia","Luigi Di Stefano"],"pdf_url":"https://arxiv.org/pdf/2301.08245v2.pdf","comment":"Extension of the paper \"Open Challenges in Deep Stereo: the Booster\n  Dataset\" presented at CVPR 2022. Accepted at TPAMI"},{"id":"http://arxiv.org/abs/2306.00977v2","updated":"2023-10-09T17:51:12Z","published":"2023-06-01T17:59:10Z","title":"AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation","summary":"  During interactive segmentation, a model and a user work together to\ndelineate objects of interest in a 3D point cloud. In an iterative process, the\nmodel assigns each data point to an object (or the background), while the user\ncorrects errors in the resulting segmentation and feeds them back into the\nmodel. The current best practice formulates the problem as binary\nclassification and segments objects one at a time. The model expects the user\nto provide positive clicks to indicate regions wrongly assigned to the\nbackground and negative clicks on regions wrongly assigned to the object.\nSequentially visiting objects is wasteful since it disregards synergies between\nobjects: a positive click for a given object can, by definition, serve as a\nnegative click for nearby objects. Moreover, a direct competition between\nadjacent objects can speed up the identification of their common boundary. We\nintroduce AGILE3D, an efficient, attention-based model that (1) supports\nsimultaneous segmentation of multiple 3D objects, (2) yields more accurate\nsegmentation masks with fewer user clicks, and (3) offers faster inference. Our\ncore idea is to encode user clicks as spatial-temporal queries and enable\nexplicit interactions between click queries as well as between them and the 3D\nscene through a click attention module. Every time new clicks are added, we\nonly need to run a lightweight decoder that produces updated segmentation\nmasks. In experiments with four different 3D point cloud datasets, AGILE3D sets\na new state-of-the-art. Moreover, we also verify its practicality in real-world\nsetups with real user studies.\n","authors":["Yuanwen Yue","Sabarinath Mahadevan","Jonas Schult","Francis Engelmann","Bastian Leibe","Konrad Schindler","Theodora Kontogianni"],"pdf_url":"https://arxiv.org/pdf/2306.00977v2.pdf","comment":"Project page: https://ywyue.github.io/AGILE3D"},{"id":"http://arxiv.org/abs/2310.05886v1","updated":"2023-10-09T17:28:35Z","published":"2023-10-09T17:28:35Z","title":"Streaming Anchor Loss: Augmenting Supervision with Temporal Significance","summary":"  Streaming neural network models for fast frame-wise responses to various\nspeech and sensory signals are widely adopted on resource-constrained\nplatforms. Hence, increasing the learning capacity of such streaming models\n(i.e., by adding more parameters) to improve the predictive power may not be\nviable for real-world tasks. In this work, we propose a new loss, Streaming\nAnchor Loss (SAL), to better utilize the given learning capacity by encouraging\nthe model to learn more from essential frames. More specifically, our SAL and\nits focal variations dynamically modulate the frame-wise cross entropy loss\nbased on the importance of the corresponding frames so that a higher loss\npenalty is assigned for frames within the temporal proximity of semantically\ncritical events. Therefore, our loss ensures that the model training focuses on\npredicting the relatively rare but task-relevant frames. Experimental results\nwith standard lightweight convolutional and recurrent streaming networks on\nthree different speech based detection tasks demonstrate that SAL enables the\nmodel to learn the overall task more effectively with improved accuracy and\nlatency, without any additional data, model parameters, or architectural\nchanges.\n","authors":[" Utkarsh"," Sarawgi","John Berkowitz","Vineet Garg","Arnav Kundu","Minsik Cho","Sai Srujana Buddi","Saurabh Adya","Ahmed Tewfik"],"pdf_url":"https://arxiv.org/pdf/2310.05886v1.pdf","comment":"Under review for ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.05881v1","updated":"2023-10-09T17:22:58Z","published":"2023-10-09T17:22:58Z","title":"Controllable Chest X-Ray Report Generation from Longitudinal\n  Representations","summary":"  Radiology reports are detailed text descriptions of the content of medical\nscans. Each report describes the presence/absence and location of relevant\nclinical findings, commonly including comparison with prior exams of the same\npatient to describe how they evolved. Radiology reporting is a time-consuming\nprocess, and scan results are often subject to delays. One strategy to speed up\nreporting is to integrate automated reporting systems, however clinical\ndeployment requires high accuracy and interpretability. Previous approaches to\nautomated radiology reporting generally do not provide the prior study as\ninput, precluding comparison which is required for clinical accuracy in some\ntypes of scans, and offer only unreliable methods of interpretability.\nTherefore, leveraging an existing visual input format of anatomical tokens, we\nintroduce two novel aspects: (1) longitudinal representation learning -- we\ninput the prior scan as an additional input, proposing a method to align,\nconcatenate and fuse the current and prior visual information into a joint\nlongitudinal representation which can be provided to the multimodal report\ngeneration model; (2) sentence-anatomy dropout -- a training strategy for\ncontrollability in which the report generator model is trained to predict only\nsentences from the original report which correspond to the subset of anatomical\nregions given as input. We show through in-depth experiments on the MIMIC-CXR\ndataset how the proposed approach achieves state-of-the-art results while\nenabling anatomy-wise controllable report generation.\n","authors":["Francesco Dalla Serra","Chaoyang Wang","Fani Deligianni","Jeffrey Dalton","Alison Q O'Neil"],"pdf_url":"https://arxiv.org/pdf/2310.05881v1.pdf","comment":"Accepted to the Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.05873v1","updated":"2023-10-09T17:13:10Z","published":"2023-10-09T17:13:10Z","title":"Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion\n  Models","summary":"  Fine-tuning diffusion models through personalized datasets is an acknowledged\nmethod for improving generation quality across downstream tasks, which,\nhowever, often inadvertently generates unintended concepts such as watermarks\nand QR codes, attributed to the limitations in image sources and collecting\nmethods within specific downstream tasks. Existing solutions suffer from\neliminating these unintentionally learned implicit concepts, primarily due to\nthe dependency on the model's ability to recognize concepts that it actually\ncannot discern. In this work, we introduce \\methodname, a novel approach that\nsuccessfully removes the implicit concepts with either an additional accessible\nclassifier or detector model to encode geometric information of these concepts\ninto text domain. Moreover, we propose \\textit{Implicit Concept}, a novel\nimage-text dataset imbued with three implicit concepts (\\ie, watermarks, QR\ncodes, and text) for training and evaluation. Experimental results demonstrate\nthat \\methodname not only identifies but also proficiently eradicates implicit\nconcepts, revealing a significant improvement over the existing methods. The\nintegration of geometric information marks a substantial progression in the\nprecise removal of implicit concepts in diffusion models.\n","authors":["Zhili Liu","Kai Chen","Yifan Zhang","Jianhua Han","Lanqing Hong","Hang Xu","Zhenguo Li","Dit-Yan Yeung","James Kwok"],"pdf_url":"https://arxiv.org/pdf/2310.05873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05872v1","updated":"2023-10-09T17:10:35Z","published":"2023-10-09T17:10:35Z","title":"ViCor: Bridging Visual Understanding and Commonsense Reasoning with\n  Large Language Models","summary":"  In our work, we explore the synergistic capabilities of pre-trained\nvision-and-language models (VLMs) and large language models (LLMs) for visual\ncommonsense reasoning (VCR). We categorize the problem of VCR into visual\ncommonsense understanding (VCU) and visual commonsense inference (VCI). For\nVCU, which involves perceiving the literal visual content, pre-trained VLMs\nexhibit strong cross-dataset generalization. On the other hand, in VCI, where\nthe goal is to infer conclusions beyond image content, VLMs face difficulties.\nWe find that a baseline where VLMs provide perception results (image captions)\nto LLMs leads to improved performance on VCI. However, we identify a challenge\nwith VLMs' passive perception, which often misses crucial context information,\nleading to incorrect or uncertain reasoning by LLMs. To mitigate this issue, we\nsuggest a collaborative approach where LLMs, when uncertain about their\nreasoning, actively direct VLMs to concentrate on and gather relevant visual\nelements to support potential commonsense inferences. In our method, named\nViCor, pre-trained LLMs serve as problem classifiers to analyze the problem\ncategory, VLM commanders to leverage VLMs differently based on the problem\nclassification, and visual commonsense reasoners to answer the question. VLMs\nwill perform visual recognition and understanding. We evaluate our framework on\ntwo VCR benchmark datasets and outperform all other methods that do not require\nin-domain supervised fine-tuning.\n","authors":["Kaiwen Zhou","Kwonjoon Lee","Teruhisa Misu","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05867v1","updated":"2023-10-09T17:03:39Z","published":"2023-10-09T17:03:39Z","title":"Domain-wise Invariant Learning for Panoptic Scene Graph Generation","summary":"  Panoptic Scene Graph Generation (PSG) involves the detection of objects and\nthe prediction of their corresponding relationships (predicates). However, the\npresence of biased predicate annotations poses a significant challenge for PSG\nmodels, as it hinders their ability to establish a clear decision boundary\namong different predicates. This issue substantially impedes the practical\nutility and real-world applicability of PSG models. To address the intrinsic\nbias above, we propose a novel framework to infer potentially biased\nannotations by measuring the predicate prediction risks within each\nsubject-object pair (domain), and adaptively transfer the biased annotations to\nconsistent ones by learning invariant predicate representation embeddings.\nExperiments show that our method significantly improves the performance of\nbenchmark models, achieving a new state-of-the-art performance, and shows great\ngeneralization and effectiveness on PSG dataset.\n","authors":["Li Li","You Qin","Wei Ji","Yuxiao Zhou","Roger Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2310.05867v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2307.15567"},{"id":"http://arxiv.org/abs/2208.10765v2","updated":"2023-10-09T17:03:26Z","published":"2022-08-23T06:55:53Z","title":"A Low-Cost Lane-Following Algorithm for Cyber-Physical Robots","summary":"  Duckiebots are low-cost mobile robots that are widely used in the fields of\nresearch and education. Although there are existing self-driving algorithms for\nthe Duckietown platform, they are either too complex or perform too poorly to\nnavigate a multi-lane track. Moreover, it is essential to give memory and\ncomputational resources to a Duckiebot so it can perform additional tasks such\nas out-of-distribution input detection. In order to satisfy these constraints,\nwe built a low-cost autonomous driving algorithm capable of driving on a\ntwo-lane track. The algorithm uses traditional computer vision techniques to\nidentify the central lane on the track and obtain the relevant steering angle.\nThe steering is then controlled by a PID controller that smoothens the movement\nof the Duckiebot. The performance of the algorithm was compared to that of the\nNeurIPS 2018 AI Driving Olympics (AIDO) finalists, and it outperformed all but\none finalists. The two main contributions of our algorithm are its low\ncomputational requirements and very quick set-up, with ongoing efforts to make\nit more reliable.\n","authors":["Archit Gupta","Arvind Easwaran"],"pdf_url":"https://arxiv.org/pdf/2208.10765v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05863v1","updated":"2023-10-09T17:00:20Z","published":"2023-10-09T17:00:20Z","title":"Fine-grained Audio-Visual Joint Representations for Multimodal Large\n  Language Models","summary":"  Audio-visual large language models (LLM) have drawn significant attention,\nyet the fine-grained combination of both input streams is rather\nunder-explored, which is challenging but necessary for LLMs to understand\ngeneral video inputs. To this end, a fine-grained audio-visual joint\nrepresentation (FAVOR) learning framework for multimodal LLMs is proposed in\nthis paper, which extends a text-based LLM to simultaneously perceive speech\nand audio events in the audio input stream and images or videos in the visual\ninput stream, at the frame level. To fuse the audio and visual feature streams\ninto joint representations and to align the joint space with the LLM input\nembedding space, we propose a causal Q-Former structure with a causal attention\nmodule to enhance the capture of causal relations of the audio-visual frames\nacross time. An audio-visual evaluation benchmark (AVEB) is also proposed which\ncomprises six representative single-modal tasks with five cross-modal tasks\nreflecting audio-visual co-reasoning abilities. While achieving competitive\nsingle-modal performance on audio, speech and image tasks in AVEB, FAVOR\nachieved over 20% accuracy improvements on the video question-answering task\nwhen fine-grained information or temporal causal reasoning is required. FAVOR,\nin addition, demonstrated remarkable video comprehension and reasoning\nabilities on tasks that are unprecedented by other multimodal LLMs. An\ninteractive demo of FAVOR is available at\nhttps://github.com/the-anonymous-bs/FAVOR.git, and the training code and model\ncheckpoints will be released upon acceptance.\n","authors":["Guangzhi Sun","Wenyi Yu","Changli Tang","Xianzhao Chen","Tian Tan","Wei Li","Lu Lu","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05861v1","updated":"2023-10-09T16:57:57Z","published":"2023-10-09T16:57:57Z","title":"Rephrase, Augment, Reason: Visual Grounding of Questions for\n  Vision-Language Models","summary":"  An increasing number of vision-language tasks can be handled with little to\nno training, i.e., in a zero and few-shot manner, by marrying large language\nmodels (LLMs) to vision encoders, resulting in large vision-language models\n(LVLMs). While this has huge upsides, such as not requiring training data or\ncustom architectures, how an input is presented to a LVLM can have a major\nimpact on zero-shot model performance. In particular, inputs phrased in an\nunderspecified way can result in incorrect answers due to factors like missing\nvisual information, complex implicit reasoning, or linguistic ambiguity.\nTherefore, adding visually grounded information to the input as a preemptive\nclarification should improve model performance by reducing underspecification,\ne.g., by localizing objects and disambiguating references. Similarly, in the\nVQA setting, changing the way questions are framed can make them easier for\nmodels to answer. To this end, we present Rephrase, Augment and Reason\n(RepARe), a gradient-free framework that extracts salient details about the\nimage using the underlying LVLM as a captioner and reasoner, in order to\npropose modifications to the original question. We then use the LVLM's\nconfidence over a generated answer as an unsupervised scoring function to\nselect the rephrased question most likely to improve zero-shot performance.\nFocusing on two visual question answering tasks, we show that RepARe can result\nin a 3.85% (absolute) increase in zero-shot performance on VQAv2 and a 6.41%\npoint increase on A-OKVQA. Additionally, we find that using gold answers for\noracle question candidate selection achieves a substantial gain in VQA accuracy\nby up to 14.41%. Through extensive analysis, we demonstrate that outputs from\nRepARe increase syntactic complexity, and effectively utilize vision-language\ninteraction and the frozen language model in LVLMs.\n","authors":["Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2310.05861v1.pdf","comment":"22 pages, 4 figures, Code: https://github.com/archiki/RepARe"},{"id":"http://arxiv.org/abs/2303.14465v2","updated":"2023-10-09T16:55:08Z","published":"2023-03-25T13:22:56Z","title":"Equivariant Similarity for Vision-Language Foundation Models","summary":"  This study explores the concept of equivariance in vision-language foundation\nmodels (VLMs), focusing specifically on the multimodal similarity function that\nis not only the major training objective but also the core delivery to support\ndownstream tasks. Unlike the existing image-text similarity objective which\nonly categorizes matched pairs as similar and unmatched pairs as dissimilar,\nequivariance also requires similarity to vary faithfully according to the\nsemantic changes. This allows VLMs to generalize better to nuanced and unseen\nmultimodal compositions. However, modeling equivariance is challenging as the\nground truth of semantic change is difficult to collect. For example, given an\nimage-text pair about a dog, it is unclear to what extent the similarity\nchanges when the pixel is changed from dog to cat? To this end, we propose\nEqSim, a regularization loss that can be efficiently calculated from any two\nmatched training pairs and easily pluggable into existing image-text retrieval\nfine-tuning. Meanwhile, to further diagnose the equivariance of VLMs, we\npresent a new challenging benchmark EqBen. Compared to the existing evaluation\nsets, EqBen is the first to focus on \"visual-minimal change\". Extensive\nexperiments show the lack of equivariance in current VLMs and validate the\neffectiveness of EqSim. Code is available at https://github.com/Wangt-CN/EqBen.\n","authors":["Tan Wang","Kevin Lin","Linjie Li","Chung-Ching Lin","Zhengyuan Yang","Hanwang Zhang","Zicheng Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2303.14465v2.pdf","comment":"Accepted by ICCV'23 (Oral); Add evaluation on MLLM"},{"id":"http://arxiv.org/abs/2310.05837v1","updated":"2023-10-09T16:26:34Z","published":"2023-10-09T16:26:34Z","title":"A Real-time Method for Inserting Virtual Objects into Neural Radiance\n  Fields","summary":"  We present the first real-time method for inserting a rigid virtual object\ninto a neural radiance field, which produces realistic lighting and shadowing\neffects, as well as allows interactive manipulation of the object. By\nexploiting the rich information about lighting and geometry in a NeRF, our\nmethod overcomes several challenges of object insertion in augmented reality.\nFor lighting estimation, we produce accurate, robust and 3D spatially-varying\nincident lighting that combines the near-field lighting from NeRF and an\nenvironment lighting to account for sources not covered by the NeRF. For\nocclusion, we blend the rendered virtual object with the background scene using\nan opacity map integrated from the NeRF. For shadows, with a precomputed field\nof spherical signed distance field, we query the visibility term for any point\naround the virtual object, and cast soft, detailed shadows onto 3D surfaces.\nCompared with state-of-the-art techniques, our approach can insert virtual\nobject into scenes with superior fidelity, and has a great potential to be\nfurther applied to augmented reality systems.\n","authors":["Keyang Ye","Hongzhi Wu","Xin Tong","Kun Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.05837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.04819v2","updated":"2023-10-09T16:24:26Z","published":"2023-04-10T19:00:29Z","title":"Recent Advancements in Machine Learning For Cybercrime Prediction","summary":"  Cybercrime is a growing threat to organizations and individuals worldwide,\nwith criminals using sophisticated techniques to breach security systems and\nsteal sensitive data. This paper aims to comprehensively survey the latest\nadvancements in cybercrime prediction, highlighting the relevant research. For\nthis purpose, we reviewed more than 150 research articles and discussed 50 most\nrecent and appropriate ones. We start the review with some standard methods\ncybercriminals use and then focus on the latest machine and deep learning\ntechniques, which detect anomalous behavior and identify potential threats. We\nalso discuss transfer learning, which allows models trained on one dataset to\nbe adapted for use on another dataset. We then focus on active and\nreinforcement learning as part of early-stage algorithmic research in\ncybercrime prediction. Finally, we discuss critical innovations, research gaps,\nand future research opportunities in Cybercrime prediction. This paper presents\na holistic view of cutting-edge developments and publicly available datasets.\n","authors":["Lavanya Elluri","Varun Mandalapu","Piyush Vyas","Nirmalya Roy"],"pdf_url":"https://arxiv.org/pdf/2304.04819v2.pdf","comment":"Accepted in Journal of Computer Information Systems, 2023"},{"id":"http://arxiv.org/abs/2304.06385v3","updated":"2023-10-09T16:22:55Z","published":"2023-04-13T10:37:41Z","title":"TransHP: Image Classification with Hierarchical Prompting","summary":"  This paper explores a hierarchical prompting mechanism for the hierarchical\nimage classification (HIC) task. Different from prior HIC methods, our\nhierarchical prompting is the first to explicitly inject ancestor-class\ninformation as a tokenized hint that benefits the descendant-class\ndiscrimination. We think it well imitates human visual recognition, i.e.,\nhumans may use the ancestor class as a prompt to draw focus on the subtle\ndifferences among descendant classes. We model this prompting mechanism into a\nTransformer with Hierarchical Prompting (TransHP). TransHP consists of three\nsteps: 1) learning a set of prompt tokens to represent the coarse (ancestor)\nclasses, 2) on-the-fly predicting the coarse class of the input image at an\nintermediate block, and 3) injecting the prompt token of the predicted coarse\nclass into the intermediate feature. Though the parameters of TransHP maintain\nthe same for all input images, the injected coarse-class prompt conditions\n(modifies) the subsequent feature extraction and encourages a dynamic focus on\nrelatively subtle differences among the descendant classes. Extensive\nexperiments show that TransHP improves image classification on accuracy (e.g.,\nimproving ViT-B/16 by +2.83% ImageNet classification accuracy), training data\nefficiency (e.g., +12.69% improvement under 10% ImageNet training data), and\nmodel explainability. Moreover, TransHP also performs favorably against prior\nHIC methods, showing that TransHP well exploits the hierarchical information.\n","authors":["Wenhao Wang","Yifan Sun","Wei Li","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2304.06385v3.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05829v1","updated":"2023-10-09T16:17:42Z","published":"2023-10-09T16:17:42Z","title":"Revisiting the Temporal Modeling in Spatio-Temporal Predictive Learning\n  under A Unified View","summary":"  Spatio-temporal predictive learning plays a crucial role in self-supervised\nlearning, with wide-ranging applications across a diverse range of fields.\nPrevious approaches for temporal modeling fall into two categories:\nrecurrent-based and recurrent-free methods. The former, while meticulously\nprocessing frames one by one, neglect short-term spatio-temporal information\nredundancies, leading to inefficiencies. The latter naively stack frames\nsequentially, overlooking the inherent temporal dependencies. In this paper, we\nre-examine the two dominant temporal modeling approaches within the realm of\nspatio-temporal predictive learning, offering a unified perspective. Building\nupon this analysis, we introduce USTEP (Unified Spatio-TEmporal Predictive\nlearning), an innovative framework that reconciles the recurrent-based and\nrecurrent-free methods by integrating both micro-temporal and macro-temporal\nscales. Extensive experiments on a wide range of spatio-temporal predictive\nlearning demonstrate that USTEP achieves significant improvements over existing\ntemporal modeling approaches, thereby establishing it as a robust solution for\na wide range of spatio-temporal applications.\n","authors":["Cheng Tan","Jue Wang","Zhangyang Gao","Siyuan Li","Lirong Wu","Jun Xia","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2310.05829v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2208.08288v2","updated":"2023-10-09T16:01:45Z","published":"2022-08-17T13:31:38Z","title":"Deep learning based projection domain metal segmentation for metal\n  artifact reduction in cone beam computed tomography","summary":"  Metal artifact correction is a challenging problem in cone beam computed\ntomography (CBCT) scanning. Metal implants inserted into the anatomy cause\nsevere artifacts in reconstructed images. Widely used inpainting-based metal\nartifact reduction (MAR) methods require segmentation of metal traces in the\nprojections as a first step, which is a challenging task. One approach is to\nuse a deep learning method to segment metals in the projections. However, the\nsuccess of deep learning methods is limited by the availability of realistic\ntraining data. It is laborious and time consuming to get reliable ground truth\nannotations due to unclear implant boundaries and large numbers of projections.\nWe propose to use X-ray simulations to generate synthetic metal segmentation\ntraining dataset from clinical CBCT scans. We compare the effect of simulations\nwith different numbers of photons and also compare several training strategies\nto augment the available data. We compare our model's performance on real\nclinical scans with conventional region growing threshold-based MAR, moving\nmetal artifact reduction method, and a recent deep learning method. We show\nthat simulations with relatively small number of photons are suitable for the\nmetal segmentation task and that training the deep learning model with full\nsize and cropped projections together improves the robustness of the model. We\nshow substantial improvement in the image quality affected by severe motion,\nvoxel size under-sampling, and out-of-FOV metals. Our method can be easily\nintegrated into the existing projection-based MAR pipeline to get improved\nimage quality. This method can provide a novel paradigm to accurately segment\nmetals in CBCT projections.\n","authors":["Harshit Agrawal","Ari Hietanen","Simo Särkkä"],"pdf_url":"https://arxiv.org/pdf/2208.08288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01830v2","updated":"2023-10-09T16:01:32Z","published":"2023-10-03T06:55:19Z","title":"AI-Generated Images as Data Source: The Dawn of Synthetic Era","summary":"  The advancement of visual intelligence is intrinsically tethered to the\navailability of data. In parallel, generative Artificial Intelligence (AI) has\nunlocked the potential to create synthetic images that closely resemble\nreal-world photographs, which prompts a compelling inquiry: how visual\nintelligence benefit from the advance of generative AI? This paper explores the\ninnovative concept of harnessing these AI-generated images as a new data\nsource, reshaping traditional model paradigms in visual intelligence. In\ncontrast to real data, AI-generated data sources exhibit remarkable advantages,\nincluding unmatched abundance and scalability, the rapid generation of vast\ndatasets, and the effortless simulation of edge cases. Built on the success of\ngenerative AI models, we examines the potential of their generated data in a\nrange of applications, from training machine learning models to simulating\nscenarios for computational modeling, testing, and validation. We probe the\ntechnological foundations that support this groundbreaking use of generative\nAI, engaging in an in-depth discussion on the ethical, legal, and practical\nconsiderations that accompany this transformative paradigm shift. Through an\nexhaustive survey of current technologies and applications, this paper presents\na comprehensive view of the synthetic era in visual intelligence. A project\nassociated with this paper can be found at https://github.com/mwxely/AIGS .\n","authors":["Zuhao Yang","Fangneng Zhan","Kunhao Liu","Muyu Xu","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2310.01830v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2310.05812v1","updated":"2023-10-09T15:52:59Z","published":"2023-10-09T15:52:59Z","title":"Provably Convergent Data-Driven Convex-Nonconvex Regularization","summary":"  An emerging new paradigm for solving inverse problems is via the use of deep\nlearning to learn a regularizer from data. This leads to high-quality results,\nbut often at the cost of provable guarantees. In this work, we show how\nwell-posedness and convergent regularization arises within the convex-nonconvex\n(CNC) framework for inverse problems. We introduce a novel input weakly convex\nneural network (IWCNN) construction to adapt the method of learned adversarial\nregularization to the CNC framework. Empirically we show that our method\novercomes numerical issues of previous adversarial methods.\n","authors":["Zakhar Shumaylov","Jeremy Budd","Subhadip Mukherjee","Carola-Bibiane Schönlieb"],"pdf_url":"https://arxiv.org/pdf/2310.05812v1.pdf","comment":"4 pages + 3 pages appendices; preprint"},{"id":"http://arxiv.org/abs/2303.05118v4","updated":"2023-10-09T15:50:00Z","published":"2023-03-09T08:57:01Z","title":"SLCA: Slow Learner with Classifier Alignment for Continual Learning on a\n  Pre-trained Model","summary":"  The goal of continual learning is to improve the performance of recognition\nmodels in learning sequentially arrived data. Although most existing works are\nestablished on the premise of learning from scratch, growing efforts have been\ndevoted to incorporating the benefits of pre-training. However, how to\nadaptively exploit the pre-trained knowledge for each incremental task while\nmaintaining its generalizability remains an open question. In this work, we\npresent an extensive analysis for continual learning on a pre-trained model\n(CLPM), and attribute the key challenge to a progressive overfitting problem.\nObserving that selectively reducing the learning rate can almost resolve this\nissue in the representation layer, we propose a simple but extremely effective\napproach named Slow Learner with Classifier Alignment (SLCA), which further\nimproves the classification layer by modeling the class-wise distributions and\naligning the classification layers in a post-hoc fashion. Across a variety of\nscenarios, our proposal provides substantial improvements for CLPM (e.g., up to\n49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split\nCUB-200 and Split Cars-196, respectively), and thus outperforms\nstate-of-the-art approaches by a large margin. Based on such a strong baseline,\ncritical factors and promising directions are analyzed in-depth to facilitate\nsubsequent research. Code has been made available at:\nhttps://github.com/GengDavid/SLCA.\n","authors":["Gengwei Zhang","Liyuan Wang","Guoliang Kang","Ling Chen","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2303.05118v4.pdf","comment":"ICCV 2023, code released"},{"id":"http://arxiv.org/abs/2310.05804v1","updated":"2023-10-09T15:43:07Z","published":"2023-10-09T15:43:07Z","title":"Learning Language-guided Adaptive Hyper-modality Representation for\n  Multimodal Sentiment Analysis","summary":"  Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich\ninformation from multiple sources (e.g., language, video, and audio), the\npotential sentiment-irrelevant and conflicting information across modalities\nmay hinder the performance from being further improved. To alleviate this, we\npresent Adaptive Language-guided Multimodal Transformer (ALMT), which\nincorporates an Adaptive Hyper-modality Learning (AHL) module to learn an\nirrelevance/conflict-suppressing representation from visual and audio features\nunder the guidance of language features at different scales. With the obtained\nhyper-modality representation, the model can obtain a complementary and joint\nrepresentation through multimodal fusion for effective MSA. In practice, ALMT\nachieves state-of-the-art performance on several popular datasets (e.g., MOSI,\nMOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and\nnecessity of our irrelevance/conflict suppression mechanism.\n","authors":["Haoyu Zhang","Yu Wang","Guanghao Yin","Kejun Liu","Yuanyuan Liu","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.11723v6","updated":"2023-10-09T15:18:32Z","published":"2022-06-23T14:16:30Z","title":"Self-Supervised Training with Autoencoders for Visual Anomaly Detection","summary":"  Deep autoencoders provide an effective tool for learning non-linear\ndimensionality reduction in an unsupervised way. Recently, they have been used\nfor the task of anomaly detection in the visual domain. By optimizing for the\nreconstruction error using anomaly-free examples, the common belief is that a\ncorresponding network should fail to accurately reconstruct anomalous regions\nin the application phase. This goal is typically addressed by controlling the\ncapacity of the network, either by reducing the size of the bottleneck layer or\nby enforcing sparsity constraints on the activations. However, neither of these\ntechniques does explicitly penalize reconstruction of anomalous signals often\nresulting in poor detection. We tackle this problem by adapting a\nself-supervised learning regime that allows the use of discriminative\ninformation during training but focuses on the data manifold of normal\nexamples. We emphasize that inference with our approach is very efficient\nduring training and prediction requiring a single forward pass for each input\nimage. Our experiments on the MVTec AD dataset demonstrate high detection and\nlocalization performance. On the texture-subset, in particular, our approach\nconsistently outperforms recent anomaly detection methods by a significant\nmargin.\n","authors":["Alexander Bauer","Shinichi Nakajima","Klaus-Robert Müller"],"pdf_url":"https://arxiv.org/pdf/2206.11723v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09268v2","updated":"2023-10-09T15:17:25Z","published":"2023-03-16T12:44:44Z","title":"StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized\n  Tokenizer of a Large-Scale Generative Model","summary":"  Despite the progress made in the style transfer task, most previous work\nfocus on transferring only relatively simple features like color or texture,\nwhile missing more abstract concepts such as overall art expression or\npainter-specific traits. However, these abstract semantics can be captured by\nmodels like DALL-E or CLIP, which have been trained using huge datasets of\nimages and textual documents. In this paper, we propose StylerDALLE, a style\ntransfer method that exploits both of these models and uses natural language to\ndescribe abstract art styles. Specifically, we formulate the language-guided\nstyle transfer task as a non-autoregressive token sequence translation, i.e.,\nfrom input content image to output stylized image, in the discrete latent space\nof a large-scale pretrained vector-quantized tokenizer, e.g., the discrete\nvariational auto-encoder (dVAE) of DALL-E. To incorporate style information, we\npropose a Reinforcement Learning strategy with CLIP-based language supervision\nthat ensures stylization and content preservation simultaneously. Experimental\nresults demonstrate the superiority of our method, which can effectively\ntransfer art styles using language instructions at different granularities.\nCode is available at https://github.com/zipengxuc/StylerDALLE.\n","authors":["Zipeng Xu","Enver Sangineto","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2303.09268v2.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2310.05785v1","updated":"2023-10-09T15:16:35Z","published":"2023-10-09T15:16:35Z","title":"Joint object detection and re-identification for 3D obstacle\n  multi-camera systems","summary":"  In recent years, the field of autonomous driving has witnessed remarkable\nadvancements, driven by the integration of a multitude of sensors, including\ncameras and LiDAR systems, in different prototypes. However, with the\nproliferation of sensor data comes the pressing need for more sophisticated\ninformation processing techniques. This research paper introduces a novel\nmodification to an object detection network that uses camera and lidar\ninformation, incorporating an additional branch designed for the task of\nre-identifying objects across adjacent cameras within the same vehicle while\nelevating the quality of the baseline 3D object detection outcomes. The\nproposed methodology employs a two-step detection pipeline: initially, an\nobject detection network is employed, followed by a 3D box estimator that\noperates on the filtered point cloud generated from the network's detections.\nExtensive experimental evaluations encompassing both 2D and 3D domains validate\nthe effectiveness of the proposed approach and the results underscore the\nsuperiority of this method over traditional Non-Maximum Suppression (NMS)\ntechniques, with an improvement of more than 5\\% in the car category in the\noverlapping areas.\n","authors":["Irene Cortés","Jorge Beltrán","Arturo de la Escalera","Fernando García"],"pdf_url":"https://arxiv.org/pdf/2310.05785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05773v1","updated":"2023-10-09T14:57:41Z","published":"2023-10-09T14:57:41Z","title":"Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory\n  Matching","summary":"  The ultimate goal of Dataset Distillation is to synthesize a small synthetic\ndataset such that a model trained on this synthetic set will perform equally\nwell as a model trained on the full, real dataset. Until now, no method of\nDataset Distillation has reached this completely lossless goal, in part due to\nthe fact that previous methods only remain effective when the total number of\nsynthetic samples is extremely small. Since only so much information can be\ncontained in such a small number of samples, it seems that to achieve truly\nloss dataset distillation, we must develop a distillation method that remains\neffective as the size of the synthetic dataset grows. In this work, we present\nsuch an algorithm and elucidate why existing methods fail to generate larger,\nhigh-quality synthetic sets. Current state-of-the-art methods rely on\ntrajectory-matching, or optimizing the synthetic data to induce similar\nlong-term training dynamics as the real data. We empirically find that the\ntraining stage of the trajectories we choose to match (i.e., early or late)\ngreatly affects the effectiveness of the distilled dataset. Specifically, early\ntrajectories (where the teacher network learns easy patterns) work well for a\nlow-cardinality synthetic set since there are fewer examples wherein to\ndistribute the necessary information. Conversely, late trajectories (where the\nteacher network learns hard patterns) provide better signals for larger\nsynthetic sets since there are now enough samples to represent the necessary\ncomplex patterns. Based on our findings, we propose to align the difficulty of\nthe generated patterns with the size of the synthetic dataset. In doing so, we\nsuccessfully scale trajectory matching-based methods to larger synthetic\ndatasets, achieving lossless dataset distillation for the very first time. Code\nand distilled datasets are available at https://gzyaftermath.github.io/DATM.\n","authors":["Ziyao Guo","Kai Wang","George Cazenavette","Hui Li","Kaipeng Zhang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2310.05773v1.pdf","comment":"First lossless dataset distillation method"},{"id":"http://arxiv.org/abs/2310.05768v1","updated":"2023-10-09T14:54:37Z","published":"2023-10-09T14:54:37Z","title":"DANet: Enhancing Small Object Detection through an Efficient Deformable\n  Attention Network","summary":"  Efficient and accurate detection of small objects in manufacturing settings,\nsuch as defects and cracks, is crucial for ensuring product quality and safety.\nTo address this issue, we proposed a comprehensive strategy by synergizing\nFaster R-CNN with cutting-edge methods. By combining Faster R-CNN with Feature\nPyramid Network, we enable the model to efficiently handle multi-scale features\nintrinsic to manufacturing environments. Additionally, Deformable Net is used\nthat contorts and conforms to the geometric variations of defects, bringing\nprecision in detecting even the minuscule and complex features. Then, we\nincorporated an attention mechanism called Convolutional Block Attention Module\nin each block of our base ResNet50 network to selectively emphasize informative\nfeatures and suppress less useful ones. After that we incorporated RoI Align,\nreplacing RoI Pooling for finer region-of-interest alignment and finally the\nintegration of Focal Loss effectively handles class imbalance, crucial for rare\ndefect occurrences. The rigorous evaluation of our model on both the NEU-DET\nand Pascal VOC datasets underscores its robust performance and generalization\ncapabilities. On the NEU-DET dataset, our model exhibited a profound\nunderstanding of steel defects, achieving state-of-the-art accuracy in\nidentifying various defects. Simultaneously, when evaluated on the Pascal VOC\ndataset, our model showcases its ability to detect objects across a wide\nspectrum of categories within complex and small scenes.\n","authors":["Md Sohag Mia","Abdullah Al Bary Voban","Abu Bakor Hayat Arnob","Abdu Naim","Md Kawsar Ahmed","Md Shariful Islam"],"pdf_url":"https://arxiv.org/pdf/2310.05768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05762v1","updated":"2023-10-09T14:44:01Z","published":"2023-10-09T14:44:01Z","title":"3D tomatoes' localisation with monocular cameras using histogram filters","summary":"  Performing tasks in agriculture, such as fruit monitoring or harvesting,\nrequires perceiving the objects' spatial position. RGB-D cameras are limited\nunder open-field environments due to lightning interferences. Therefore, in\nthis study, we approach the use of Histogram Filters (Bayesian Discrete\nFilters) to estimate the position of tomatoes in the tomato plant. Two kernel\nfilters were studied: the square kernel and the Gaussian kernel. The\nimplemented algorithm was essayed in simulation, with and without Gaussian\nnoise and random noise, and in a testbed at laboratory conditions. The\nalgorithm reported a mean absolute error lower than 10 mm in simulation and 20\nmm in the testbed at laboratory conditions with an assessing distance of about\n0.5 m. So, the results are viable for real environments and should be improved\nat closer distances.\n","authors":["Sandro Costa Magalhães","Filipe Neves dos Santos","António Paulo Moreira","Jorge Dias"],"pdf_url":"https://arxiv.org/pdf/2310.05762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15112v4","updated":"2023-10-09T14:41:11Z","published":"2023-09-26T17:58:20Z","title":"InternLM-XComposer: A Vision-Language Large Model for Advanced\n  Text-image Comprehension and Composition","summary":"  We propose InternLM-XComposer, a vision-language large model that enables\nadvanced image-text comprehension and composition. The innovative nature of our\nmodel is highlighted by three appealing properties: 1) Interleaved Text-Image\nComposition: InternLM-XComposer can effortlessly generate coherent and\ncontextual articles that seamlessly integrate images, providing a more engaging\nand immersive reading experience. Simply provide a title, and our system will\ngenerate the corresponding manuscript. It can intelligently identify the areas\nin the text where images would enhance the content and automatically insert the\nmost appropriate visual candidates. 2) Comprehension with Rich Multilingual\nKnowledge: The text-image comprehension is empowered by training on extensive\nmulti-modal multilingual concepts with carefully crafted strategies, resulting\nin a deep understanding of visual content. 3) State-of-the-art Performance: Our\nmodel consistently achieves state-of-the-art results across various mainstream\nbenchmarks for vision-language foundational models, including MME Benchmark,\nMMBench, MMBench-CN, Seed-Bench, and CCBench (Chinese Cultural Benchmark).\nCollectively, InternLM-XComposer seamlessly blends advanced text-image\ncomprehension and composition, revolutionizing vision-language interaction and\noffering new insights and opportunities. The InternLM-XComposer model series\nwith 7B parameters are publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.\n","authors":["Pan Zhang","Xiaoyi Dong","Bin Wang","Yuhang Cao","Chao Xu","Linke Ouyang","Zhiyuan Zhao","Shuangrui Ding","Songyang Zhang","Haodong Duan","Wenwei Zhang","Hang Yan","Xinyue Zhang","Wei Li","Jingwen Li","Kai Chen","Conghui He","Xingcheng Zhang","Yu Qiao","Dahua Lin","Jiaqi Wang"],"pdf_url":"https://arxiv.org/pdf/2309.15112v4.pdf","comment":"Code and models are available at\n  https://github.com/InternLM/InternLM-XComposer"},{"id":"http://arxiv.org/abs/2307.09052v2","updated":"2023-10-09T14:32:21Z","published":"2023-07-18T08:06:14Z","title":"Connections between Operator-splitting Methods and Deep Neural Networks\n  with Applications in Image Segmentation","summary":"  Deep neural network is a powerful tool for many tasks. Understanding why it\nis so successful and providing a mathematical explanation is an important\nproblem and has been one popular research direction in past years. In the\nliterature of mathematical analysis of deep neural networks, a lot of works is\ndedicated to establishing representation theories. How to make connections\nbetween deep neural networks and mathematical algorithms is still under\ndevelopment. In this paper, we give an algorithmic explanation for deep neural\nnetworks, especially in their connections with operator splitting. We show that\nwith certain splitting strategies, operator-splitting methods have the same\nstructure as networks. Utilizing this connection and the Potts model for image\nsegmentation, two networks inspired by operator-splitting methods are proposed.\nThe two networks are essentially two operator-splitting algorithms solving the\nPotts model. Numerical experiments are presented to demonstrate the\neffectiveness of the proposed networks.\n","authors":["Hao Liu","Xue-Cheng Tai","Raymond Chan"],"pdf_url":"https://arxiv.org/pdf/2307.09052v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05754v1","updated":"2023-10-09T14:30:10Z","published":"2023-10-09T14:30:10Z","title":"Unleashing the power of Neural Collapse for Transferability Estimation","summary":"  Transferability estimation aims to provide heuristics for quantifying how\nsuitable a pre-trained model is for a specific downstream task, without\nfine-tuning them all. Prior studies have revealed that well-trained models\nexhibit the phenomenon of Neural Collapse. Based on a widely used neural\ncollapse metric in existing literature, we observe a strong correlation between\nthe neural collapse of pre-trained models and their corresponding fine-tuned\nmodels. Inspired by this observation, we propose a novel method termed Fair\nCollapse (FaCe) for transferability estimation by comprehensively measuring the\ndegree of neural collapse in the pre-trained model. Typically, FaCe comprises\ntwo different terms: the variance collapse term, which assesses the class\nseparation and within-class compactness, and the class fairness term, which\nquantifies the fairness of the pre-trained model towards each class. We\ninvestigate FaCe on a variety of pre-trained classification models across\ndifferent network architectures, source datasets, and training loss functions.\nResults show that FaCe yields state-of-the-art performance on different tasks\nincluding image classification, semantic segmentation, and text classification,\nwhich demonstrate the effectiveness and generalization of our method.\n","authors":["Yuhe Ding","Bo Jiang","Lijun Sheng","Aihua Zheng","Jian Liang"],"pdf_url":"https://arxiv.org/pdf/2310.05754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05500v2","updated":"2023-10-09T14:17:15Z","published":"2023-01-13T12:03:58Z","title":"RCPS: Rectified Contrastive Pseudo Supervision for Semi-Supervised\n  Medical Image Segmentation","summary":"  Medical image segmentation methods are generally designed as fully-supervised\nto guarantee model performance, which require a significant amount of expert\nannotated samples that are high-cost and laborious. Semi-supervised image\nsegmentation can alleviate the problem by utilizing a large number of unlabeled\nimages along with limited labeled images. However, learning a robust\nrepresentation from numerous unlabeled images remains challenging due to\npotential noise in pseudo labels and insufficient class separability in feature\nspace, which undermines the performance of current semi-supervised segmentation\napproaches. To address the issues above, we propose a novel semi-supervised\nsegmentation method named as Rectified Contrastive Pseudo Supervision (RCPS),\nwhich combines a rectified pseudo supervision and voxel-level contrastive\nlearning to improve the effectiveness of semi-supervised segmentation.\nParticularly, we design a novel rectification strategy for the pseudo\nsupervision method based on uncertainty estimation and consistency\nregularization to reduce the noise influence in pseudo labels. Furthermore, we\nintroduce a bidirectional voxel contrastive loss to the network to ensure\nintra-class consistency and inter-class contrast in feature space, which\nincreases class separability in the segmentation. The proposed RCPS\nsegmentation method has been validated on two public datasets and an in-house\nclinical dataset. Experimental results reveal that the proposed method yields\nbetter segmentation performance compared with the state-of-the-art methods in\nsemi-supervised medical image segmentation. The source code is available at\nhttps://github.com/hsiangyuzhao/RCPS.\n","authors":["Xiangyu Zhao","Zengxin Qi","Sheng Wang","Qian Wang","Xuehai Wu","Ying Mao","Lichi Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.05500v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05737v1","updated":"2023-10-09T14:10:29Z","published":"2023-10-09T14:10:29Z","title":"Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation","summary":"  While Large Language Models (LLMs) are the dominant models for generative\ntasks in language, they do not perform as well as diffusion models on image and\nvideo generation. To effectively use LLMs for visual generation, one crucial\ncomponent is the visual tokenizer that maps pixel-space inputs to discrete\ntokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a\nvideo tokenizer designed to generate concise and expressive tokens for both\nvideos and images using a common token vocabulary. Equipped with this new\ntokenizer, we show that LLMs outperform diffusion models on standard image and\nvideo generation benchmarks including ImageNet and Kinetics. In addition, we\ndemonstrate that our tokenizer surpasses the previously top-performing video\ntokenizer on two more tasks: (1) video compression comparable to the\nnext-generation video codec (VCC) according to human evaluations, and (2)\nlearning effective representations for action recognition tasks.\n","authors":["Lijun Yu","José Lezama","Nitesh B. Gundavarapu","Luca Versari","Kihyuk Sohn","David Minnen","Yong Cheng","Agrim Gupta","Xiuye Gu","Alexander G. Hauptmann","Boqing Gong","Ming-Hsuan Yang","Irfan Essa","David A. Ross","Lu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.05737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08010v3","updated":"2023-10-09T13:56:25Z","published":"2023-03-14T15:57:54Z","title":"Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep\n  Ensembles are More Efficient than Single Models","summary":"  Deep Ensembles are a simple, reliable, and effective method of improving both\nthe predictive performance and uncertainty estimates of deep learning\napproaches. However, they are widely criticised as being computationally\nexpensive, due to the need to deploy multiple independent models. Recent work\nhas challenged this view, showing that for predictive accuracy, ensembles can\nbe more computationally efficient (at inference) than scaling single models\nwithin an architecture family. This is achieved by cascading ensemble members\nvia an early-exit approach. In this work, we investigate extending these\nefficiency gains to tasks related to uncertainty estimation. As many such\ntasks, e.g. selective classification, are binary classification, our key novel\ninsight is to only pass samples within a window close to the binary decision\nboundary to later cascade stages. Experiments on ImageNet-scale data across a\nnumber of network architectures and uncertainty tasks show that the proposed\nwindow-based early-exit approach is able to achieve a superior\nuncertainty-computation trade-off compared to scaling single models. For\nexample, a cascaded EfficientNet-B2 ensemble is able to achieve similar\ncoverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.\nWe also find that cascades/ensembles give more reliable improvements on OOD\ndata vs scaling models up. Code for this work is available at:\nhttps://github.com/Guoxoug/window-early-exit.\n","authors":["Guoxuan Xia","Christos-Savvas Bouganis"],"pdf_url":"https://arxiv.org/pdf/2303.08010v3.pdf","comment":"Accepted to ICCV 2023 (camera-ready version, 9 pages)"},{"id":"http://arxiv.org/abs/2305.08685v3","updated":"2023-10-09T13:45:46Z","published":"2023-05-15T14:42:02Z","title":"CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding","summary":"  Visual Grounding (VG) is a crucial topic in the field of vision and language,\nwhich involves locating a specific region described by expressions within an\nimage. To reduce the reliance on manually labeled data, unsupervised methods\nhave been developed to locate regions using pseudo-labels. However, the\nperformance of existing unsupervised methods is highly dependent on the quality\nof pseudo-labels and these methods always encounter issues with limited\ndiversity. In order to utilize vision and language pre-trained models to\naddress the grounding problem, and reasonably take advantage of pseudo-labels,\nwe propose CLIP-VG, a novel method that can conduct self-paced curriculum\nadapting of CLIP with pseudo-language labels. We propose a simple yet efficient\nend-to-end network architecture to realize the transfer of CLIP to the visual\ngrounding. Based on the CLIP-based architecture, we further propose\nsingle-source and multi-source curriculum adapting algorithms, which can\nprogressively find more reliable pseudo-labels to learn an optimal model,\nthereby achieving a balance between reliability and diversity for the\npseudo-language labels. Our method outperforms the current state-of-the-art\nunsupervised method by a significant margin on RefCOCO/+/g datasets in both\nsingle-source and multi-source scenarios, with improvements ranging from 6.78%\nto 10.67% and 11.39% to 14.87%, respectively. Furthermore, our approach even\noutperforms existing weakly supervised methods. The code and models are\navailable at https://github.com/linhuixiao/CLIP-VG.\n","authors":["Linhui Xiao","Xiaoshan Yang","Fang Peng","Ming Yan","Yaowei Wang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2305.08685v3.pdf","comment":"Accepted by IEEE Transaction on Multimedia (2023), Paper page:\n  https://ieeexplore.ieee.org/abstract/document/10269126. Code will be released\n  at https://github.com/linhuixiao/CLIP-VG"},{"id":"http://arxiv.org/abs/2310.05720v1","updated":"2023-10-09T13:45:21Z","published":"2023-10-09T13:45:21Z","title":"HyperLips: Hyper Control Lips with High Resolution Decoder for Talking\n  Face Generation","summary":"  Talking face generation has a wide range of potential applications in the\nfield of virtual digital humans. However, rendering high-fidelity facial video\nwhile ensuring lip synchronization is still a challenge for existing\naudio-driven talking face generation approaches. To address this issue, we\npropose HyperLips, a two-stage framework consisting of a hypernetwork for\ncontrolling lips and a high-resolution decoder for rendering high-fidelity\nfaces.In the first stage, we construct a base face generation network that uses\nthe hypernetwork to control the encoding latent code of the visual face\ninformation over audio. First, FaceEncoder is used to obtain latent code by\nextracting features from the visual face information taken from the video\nsource containing the face frame.Then, HyperConv, which weighting parameters\nare updated by HyperNet with the audio features as input, will modify the\nlatent code to synchronize the lip movement with the audio. Finally,\nFaceDecoder will decode the modified and synchronized latent code into visual\nface content. In the second stage, we obtain higher quality face videos through\na high-resolution decoder. To further improve the quality of face generation,\nwe trained a high-resolution decoder, HRDecoder, using face images and detected\nsketches generated from the first stage as input.Extensive quantitative and\nqualitative experiments show that our method outperforms state-of-the-art work\nwith more realistic, high-fidelity, and lip synchronization. Project page:\nhttps://semchan.github.io/HyperLips/\n","authors":["Yaosen Chen","Yu Yao","Zhiqiang Li","Wei Wang","Yanru Zhang","Han Yang","Xuming Wen"],"pdf_url":"https://arxiv.org/pdf/2310.05720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05718v1","updated":"2023-10-09T13:39:26Z","published":"2023-10-09T13:39:26Z","title":"EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational\n  Autoencoders","summary":"  Codebook collapse is a common problem in training deep generative models with\ndiscrete representation spaces like Vector Quantized Variational Autoencoders\n(VQ-VAEs). We observe that the same problem arises for the alternatively\ndesigned discrete variational autoencoders (dVAEs) whose encoder directly\nlearns a distribution over the codebook embeddings to represent the data. We\nhypothesize that using the softmax function to obtain a probability\ndistribution causes the codebook collapse by assigning overconfident\nprobabilities to the best matching codebook elements. In this paper, we propose\na novel way to incorporate evidential deep learning (EDL) instead of softmax to\ncombat the codebook collapse problem of dVAE. We evidentially monitor the\nsignificance of attaining the probability distribution over the codebook\nembeddings, in contrast to softmax usage. Our experiments using various\ndatasets show that our model, called EdVAE, mitigates codebook collapse while\nimproving the reconstruction performance, and enhances the codebook usage\ncompared to dVAE and VQ-VAE based models.\n","authors":["Gulcin Baykal","Melih Kandemir","Gozde Unal"],"pdf_url":"https://arxiv.org/pdf/2310.05718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05717v1","updated":"2023-10-09T13:39:06Z","published":"2023-10-09T13:39:06Z","title":"STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects\n  on Production Lines","summary":"  In this work, we present STOPNet, a framework for 6-DoF object suction\ndetection on production lines, with a focus on but not limited to transparent\nobjects, which is an important and challenging problem in robotic systems and\nmodern industry. Current methods requiring depth input fail on transparent\nobjects due to depth cameras' deficiency in sensing their geometry, while we\nproposed a novel framework to reconstruct the scene on the production line\ndepending only on RGB input, based on multiview stereo. Compared to existing\nworks, our method not only reconstructs the whole 3D scene in order to obtain\nhigh-quality 6-DoF suction poses in real time but also generalizes to novel\nenvironments, novel arrangements and novel objects, including challenging\ntransparent objects, both in simulation and the real world. Extensive\nexperiments in simulation and the real world show that our method significantly\nsurpasses the baselines and has better generalizability, which caters to\npractical industrial needs.\n","authors":["Yuxuan Kuang","Qin Han","Danshi Li","Qiyu Dai","Lian Ding","Dong Sun","Hanlin Zhao","He Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05717v1.pdf","comment":"Under Review. ICRA 2024 submission"},{"id":"http://arxiv.org/abs/2306.11891v2","updated":"2023-10-09T13:24:54Z","published":"2023-06-02T17:47:29Z","title":"Vital Videos: A dataset of face videos with PPG and blood pressure\n  ground truths","summary":"  We collected a large dataset consisting of nearly 900 unique participants.\nFor every participant we recorded two 30 second uncompressed videos,\nsynchronized PPG waveforms and a single blood pressure measurement. Gender, age\nand skin color were also registered for every participant. The dataset includes\nroughly equal numbers of males and females, as well as participants of all\nages. While the skin color distribution could have been more balanced, the\ndataset contains individuals from every skin color. The data was collected in a\ndiverse set of locations to ensure a wide variety of backgrounds and lighting\nconditions. In an effort to assist in the research and development of remote\nvital sign measurement we are now opening up access to this dataset.\n","authors":["Pieter-Jan Toye"],"pdf_url":"https://arxiv.org/pdf/2306.11891v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2310.05699v1","updated":"2023-10-09T13:20:20Z","published":"2023-10-09T13:20:20Z","title":"Uni3DETR: Unified 3D Detection Transformer","summary":"  Existing point cloud based 3D detectors are designed for the particular\nscene, either indoor or outdoor ones. Because of the substantial differences in\nobject distribution and point density within point clouds collected from\nvarious environments, coupled with the intricate nature of 3D metrics, there is\nstill a lack of a unified network architecture that can accommodate diverse\nscenes. In this paper, we propose Uni3DETR, a unified 3D detector that\naddresses indoor and outdoor 3D detection within the same framework.\nSpecifically, we employ the detection transformer with point-voxel interaction\nfor object prediction, which leverages voxel features and points for\ncross-attention and behaves resistant to the discrepancies from data. We then\npropose the mixture of query points, which sufficiently exploits global\ninformation for dense small-range indoor scenes and local information for\nlarge-range sparse outdoor ones. Furthermore, our proposed decoupled IoU\nprovides an easy-to-optimize training target for localization by disentangling\nthe xy and z space. Extensive experiments validate that Uni3DETR exhibits\nexcellent performance consistently on both indoor and outdoor 3D detection. In\ncontrast to previous specialized detectors, which may perform well on some\nparticular datasets but suffer a substantial degradation on different scenes,\nUni3DETR demonstrates the strong generalization ability under heterogeneous\nconditions (Fig. 1).\n  Codes are available at\n\\href{https://github.com/zhenyuw16/Uni3DETR}{https://github.com/zhenyuw16/Uni3DETR}.\n","authors":["Zhenyu Wang","Yali Li","Xi Chen","Hengshuang Zhao","Shengjin Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05699v1.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05697v1","updated":"2023-10-09T13:16:20Z","published":"2023-10-09T13:16:20Z","title":"Combining recurrent and residual learning for deforestation monitoring\n  using multitemporal SAR images","summary":"  With its vast expanse, exceeding that of Western Europe by twice, the Amazon\nrainforest stands as the largest forest of the Earth, holding immense\nimportance in global climate regulation. Yet, deforestation detection from\nremote sensing data in this region poses a critical challenge, often hindered\nby the persistent cloud cover that obscures optical satellite data for much of\nthe year. Addressing this need, this paper proposes three deep-learning models\ntailored for deforestation monitoring, utilizing SAR (Synthetic Aperture Radar)\nmultitemporal data moved by its independence on atmospheric conditions.\nSpecifically, the study proposes three novel recurrent fully convolutional\nnetwork architectures-namely, RRCNN-1, RRCNN-2, and RRCNN-3, crafted to enhance\nthe accuracy of deforestation detection. Additionally, this research explores\nreplacing a bitemporal with multitemporal SAR sequences, motivated by the\nhypothesis that deforestation signs quickly fade in SAR images over time. A\ncomprehensive assessment of the proposed approaches was conducted using a\nSentinel-1 multitemporal sequence from a sample site in the Brazilian\nrainforest. The experimental analysis confirmed that analyzing a sequence of\nSAR images over an observation period can reveal deforestation spots\nundetectable in a pair of images. Notably, experimental results underscored the\nsuperiority of the multitemporal approach, yielding approximately a five\npercent enhancement in F1-Score across all tested network architectures.\nParticularly the RRCNN-1 achieved the highest accuracy and also boasted half\nthe processing time of its closest counterpart.\n","authors":["Carla Nascimento Neves","Raul Queiroz Feitosa","Mabel X. Ortega Adarme","Gilson Antonio Giraldi"],"pdf_url":"https://arxiv.org/pdf/2310.05697v1.pdf","comment":"21 pages, 19 Figures"},{"id":"http://arxiv.org/abs/2205.11521v4","updated":"2023-10-09T13:09:25Z","published":"2022-05-23T17:59:58Z","title":"From Hours to Seconds: Towards 100x Faster Quantitative Phase Imaging\n  via Differentiable Microscopy","summary":"  With applications ranging from metabolomics to histopathology, quantitative\nphase microscopy (QPM) is a powerful label-free imaging modality. Despite\nsignificant advances in fast multiplexed imaging sensors and\ndeep-learning-based inverse solvers, the throughput of QPM is currently limited\nby the speed of electronic hardware. Complementarily, to improve throughput\nfurther, here we propose to acquire images in a compressed form such that more\ninformation can be transferred beyond the existing electronic hardware\nbottleneck. To this end, we present a learnable optical\ncompression-decompression framework that learns content-specific features. The\nproposed differentiable quantitative phase microscopy ($\\partial \\mu$) first\nuses learnable optical feature extractors as image compressors. The intensity\nrepresentation produced by these networks is then captured by the imaging\nsensor. Finally, a reconstruction network running on electronic hardware\ndecompresses the QPM images. In numerical experiments, the proposed system\nachieves compression of $\\times$ 64 while maintaining the SSIM of $\\sim 0.90$\nand PSNR of $\\sim 30$ dB on cells. The results demonstrated by our experiments\nopen up a new pathway for achieving end-to-end optimized (i.e., optics and\nelectronic) compact QPM systems that may provide unprecedented throughput\nimprovements.\n","authors":["Udith Haputhanthri","Kithmini Herath","Ramith Hettiarachchi","Hasindu Kariyawasam","Azeem Ahmad","Balpreet S. Ahluwalia","Chamira U. S. Edussooriya","Dushan N. Wadduwage"],"pdf_url":"https://arxiv.org/pdf/2205.11521v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05691v1","updated":"2023-10-09T13:07:23Z","published":"2023-10-09T13:07:23Z","title":"Climate-sensitive Urban Planning through Optimization of Tree Placements","summary":"  Climate change is increasing the intensity and frequency of many extreme\nweather events, including heatwaves, which results in increased thermal\ndiscomfort and mortality rates. While global mitigation action is undoubtedly\nnecessary, so is climate adaptation, e.g., through climate-sensitive urban\nplanning. Among the most promising strategies is harnessing the benefits of\nurban trees in shading and cooling pedestrian-level environments. Our work\ninvestigates the challenge of optimal placement of such trees. Physical\nsimulations can estimate the radiative and thermal impact of trees on human\nthermal comfort but induce high computational costs. This rules out\noptimization of tree placements over large areas and considering effects over\nlonger time scales. Hence, we employ neural networks to simulate the point-wise\nmean radiant temperatures--a driving factor of outdoor human thermal\ncomfort--across various time scales, spanning from daily variations to extended\ntime scales of heatwave events and even decades. To optimize tree placements,\nwe harness the innate local effect of trees within the iterated local search\nframework with tailored adaptations. We show the efficacy of our approach\nacross a wide spectrum of study areas and time scales. We believe that our\napproach is a step towards empowering decision-makers, urban designers and\nplanners to proactively and effectively assess the potential of urban trees to\nmitigate heat stress.\n","authors":["Simon Schrodi","Ferdinand Briegel","Max Argus","Andreas Christen","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2310.05691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.01587v2","updated":"2023-10-09T12:56:17Z","published":"2023-08-03T07:45:53Z","title":"Consistency Regularization for Generalizable Source-free Domain\n  Adaptation","summary":"  Source-free domain adaptation (SFDA) aims to adapt a well-trained source\nmodel to an unlabelled target domain without accessing the source dataset,\nmaking it applicable in a variety of real-world scenarios. Existing SFDA\nmethods ONLY assess their adapted models on the target training set, neglecting\nthe data from unseen but identically distributed testing sets. This oversight\nleads to overfitting issues and constrains the model's generalization ability.\nIn this paper, we propose a consistency regularization framework to develop a\nmore generalizable SFDA method, which simultaneously boosts model performance\non both target training and testing datasets. Our method leverages soft\npseudo-labels generated from weakly augmented images to supervise strongly\naugmented images, facilitating the model training process and enhancing the\ngeneralization ability of the adapted model. To leverage more potentially\nuseful supervision, we present a sampling-based pseudo-label selection\nstrategy, taking samples with severer domain shift into consideration.\nMoreover, global-oriented calibration methods are introduced to exploit global\nclass distribution and feature cluster information, further improving the\nadaptation process. Extensive experiments demonstrate our method achieves\nstate-of-the-art performance on several SFDA benchmarks, and exhibits\nrobustness on unseen testing datasets.\n","authors":["Longxiang Tang","Kai Li","Chunming He","Yulun Zhang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2308.01587v2.pdf","comment":"Accepted by ICCV 2023 workshop"},{"id":"http://arxiv.org/abs/2310.05682v1","updated":"2023-10-09T12:51:46Z","published":"2023-10-09T12:51:46Z","title":"Analysis of Rainfall Variability and Water Extent of Selected Hydropower\n  Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical\n  Countries, Sri Lanka and Vietnam","summary":"  This study presents a comprehensive remote sensing analysis of rainfall\npatterns and selected hydropower reservoir water extent in two tropical monsoon\ncountries, Vietnam and Sri Lanka. The aim is to understand the relationship\nbetween remotely sensed rainfall data and the dynamic changes (monthly) in\nreservoir water extent. The analysis utilizes high-resolution optical imagery\nand Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water\nbodies during different weather conditions, especially during the monsoon\nseason. The average annual rainfall for both countries is determined, and\nspatiotemporal variations in monthly average rainfall are examined at regional\nand reservoir basin levels using the Climate Hazards Group InfraRed\nPrecipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents\nare derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected\n(GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are\npre-processed and corrected using terrain correction and refined Lee filter. An\nautomated thresholding algorithm, OTSU, distinguishes water and land, taking\nadvantage of both VV and VH polarization data. The connected pixel count\nthreshold is applied to enhance result accuracy. The results indicate a clear\nrelationship between rainfall patterns and reservoir water extent, with\nincreased precipitation during the monsoon season leading to higher water\nextents in the later months. This study contributes to understanding how\nrainfall variability impacts reservoir water resources in tropical monsoon\nregions. The preliminary findings can inform water resource management\nstrategies and support these countries' decision-making processes related to\nhydropower generation, flood management, and irrigation.\n","authors":["Punsisi Rajakaruna","Surajit Ghosh","Bunyod Holmatov"],"pdf_url":"https://arxiv.org/pdf/2310.05682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05666v1","updated":"2023-10-09T12:35:05Z","published":"2023-10-09T12:35:05Z","title":"Anchor-Intermediate Detector: Decoupling and Coupling Bounding Boxes for\n  Accurate Object Detection","summary":"  Anchor-based detectors have been continuously developed for object detection.\nHowever, the individual anchor box makes it difficult to predict the boundary's\noffset accurately. Instead of taking each bounding box as a closed individual,\nwe consider using multiple boxes together to get prediction boxes. To this end,\nthis paper proposes the \\textbf{Box Decouple-Couple(BDC) strategy} in the\ninference, which no longer discards the overlapping boxes, but decouples the\ncorner points of these boxes. Then, according to each corner's score, we couple\nthe corner points to select the most accurate corner pairs. To meet the BDC\nstrategy, a simple but novel model is designed named the\n\\textbf{Anchor-Intermediate Detector(AID)}, which contains two head networks,\ni.e., an anchor-based head and an anchor-free \\textbf{Corner-aware head}. The\ncorner-aware head is able to score the corners of each bounding box to\nfacilitate the coupling between corner points. Extensive experiments on MS COCO\nshow that the proposed anchor-intermediate detector respectively outperforms\ntheir baseline RetinaNet and GFL method by $\\sim$2.4 and $\\sim$1.2 AP on the MS\nCOCO test-dev dataset without any bells and whistles. Code is available at:\nhttps://github.com/YilongLv/AID.\n","authors":["Yilong Lv","Min Li","Yujie He","Shaopeng Li","Zhuzhen He","Aitao Yang"],"pdf_url":"https://arxiv.org/pdf/2310.05666v1.pdf","comment":"Submitted 29 September, 2023; originally announced October 2023.\n  Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2206.09900v7","updated":"2023-10-09T12:34:02Z","published":"2022-06-20T17:15:50Z","title":"Occupancy-MAE: Self-supervised Pre-training Large-scale LiDAR Point\n  Clouds with Masked Occupancy Autoencoders","summary":"  Current perception models in autonomous driving heavily rely on large-scale\nlabelled 3D data, which is both costly and time-consuming to annotate. This\nwork proposes a solution to reduce the dependence on labelled 3D training data\nby leveraging pre-training on large-scale unlabeled outdoor LiDAR point clouds\nusing masked autoencoders (MAE). While existing masked point autoencoding\nmethods mainly focus on small-scale indoor point clouds or pillar-based\nlarge-scale outdoor LiDAR data, our approach introduces a new self-supervised\nmasked occupancy pre-training method called Occupancy-MAE, specifically\ndesigned for voxel-based large-scale outdoor LiDAR point clouds. Occupancy-MAE\ntakes advantage of the gradually sparse voxel occupancy structure of outdoor\nLiDAR point clouds and incorporates a range-aware random masking strategy and a\npretext task of occupancy prediction. By randomly masking voxels based on their\ndistance to the LiDAR and predicting the masked occupancy structure of the\nentire 3D surrounding scene, Occupancy-MAE encourages the extraction of\nhigh-level semantic information to reconstruct the masked voxel using only a\nsmall number of visible voxels. Extensive experiments demonstrate the\neffectiveness of Occupancy-MAE across several downstream tasks. For 3D object\ndetection, Occupancy-MAE reduces the labelled data required for car detection\non the KITTI dataset by half and improves small object detection by\napproximately 2% in AP on the Waymo dataset. For 3D semantic segmentation,\nOccupancy-MAE outperforms training from scratch by around 2% in mIoU. For\nmulti-object tracking, Occupancy-MAE enhances training from scratch by\napproximately 1% in terms of AMOTA and AMOTP. Codes are publicly available at\nhttps://github.com/chaytonmin/Occupancy-MAE.\n","authors":["Chen Min","Xinli Xu","Dawei Zhao","Liang Xiao","Yiming Nie","Bin Dai"],"pdf_url":"https://arxiv.org/pdf/2206.09900v7.pdf","comment":"Accepted by TIV"},{"id":"http://arxiv.org/abs/2310.05664v1","updated":"2023-10-09T12:31:30Z","published":"2023-10-09T12:31:30Z","title":"ViTs are Everywhere: A Comprehensive Study Showcasing Vision\n  Transformers in Different Domain","summary":"  Transformer design is the de facto standard for natural language processing\ntasks. The success of the transformer design in natural language processing has\nlately piqued the interest of researchers in the domain of computer vision.\nWhen compared to Convolutional Neural Networks (CNNs), Vision Transformers\n(ViTs) are becoming more popular and dominant solutions for many vision\nproblems. Transformer-based models outperform other types of networks, such as\nconvolutional and recurrent neural networks, in a range of visual benchmarks.\nWe evaluate various vision transformer models in this work by dividing them\ninto distinct jobs and examining their benefits and drawbacks. ViTs can\novercome several possible difficulties with convolutional neural networks\n(CNNs). The goal of this survey is to show the first use of ViTs in CV. In the\nfirst phase, we categorize various CV applications where ViTs are appropriate.\nImage classification, object identification, image segmentation, video\ntransformer, image denoising, and NAS are all CV applications. Our next step\nwill be to analyze the state-of-the-art in each area and identify the models\nthat are currently available. In addition, we outline numerous open research\ndifficulties as well as prospective research possibilities.\n","authors":["Md Sohag Mia","Abu Bakor Hayat Arnob","Abdu Naim+","Abdullah Al Bary Voban","Md Shariful Islam"],"pdf_url":"https://arxiv.org/pdf/2310.05664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05654v1","updated":"2023-10-09T12:10:41Z","published":"2023-10-09T12:10:41Z","title":"No Token Left Behind: Efficient Vision Transformer via Dynamic Token\n  Idling","summary":"  Vision Transformers (ViTs) have demonstrated outstanding performance in\ncomputer vision tasks, yet their high computational complexity prevents their\ndeployment in computing resource-constrained environments. Various token\npruning techniques have been introduced to alleviate the high computational\nburden of ViTs by dynamically dropping image tokens. However, some undesirable\npruning at early stages may result in permanent loss of image information in\nsubsequent layers, consequently hindering model performance. To address this\nproblem, we propose IdleViT, a dynamic token-idle-based method that achieves an\nexcellent trade-off between performance and efficiency. Specifically, in each\nlayer, IdleViT selects a subset of the image tokens to participate in\ncomputations while keeping the rest of the tokens idle and directly passing\nthem to this layer's output. By allowing the idle tokens to be re-selected in\nthe following layers, IdleViT mitigates the negative impact of improper pruning\nin the early stages. Furthermore, inspired by the normalized graph cut, we\ndevise a token cut loss on the attention map as regularization to improve\nIdleViT's token selection ability. Our method is simple yet effective and can\nbe extended to pyramid ViTs since no token is completely dropped. Extensive\nexperimental results on various ViT architectures have shown that IdleViT can\ndiminish the complexity of pretrained ViTs by up to 33\\% with no more than\n0.2\\% accuracy decrease on ImageNet, after finetuning for only 30 epochs.\nNotably, when the keep ratio is 0.5, IdleViT outperforms the state-of-the-art\nEViT on DeiT-S by 0.5\\% higher accuracy and even faster inference speed. The\nsource code is available in the supplementary material.\n","authors":["Xuwei Xu","Changlin Li","Yudong Chen","Xiaojun Chang","Jiajun Liu","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05654v1.pdf","comment":"Accepted to AJCAI2023"},{"id":"http://arxiv.org/abs/2305.18829v3","updated":"2023-10-09T11:59:31Z","published":"2023-05-30T08:23:06Z","title":"UniScene: Multi-Camera Unified Pre-training via 3D Scene Reconstruction","summary":"  Multi-camera 3D perception has emerged as a prominent research field in\nautonomous driving, offering a viable and cost-effective alternative to\nLiDAR-based solutions. The existing multi-camera algorithms primarily rely on\nmonocular 2D pre-training. However, the monocular 2D pre-training overlooks the\nspatial and temporal correlations among the multi-camera system. To address\nthis limitation, we propose the first multi-camera unified pre-training\nframework, called UniScene, which involves initially reconstructing the 3D\nscene as the foundational stage and subsequently fine-tuning the model on\ndownstream tasks. Specifically, we employ Occupancy as the general\nrepresentation for the 3D scene, enabling the model to grasp geometric priors\nof the surrounding world through pre-training. A significant benefit of\nUniScene is its capability to utilize a considerable volume of unlabeled\nimage-LiDAR pairs for pre-training purposes. The proposed multi-camera unified\npre-training framework demonstrates promising results in key tasks such as\nmulti-camera 3D object detection and surrounding semantic scene completion.\nWhen compared to monocular pre-training methods on the nuScenes dataset,\nUniScene shows a significant improvement of about 2.0% in mAP and 2.0% in NDS\nfor multi-camera 3D object detection, as well as a 3% increase in mIoU for\nsurrounding semantic scene completion. By adopting our unified pre-training\nmethod, a 25% reduction in 3D training annotation costs can be achieved,\noffering significant practical value for the implementation of real-world\nautonomous driving. Codes are publicly available at\nhttps://github.com/chaytonmin/UniScene.\n","authors":["Chen Min","Liang Xiao","Dawei Zhao","Yiming Nie","Bin Dai"],"pdf_url":"https://arxiv.org/pdf/2305.18829v3.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.05647v1","updated":"2023-10-09T11:59:11Z","published":"2023-10-09T11:59:11Z","title":"Exploiting Manifold Structured Data Priors for Improved MR\n  Fingerprinting Reconstruction","summary":"  Estimating tissue parameter maps with high accuracy and precision from highly\nundersampled measurements presents one of the major challenges in MR\nfingerprinting (MRF). Many existing works project the recovered voxel\nfingerprints onto the Bloch manifold to improve reconstruction performance.\nHowever, little research focuses on exploiting the latent manifold structure\npriors among fingerprints. To fill this gap, we propose a novel MRF\nreconstruction framework based on manifold structured data priors. Since it is\ndifficult to directly estimate the fingerprint manifold structure, we model the\ntissue parameters as points on a low-dimensional parameter manifold. We reveal\nthat the fingerprint manifold shares the same intrinsic topology as the\nparameter manifold, although being embedded in different Euclidean spaces. To\nexploit the non-linear and non-local redundancies in MRF data, we divide the\nMRF data into spatial patches, and the similarity measurement among data\npatches can be accurately obtained using the Euclidean distance between the\ncorresponding patches in the parameter manifold. The measured similarity is\nthen used to construct the graph Laplacian operator, which represents the\nfingerprint manifold structure. Thus, the fingerprint manifold structure is\nintroduced in the reconstruction framework by using the low-dimensional\nparameter manifold. Additionally, we incorporate the locally low-rank prior in\nthe reconstruction framework to further utilize the local correlations within\neach patch for improved reconstruction performance. We also adopt a\nGPU-accelerated NUFFT library to accelerate reconstruction in non-Cartesian\nsampling scenarios. Experimental results demonstrate that our method can\nachieve significantly improved reconstruction performance with reduced\ncomputational time over the state-of-the-art methods.\n","authors":["Peng Li","Yuping Ji","Yue Hu"],"pdf_url":"https://arxiv.org/pdf/2310.05647v1.pdf","comment":"10 pages, 10 figures, will submit to IEEE Transactions on Medical\n  Imaging"},{"id":"http://arxiv.org/abs/2310.05644v1","updated":"2023-10-09T11:57:46Z","published":"2023-10-09T11:57:46Z","title":"Diagnosing Catastrophe: Large parts of accuracy loss in continual\n  learning can be accounted for by readout misalignment","summary":"  Unlike primates, training artificial neural networks on changing data\ndistributions leads to a rapid decrease in performance on old tasks. This\nphenomenon is commonly referred to as catastrophic forgetting. In this paper,\nwe investigate the representational changes that underlie this performance\ndecrease and identify three distinct processes that together account for the\nphenomenon. The largest component is a misalignment between hidden\nrepresentations and readout layers. Misalignment occurs due to learning on\nadditional tasks and causes internal representations to shift. Representational\ngeometry is partially conserved under this misalignment and only a small part\nof the information is irrecoverably lost. All types of representational changes\nscale with the dimensionality of hidden representations. These insights have\nimplications for deep learning applications that need to be continuously\nupdated, but may also aid aligning ANN models to the rather robust biological\nvision.\n","authors":["Daniel Anthes","Sushrut Thorat","Peter König","Tim C. Kietzmann"],"pdf_url":"https://arxiv.org/pdf/2310.05644v1.pdf","comment":"3 pages, 1 figure; published at the 2023 Conference on Cognitive\n  Computational Neuroscience"},{"id":"http://arxiv.org/abs/2310.05642v1","updated":"2023-10-09T11:56:35Z","published":"2023-10-09T11:56:35Z","title":"Plug n' Play: Channel Shuffle Module for Enhancing Tiny Vision\n  Transformers","summary":"  Vision Transformers (ViTs) have demonstrated remarkable performance in\nvarious computer vision tasks. However, the high computational complexity\nhinders ViTs' applicability on devices with limited memory and computing\nresources. Although certain investigations have delved into the fusion of\nconvolutional layers with self-attention mechanisms to enhance the efficiency\nof ViTs, there remains a knowledge gap in constructing tiny yet effective ViTs\nsolely based on the self-attention mechanism. Furthermore, the straightforward\nstrategy of reducing the feature channels in a large but outperforming ViT\noften results in significant performance degradation despite improved\nefficiency. To address these challenges, we propose a novel channel shuffle\nmodule to improve tiny-size ViTs, showing the potential of pure self-attention\nmodels in environments with constrained computing resources. Inspired by the\nchannel shuffle design in ShuffleNetV2 \\cite{ma2018shufflenet}, our module\nexpands the feature channels of a tiny ViT and partitions the channels into two\ngroups: the \\textit{Attended} and \\textit{Idle} groups. Self-attention\ncomputations are exclusively employed on the designated \\textit{Attended}\ngroup, followed by a channel shuffle operation that facilitates information\nexchange between the two groups. By incorporating our module into a tiny ViT,\nwe can achieve superior performance while maintaining a comparable\ncomputational complexity to the vanilla model. Specifically, our proposed\nchannel shuffle module consistently improves the top-1 accuracy on the\nImageNet-1K dataset for various tiny ViT models by up to 2.8\\%, with the\nchanges in model complexity being less than 0.03 GMACs.\n","authors":["Xuwei Xu","Sen Wang","Yudong Chen","Jiajun Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02722v3","updated":"2023-10-09T11:52:48Z","published":"2023-05-04T10:43:11Z","title":"Avatar Knowledge Distillation: Self-ensemble Teacher Paradigm with\n  Uncertainty","summary":"  Knowledge distillation is an effective paradigm for boosting the performance\nof pocket-size model, especially when multiple teacher models are available,\nthe student would break the upper limit again. However, it is not economical to\ntrain diverse teacher models for the disposable distillation. In this paper, we\nintroduce a new concept dubbed Avatars for distillation, which are the\ninference ensemble models derived from the teacher. Concretely, (1) For each\niteration of distillation training, various Avatars are generated by a\nperturbation transformation. We validate that Avatars own higher upper limit of\nworking capacity and teaching ability, aiding the student model in learning\ndiverse and receptive knowledge perspectives from the teacher model. (2) During\nthe distillation, we propose an uncertainty-aware factor from the variance of\nstatistical differences between the vanilla teacher and Avatars, to adjust\nAvatars' contribution on knowledge transfer adaptively. Avatar Knowledge\nDistillation AKD is fundamentally different from existing methods and refines\nwith the innovative view of unequal training. Comprehensive experiments\ndemonstrate the effectiveness of our Avatars mechanism, which polishes up the\nstate-of-the-art distillation methods for dense prediction without more extra\ncomputational cost. The AKD brings at most 0.7 AP gains on COCO 2017 for Object\nDetection and 1.83 mIoU gains on Cityscapes for Semantic Segmentation,\nrespectively.\n","authors":["Yuan Zhang","Weihua Chen","Yichen Lu","Tao Huang","Xiuyu Sun","Jian Cao"],"pdf_url":"https://arxiv.org/pdf/2305.02722v3.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.05638v1","updated":"2023-10-09T11:50:52Z","published":"2023-10-09T11:50:52Z","title":"High Accuracy and Cost-Saving Active Learning 3D WD-UNet for Airway\n  Segmentation","summary":"  We propose a novel Deep Active Learning (DeepAL) model-3D Wasserstein\nDiscriminative UNet (WD-UNet) for reducing the annotation effort of medical 3D\nComputed Tomography (CT) segmentation. The proposed WD-UNet learns in a\nsemi-supervised way and accelerates learning convergence to meet or exceed the\nprediction metrics of supervised learning models. Our method can be embedded\nwith different Active Learning (AL) strategies and different network\nstructures. The model is evaluated on 3D lung airway CT scans for medical\nsegmentation and show that the use of uncertainty metric, which is parametrized\nas an input of query strategy, leads to more accurate prediction results than\nsome state-of-the-art Deep Learning (DL) supervised models, e.g.,3DUNet and 3D\nCEUNet. Compared to the above supervised DL methods, our WD-UNet not only saves\nthe cost of annotation for radiologists but also saves computational resources.\nWD-UNet uses a limited amount of annotated data (35% of the total) to achieve\nbetter predictive metrics with a more efficient deep learning model algorithm.\n","authors":["Shiyi Wang","Yang Nan","Simon Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2310.05638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05624v1","updated":"2023-10-09T11:26:58Z","published":"2023-10-09T11:26:58Z","title":"Locality-Aware Generalizable Implicit Neural Representation}","summary":"  Generalizable implicit neural representation (INR) enables a single\ncontinuous function, i.e., a coordinate-based neural network, to represent\nmultiple data instances by modulating its weights or intermediate features\nusing latent codes. However, the expressive power of the state-of-the-art\nmodulation is limited due to its inability to localize and capture fine-grained\ndetails of data entities such as specific pixels and rays. To address this\nissue, we propose a novel framework for generalizable INR that combines a\ntransformer encoder with a locality-aware INR decoder. The transformer encoder\npredicts a set of latent tokens from a data instance to encode local\ninformation into each latent token. The locality-aware INR decoder extracts a\nmodulation vector by selectively aggregating the latent tokens via\ncross-attention for a coordinate input and then predicts the output by\nprogressively decoding with coarse-to-fine modulation through multiple\nfrequency bandwidths. The selective token aggregation and the multi-band\nfeature modulation enable us to learn locality-aware representation in spatial\nand spectral aspects, respectively. Our framework significantly outperforms\nprevious generalizable INRs and validates the usefulness of the locality-aware\nlatents for downstream tasks such as image generation.\n","authors":["Doyup Lee","Chiheon Kim","Minsu Cho","Wook-Shin Han"],"pdf_url":"https://arxiv.org/pdf/2310.05624v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.05618v1","updated":"2023-10-09T11:18:22Z","published":"2023-10-09T11:18:22Z","title":"ASM: Adaptive Sample Mining for In-The-Wild Facial Expression\n  Recognition","summary":"  Given the similarity between facial expression categories, the presence of\ncompound facial expressions, and the subjectivity of annotators, facial\nexpression recognition (FER) datasets often suffer from ambiguity and noisy\nlabels. Ambiguous expressions are challenging to differentiate from expressions\nwith noisy labels, which hurt the robustness of FER models. Furthermore, the\ndifficulty of recognition varies across different expression categories,\nrendering a uniform approach unfair for all expressions. In this paper, we\nintroduce a novel approach called Adaptive Sample Mining (ASM) to dynamically\naddress ambiguity and noise within each expression category. First, the\nAdaptive Threshold Learning module generates two thresholds, namely the clean\nand noisy thresholds, for each category. These thresholds are based on the mean\nclass probabilities at each training epoch. Next, the Sample Mining module\npartitions the dataset into three subsets: clean, ambiguity, and noise, by\ncomparing the sample confidence with the clean and noisy thresholds. Finally,\nthe Tri-Regularization module employs a mutual learning strategy for the\nambiguity subset to enhance discrimination ability, and an unsupervised\nlearning strategy for the noise subset to mitigate the impact of noisy labels.\nExtensive experiments prove that our method can effectively mine both ambiguity\nand noise, and outperform SOTA methods on both synthetic noisy and original\ndatasets. The supplement material is available at\nhttps://github.com/zzzzzzyang/ASM.\n","authors":["Ziyang Zhang","Xiao Sun","Liuwei An","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.05618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05615v1","updated":"2023-10-09T11:08:34Z","published":"2023-10-09T11:08:34Z","title":"Adaptive Multi-head Contrastive Learning","summary":"  In contrastive learning, two views of an original image generated by\ndifferent augmentations are considered as a positive pair whose similarity is\nrequired to be high. Moreover, two views of two different images are considered\nas a negative pair, and their similarity is encouraged to be low. Normally, a\nsingle similarity measure given by a single projection head is used to evaluate\npositive and negative sample pairs, respectively. However, due to the various\naugmentation strategies and varying intra-sample similarity, augmented views\nfrom the same image are often not similar. Moreover, due to inter-sample\nsimilarity, augmented views of two different images may be more similar than\naugmented views from the same image. As such, enforcing a high similarity for\npositive pairs and a low similarity for negative pairs may not always be\nachievable, and in the case of some pairs, forcing so may be detrimental to the\nperformance. To address this issue, we propose to use multiple projection\nheads, each producing a separate set of features. Our loss function for\npre-training emerges from a solution to the maximum likelihood estimation over\nhead-wise posterior distributions of positive samples given observations. The\nloss contains the similarity measure over positive and negative pairs, each\nre-weighted by an individual adaptive temperature that is regularized to\nprevent ill solutions. Our adaptive multi-head contrastive learning (AMCL) can\nbe applied to and experimentally improves several popular contrastive learning\nmethods such as SimCLR, MoCo and Barlow Twins. Such improvement is consistent\nunder various backbones and linear probing epoches and is more significant when\nmultiple augmentation methods are used.\n","authors":["Lei Wang","Piotr Koniusz","Tom Gedeon","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.05615v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2310.05600v1","updated":"2023-10-09T10:35:37Z","published":"2023-10-09T10:35:37Z","title":"Care3D: An Active 3D Object Detection Dataset of Real Robotic-Care\n  Environments","summary":"  As labor shortage increases in the health sector, the demand for assistive\nrobotics grows. However, the needed test data to develop those robots is\nscarce, especially for the application of active 3D object detection, where no\nreal data exists at all. This short paper counters this by introducing such an\nannotated dataset of real environments. The captured environments represent\nareas which are already in use in the field of robotic health care research. We\nfurther provide ground truth data within one room, for assessing SLAM\nalgorithms running directly on a health care robot.\n","authors":["Michael G. Adam","Sebastian Eger","Martin Piccolrovazzi","Maged Iskandar","Joern Vogel","Alexander Dietrich","Seongjien Bien","Jon Skerlj","Abdeldjallil Naceri","Eckehard Steinbach","Alin Albu-Schaeffer","Sami Haddadin","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2310.05600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10764v4","updated":"2023-10-09T10:35:01Z","published":"2023-02-14T13:41:57Z","title":"On The Coherence of Quantitative Evaluation of Visual Explanations","summary":"  Recent years have shown an increased development of methods for justifying\nthe predictions of neural networks through visual explanations. These\nexplanations usually take the form of heatmaps which assign a saliency (or\nrelevance) value to each pixel of the input image that expresses how relevant\nthe pixel is for the prediction of a label.\n  Complementing this development, evaluation methods have been proposed to\nassess the \"goodness\" of such explanations. On the one hand, some of these\nmethods rely on synthetic datasets. However, this introduces the weakness of\nhaving limited guarantees regarding their applicability on more realistic\nsettings. On the other hand, some methods rely on metrics for objective\nevaluation. However the level to which some of these evaluation methods perform\nwith respect to each other is uncertain.\n  Taking this into account, we conduct a comprehensive study on a subset of the\nImageNet-1k validation set where we evaluate a number of different\ncommonly-used explanation methods following a set of evaluation methods. We\ncomplement our study with sanity checks on the studied evaluation methods as a\nmeans to investigate their reliability and the impact of characteristics of the\nexplanations on the evaluation methods.\n  Results of our study suggest that there is a lack of coherency on the grading\nprovided by some of the considered evaluation methods. Moreover, we have\nidentified some characteristics of the explanations, e.g. sparsity, which can\nhave a significant effect on the performance.\n","authors":["Benjamin Vandersmissen","Jose Oramas"],"pdf_url":"https://arxiv.org/pdf/2302.10764v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05590v1","updated":"2023-10-09T10:22:08Z","published":"2023-10-09T10:22:08Z","title":"Perceptual Artifacts Localization for Image Synthesis Tasks","summary":"  Recent advancements in deep generative models have facilitated the creation\nof photo-realistic images across various tasks. However, these generated images\noften exhibit perceptual artifacts in specific regions, necessitating manual\ncorrection. In this study, we present a comprehensive empirical examination of\nPerceptual Artifacts Localization (PAL) spanning diverse image synthesis\nendeavors. We introduce a novel dataset comprising 10,168 generated images,\neach annotated with per-pixel perceptual artifact labels across ten synthesis\ntasks. A segmentation model, trained on our proposed dataset, effectively\nlocalizes artifacts across a range of tasks. Additionally, we illustrate its\nproficiency in adapting to previously unseen models using minimal training\nsamples. We further propose an innovative zoom-in inpainting pipeline that\nseamlessly rectifies perceptual artifacts in the generated images. Through our\nexperimental analyses, we elucidate several practical downstream applications,\nsuch as automated artifact rectification, non-referential image quality\nevaluation, and abnormal region detection in images. The dataset and code are\nreleased.\n","authors":["Lingzhi Zhang","Zhengjie Xu","Connelly Barnes","Yuqian Zhou","Qing Liu","He Zhang","Sohrab Amirghodsi","Zhe Lin","Eli Shechtman","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2310.05590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07688v2","updated":"2023-10-09T10:08:00Z","published":"2023-08-15T10:37:13Z","title":"Enhancing Network Initialization for Medical AI Models Using\n  Large-Scale, Unlabeled Natural Images","summary":"  Pre-training datasets, like ImageNet, have become the gold standard in\nmedical image analysis. However, the emergence of self-supervised learning\n(SSL), which leverages unlabeled data to learn robust features, presents an\nopportunity to bypass the intensive labeling process. In this study, we\nexplored if SSL for pre-training on non-medical images can be applied to chest\nradiographs and how it compares to supervised pre-training on non-medical\nimages and on medical images. We utilized a vision transformer and initialized\nits weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL\npre-training on natural images (ImageNet dataset), and (iii) SL pre-training on\nchest radiographs from the MIMIC-CXR database. We tested our approach on over\n800,000 chest radiographs from six large global datasets, diagnosing more than\n20 different imaging findings. Our SSL pre-training on curated images not only\noutperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in\ncertain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest\nthat selecting the right pre-training strategy, especially with SSL, can be\npivotal for improving artificial intelligence (AI)'s diagnostic accuracy in\nmedical imaging. By demonstrating the promise of SSL in chest radiograph\nanalysis, we underline a transformative shift towards more efficient and\naccurate AI models in medical imaging.\n","authors":["Soroosh Tayebi Arasteh","Leo Misera","Jakob Nikolas Kather","Daniel Truhn","Sven Nebelung"],"pdf_url":"https://arxiv.org/pdf/2308.07688v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05572v1","updated":"2023-10-09T09:51:44Z","published":"2023-10-09T09:51:44Z","title":"A Simple and Robust Framework for Cross-Modality Medical Image\n  Segmentation applied to Vision Transformers","summary":"  When it comes to clinical images, automatic segmentation has a wide variety\nof applications and a considerable diversity of input domains, such as\ndifferent types of Magnetic Resonance Images (MRIs) and Computerized Tomography\n(CT) scans. This heterogeneity is a challenge for cross-modality algorithms\nthat should equally perform independently of the input image type fed to them.\nOften, segmentation models are trained using a single modality, preventing\ngeneralization to other types of input data without resorting to transfer\nlearning techniques. Furthermore, the multi-modal or cross-modality\narchitectures proposed in the literature frequently require registered images,\nwhich are not easy to collect in clinical environments, or need additional\nprocessing steps, such as synthetic image generation. In this work, we propose\na simple framework to achieve fair image segmentation of multiple modalities\nusing a single conditional model that adapts its normalization layers based on\nthe input type, trained with non-registered interleaved mixed data. We show\nthat our framework outperforms other cross-modality segmentation methods, when\napplied to the same 3D UNet baseline model, on the Multi-Modality Whole Heart\nSegmentation Challenge. Furthermore, we define the Conditional Vision\nTransformer (C-ViT) encoder, based on the proposed cross-modality framework,\nand we show that it brings significant improvements to the resulting\nsegmentation, up to 6.87\\% of Dice accuracy, with respect to its baseline\nreference. The code to reproduce our experiments and the trained model weights\nare available at https://github.com/matteo-bastico/MI-Seg.\n","authors":["Matteo Bastico","David Ryckelynck","Laurent Corté","Yannick Tillier","Etienne Decencière"],"pdf_url":"https://arxiv.org/pdf/2310.05572v1.pdf","comment":"This paper has been accepted in International Conference on Computer\n  Vision Workshops (ICCVW) 2023"},{"id":"http://arxiv.org/abs/2310.05556v1","updated":"2023-10-09T09:26:27Z","published":"2023-10-09T09:26:27Z","title":"WeatherDepth: Curriculum Contrastive Learning for Self-Supervised Depth\n  Estimation under Adverse Weather Conditions","summary":"  Depth estimation models have shown promising performance on clear scenes but\nfail to generalize to adverse weather conditions due to illumination\nvariations, weather particles, etc. In this paper, we propose WeatherDepth, a\nself-supervised robust depth estimation model with curriculum contrastive\nlearning, to tackle performance degradation in complex weather conditions.\nConcretely, we first present a progressive curriculum learning scheme with\nthree simple-to-complex curricula to gradually adapt the model from clear to\nrelative adverse, and then to adverse weather scenes. It encourages the model\nto gradually grasp beneficial depth cues against the weather effect, yielding\nsmoother and better domain adaption. Meanwhile, to prevent the model from\nforgetting previous curricula, we integrate contrastive learning into different\ncurricula. Drawn the reference knowledge from the previous course, our strategy\nestablishes a depth consistency constraint between different courses towards\nrobust depth estimation in diverse weather. Besides, to reduce manual\nintervention and better adapt to different models, we designed an adaptive\ncurriculum scheduler to automatically search for the best timing for course\nswitching. In the experiment, the proposed solution is proven to be easily\nincorporated into various architectures and demonstrates state-of-the-art\n(SoTA) performance on both synthetic and real weather datasets.\n","authors":["Jiyuan Wang","Chunyu Lin","Lang Nie","Shujun Huang","Yao Zhao","Xing Pan","Rui Ai"],"pdf_url":"https://arxiv.org/pdf/2310.05556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08653v2","updated":"2023-10-09T09:03:12Z","published":"2022-12-16T18:59:12Z","title":"Attentive Mask CLIP","summary":"  Image token removal is an efficient augmentation strategy for reducing the\ncost of computing image features. However, this efficient augmentation strategy\nhas been found to adversely affect the accuracy of CLIP-based training. We\nhypothesize that removing a large portion of image tokens may improperly\ndiscard the semantic content associated with a given text description, thus\nconstituting an incorrect pairing target in CLIP training. To address this\nissue, we propose an attentive token removal approach for CLIP training, which\nretains tokens with a high semantic correlation to the text description. The\ncorrelation scores are computed in an online fashion using the EMA version of\nthe visual encoder. Our experiments show that the proposed attentive masking\napproach performs better than the previous method of random token removal for\nCLIP training. The approach also makes it efficient to apply multiple\naugmentation views to the image, as well as introducing instance contrastive\nlearning tasks between these views into the CLIP framework. Compared to other\nCLIP improvements that combine different pre-training targets such as SLIP and\nMaskCLIP, our method is not only more effective, but also much more efficient.\nSpecifically, using ViT-B and YFCC-15M dataset, our approach achieves $43.9\\%$\ntop-1 accuracy on ImageNet-1K zero-shot classification, as well as $62.7/42.1$\nand $38.0/23.2$ I2T/T2I retrieval accuracy on Flickr30K and MS COCO, which are\n$+1.1\\%$, $+5.5/+0.9$, and $+4.4/+1.3$ higher than the SLIP method, while being\n$2.30\\times$ faster. An efficient version of our approach running $1.16\\times$\nfaster than the plain CLIP model achieves significant gains of $+5.3\\%$,\n$+11.3/+8.0$, and $+9.5/+4.9$ on these benchmarks.\n","authors":["Yifan Yang","Weiquan Huang","Yixuan Wei","Houwen Peng","Xinyang Jiang","Huiqiang Jiang","Fangyun Wei","Yin Wang","Han Hu","Lili Qiu","Yuqing Yang"],"pdf_url":"https://arxiv.org/pdf/2212.08653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05538v1","updated":"2023-10-09T09:01:53Z","published":"2023-10-09T09:01:53Z","title":"M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion\n  for Polyp Localization in Colonoscopy Images","summary":"  Polyp segmentation is crucial for preventing colorectal cancer a common type\nof cancer. Deep learning has been used to segment polyps automatically, which\nreduces the risk of misdiagnosis. Localizing small polyps in colonoscopy images\nis challenging because of its complex characteristics, such as color,\nocclusion, and various shapes of polyps. To address this challenge, a novel\nfrequency-based fully convolutional neural network, Multi-Frequency Feature\nFusion Polyp Segmentation Network (M3FPolypSegNet) was proposed to decompose\nthe input image into low/high/full-frequency components to use the\ncharacteristics of each component. We used three independent multi-frequency\nencoders to map multiple input images into a high-dimensional feature space. In\nthe Frequency-ASPP Scalable Attention Module (F-ASPP SAM), ASPP was applied\nbetween each frequency component to preserve scale information. Subsequently,\nscalable attention was applied to emphasize polyp regions in a high-dimensional\nfeature space. Finally, we designed three multi-task learning (i.e., region,\nedge, and distance) in four decoder blocks to learn the structural\ncharacteristics of the region. The proposed model outperformed various\nsegmentation models with performance gains of 6.92% and 7.52% on average for\nall metrics on CVC-ClinicDB and BKAI-IGH-NeoPolyp, respectively.\n","authors":["Ju-Hyeon Nam","Seo-Hyeong Park","Nur Suriza Syazwany","Yerim Jung","Yu-Han Im","Sang-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2310.05538v1.pdf","comment":"5pages. 2023 IEEE International Conference on Image Processing\n  (ICIP). IEEE, 2023"},{"id":"http://arxiv.org/abs/2305.14951v2","updated":"2023-10-09T08:57:38Z","published":"2023-05-24T09:42:08Z","title":"DSFFNet: Dual-Side Feature Fusion Network for 3D Pose Transfer","summary":"  To solve the problem of pose distortion in the forward propagation of pose\nfeatures in existing methods, this pa-per proposes a Dual-Side Feature Fusion\nNetwork for pose transfer (DSFFNet). Firstly, a fixed-length pose code is\nextracted from the source mesh by a pose encoder and combined with the target\nvertices to form a mixed feature; Then, a Feature Fusion Adaptive Instance\nNormalization module (FFAdaIN) is designed, which can process both pose and\nidentity features simultaneously, so that the pose features can be compensated\nin layer-by-layer for-ward propagation, thus solving the pose distortion\nproblem; Finally, using the mesh decoder composed of this module, the pose are\ngradually transferred to the target mesh. Experimental results on SMPL, SMAL,\nFAUST and MultiGarment datasets show that DSFFNet successfully solves the pose\ndistortion problem while maintaining a smaller network structure with stronger\npose transfer capability and faster convergence speed, and can adapt to meshes\nwith different numbers of vertices. Code is available at\nhttps://github.com/YikiDragon/DSFFNet\n","authors":["Jue Liu"],"pdf_url":"https://arxiv.org/pdf/2305.14951v2.pdf","comment":"in Chinese language"},{"id":"http://arxiv.org/abs/2310.05524v1","updated":"2023-10-09T08:42:40Z","published":"2023-10-09T08:42:40Z","title":"Bi-directional Deformation for Parameterization of Neural Implicit\n  Surfaces","summary":"  The growing capabilities of neural rendering have increased the demand for\nnew techniques that enable the intuitive editing of 3D objects, particularly\nwhen they are represented as neural implicit surfaces. In this paper, we\npresent a novel neural algorithm to parameterize neural implicit surfaces to\nsimple parametric domains, such as spheres, cubes or polycubes, where 3D\nradiance field can be represented as a 2D field, thereby facilitating\nvisualization and various editing tasks. Technically, our method computes a\nbi-directional deformation between 3D objects and their chosen parametric\ndomains, eliminating the need for any prior information. We adopt a forward\nmapping of points on the zero level set of the 3D object to a parametric\ndomain, followed by a backward mapping through inverse deformation. To ensure\nthe map is bijective, we employ a cycle loss while optimizing the smoothness of\nboth deformations. Additionally, we leverage a Laplacian regularizer to\neffectively control angle distortion and offer the flexibility to choose from a\nrange of parametric domains for managing area distortion. Designed for\ncompatibility, our framework integrates seamlessly with existing neural\nrendering pipelines, taking multi-view images as input to reconstruct 3D\ngeometry and compute the corresponding texture map. We also introduce a simple\nyet effective technique for intrinsic radiance decomposition, facilitating both\nview-independent material editing and view-dependent shading editing. Our\nmethod allows for the immediate rendering of edited textures through volume\nrendering, without the need for network re-training. Moreover, our approach\nsupports the co-parameterization of multiple objects and enables texture\ntransfer between them. We demonstrate the effectiveness of our method on images\nof human heads and man-made objects. We will make the source code publicly\navailable.\n","authors":["Baixin Xu","Jiangbei Hu","Fei Hou","Kwan-Yee Lin","Wayne Wu","Chen Qian","Ying He"],"pdf_url":"https://arxiv.org/pdf/2310.05524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05512v1","updated":"2023-10-09T08:27:35Z","published":"2023-10-09T08:27:35Z","title":"UAVs and Neural Networks for search and rescue missions","summary":"  In this paper, we present a method for detecting objects of interest,\nincluding cars, humans, and fire, in aerial images captured by unmanned aerial\nvehicles (UAVs) usually during vegetation fires. To achieve this, we use\nartificial neural networks and create a dataset for supervised learning. We\naccomplish the assisted labeling of the dataset through the implementation of\nan object detection pipeline that combines classic image processing techniques\nwith pretrained neural networks. In addition, we develop a data augmentation\npipeline to augment the dataset with automatically labeled images. Finally, we\nevaluate the performance of different neural networks.\n","authors":["Hartmut Surmann","Artur Leinweber","Gerhard Senkowski","Julien Meine","Dominik Slomma"],"pdf_url":"https://arxiv.org/pdf/2310.05512v1.pdf","comment":"8 pages, 56th International Symposium on Robotics (ISR Europe) |\n  September 26-27, 2023"},{"id":"http://arxiv.org/abs/2310.05511v1","updated":"2023-10-09T08:27:05Z","published":"2023-10-09T08:27:05Z","title":"Proposal-based Temporal Action Localization with Point-level Supervision","summary":"  Point-level supervised temporal action localization (PTAL) aims at\nrecognizing and localizing actions in untrimmed videos where only a single\npoint (frame) within every action instance is annotated in training data.\nWithout temporal annotations, most previous works adopt the multiple instance\nlearning (MIL) framework, where the input video is segmented into\nnon-overlapped short snippets, and action classification is performed\nindependently on every short snippet. We argue that the MIL framework is\nsuboptimal for PTAL because it operates on separated short snippets that\ncontain limited temporal information. Therefore, the classifier only focuses on\nseveral easy-to-distinguish snippets instead of discovering the whole action\ninstance without missing any relevant snippets. To alleviate this problem, we\npropose a novel method that localizes actions by generating and evaluating\naction proposals of flexible duration that involve more comprehensive temporal\ninformation. Moreover, we introduce an efficient clustering algorithm to\nefficiently generate dense pseudo labels that provide stronger supervision, and\na fine-grained contrastive loss to further refine the quality of pseudo labels.\nExperiments show that our proposed method achieves competitive or superior\nperformance to the state-of-the-art methods and some fully-supervised methods\non four benchmarks: ActivityNet 1.3, THUMOS 14, GTEA, and BEOID datasets.\n","authors":["Yuan Yin","Yifei Huang","Ryosuke Furuta","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2310.05511v1.pdf","comment":"BMVC 2023"},{"id":"http://arxiv.org/abs/2310.05504v1","updated":"2023-10-09T08:09:15Z","published":"2023-10-09T08:09:15Z","title":"Colmap-PCD: An Open-source Tool for Fine Image-to-point cloud\n  Registration","summary":"  State-of-the-art techniques for monocular camera reconstruction predominantly\nrely on the Structure from Motion (SfM) pipeline. However, such methods often\nyield reconstruction outcomes that lack crucial scale information, and over\ntime, accumulation of images leads to inevitable drift issues. In contrast,\nmapping methods based on LiDAR scans are popular in large-scale urban scene\nreconstruction due to their precise distance measurements, a capability\nfundamentally absent in visual-based approaches. Researchers have made attempts\nto utilize concurrent LiDAR and camera measurements in pursuit of precise\nscaling and color details within mapping outcomes. However, the outcomes are\nsubject to extrinsic calibration and time synchronization precision. In this\npaper, we propose a novel cost-effective reconstruction pipeline that utilizes\na pre-established LiDAR map as a fixed constraint to effectively address the\ninherent scale challenges present in monocular camera reconstruction. To our\nknowledge, our method is the first to register images onto the point cloud map\nwithout requiring synchronous capture of camera and LiDAR data, granting us the\nflexibility to manage reconstruction detail levels across various areas of\ninterest. To facilitate further research in this domain, we have released\nColmap-PCD${^{3}}$, an open-source tool leveraging the Colmap algorithm, that\nenables precise fine-scale registration of images to the point cloud map.\n","authors":["Chunge Bai","Ruijie Fu","Xiang Gao"],"pdf_url":"https://arxiv.org/pdf/2310.05504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05498v1","updated":"2023-10-09T07:59:31Z","published":"2023-10-09T07:59:31Z","title":"Semi-Supervised Object Detection with Uncurated Unlabeled Data for\n  Remote Sensing Images","summary":"  Annotating remote sensing images (RSIs) presents a notable challenge due to\nits labor-intensive nature. Semi-supervised object detection (SSOD) methods\ntackle this issue by generating pseudo-labels for the unlabeled data, assuming\nthat all classes found in the unlabeled dataset are also represented in the\nlabeled data. However, real-world situations introduce the possibility of\nout-of-distribution (OOD) samples being mixed with in-distribution (ID) samples\nwithin the unlabeled dataset. In this paper, we delve into techniques for\nconducting SSOD directly on uncurated unlabeled data, which is termed Open-Set\nSemi-Supervised Object Detection (OSSOD). Our approach commences by employing\nlabeled in-distribution data to dynamically construct a class-wise feature bank\n(CFB) that captures features specific to each class. Subsequently, we compare\nthe features of predicted object bounding boxes with the corresponding entries\nin the CFB to calculate OOD scores. We design an adaptive threshold based on\nthe statistical properties of the CFB, allowing us to filter out OOD samples\neffectively. The effectiveness of our proposed method is substantiated through\nextensive experiments on two widely used remote sensing object detection\ndatasets: DIOR and DOTA. These experiments showcase the superior performance\nand efficacy of our approach for OSSOD on RSIs.\n","authors":["Nanqing Liu","Xun Xu","Yingjie Gao","Heng-Chao Li"],"pdf_url":"https://arxiv.org/pdf/2310.05498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16177v4","updated":"2023-10-09T07:58:13Z","published":"2023-07-30T09:15:38Z","title":"Fusing VHR Post-disaster Aerial Imagery and LiDAR Data for Roof\n  Classification in the Caribbean","summary":"  Accurate and up-to-date information on building characteristics is essential\nfor vulnerability assessment; however, the high costs and long timeframes\nassociated with conducting traditional field surveys can be an obstacle to\nobtaining critical exposure datasets needed for disaster risk management. In\nthis work, we leverage deep learning techniques for the automated\nclassification of roof characteristics from very high-resolution orthophotos\nand airborne LiDAR data obtained in Dominica following Hurricane Maria in 2017.\nWe demonstrate that the fusion of multimodal earth observation data performs\nbetter than using any single data source alone. Using our proposed methods, we\nachieve F1 scores of 0.93 and 0.92 for roof type and roof material\nclassification, respectively. This work is intended to help governments produce\nmore timely building information to improve resilience and disaster response in\nthe Caribbean.\n","authors":["Isabelle Tingzon","Nuala Margaret Cowan","Pierre Chrzanowski"],"pdf_url":"https://arxiv.org/pdf/2307.16177v4.pdf","comment":"ICCV 2023 Workshop on Artificial Intelligence for Humanitarian\n  Assistance and Disaster Response"},{"id":"http://arxiv.org/abs/2309.13038v2","updated":"2023-10-09T07:56:19Z","published":"2023-09-22T17:58:04Z","title":"Privacy Assessment on Reconstructed Images: Are Existing Evaluation\n  Metrics Faithful to Human Perception?","summary":"  Hand-crafted image quality metrics, such as PSNR and SSIM, are commonly used\nto evaluate model privacy risk under reconstruction attacks. Under these\nmetrics, reconstructed images that are determined to resemble the original one\ngenerally indicate more privacy leakage. Images determined as overall\ndissimilar, on the other hand, indicate higher robustness against attack.\nHowever, there is no guarantee that these metrics well reflect human opinions,\nwhich, as a judgement for model privacy leakage, are more trustworthy. In this\npaper, we comprehensively study the faithfulness of these hand-crafted metrics\nto human perception of privacy information from the reconstructed images. On 5\ndatasets ranging from natural images, faces, to fine-grained classes, we use 4\nexisting attack methods to reconstruct images from many different\nclassification models and, for each reconstructed image, we ask multiple human\nannotators to assess whether this image is recognizable. Our studies reveal\nthat the hand-crafted metrics only have a weak correlation with the human\nevaluation of privacy leakage and that even these metrics themselves often\ncontradict each other. These observations suggest risks of current metrics in\nthe community. To address this potential risk, we propose a learning-based\nmeasure called SemSim to evaluate the Semantic Similarity between the original\nand reconstructed images. SemSim is trained with a standard triplet loss, using\nan original image as an anchor, one of its recognizable reconstructed images as\na positive sample, and an unrecognizable one as a negative. By training on\nhuman annotations, SemSim exhibits a greater reflection of privacy leakage on\nthe semantic level. We show that SemSim has a significantly higher correlation\nwith human judgment compared with existing metrics. Moreover, this strong\ncorrelation generalizes to unseen datasets, models and attack methods.\n","authors":["Xiaoxiao Sun","Nidham Gazagnadou","Vivek Sharma","Lingjuan Lyu","Hongdong Li","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2309.13038v2.pdf","comment":"15 pages, 9 figures and 3 tables"},{"id":"http://arxiv.org/abs/2310.04414v2","updated":"2023-10-09T07:54:54Z","published":"2023-10-06T17:58:20Z","title":"CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model\n  Generalization Analysis","summary":"  Analyzing model performance in various unseen environments is a critical\nresearch problem in the machine learning community. To study this problem, it\nis important to construct a testbed with out-of-distribution test sets that\nhave broad coverage of environmental discrepancies. However, existing testbeds\ntypically either have a small number of domains or are synthesized by image\ncorruptions, hindering algorithm design that demonstrates real-world\neffectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of\n180 datasets collected by prompting image search engines and diffusion models\nin various ways. Generally sized between 300 and 8,000 images, the datasets\ncontain natural images, cartoons, certain colors, or objects that do not\nnaturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen\nthe understanding of two generalization tasks: domain generalization and model\naccuracy prediction in various out-of-distribution environments. We conduct\nextensive benchmarking and comparison experiments and show that CIFAR-10-W\noffers new and interesting insights inherent to these tasks. We also discuss\nother fields that would benefit from CIFAR-10-W.\n","authors":["Xiaoxiao Sun","Xingjian Leng","Zijian Wang","Yang Yang","Zi Huang","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.04414v2.pdf","comment":"9 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2310.05483v1","updated":"2023-10-09T07:42:33Z","published":"2023-10-09T07:42:33Z","title":"Geometry-Guided Ray Augmentation for Neural Surface Reconstruction with\n  Sparse Views","summary":"  In this paper, we propose a novel method for 3D scene and object\nreconstruction from sparse multi-view images. Different from previous methods\nthat leverage extra information such as depth or generalizable features across\nscenes, our approach leverages the scene properties embedded in the multi-view\ninputs to create precise pseudo-labels for optimization without any prior\ntraining. Specifically, we introduce a geometry-guided approach that improves\nsurface reconstruction accuracy from sparse views by leveraging spherical\nharmonics to predict the novel radiance while holistically considering all\ncolor observations for a point in the scene. Also, our pipeline exploits proxy\ngeometry and correctly handles the occlusion in generating the pseudo-labels of\nradiance, which previous image-warping methods fail to avoid. Our method,\ndubbed Ray Augmentation (RayAug), achieves superior results on DTU and Blender\ndatasets without requiring prior training, demonstrating its effectiveness in\naddressing the problem of sparse view reconstruction. Our pipeline is flexible\nand can be integrated into other implicit neural reconstruction methods for\nsparse views.\n","authors":["Jiawei Yao","Chen Wang","Tong Wu","Chuming Li"],"pdf_url":"https://arxiv.org/pdf/2310.05483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05473v1","updated":"2023-10-09T07:31:44Z","published":"2023-10-09T07:31:44Z","title":"Sentence-level Prompts Benefit Composed Image Retrieval","summary":"  Composed image retrieval (CIR) is the task of retrieving specific images by\nusing a query that involves both a reference image and a relative caption. Most\nexisting CIR models adopt the late-fusion strategy to combine visual and\nlanguage features. Besides, several approaches have also been suggested to\ngenerate a pseudo-word token from the reference image, which is further\nintegrated into the relative caption for CIR. However, these pseudo-word-based\nprompting methods have limitations when target image encompasses complex\nchanges on reference image, e.g., object removal and attribute modification. In\nthis work, we demonstrate that learning an appropriate sentence-level prompt\nfor the relative caption (SPRC) is sufficient for achieving effective composed\nimage retrieval. Instead of relying on pseudo-word-based prompts, we propose to\nleverage pretrained V-L models, e.g., BLIP-2, to generate sentence-level\nprompts. By concatenating the learned sentence-level prompt with the relative\ncaption, one can readily use existing text-based image retrieval models to\nenhance CIR performance. Furthermore, we introduce both image-text contrastive\nloss and text prompt alignment loss to enforce the learning of suitable\nsentence-level prompts. Experiments show that our proposed method performs\nfavorably against the state-of-the-art CIR methods on the Fashion-IQ and CIRR\ndatasets. The source code and pretrained model are publicly available at\nhttps://github.com/chunmeifeng/SPRC\n","authors":["Yang Bai","Xinxing Xu","Yong Liu","Salman Khan","Fahad Khan","Wangmeng Zuo","Rick Siow Mong Goh","Chun-Mei Feng"],"pdf_url":"https://arxiv.org/pdf/2310.05473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15521v2","updated":"2023-10-09T07:24:45Z","published":"2023-06-27T14:47:43Z","title":"What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation","summary":"  While semantic segmentation has seen tremendous improvements in the past,\nthere are still significant labeling efforts necessary and the problem of\nlimited generalization to classes that have not been present during training.\nTo address this problem, zero-shot semantic segmentation makes use of large\nself-supervised vision-language models, allowing zero-shot transfer to unseen\nclasses. In this work, we build a benchmark for Multi-domain Evaluation of\nSemantic Segmentation (MESS), which allows a holistic analysis of performance\nacross a wide range of domain-specific datasets such as medicine, engineering,\nearth monitoring, biology, and agriculture. To do this, we reviewed 120\ndatasets, developed a taxonomy, and classified the datasets according to the\ndeveloped taxonomy. We select a representative subset consisting of 22 datasets\nand propose it as the MESS benchmark. We evaluate eight recently published\nmodels on the proposed MESS benchmark and analyze characteristics for the\nperformance of zero-shot transfer models. The toolkit is available at\nhttps://github.com/blumenstiel/MESS.\n","authors":["Benedikt Blumenstiel","Johannes Jakubik","Hilde Kühne","Michael Vössing"],"pdf_url":"https://arxiv.org/pdf/2306.15521v2.pdf","comment":"Accepted at NeurIPS 2023 Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2307.10864v2","updated":"2023-10-09T07:20:00Z","published":"2023-07-20T13:33:28Z","title":"Divide & Bind Your Attention for Improved Generative Semantic Nursing","summary":"  Emerging large-scale text-to-image generative models, e.g., Stable Diffusion\n(SD), have exhibited overwhelming results with high fidelity. Despite the\nmagnificent progress, current state-of-the-art models still struggle to\ngenerate images fully adhering to the input prompt. Prior work, Attend &\nExcite, has introduced the concept of Generative Semantic Nursing (GSN), aiming\nto optimize cross-attention during inference time to better incorporate the\nsemantics. It demonstrates promising results in generating simple prompts,\ne.g., ``a cat and a dog''. However, its efficacy declines when dealing with\nmore complex prompts, and it does not explicitly address the problem of\nimproper attribute binding. To address the challenges posed by complex prompts\nor scenarios involving multiple entities and to achieve improved attribute\nbinding, we propose Divide & Bind. We introduce two novel loss objectives for\nGSN: a novel attendance loss and a binding loss. Our approach stands out in its\nability to faithfully synthesize desired objects with improved attribute\nalignment from complex prompts and exhibits superior performance across\nmultiple evaluation benchmarks.\n","authors":["Yumeng Li","Margret Keuper","Dan Zhang","Anna Khoreva"],"pdf_url":"https://arxiv.org/pdf/2307.10864v2.pdf","comment":"Accepted at BMVC 2023 as Oral. Code:\n  https://github.com/boschresearch/Divide-and-Bind and project page:\n  https://sites.google.com/view/divide-and-bind"},{"id":"http://arxiv.org/abs/2309.07616v2","updated":"2023-10-09T07:18:01Z","published":"2023-09-14T11:25:19Z","title":"Road Disease Detection based on Latent Domain Background Feature\n  Separation and Suppression","summary":"  Road disease detection is challenging due to the the small proportion of road\ndamage in target region and the diverse background,which introduce lots of\ndomain information.Besides, disease categories have high similarity,makes the\ndetection more difficult. In this paper, we propose a new LDBFSS(Latent Domain\nBackground Feature Separation and Suppression) network which could perform\nbackground information separation and suppression without domain supervision\nand contrastive enhancement of object features.We combine our LDBFSS network\nwith YOLOv5 model to enhance disease features for better road disease\ndetection. As the components of LDBFSS network, we first design a latent domain\ndiscovery module and a domain adversarial learning module to obtain pseudo\ndomain labels through unsupervised method, guiding domain discriminator and\nmodel to train adversarially to suppress background information. In addition,\nwe introduce a contrastive learning module and design k-instance contrastive\nloss, optimize the disease feature representation by increasing the inter-class\ndistance and reducing the intra-class distance for object features. We\nconducted experiments on two road disease detection datasets, GRDDC and CNRDD,\nand compared with other models,which show an increase of nearly 4% on GRDDC\ndataset compared with optimal model, and an increase of 4.6% on CNRDD dataset.\nExperimental results prove the effectiveness and superiority of our model.\n","authors":["Juwu Zheng","Jiangtao Ren"],"pdf_url":"https://arxiv.org/pdf/2309.07616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05462v1","updated":"2023-10-09T07:10:30Z","published":"2023-10-09T07:10:30Z","title":"AdaFuse: Adaptive Medical Image Fusion Based on Spatial-Frequential\n  Cross Attention","summary":"  Multi-modal medical image fusion is essential for the precise clinical\ndiagnosis and surgical navigation since it can merge the complementary\ninformation in multi-modalities into a single image. The quality of the fused\nimage depends on the extracted single modality features as well as the fusion\nrules for multi-modal information. Existing deep learning-based fusion methods\ncan fully exploit the semantic features of each modality, they cannot\ndistinguish the effective low and high frequency information of each modality\nand fuse them adaptively. To address this issue, we propose AdaFuse, in which\nmultimodal image information is fused adaptively through frequency-guided\nattention mechanism based on Fourier transform. Specifically, we propose the\ncross-attention fusion (CAF) block, which adaptively fuses features of two\nmodalities in the spatial and frequency domains by exchanging key and query\nvalues, and then calculates the cross-attention scores between the spatial and\nfrequency features to further guide the spatial-frequential information fusion.\nThe CAF block enhances the high-frequency features of the different modalities\nso that the details in the fused images can be retained. Moreover, we design a\nnovel loss function composed of structure loss and content loss to preserve\nboth low and high frequency information. Extensive comparison experiments on\nseveral datasets demonstrate that the proposed method outperforms\nstate-of-the-art methods in terms of both visual quality and quantitative\nmetrics. The ablation experiments also validate the effectiveness of the\nproposed loss and fusion strategy. Our code is publicly available at\nhttps://github.com/xianming-gu/AdaFuse.\n","authors":["Xianming Gu","Lihui Wang","Zeyu Deng","Ying Cao","Xingyu Huang","Yue-min Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.05462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2006.09565v3","updated":"2023-10-09T07:10:20Z","published":"2020-06-16T23:41:42Z","title":"Mining Label Distribution Drift in Unsupervised Domain Adaptation","summary":"  Unsupervised domain adaptation targets to transfer task-related knowledge\nfrom labeled source domain to unlabeled target domain. Although tremendous\nefforts have been made to minimize domain divergence, most existing methods\nonly partially manage by aligning feature representations from diverse domains.\nBeyond the discrepancy in data distribution, the gap between source and target\nlabel distribution, recognized as label distribution drift, is another crucial\nfactor raising domain divergence, and has been under insufficient exploration.\nFrom this perspective, we first reveal how label distribution drift brings\nnegative influence. Next, we propose Label distribution Matching Domain\nAdversarial Network (LMDAN) to handle data distribution shift and label\ndistribution drift jointly. In LMDAN, label distribution drift is addressed by\na source sample weighting strategy, which selects samples that contribute to\npositive adaptation and avoid adverse effects brought by the mismatched\nsamples. Experiments show that LMDAN delivers superior performance under\nconsiderable label distribution drift.\n","authors":["Peizhao Li","Zhengming Ding","Hongfu Liu"],"pdf_url":"https://arxiv.org/pdf/2006.09565v3.pdf","comment":"Accepted to AJCAI'23"},{"id":"http://arxiv.org/abs/2303.15413v3","updated":"2023-10-09T07:02:43Z","published":"2023-03-27T17:31:13Z","title":"Debiasing Scores and Prompts of 2D Diffusion for View-consistent\n  Text-to-3D Generation","summary":"  Existing score-distilling text-to-3D generation techniques, despite their\nconsiderable promise, often encounter the view inconsistency problem. One of\nthe most notable issues is the Janus problem, where the most canonical view of\nan object (\\textit{e.g}., face or head) appears in other views. In this work,\nwe explore existing frameworks for score-distilling text-to-3D generation and\nidentify the main causes of the view inconsistency problem -- the embedded bias\nof 2D diffusion models. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for view-consistent text-to-3D\ngeneration. Our first approach, called score debiasing, involves cutting off\nthe score estimated by 2D diffusion models and gradually increasing the\ntruncation value throughout the optimization process. Our second approach,\ncalled prompt debiasing, identifies conflicting words between user prompts and\nview prompts using a language model, and adjusts the discrepancy between view\nprompts and the viewing direction of an object. Our experimental results show\nthat our methods improve the realism of the generated 3D objects by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead. Our project page is available\nat~\\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.\n","authors":["Susung Hong","Donghoon Ahn","Seungryong Kim"],"pdf_url":"https://arxiv.org/pdf/2303.15413v3.pdf","comment":"Accepted to NeurIPS 2023. Project Page:\n  https://susunghong.github.io/Debiased-Score-Distillation-Sampling/"},{"id":"http://arxiv.org/abs/2310.05453v1","updated":"2023-10-09T06:57:55Z","published":"2023-10-09T06:57:55Z","title":"Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation","summary":"  Universal domain adaptation aims to align the classes and reduce the feature\ngap between the same category of the source and target domains. The target\nprivate category is set as the unknown class during the adaptation process, as\nit is not included in the source domain. However, most existing methods\noverlook the intra-class structure within a category, especially in cases where\nthere exists significant concept shift between the samples belonging to the\nsame category. When samples with large concept shift are forced to be pushed\ntogether, it may negatively affect the adaptation performance. Moreover, from\nthe interpretability aspect, it is unreasonable to align visual features with\nsignificant differences, such as fighter jets and civil aircraft, into the same\ncategory. Unfortunately, due to such semantic ambiguity and annotation cost,\ncategories are not always classified in detail, making it difficult for the\nmodel to perform precise adaptation. To address these issues, we propose a\nnovel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the\ndifferences between samples belonging to the same category and mine sub-classes\nwhen there exists significant concept shift between them. By doing so, our\nmodel learns a more reasonable feature space that enhances the transferability\nand reflects the inherent differences among samples annotated as the same\ncategory. We evaluate the effectiveness of our MemSPM method over multiple\nscenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art\nperformance on four benchmarks in most cases.\n","authors":["Yuxiang Lai","Xinghong Liu","Tao Zhou","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.05453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05447v1","updated":"2023-10-09T06:43:48Z","published":"2023-10-09T06:43:48Z","title":"Towards Fair and Comprehensive Comparisons for Image-Based 3D Object\n  Detection","summary":"  In this work, we build a modular-designed codebase, formulate strong training\nrecipes, design an error diagnosis toolbox, and discuss current methods for\nimage-based 3D object detection. In particular, different from other highly\nmature tasks, e.g., 2D object detection, the community of image-based 3D object\ndetection is still evolving, where methods often adopt different training\nrecipes and tricks resulting in unfair evaluations and comparisons. What is\nworse, these tricks may overwhelm their proposed designs in performance, even\nleading to wrong conclusions. To address this issue, we build a module-designed\ncodebase and formulate unified training standards for the community.\nFurthermore, we also design an error diagnosis toolbox to measure the detailed\ncharacterization of detection models. Using these tools, we analyze current\nmethods in-depth under varying settings and provide discussions for some open\nquestions, e.g., discrepancies in conclusions on KITTI-3D and nuScenes\ndatasets, which have led to different dominant methods for these datasets. We\nhope that this work will facilitate future research in image-based 3D object\ndetection. Our codes will be released at\n\\url{https://github.com/OpenGVLab/3dodi}\n","authors":["Xinzhu Ma","Yongtao Wan","Yinmin Zhang","Zhiyi Xia","Yuan Meng","Zhihui Wang","Haojie Li","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.05447v1.pdf","comment":"ICCV23, code will be released soon"},{"id":"http://arxiv.org/abs/2310.05446v1","updated":"2023-10-09T06:43:38Z","published":"2023-10-09T06:43:38Z","title":"RetSeg: Retention-based Colorectal Polyps Segmentation Network","summary":"  Vision Transformers (ViTs) have revolutionized medical imaging analysis,\nshowcasing superior efficacy compared to conventional Convolutional Neural\nNetworks (CNNs) in vital tasks such as polyp classification, detection, and\nsegmentation. Leveraging attention mechanisms to focus on specific image\nregions, ViTs exhibit contextual awareness in processing visual data,\nculminating in robust and precise predictions, even for intricate medical\nimages. Moreover, the inherent self-attention mechanism in Transformers\naccommodates varying input sizes and resolutions, granting an unprecedented\nflexibility absent in traditional CNNs. However, Transformers grapple with\nchallenges like excessive memory usage and limited training parallelism due to\nself-attention, rendering them impractical for real-time disease detection on\nresource-constrained devices. In this study, we address these hurdles by\ninvestigating the integration of the recently introduced retention mechanism\ninto polyp segmentation, introducing RetSeg, an encoder-decoder network\nfeaturing multi-head retention blocks. Drawing inspiration from Retentive\nNetworks (RetNet), RetSeg is designed to bridge the gap between precise polyp\nsegmentation and resource utilization, particularly tailored for colonoscopy\nimages. We train and validate RetSeg for polyp segmentation employing two\npublicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we\nshowcase RetSeg's promising performance across diverse public datasets,\nincluding CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While\nour work represents an early-stage exploration, further in-depth studies are\nimperative to advance these promising findings.\n","authors":["Khaled ELKarazle","Valliappan Raman","Caslon Chua","Patrick Then"],"pdf_url":"https://arxiv.org/pdf/2310.05446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05445v1","updated":"2023-10-09T06:42:43Z","published":"2023-10-09T06:42:43Z","title":"AngioMoCo: Learning-based Motion Correction in Cerebral Digital\n  Subtraction Angiography","summary":"  Cerebral X-ray digital subtraction angiography (DSA) is the standard imaging\ntechnique for visualizing blood flow and guiding endovascular treatments. The\nquality of DSA is often negatively impacted by body motion during acquisition,\nleading to decreased diagnostic value. Time-consuming iterative methods address\nmotion correction based on non-rigid registration, and employ sparse key points\nand non-rigidity penalties to limit vessel distortion. Recent methods alleviate\nsubtraction artifacts by predicting the subtracted frame from the corresponding\nunsubtracted frame, but do not explicitly compensate for motion-induced\nmisalignment between frames. This hinders the serial evaluation of blood flow,\nand often causes undesired vasculature and contrast flow alterations, leading\nto impeded usability in clinical practice. To address these limitations, we\npresent AngioMoCo, a learning-based framework that generates motion-compensated\nDSA sequences from X-ray angiography. AngioMoCo integrates contrast extraction\nand motion correction, enabling differentiation between patient motion and\nintensity changes caused by contrast flow. This strategy improves registration\nquality while being substantially faster than iterative elastix-based methods.\nWe demonstrate AngioMoCo on a large national multi-center dataset (MR CLEAN\nRegistry) of clinically acquired angiographic images through comprehensive\nqualitative and quantitative analyses. AngioMoCo produces high-quality\nmotion-compensated DSA, removing motion artifacts while preserving contrast\nflow. Code is publicly available at https://github.com/RuishengSu/AngioMoCo.\n","authors":["Ruisheng Su","Matthijs van der Sluijs","Sandra Cornelissen","Wim van Zwam","Aad van der Lugt","Wiro Niessen","Danny Ruijters","Theo van Walsum","Adrian Dalca"],"pdf_url":"https://arxiv.org/pdf/2310.05445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05428v1","updated":"2023-10-09T05:57:01Z","published":"2023-10-09T05:57:01Z","title":"Semantic-aware Temporal Channel-wise Attention for Cardiac Function\n  Assessment","summary":"  Cardiac function assessment aims at predicting left ventricular ejection\nfraction (LVEF) given an echocardiogram video, which requests models to focus\non the changes in the left ventricle during the cardiac cycle. How to assess\ncardiac function accurately and automatically from an echocardiogram video is a\nvaluable topic in intelligent assisted healthcare. Existing video-based methods\ndo not pay much attention to the left ventricular region, nor the left\nventricular changes caused by motion. In this work, we propose a\nsemi-supervised auxiliary learning paradigm with a left ventricular\nsegmentation task, which contributes to the representation learning for the\nleft ventricular region. To better model the importance of motion information,\nwe introduce a temporal channel-wise attention (TCA) module to excite those\nchannels used to describe motion. Furthermore, we reform the TCA module with\nsemantic perception by taking the segmentation map of the left ventricle as\ninput to focus on the motion patterns of the left ventricle. Finally, to reduce\nthe difficulty of direct LVEF regression, we utilize an anchor-based\nclassification and regression method to predict LVEF. Our approach achieves\nstate-of-the-art performance on the Stanford dataset with an improvement of\n0.22 MAE, 0.26 RMSE, and 1.9% $R^2$.\n","authors":["Guanqi Chen","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2310.05428v1.pdf","comment":"Accepted by ISBI 2022 (oral)"},{"id":"http://arxiv.org/abs/2310.05425v1","updated":"2023-10-09T05:55:17Z","published":"2023-10-09T05:55:17Z","title":"Divide and Ensemble: Progressively Learning for the Unknown","summary":"  In the wheat nutrient deficiencies classification challenge, we present the\nDividE and EnseMble (DEEM) method for progressive test data predictions. We\nfind that (1) test images are provided in the challenge; (2) samples are\nequipped with their collection dates; (3) the samples of different dates show\nnotable discrepancies. Based on the findings, we partition the dataset into\ndiscrete groups by the dates and train models on each divided group. We then\nadopt the pseudo-labeling approach to label the test data and incorporate those\nwith high confidence into the training set. In pseudo-labeling, we leverage\nmodels ensemble with different architectures to enhance the reliability of\npredictions. The pseudo-labeling and ensembled model training are iteratively\nconducted until all test samples are labeled. Finally, the separated models for\neach group are unified to obtain the model for the whole dataset. Our method\nachieves an average of 93.6\\% Top-1 test accuracy~(94.0\\% on WW2020 and 93.2\\%\non WR2021) and wins the 1$st$ place in the Deep Nutrient Deficiency\nChallenge~\\footnote{https://cvppa2023.github.io/challenges/}.\n","authors":["Hu Zhang","Xin Shen","Heming Du","Huiqiang Chen","Chen Liu","Hongwei Sheng","Qingzheng Xu","MD Wahiduzzaman Khan","Qingtao Yu","Tianqing Zhu","Scott Chapman","Zi Huang","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12270v2","updated":"2023-10-09T05:48:11Z","published":"2023-07-23T09:04:13Z","title":"Context Perception Parallel Decoder for Scene Text Recognition","summary":"  Scene text recognition (STR) methods have struggled to attain high accuracy\nand fast inference speed. Autoregressive (AR)-based models implement the\nrecognition in a character-by-character manner, showing superiority in accuracy\nbut with slow inference speed. Alternatively, parallel decoding (PD)-based\nmodels infer all characters in a single decoding pass, offering faster\ninference speed but generally worse accuracy. We first present an empirical\nstudy of AR decoding in STR, and discover that the AR decoder not only models\nlinguistic context, but also provides guidance on visual context perception.\nConsequently, we propose Context Perception Parallel Decoder (CPPD) to predict\nthe character sequence in a PD pass. CPPD devises a character counting module\nto infer the occurrence count of each character, and a character ordering\nmodule to deduce the content-free reading order and placeholders. Meanwhile,\nthe character prediction task associates the placeholders with characters. They\ntogether build a comprehensive recognition context. We construct a series of\nCPPD models and also plug the proposed modules into existing STR decoders.\nExperiments on both English and Chinese benchmarks demonstrate that the CPPD\nmodels achieve highly competitive accuracy while running approximately 8x\nfaster than their AR-based counterparts. Moreover, the plugged models achieve\nsignificant accuracy improvements. Code is at\n\\href{https://github.com/PaddlePaddle/PaddleOCR/blob/dygraph/doc/doc_en/algorithm_rec_cppd_en.md}{this\nhttps URL}.\n","authors":["Yongkun Du","Zhineng Chen","Caiyan Jia","Xiaoting Yin","Chenxia Li","Yuning Du","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2307.12270v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05406v1","updated":"2023-10-09T04:54:30Z","published":"2023-10-09T04:54:30Z","title":"GradientSurf: Gradient-Domain Neural Surface Reconstruction from RGB\n  Video","summary":"  This paper proposes GradientSurf, a novel algorithm for real time surface\nreconstruction from monocular RGB video. Inspired by Poisson Surface\nReconstruction, the proposed method builds on the tight coupling between\nsurface, volume, and oriented point cloud and solves the reconstruction problem\nin gradient-domain. Unlike Poisson Surface Reconstruction which finds an\noffline solution to the Poisson equation by solving a linear system after the\nscanning process is finished, our method finds online solutions from partial\nscans with a neural network incrementally where the Poisson layer is designed\nto supervise both local and global reconstruction. The main challenge that\nexisting methods suffer from when reconstructing from RGB signal is a lack of\ndetails in the reconstructed surface. We hypothesize this is due to the\nspectral bias of neural networks towards learning low frequency geometric\nfeatures. To address this issue, the reconstruction problem is cast onto\ngradient domain, where zeroth-order and first-order energies are minimized. The\nzeroth-order term penalizes location of the surface. The first-order term\npenalizes the difference between the gradient of reconstructed implicit\nfunction and the vector field formulated from oriented point clouds sampled at\nadaptive local densities. For the task of indoor scene reconstruction, visual\nand quantitative experimental results show that the proposed method\nreconstructs surfaces with more details in curved regions and higher fidelity\nfor small objects than previous methods.\n","authors":["Crane He Chen","Joerg Liebelt"],"pdf_url":"https://arxiv.org/pdf/2310.05406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02127v2","updated":"2023-10-09T04:46:38Z","published":"2022-09-05T20:21:37Z","title":"Design of the topology for contrastive visual-textual alignment","summary":"  Cosine similarity is the common choice for measuring the distance between the\nfeature representations in contrastive visual-textual alignment learning.\nHowever, empirically a learnable softmax temperature parameter is required when\nlearning on large-scale noisy training data. In this work, we first discuss the\nrole of softmax temperature from the embedding space's topological properties.\nWe argue that the softmax temperature is the key mechanism for contrastive\nlearning on noisy training data. It acts as a scaling factor of the distance\nrange (e.g. [-1, 1] for the cosine similarity), and its learned value indicates\nthe level of noise in the training data. Then, we propose an alternative design\nof the topology for the embedding alignment. We make use of multiple class\ntokens in the transformer architecture; then map the feature representations\nonto an oblique manifold endowed with the negative inner product as the\ndistance function. With this configuration, we largely improve the zero-shot\nclassification performance of baseline CLIP models pre-trained on large-scale\ndatasets by an average of 6.1\\%.\n","authors":["Zhun Sun"],"pdf_url":"https://arxiv.org/pdf/2209.02127v2.pdf","comment":"https://github.com/minogame/clip-mtob"},{"id":"http://arxiv.org/abs/2310.05400v1","updated":"2023-10-09T04:38:52Z","published":"2023-10-09T04:38:52Z","title":"Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient\n  Vision Transformers","summary":"  Vector-quantized image modeling has shown great potential in synthesizing\nhigh-quality images. However, generating high-resolution images remains a\nchallenging task due to the quadratic computational overhead of the\nself-attention process. In this study, we seek to explore a more efficient\ntwo-stage framework for high-resolution image generation with improvements in\nthe following three aspects. (1) Based on the observation that the first\nquantization stage has solid local property, we employ a local attention-based\nquantization model instead of the global attention mechanism used in previous\nmethods, leading to better efficiency and reconstruction quality. (2) We\nemphasize the importance of multi-grained feature interaction during image\ngeneration and introduce an efficient attention mechanism that combines global\nattention (long-range semantic consistency within the whole image) and local\nattention (fined-grained details). This approach results in faster generation\nspeed, higher generation fidelity, and improved resolution. (3) We propose a\nnew generation pipeline incorporating autoencoding training and autoregressive\ngeneration strategy, demonstrating a better paradigm for image synthesis.\nExtensive experiments demonstrate the superiority of our approach in\nhigh-quality and high-resolution image reconstruction and generation.\n","authors":["Shiyue Cao","Yueqin Yin","Lianghua Huang","Yu Liu","Xin Zhao","Deli Zhao","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2310.05400v1.pdf","comment":"This paper is accepted to ICCV2023"},{"id":"http://arxiv.org/abs/2307.12493v3","updated":"2023-10-09T04:37:52Z","published":"2023-07-24T02:50:44Z","title":"TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition","summary":"  Text-driven diffusion models have exhibited impressive generative\ncapabilities, enabling various image editing tasks. In this paper, we propose\nTF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the\npower of text-driven diffusion models for cross-domain image-guided\ncomposition. This task aims to seamlessly integrate user-provided objects into\na specific visual context. Current diffusion-based methods often involve costly\ninstance-based optimization or finetuning of pretrained models on customized\ndatasets, which can potentially undermine their rich prior. In contrast,\nTF-ICON can leverage off-the-shelf diffusion models to perform cross-domain\nimage-guided composition without requiring additional training, finetuning, or\noptimization. Moreover, we introduce the exceptional prompt, which contains no\ninformation, to facilitate text-driven diffusion models in accurately inverting\nreal images into latent representations, forming the basis for compositing. Our\nexperiments show that equipping Stable Diffusion with the exceptional prompt\noutperforms state-of-the-art inversion methods on various datasets (CelebA-HQ,\nCOCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile\nvisual domains. Code is available at https://github.com/Shilin-LU/TF-ICON\n","authors":["Shilin Lu","Yanzhu Liu","Adams Wai-Kin Kong"],"pdf_url":"https://arxiv.org/pdf/2307.12493v3.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2310.05394v1","updated":"2023-10-09T04:17:21Z","published":"2023-10-09T04:17:21Z","title":"CAMEL2: Enhancing weakly supervised learning for histopathology images\n  by incorporating the significance ratio","summary":"  Histopathology image analysis plays a crucial role in cancer diagnosis.\nHowever, training a clinically applicable segmentation algorithm requires\npathologists to engage in labour-intensive labelling. In contrast, weakly\nsupervised learning methods, which only require coarse-grained labels at the\nimage level, can significantly reduce the labeling efforts. Unfortunately,\nwhile these methods perform reasonably well in slide-level prediction, their\nability to locate cancerous regions, which is essential for many clinical\napplications, remains unsatisfactory. Previously, we proposed CAMEL, which\nachieves comparable results to those of fully supervised baselines in\npixel-level segmentation. However, CAMEL requires 1,280x1,280 image-level\nbinary annotations for positive WSIs. Here, we present CAMEL2, by introducing a\nthreshold of the cancerous ratio for positive bags, it allows us to better\nutilize the information, consequently enabling us to scale up the image-level\nsetting from 1,280x1,280 to 5,120x5,120 while maintaining the accuracy. Our\nresults with various datasets, demonstrate that CAMEL2, with the help of\n5,120x5,120 image-level binary annotations, which are easy to annotate,\nachieves comparable performance to that of a fully supervised baseline in both\ninstance- and slide-level classifications.\n","authors":["Gang Xu","Shuhao Wang","Lingyu Zhao","Xiao Chen","Tongwei Wang","Lang Wang","Zhenwei Luo","Dahan Wang","Zewen Zhang","Aijun Liu","Wei Ba","Zhigang Song","Huaiyin Shi","Dingrong Zhong","Jianpeng Ma"],"pdf_url":"https://arxiv.org/pdf/2310.05394v1.pdf","comment":"41 pages, 13 figures"},{"id":"http://arxiv.org/abs/2310.05393v1","updated":"2023-10-09T04:16:35Z","published":"2023-10-09T04:16:35Z","title":"Hierarchical Side-Tuning for Vision Transformers","summary":"  Fine-tuning pre-trained Vision Transformers (ViT) has consistently\ndemonstrated promising performance in the realm of visual recognition. However,\nadapting large pre-trained models to various tasks poses a significant\nchallenge. This challenge arises from the need for each model to undergo an\nindependent and comprehensive fine-tuning process, leading to substantial\ncomputational and memory demands. While recent advancements in\nParameter-efficient Transfer Learning (PETL) have demonstrated their ability to\nachieve superior performance compared to full fine-tuning with a smaller subset\nof parameter updates, they tend to overlook dense prediction tasks such as\nobject detection and segmentation. In this paper, we introduce Hierarchical\nSide-Tuning (HST), a novel PETL approach that enables ViT transfer to various\ndownstream tasks effectively. Diverging from existing methods that exclusively\nfine-tune parameters within input spaces or certain modules connected to the\nbackbone, we tune a lightweight and hierarchical side network (HSN) that\nleverages intermediate activations extracted from the backbone and generates\nmulti-scale features to make predictions. To validate HST, we conducted\nextensive experiments encompassing diverse visual tasks, including\nclassification, object detection, instance segmentation, and semantic\nsegmentation. Notably, our method achieves state-of-the-art average Top-1\naccuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters.\nWhen applied to object detection tasks on COCO testdev benchmark, HST even\nsurpasses full fine-tuning and obtains better performance with 49.7 box AP and\n43.2 mask AP using Cascade Mask R-CNN.\n","authors":["Weifeng Lin","Ziheng Wu","Jiayu Chen","Wentao Yang","Mingxin Huang","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2310.05393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05392v1","updated":"2023-10-09T04:07:35Z","published":"2023-10-09T04:07:35Z","title":"Lightweight Full-Convolutional Siamese Tracker","summary":"  Although single object trackers have achieved advanced performance, their\nlarge-scale network models make it difficult to apply them on the platforms\nwith limited resources. Moreover, existing lightweight trackers only achieve\nbalance between 2-3 points in terms of parameters, performance, Flops and FPS.\nTo achieve the balance among all 4 points, this paper propose a lightweight\nfull-convolutional Siamese tracker called lightFC. LightFC employs a noval\nefficient cross-correlation module (ECM) and a noval efficient rep-center head\n(ERH) to enhance the nonlinear expressiveness of the convoluational tracking\npipeline. The ECM adopts an architecture of attention-like module and fuses\nlocal spatial and channel features from the pixel-wise correlation fusion\nfeatures and enhance model nonlinearity with an inversion activation block.\nAdditionally, skip-connections and the reuse of search area features are\nintroduced by the ECM to improve its performance. The ERH reasonably introduces\nreparameterization technology and channel attention to enhance the nonlinear\nexpressiveness of the center head. Comprehensive experiments show that LightFC\nachieves a good balance between performance, parameters, Flops and FPS. The\nprecision score of LightFC outperforms MixFormerV2-S by 3.7 \\% and 6.5 \\% on\nLaSOT and TNL2K, respectively, while using 5x fewer parameters and 4.6x fewer\nFlops. Besides, LightFC runs 2x faster than MixFormerV2-S on CPUs. Our code and\nraw results can be found at https://github.com/LiYunfengLYF/LightFC\n","authors":["Li Yunfeng","Wang Bo","Li Ye","Liu Zhuoyan","Wu Xueyi"],"pdf_url":"https://arxiv.org/pdf/2310.05392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05391v1","updated":"2023-10-09T04:07:00Z","published":"2023-10-09T04:07:00Z","title":"Neural Impostor: Editing Neural Radiance Fields with Explicit Shape\n  Manipulation","summary":"  Neural Radiance Fields (NeRF) have significantly advanced the generation of\nhighly realistic and expressive 3D scenes. However, the task of editing NeRF,\nparticularly in terms of geometry modification, poses a significant challenge.\nThis issue has obstructed NeRF's wider adoption across various applications. To\ntackle the problem of efficiently editing neural implicit fields, we introduce\nNeural Impostor, a hybrid representation incorporating an explicit tetrahedral\nmesh alongside a multigrid implicit field designated for each tetrahedron\nwithin the explicit mesh. Our framework bridges the explicit shape manipulation\nand the geometric editing of implicit fields by utilizing multigrid barycentric\ncoordinate encoding, thus offering a pragmatic solution to deform, composite,\nand generate neural implicit fields while maintaining a complex volumetric\nappearance. Furthermore, we propose a comprehensive pipeline for editing neural\nimplicit fields based on a set of explicit geometric editing operations. We\nshow the robustness and adaptability of our system through diverse examples and\nexperiments, including the editing of both synthetic objects and real captured\ndata. Finally, we demonstrate the authoring process of a hybrid\nsynthetic-captured object utilizing a variety of editing operations,\nunderlining the transformative potential of Neural Impostor in the field of 3D\ncontent creation and manipulation.\n","authors":["Ruiyang Liu","Jinxu Xiang","Bowen Zhao","Ran Zhang","Jingyi Yu","Changxi Zheng"],"pdf_url":"https://arxiv.org/pdf/2310.05391v1.pdf","comment":"Accepted at Pacific Graphics 2023 and Computer Graphics Forum"},{"id":"http://arxiv.org/abs/2305.12653v2","updated":"2023-10-09T03:56:34Z","published":"2023-05-22T02:52:29Z","title":"Estimating Discrete Total Curvature with Per Triangle Normal Variation","summary":"  We introduce a novel approach for measuring the total curvature at every\ntriangle of a discrete surface. This method takes advantage of the relationship\nbetween per triangle total curvature and the Dirichlet energy of the Gauss map.\nThis new tool can be used on both triangle meshes and point clouds and has\nnumerous applications. In this study, we demonstrate the effectiveness of our\ntechnique by using it for feature-aware mesh decimation, and show that it\noutperforms existing curvature-estimation methods from popular libraries such\nas Meshlab, Trimesh2, and Libigl. When estimating curvature on point clouds,\nour method outperforms popular libraries PCL and CGAL.\n","authors":["Crane He Chen"],"pdf_url":"https://arxiv.org/pdf/2305.12653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16940v2","updated":"2023-10-09T03:46:41Z","published":"2023-09-29T02:45:56Z","title":"Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow","summary":"  Collaborative perception can substantially boost each agent's perception\nability by facilitating communication among multiple agents. However, temporal\nasynchrony among agents is inevitable in the real world due to communication\ndelays, interruptions, and clock misalignments. This issue causes information\nmismatch during multi-agent fusion, seriously shaking the foundation of\ncollaboration. To address this issue, we propose CoBEVFlow, an\nasynchrony-robust collaborative perception system based on bird's eye view\n(BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align\nasynchronous collaboration messages sent by multiple agents. To model the\nmotion in a scene, we propose BEV flow, which is a collection of the motion\nvector corresponding to each spatial location. Based on BEV flow, asynchronous\nperceptual features can be reassigned to appropriate positions, mitigating the\nimpact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle\nasynchronous collaboration messages sent at irregular, continuous time stamps\nwithout discretization; and (ii) with BEV flow, CoBEVFlow only transports the\noriginal perceptual features, instead of generating new perceptual features,\navoiding additional noises. To validate CoBEVFlow's efficacy, we create\nIRregular V2V(IRV2V), the first synthetic collaborative perception dataset with\nvarious temporal asynchronies that simulate different real-world scenarios.\nExtensive experiments conducted on both IRV2V and the real-world dataset\nDAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is\nrobust in extremely asynchronous settings. The code is available at\nhttps://github.com/MediaBrain-SJTU/CoBEVFlow.\n","authors":["Sizhe Wei","Yuxi Wei","Yue Hu","Yifan Lu","Yiqi Zhong","Siheng Chen","Ya Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.16940v2.pdf","comment":"16 pages, 9 figures. Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.05383v1","updated":"2023-10-09T03:37:30Z","published":"2023-10-09T03:37:30Z","title":"Three-Stage Cascade Framework for Blurry Video Frame Interpolation","summary":"  Blurry video frame interpolation (BVFI) aims to generate high-frame-rate\nclear videos from low-frame-rate blurry videos, is a challenging but important\ntopic in the computer vision community. Blurry videos not only provide spatial\nand temporal information like clear videos, but also contain additional motion\ninformation hidden in each blurry frame. However, existing BVFI methods usually\nfail to fully leverage all valuable information, which ultimately hinders their\nperformance. In this paper, we propose a simple end-to-end three-stage\nframework to fully explore useful information from blurry videos. The frame\ninterpolation stage designs a temporal deformable network to directly sample\nuseful information from blurry inputs and synthesize an intermediate frame at\nan arbitrary time interval. The temporal feature fusion stage explores the\nlong-term temporal information for each target frame through a bi-directional\nrecurrent deformable alignment network. And the deblurring stage applies a\ntransformer-empowered Taylor approximation network to recursively recover the\nhigh-frequency details. The proposed three-stage framework has clear task\nassignment for each module and offers good expandability, the effectiveness of\nwhich are demonstrated by various experimental results. We evaluate our model\non four benchmarks, including the Adobe240 dataset, GoPro dataset, YouTube240\ndataset and Sony dataset. Quantitative and qualitative results indicate that\nour model outperforms existing SOTA methods. Besides, experiments on real-world\nblurry videos also indicate the good generalization ability of our model.\n","authors":["Pengcheng Lei","Zaoming Yan","Tingting Wang","Faming Fang","Guixu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03602v2","updated":"2023-10-09T03:12:46Z","published":"2023-10-05T15:29:52Z","title":"Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout\n  Constraints","summary":"  Text-driven 3D indoor scene generation could be useful for gaming, film\nindustry, and AR/VR applications. However, existing methods cannot faithfully\ncapture the room layout, nor do they allow flexible editing of individual\nobjects in the room. To address these problems, we present Ctrl-Room, which is\nable to generate convincing 3D rooms with designer-style layouts and\nhigh-fidelity textures from just a text prompt. Moreover, Ctrl-Room enables\nversatile interactive editing operations such as resizing or moving individual\nfurniture items. Our key insight is to separate the modeling of layouts and\nappearance. %how to model the room that takes into account both scene texture\nand geometry at the same time. To this end, Our proposed method consists of two\nstages, a `Layout Generation Stage' and an `Appearance Generation Stage'. The\n`Layout Generation Stage' trains a text-conditional diffusion model to learn\nthe layout distribution with our holistic scene code parameterization. Next,\nthe `Appearance Generation Stage' employs a fine-tuned ControlNet to produce a\nvivid panoramic image of the room guided by the 3D scene layout and text\nprompt. In this way, we achieve a high-quality 3D room with convincing layouts\nand lively textures. Benefiting from the scene code parameterization, we can\neasily edit the generated room model through our mask-guided editing module,\nwithout expensive editing-specific training. Extensive experiments on the\nStructured3D dataset demonstrate that our method outperforms existing methods\nin producing more reasonable, view-consistent, and editable 3D rooms from\nnatural language prompts.\n","authors":["Chuan Fang","Xiaotao Hu","Kunming Luo","Ping Tan"],"pdf_url":"https://arxiv.org/pdf/2310.03602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05375v1","updated":"2023-10-09T03:11:08Z","published":"2023-10-09T03:11:08Z","title":"IPDreamer: Appearance-Controllable 3D Object Generation with Image\n  Prompts","summary":"  Recent advances in text-to-3D generation have been remarkable, with methods\nsuch as DreamFusion leveraging large-scale text-to-image diffusion-based models\nto supervise 3D generation. These methods, including the variational score\ndistillation proposed by ProlificDreamer, enable the synthesis of detailed and\nphotorealistic textured meshes. However, the appearance of 3D objects generated\nby these methods is often random and uncontrollable, posing a challenge in\nachieving appearance-controllable 3D objects. To address this challenge, we\nintroduce IPDreamer, a novel approach that incorporates image prompts to\nprovide specific and comprehensive appearance information for 3D object\ngeneration. Our results demonstrate that IPDreamer effectively generates\nhigh-quality 3D objects that are consistent with both the provided text and\nimage prompts, demonstrating its promising capability in\nappearance-controllable 3D object generation.\n","authors":["Bohan Zeng","Shanglin Li","Yutang Feng","Hong Li","Sicheng Gao","Jiaming Liu","Huaxia Li","Xu Tang","Jianzhuang Liu","Baochang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05375v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.05371v1","updated":"2023-10-09T03:00:15Z","published":"2023-10-09T03:00:15Z","title":"Enhancing Prostate Cancer Diagnosis with Deep Learning: A Study using\n  mpMRI Segmentation and Classification","summary":"  Prostate cancer (PCa) is a severe disease among men globally. It is important\nto identify PCa early and make a precise diagnosis for effective treatment. For\nPCa diagnosis, Multi-parametric magnetic resonance imaging (mpMRI) emerged as\nan invaluable imaging modality that offers a precise anatomical view of the\nprostate gland and its tissue structure. Deep learning (DL) models can enhance\nexisting clinical systems and improve patient care by locating regions of\ninterest for physicians. Recently, DL techniques have been employed to develop\na pipeline for segmenting and classifying different cancer types. These studies\nshow that DL can be used to increase diagnostic precision and give objective\nresults without variability. This work uses well-known DL models for the\nclassification and segmentation of mpMRI images to detect PCa. Our\nimplementation involves four pipelines; Semantic DeepSegNet with ResNet50,\nDeepSegNet with recurrent neural network (RNN), U-Net with RNN, and U-Net with\na long short-term memory (LSTM). Each segmentation model is paired with a\ndifferent classifier to evaluate the performance using different metrics. The\nresults of our experiments show that the pipeline that uses the combination of\nU-Net and the LSTM model outperforms all other combinations, excelling in both\nsegmentation and classification tasks\n","authors":["Anil B. Gavade","Neel Kanwal","Priyanka A. Gavade","Rajendra Nerli"],"pdf_url":"https://arxiv.org/pdf/2310.05371v1.pdf","comment":"Accepted at CISCON-2023"},{"id":"http://arxiv.org/abs/2310.05370v1","updated":"2023-10-09T02:59:21Z","published":"2023-10-09T02:59:21Z","title":"SocialCircle: Learning the Angle-based Social Interaction Representation\n  for Pedestrian Trajectory Prediction","summary":"  Analyzing and forecasting trajectories of agents like pedestrians and cars in\ncomplex scenes has become more and more significant in many intelligent systems\nand applications. The diversity and uncertainty in socially interactive\nbehaviors among a rich variety of agents make this task more challenging than\nother deterministic computer vision tasks. Researchers have made a lot of\nefforts to quantify the effects of these interactions on future trajectories\nthrough different mathematical models and network structures, but this problem\nhas not been well solved. Inspired by marine animals that localize the\npositions of their companions underwater through echoes, we build a new\nanglebased trainable social representation, named SocialCircle, for\ncontinuously reflecting the context of social interactions at different angular\norientations relative to the target agent. We validate the effect of the\nproposed SocialCircle by training it along with several newly released\ntrajectory prediction models, and experiments show that the SocialCircle not\nonly quantitatively improves the prediction performance, but also qualitatively\nhelps better consider social interactions when forecasting pedestrian\ntrajectories in a way that is consistent with human intuitions.\n","authors":["Conghao Wong","Beihao Xia","Xinge You"],"pdf_url":"https://arxiv.org/pdf/2310.05370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05366v1","updated":"2023-10-09T02:52:22Z","published":"2023-10-09T02:52:22Z","title":"Rotation Matters: Generalized Monocular 3D Object Detection for Various\n  Camera Systems","summary":"  Research on monocular 3D object detection is being actively studied, and as a\nresult, performance has been steadily improving. However, 3D object detection\nperformance is significantly reduced when applied to a camera system different\nfrom the system used to capture the training datasets. For example, a 3D\ndetector trained on datasets from a passenger car mostly fails to regress\naccurate 3D bounding boxes for a camera mounted on a bus. In this paper, we\nconduct extensive experiments to analyze the factors that cause performance\ndegradation. We find that changing the camera pose, especially camera\norientation, relative to the road plane caused performance degradation. In\naddition, we propose a generalized 3D object detection method that can be\nuniversally applied to various camera systems. We newly design a compensation\nmodule that corrects the estimated 3D bounding box location and heading\ndirection. The proposed module can be applied to most of the recent 3D object\ndetection networks. It increases AP3D score (KITTI moderate, IoU $> 70\\%$)\nabout 6-to-10-times above the baselines without additional training. Both\nquantitative and qualitative results show the effectiveness of the proposed\nmethod.\n","authors":["SungHo Moon","JinWoo Bae","SungHoon Im"],"pdf_url":"https://arxiv.org/pdf/2310.05366v1.pdf","comment":"Accepted to CVPRw 2023"},{"id":"http://arxiv.org/abs/2302.11713v3","updated":"2023-10-09T02:44:47Z","published":"2023-02-23T00:33:54Z","title":"Can Pre-trained Vision and Language Models Answer Visual\n  Information-Seeking Questions?","summary":"  Pre-trained vision and language models have demonstrated state-of-the-art\ncapabilities over existing tasks involving images and texts, including visual\nquestion answering. However, it remains unclear whether these models possess\nthe capability to answer questions that are not only querying visual content\nbut knowledge-intensive and information-seeking. In this study, we introduce\nInfoSeek, a visual question answering dataset tailored for information-seeking\nquestions that cannot be answered with only common sense knowledge. Using\nInfoSeek, we analyze various pre-trained visual question answering models and\ngain insights into their characteristics. Our findings reveal that\nstate-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)\nface challenges in answering visual information-seeking questions, but\nfine-tuning on the InfoSeek dataset elicits models to use fine-grained\nknowledge that was learned during their pre-training. Furthermore, we show that\naccurate visual entity recognition can be used to improve performance on\nInfoSeek by retrieving relevant documents, showing a significant space for\nimprovement.\n","authors":["Yang Chen","Hexiang Hu","Yi Luan","Haitian Sun","Soravit Changpinyo","Alan Ritter","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2302.11713v3.pdf","comment":"EMNLP 2023 (main conference); Our dataset and evaluation is available\n  at https://open-vision-language.github.io/infoseek/"},{"id":"http://arxiv.org/abs/2310.05355v1","updated":"2023-10-09T02:31:36Z","published":"2023-10-09T02:31:36Z","title":"C^2M-DoT: Cross-modal consistent multi-view medical report generation\n  with domain transfer network","summary":"  In clinical scenarios, multiple medical images with different views are\nusually generated simultaneously, and these images have high semantic\nconsistency. However, most existing medical report generation methods only\nconsider single-view data. The rich multi-view mutual information of medical\nimages can help generate more accurate reports, however, the dependence of\nmulti-view models on multi-view data in the inference stage severely limits\ntheir application in clinical practice. In addition, word-level optimization\nbased on numbers ignores the semantics of reports and medical images, and the\ngenerated reports often cannot achieve good performance. Therefore, we propose\na cross-modal consistent multi-view medical report generation with a domain\ntransfer network (C^2M-DoT). Specifically, (i) a semantic-based multi-view\ncontrastive learning medical report generation framework is adopted to utilize\ncross-view information to learn the semantic representation of lesions; (ii) a\ndomain transfer network is further proposed to ensure that the multi-view\nreport generation model can still achieve good inference performance under\nsingle-view input; (iii) meanwhile, optimization using a cross-modal\nconsistency loss facilitates the generation of textual reports that are\nsemantically consistent with medical images. Extensive experimental studies on\ntwo public benchmark datasets demonstrate that C^2M-DoT substantially\noutperforms state-of-the-art baselines in all metrics. Ablation studies also\nconfirmed the validity and necessity of each component in C^2M-DoT.\n","authors":["Ruizhi Wang","Xiangtao Wang","Jie Zhou","Thomas Lukasiewicz","Zhenghua Xu"],"pdf_url":"https://arxiv.org/pdf/2310.05355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05351v1","updated":"2023-10-09T02:27:04Z","published":"2023-10-09T02:27:04Z","title":"Generalized Neural Collapse for a Large Number of Classes","summary":"  Neural collapse provides an elegant mathematical characterization of learned\nlast layer representations (a.k.a. features) and classifier weights in deep\nclassification models. Such results not only provide insights but also motivate\nnew techniques for improving practical deep models. However, most of the\nexisting empirical and theoretical studies in neural collapse focus on the case\nthat the number of classes is small relative to the dimension of the feature\nspace. This paper extends neural collapse to cases where the number of classes\nare much larger than the dimension of feature space, which broadly occur for\nlanguage models, retrieval systems, and face recognition applications. We show\nthat the features and classifier exhibit a generalized neural collapse\nphenomenon, where the minimum one-vs-rest margins is maximized.We provide\nempirical study to verify the occurrence of generalized neural collapse in\npractical deep neural networks. Moreover, we provide theoretical study to show\nthat the generalized neural collapse provably occurs under unconstrained\nfeature model with spherical constraint, under certain technical conditions on\nfeature dimension and number of classes.\n","authors":["Jiachen Jiang","Jinxin Zhou","Peng Wang","Qing Qu","Dustin Mixon","Chong You","Zhihui Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.05351v1.pdf","comment":"32 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.05347v1","updated":"2023-10-09T02:17:31Z","published":"2023-10-09T02:17:31Z","title":"Infrared Small Target Detection Using Double-Weighted Multi-Granularity\n  Patch Tensor Model With Tensor-Train Decomposition","summary":"  Infrared small target detection plays an important role in the remote sensing\nfields. Therefore, many detection algorithms have been proposed, in which the\ninfrared patch-tensor (IPT) model has become a mainstream tool due to its\nexcellent performance. However, most IPT-based methods face great challenges,\nsuch as inaccurate measure of the tensor low-rankness and poor robustness to\ncomplex scenes, which will leadto poor detection performance. In order to solve\nthese problems, this paper proposes a novel double-weighted multi-granularity\ninfrared patch tensor (DWMGIPT) model. First, to capture different granularity\ninformation of tensor from multiple modes, a multi-granularity infrared patch\ntensor (MGIPT) model is constructed by collecting nonoverlapping patches and\ntensor augmentation based on the tensor train (TT) decomposition. Second, to\nexplore the latent structure of tensor more efficiently, we utilize the\nauto-weighted mechanism to balance the importance of information at different\ngranularity. Then, the steering kernel (SK) is employed to extract local\nstructure prior, which suppresses background interference such as strong edges\nand noise. Finally, an efficient optimization algorithm based on the\nalternating direction method of multipliers (ADMM) is presented to solve the\nmodel. Extensive experiments in various challenging scenes show that the\nproposed algorithm is robust to noise and different scenes. Compared with the\nother eight state-of-the-art methods, different evaluation metrics demonstrate\nthat our method achieves better detection performance in various complex\nscenes.\n","authors":["Guiyu Zhang","Qunbo Lv","Zui Tao","Baoyu Zhu","Zheng Tan","Yuan Ma"],"pdf_url":"https://arxiv.org/pdf/2310.05347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05346v1","updated":"2023-10-09T02:15:45Z","published":"2023-10-09T02:15:45Z","title":"Anyview: Generalizable Indoor 3D Object Detection with Variable Frames","summary":"  In this paper, we propose a novel network framework for indoor 3D object\ndetection to handle variable input frame numbers in practical scenarios.\nExisting methods only consider fixed frames of input data for a single\ndetector, such as monocular RGB-D images or point clouds reconstructed from\ndense multi-view RGB-D images. While in practical application scenes such as\nrobot navigation and manipulation, the raw input to the 3D detectors is the\nRGB-D images with variable frame numbers instead of the reconstructed scene\npoint cloud. However, the previous approaches can only handle fixed frame input\ndata and have poor performance with variable frame input. In order to\nfacilitate 3D object detection methods suitable for practical tasks, we present\na novel 3D detection framework named AnyView for our practical applications,\nwhich generalizes well across different numbers of input frames with a single\nmodel. To be specific, we propose a geometric learner to mine the local\ngeometric features of each input RGB-D image frame and implement local-global\nfeature interaction through a designed spatial mixture module. Meanwhile, we\nfurther utilize a dynamic token strategy to adaptively adjust the number of\nextracted features for each frame, which ensures consistent global feature\ndensity and further enhances the generalization after fusion. Extensive\nexperiments on the ScanNet dataset show our method achieves both great\ngeneralizability and high detection accuracy with a simple and clean\narchitecture containing a similar amount of parameters with the baselines.\n","authors":["Zhenyu Wu","Xiuwei Xu","Ziwei Wang","Chong Xia","Linqing Zhao","Jiwen Lu","Haibin Yan"],"pdf_url":"https://arxiv.org/pdf/2310.05346v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2306.15128v3","updated":"2023-10-09T02:15:22Z","published":"2023-06-27T00:40:12Z","title":"MIMIC: Masked Image Modeling with Image Correspondences","summary":"  Dense pixel-specific representation learning at scale has been bottlenecked\ndue to the unavailability of large-scale multi-view datasets. Current methods\nfor building effective pretraining datasets heavily rely on annotated 3D\nmeshes, point clouds, and camera parameters from simulated environments,\npreventing them from building datasets from real-world data sources where such\nmetadata is lacking. We propose a pretraining dataset-curation approach that\ndoes not require any additional annotations. Our method allows us to generate\nmulti-view datasets from both real-world videos and simulated environments at\nscale. Specifically, we experiment with two scales: MIMIC-1M with 1.3M and\nMIMIC-3M with 3.1M multi-view image pairs. We train multiple models with\ndifferent masked image modeling objectives to showcase the following findings:\nRepresentations trained on our automatically generated MIMIC-3M outperform\nthose learned from expensive crowdsourced datasets (ImageNet-1K) and those\nlearned from synthetic environments (MULTIVIEW-HABITAT) on two dense geometric\ntasks: depth estimation on NYUv2 (1.7%), and surface normals estimation on\nTaskonomy (2.05%). For dense tasks which also require object understanding, we\noutperform MULTIVIEW-HABITAT, on semantic segmentation on ADE20K (3.89%), pose\nestimation on MSCOCO (9.4%), and reduce the gap with models pre-trained on the\nobject-centric expensive ImageNet-1K. We outperform even when the\nrepresentations are frozen, and when downstream training data is limited to\nfew-shot. Larger dataset (MIMIC-3M) significantly improves performance, which\nis promising since our curation method can arbitrarily scale to produce even\nlarger datasets. MIMIC code, dataset, and pretrained models are open-sourced at\nhttps://github.com/RAIVNLab/MIMIC.\n","authors":["Kalyani Marathe","Mahtab Bigverdi","Nishat Khan","Tuhin Kundu","Aniruddha Kembhavi","Linda G. Shapiro","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2306.15128v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15490v2","updated":"2023-10-09T02:14:02Z","published":"2023-09-27T08:39:03Z","title":"Survey on Deep Face Restoration: From Non-blind to Blind and Beyond","summary":"  Face restoration (FR) is a specialized field within image restoration that\naims to recover low-quality (LQ) face images into high-quality (HQ) face\nimages. Recent advances in deep learning technology have led to significant\nprogress in FR methods. In this paper, we begin by examining the prevalent\nfactors responsible for real-world LQ images and introduce degradation\ntechniques used to synthesize LQ images. We also discuss notable benchmarks\ncommonly utilized in the field. Next, we categorize FR methods based on\ndifferent tasks and explain their evolution over time. Furthermore, we explore\nthe various facial priors commonly utilized in the restoration process and\ndiscuss strategies to enhance their effectiveness. In the experimental section,\nwe thoroughly evaluate the performance of state-of-the-art FR methods across\nvarious tasks using a unified benchmark. We analyze their performance from\ndifferent perspectives. Finally, we discuss the challenges faced in the field\nof FR and propose potential directions for future advancements. The open-source\nrepository corresponding to this work can be found at https:// github.com/\n24wenjie-li/ Awesome-Face-Restoration.\n","authors":["Wenjie Li","Mei Wang","Kai Zhang","Juncheng Li","Xiaoming Li","Yuhang Zhang","Guangwei Gao","Weihong Deng","Chia-Wen Lin"],"pdf_url":"https://arxiv.org/pdf/2309.15490v2.pdf","comment":"Face restoration, Survey, Deep learning, Non-blind/Blind, Joint\n  restoration tasks, Facial priors"},{"id":"http://arxiv.org/abs/2309.05257v3","updated":"2023-10-09T02:09:11Z","published":"2023-09-11T06:27:25Z","title":"FusionFormer: A Multi-sensory Fusion in Bird's-Eye-View and Temporal\n  Consistent Transformer for 3D Object Detection","summary":"  Multi-sensor modal fusion has demonstrated strong advantages in 3D object\ndetection tasks. However, existing methods that fuse multi-modal features\nrequire transforming features into the bird's eye view space and may lose\ncertain information on Z-axis, thus leading to inferior performance. To this\nend, we propose a novel end-to-end multi-modal fusion transformer-based\nframework, dubbed FusionFormer, that incorporates deformable attention and\nresidual structures within the fusion encoding module. Specifically, by\ndeveloping a uniform sampling strategy, our method can easily sample from 2D\nimage and 3D voxel features spontaneously, thus exploiting flexible\nadaptability and avoiding explicit transformation to the bird's eye view space\nduring the feature concatenation process. We further implement a residual\nstructure in our feature encoder to ensure the model's robustness in case of\nmissing an input modality. Through extensive experiments on a popular\nautonomous driving benchmark dataset, nuScenes, our method achieves\nstate-of-the-art single model performance of 72.6% mAP and 75.1% NDS in the 3D\nobject detection task without test time augmentation.\n","authors":["Chunyong Hu","Hang Zheng","Kun Li","Jianyun Xu","Weibo Mao","Maochun Luo","Lingxuan Wang","Mingxia Chen","Qihao Peng","Kaixuan Liu","Yiru Zhao","Peihan Hao","Minzhe Liu","Kaicheng Yu"],"pdf_url":"https://arxiv.org/pdf/2309.05257v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05341v1","updated":"2023-10-09T01:59:49Z","published":"2023-10-09T01:59:49Z","title":"A Critical Look at Classic Test-Time Adaptation Methods in Semantic\n  Segmentation","summary":"  Test-time adaptation (TTA) aims to adapt a model, initially trained on\ntraining data, to potential distribution shifts in the test data. Most existing\nTTA studies, however, focus on classification tasks, leaving a notable gap in\nthe exploration of TTA for semantic segmentation. This pronounced emphasis on\nclassification might lead numerous newcomers and engineers to mistakenly assume\nthat classic TTA methods designed for classification can be directly applied to\nsegmentation. Nonetheless, this assumption remains unverified, posing an open\nquestion. To address this, we conduct a systematic, empirical study to disclose\nthe unique challenges of segmentation TTA, and to determine whether classic TTA\nstrategies can effectively address this task. Our comprehensive results have\nled to three key observations. First, the classic batch norm updating strategy,\ncommonly used in classification TTA, only brings slight performance\nimprovement, and in some cases it might even adversely affect the results. Even\nwith the application of advanced distribution estimation techniques like batch\nrenormalization, the problem remains unresolved. Second, the teacher-student\nscheme does enhance training stability for segmentation TTA in the presence of\nnoisy pseudo-labels. However, it cannot directly result in performance\nimprovement compared to the original model without TTA. Third, segmentation TTA\nsuffers a severe long-tailed imbalance problem, which is substantially more\ncomplex than that in TTA for classification. This long-tailed challenge\nsignificantly affects segmentation TTA performance, even when the accuracy of\npseudo-labels is high.\n","authors":["Chang'an Yi","Haotian Chen","Yifan Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05338v1","updated":"2023-10-09T01:52:27Z","published":"2023-10-09T01:52:27Z","title":"Negative Object Presence Evaluation (NOPE) to Measure Object\n  Hallucination in Vision-Language Models","summary":"  Object hallucination poses a significant challenge in vision-language (VL)\nmodels, often leading to the generation of nonsensical or unfaithful responses\nwith non-existent objects. However, the absence of a general measurement for\nevaluating object hallucination in VL models has hindered our understanding and\nability to mitigate this issue. In this work, we present NOPE (Negative Object\nPresence Evaluation), a novel benchmark designed to assess object hallucination\nin VL models through visual question answering (VQA). We propose a\ncost-effective and scalable approach utilizing large language models to\ngenerate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE.\nWe extensively investigate the performance of 10 state-of-the-art VL models in\ndiscerning the non-existence of objects in visual questions, where the ground\ntruth answers are denoted as NegP (e.g., \"none\"). Additionally, we evaluate\ntheir standard performance on visual questions on 9 other VQA datasets. Through\nour experiments, we demonstrate that no VL model is immune to the vulnerability\nof object hallucination, as all models achieve accuracy below 10\\% on NegP.\nFurthermore, we uncover that lexically diverse visual questions, question types\nwith large scopes, and scene-relevant objects capitalize the risk of object\nhallucination in VL models.\n","authors":["Holy Lovenia","Wenliang Dai","Samuel Cahyawijaya","Ziwei Ji","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2310.05338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05337v1","updated":"2023-10-09T01:52:07Z","published":"2023-10-09T01:52:07Z","title":"What do larger image classifiers memorise?","summary":"  The success of modern neural networks has prompted study of the connection\nbetween memorisation and generalisation: overparameterised models generalise\nwell, despite being able to perfectly fit (memorise) completely random labels.\nTo carefully study this issue, Feldman proposed a metric to quantify the degree\nof memorisation of individual training examples, and empirically computed the\ncorresponding memorisation profile of a ResNet on image classification\nbench-marks. While an exciting first glimpse into what real-world models\nmemorise, this leaves open a fundamental question: do larger neural models\nmemorise more? We present a comprehensive empirical analysis of this question\non image classification benchmarks. We find that training examples exhibit an\nunexpectedly diverse set of memorisation trajectories across model sizes: most\nsamples experience decreased memorisation under larger models, while the rest\nexhibit cap-shaped or increasing memorisation. We show that various proxies for\nthe Feldman memorization score fail to capture these fundamental trends.\nLastly, we find that knowledge distillation, an effective and popular model\ncompression technique, tends to inhibit memorisation, while also improving\ngeneralisation. Specifically, memorisation is mostly inhibited on examples with\nincreasing memorisation trajectories, thus pointing at how distillation\nimproves generalisation.\n","authors":["Michal Lukasik","Vaishnavh Nagarajan","Ankit Singh Rawat","Aditya Krishna Menon","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2310.05337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05336v1","updated":"2023-10-09T01:44:06Z","published":"2023-10-09T01:44:06Z","title":"GReAT: A Graph Regularized Adversarial Training Method","summary":"  This paper proposes a regularization method called GReAT, Graph Regularized\nAdversarial Training, to improve deep learning models' classification\nperformance. Adversarial examples are a well-known challenge in machine\nlearning, where small, purposeful perturbations to input data can mislead\nmodels. Adversarial training, a powerful and one of the most effective defense\nstrategies, involves training models with both regular and adversarial\nexamples. However, it often neglects the underlying structure of the data. In\nresponse, we propose GReAT, a method that leverages data graph structure to\nenhance model robustness. GReAT deploys the graph structure of the data into\nthe adversarial training process, resulting in more robust models that better\ngeneralize its testing performance and defend against adversarial attacks.\nThrough extensive evaluation on benchmark datasets, we demonstrate GReAT's\neffectiveness compared to state-of-the-art classification methods, highlighting\nits potential in improving deep learning models' classification performance.\n","authors":["Samet Bayram","Kenneth Barner"],"pdf_url":"https://arxiv.org/pdf/2310.05336v1.pdf","comment":"25 pages including references. 7 figures and 4 tables"},{"id":"http://arxiv.org/abs/2310.05330v1","updated":"2023-10-09T01:23:08Z","published":"2023-10-09T01:23:08Z","title":"A Lightweight Video Anomaly Detection Model with Weak Supervision and\n  Adaptive Instance Selection","summary":"  Video anomaly detection is to determine whether there are any abnormal\nevents, behaviors or objects in a given video, which enables effective and\nintelligent public safety management. As video anomaly labeling is both\ntime-consuming and expensive, most existing works employ unsupervised or weakly\nsupervised learning methods. This paper focuses on weakly supervised video\nanomaly detection, in which the training videos are labeled whether or not they\ncontain any anomalies, but there is no information about which frames the\nanomalies are located. However, the uncertainty of weakly labeled data and the\nlarge model size prevent existing methods from wide deployment in real\nscenarios, especially the resource-limit situations such as edge-computing. In\nthis paper, we develop a lightweight video anomaly detection model. On the one\nhand, we propose an adaptive instance selection strategy, which is based on the\nmodel's current status to select confident instances, thereby mitigating the\nuncertainty of weakly labeled data and subsequently promoting the model's\nperformance. On the other hand, we design a lightweight multi-level temporal\ncorrelation attention module and an hourglass-shaped fully connected layer to\nconstruct the model, which can reduce the model parameters to only 0.56\\% of\nthe existing methods (e.g. RTFM). Our extensive experiments on two public\ndatasets UCF-Crime and ShanghaiTech show that our model can achieve comparable\nor even superior AUC score compared to the state-of-the-art methods, with a\nsignificantly reduced number of model parameters.\n","authors":["Yang Wang","Jiaogen Zhou","Jihong Guan"],"pdf_url":"https://arxiv.org/pdf/2310.05330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08851v3","updated":"2023-10-09T01:15:32Z","published":"2023-05-15T17:59:15Z","title":"MV-Map: Offboard HD-Map Generation with Multi-view Consistency","summary":"  While bird's-eye-view (BEV) perception models can be useful for building\nhigh-definition maps (HD-Maps) with less human labor, their results are often\nunreliable and demonstrate noticeable inconsistencies in the predicted HD-Maps\nfrom different viewpoints. This is because BEV perception is typically set up\nin an 'onboard' manner, which restricts the computation and consequently\nprevents algorithms from reasoning multiple views simultaneously. This paper\novercomes these limitations and advocates a more practical 'offboard' HD-Map\ngeneration setup that removes the computation constraints, based on the fact\nthat HD-Maps are commonly reusable infrastructures built offline in data\ncenters. To this end, we propose a novel offboard pipeline called MV-Map that\ncapitalizes multi-view consistency and can handle an arbitrary number of frames\nwith the key design of a 'region-centric' framework. In MV-Map, the target\nHD-Maps are created by aggregating all the frames of onboard predictions,\nweighted by the confidence scores assigned by an 'uncertainty network'. To\nfurther enhance multi-view consistency, we augment the uncertainty network with\nthe global 3D structure optimized by a voxelized neural radiance field\n(Voxel-NeRF). Extensive experiments on nuScenes show that our MV-Map\nsignificantly improves the quality of HD-Maps, further highlighting the\nimportance of offboard methods for HD-Map generation.\n","authors":["Ziyang Xie","Ziqi Pang","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2305.08851v3.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2310.05321v1","updated":"2023-10-09T00:55:41Z","published":"2023-10-09T00:55:41Z","title":"Edge Computing-Enabled Road Condition Monitoring: System Development and\n  Evaluation","summary":"  Real-time pavement condition monitoring provides highway agencies with timely\nand accurate information that could form the basis of pavement maintenance and\nrehabilitation policies. Existing technologies rely heavily on manual data\nprocessing, are expensive and therefore, difficult to scale for frequent,\nnetworklevel pavement condition monitoring. Additionally, these systems require\nsending large packets of data to the cloud which requires large storage space,\nare computationally expensive to process, and results in high latency. The\ncurrent study proposes a solution that capitalizes on the widespread\navailability of affordable Micro Electro-Mechanical System (MEMS) sensors, edge\ncomputing and internet connection capabilities of microcontrollers, and\ndeployable machine learning (ML) models to (a) design an Internet of Things\n(IoT)-enabled device that can be mounted on axles of vehicles to stream live\npavement condition data (b) reduce latency through on-device processing and\nanalytics of pavement condition sensor data before sending to the cloud\nservers. In this study, three ML models including Random Forest, LightGBM and\nXGBoost were trained to predict International Roughness Index (IRI) at every\n0.1-mile segment. XGBoost had the highest accuracy with an RMSE and MAPE of\n16.89in/mi and 20.3%, respectively. In terms of the ability to classify the IRI\nof pavement segments based on ride quality according to MAP-21 criteria, our\nproposed device achieved an average accuracy of 96.76% on I-70EB and 63.15% on\nSouth Providence. Overall, our proposed device demonstrates significant\npotential in providing real-time pavement condition data to State Highway\nAgencies (SHA) and Department of Transportation (DOTs) with a satisfactory\nlevel of accuracy.\n","authors":["Abdulateef Daud","Mark Amo-Boateng","Neema Jakisa Owor","Armstrong Aboah","Yaw Adu-Gyamfi"],"pdf_url":"https://arxiv.org/pdf/2310.05321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05316v1","updated":"2023-10-09T00:17:20Z","published":"2023-10-09T00:17:20Z","title":"Understanding the Feature Norm for Out-of-Distribution Detection","summary":"  A neural network trained on a classification dataset often exhibits a higher\nvector norm of hidden layer features for in-distribution (ID) samples, while\nproducing relatively lower norm values on unseen instances from\nout-of-distribution (OOD). Despite this intriguing phenomenon being utilized in\nmany applications, the underlying cause has not been thoroughly investigated.\nIn this study, we demystify this very phenomenon by scrutinizing the\ndiscriminative structures concealed in the intermediate layers of a neural\nnetwork. Our analysis leads to the following discoveries: (1) The feature norm\nis a confidence value of a classifier hidden in the network layer, specifically\nits maximum logit. Hence, the feature norm distinguishes OOD from ID in the\nsame manner that a classifier confidence does. (2) The feature norm is\nclass-agnostic, thus it can detect OOD samples across diverse discriminative\nmodels. (3) The conventional feature norm fails to capture the deactivation\ntendency of hidden layer neurons, which may lead to misidentification of ID\nsamples as OOD instances. To resolve this drawback, we propose a novel\nnegative-aware norm (NAN) that can capture both the activation and deactivation\ntendencies of hidden layer neurons. We conduct extensive experiments on NAN,\ndemonstrating its efficacy and compatibility with existing OOD detectors, as\nwell as its capability in label-free environments.\n","authors":["Jaewoo Park","Jacky Chen Long Chai","Jaeho Yoon","Andrew Beng Jin Teoh"],"pdf_url":"https://arxiv.org/pdf/2310.05316v1.pdf","comment":"Accepted to ICCV2023"},{"id":"http://arxiv.org/abs/2310.06196v1","updated":"2023-10-09T22:52:43Z","published":"2023-10-09T22:52:43Z","title":"DiPS: Discriminative Pseudo-Label Sampling with Self-Supervised\n  Transformers for Weakly Supervised Object Localization","summary":"  Self-supervised vision transformers (SSTs) have shown great potential to\nyield rich localization maps that highlight different objects in an image.\nHowever, these maps remain class-agnostic since the model is unsupervised. They\noften tend to decompose the image into multiple maps containing different\nobjects while being unable to distinguish the object of interest from\nbackground noise objects. In this paper, Discriminative Pseudo-label Sampling\n(DiPS) is introduced to leverage these class-agnostic maps for\nweakly-supervised object localization (WSOL), where only image-class labels are\navailable. Given multiple attention maps, DiPS relies on a pre-trained\nclassifier to identify the most discriminative regions of each attention map.\nThis ensures that the selected ROIs cover the correct image object while\ndiscarding the background ones, and, as such, provides a rich pool of diverse\nand discriminative proposals to cover different parts of the object.\nSubsequently, these proposals are used as pseudo-labels to train our new\ntransformer-based WSOL model designed to perform classification and\nlocalization tasks. Unlike standard WSOL methods, DiPS optimizes performance in\nboth tasks by using a transformer encoder and a dedicated output head for each\ntask, each trained using dedicated loss functions. To avoid overfitting a\nsingle proposal and promote better object coverage, a single proposal is\nrandomly selected among the top ones for a training image at each training\nstep. Experimental results on the challenging CUB, ILSVRC, OpenImages, and\nTelDrone datasets indicate that our architecture, in combination with our\ntransformer-based proposals, can yield better localization performance than\nstate-of-the-art methods.\n","authors":["Shakeeb Murtaza","Soufiane Belharbi","Marco Pedersoli","Aydin Sarraf","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2310.06196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.12361v2","updated":"2023-10-09T21:03:24Z","published":"2022-02-24T20:56:29Z","title":"RescueNet: A High Resolution UAV Semantic Segmentation Benchmark Dataset\n  for Natural Disaster Damage Assessment","summary":"  Recent advancements in computer vision and deep learning techniques have\nfacilitated notable progress in scene understanding, thereby assisting rescue\nteams in achieving precise damage assessment. In this paper, we present\nRescueNet, a meticulously curated high-resolution post-disaster dataset that\nincludes detailed classification and semantic segmentation annotations. This\ndataset aims to facilitate comprehensive scene understanding in the aftermath\nof natural disasters. RescueNet comprises post-disaster images collected after\nHurricane Michael, obtained using Unmanned Aerial Vehicles (UAVs) from multiple\nimpacted regions. The uniqueness of RescueNet lies in its provision of\nhigh-resolution post-disaster imagery, accompanied by comprehensive annotations\nfor each image. Unlike existing datasets that offer annotations limited to\nspecific scene elements such as buildings, RescueNet provides pixel-level\nannotations for all classes, including buildings, roads, pools, trees, and\nmore. Furthermore, we evaluate the utility of the dataset by implementing\nstate-of-the-art segmentation models on RescueNet, demonstrating its value in\nenhancing existing methodologies for natural disaster damage assessment.\n","authors":["Maryam Rahnemoonfar","Tashnim Chowdhury","Robin Murphy"],"pdf_url":"https://arxiv.org/pdf/2202.12361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06143v1","updated":"2023-10-09T20:45:29Z","published":"2023-10-09T20:45:29Z","title":"HydraViT: Adaptive Multi-Branch Transformer for Multi-Label Disease\n  Classification from Chest X-ray Images","summary":"  Chest X-ray is an essential diagnostic tool in the identification of chest\ndiseases given its high sensitivity to pathological abnormalities in the lungs.\nHowever, image-driven diagnosis is still challenging due to heterogeneity in\nsize and location of pathology, as well as visual similarities and\nco-occurrence of separate pathology. Since disease-related regions often occupy\na relatively small portion of diagnostic images, classification models based on\ntraditional convolutional neural networks (CNNs) are adversely affected given\ntheir locality bias. While CNNs were previously augmented with attention maps\nor spatial masks to guide focus on potentially critical regions, learning\nlocalization guidance under heterogeneity in the spatial distribution of\npathology is challenging. To improve multi-label classification performance,\nhere we propose a novel method, HydraViT, that synergistically combines a\ntransformer backbone with a multi-branch output module with learned weighting.\nThe transformer backbone enhances sensitivity to long-range context in X-ray\nimages, while using the self-attention mechanism to adaptively focus on\ntask-critical regions. The multi-branch output module dedicates an independent\nbranch to each disease label to attain robust learning across separate disease\nclasses, along with an aggregated branch across labels to maintain sensitivity\nto co-occurrence relationships among pathology. Experiments demonstrate that,\non average, HydraViT outperforms competing attention-guided methods by 1.2%,\nregion-guided methods by 1.4%, and semantic-guided methods by 1.0% in\nmulti-label classification performance.\n","authors":["Şaban Öztürk","M. Yiğit Turalı","Tolga Çukur"],"pdf_url":"https://arxiv.org/pdf/2310.06143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06138v1","updated":"2023-10-09T20:32:49Z","published":"2023-10-09T20:32:49Z","title":"Layout Sequence Prediction From Noisy Mobile Modality","summary":"  Trajectory prediction plays a vital role in understanding pedestrian movement\nfor applications such as autonomous driving and robotics. Current trajectory\nprediction models depend on long, complete, and accurately observed sequences\nfrom visual modalities. Nevertheless, real-world situations often involve\nobstructed cameras, missed objects, or objects out of sight due to\nenvironmental factors, leading to incomplete or noisy trajectories. To overcome\nthese limitations, we propose LTrajDiff, a novel approach that treats objects\nobstructed or out of sight as equally important as those with fully visible\ntrajectories. LTrajDiff utilizes sensor data from mobile phones to surmount\nout-of-sight constraints, albeit introducing new challenges such as modality\nfusion, noisy data, and the absence of spatial layout and object size\ninformation. We employ a denoising diffusion model to predict precise layout\nsequences from noisy mobile data using a coarse-to-fine diffusion strategy,\nincorporating the RMS, Siamese Masked Encoding Module, and MFM. Our model\npredicts layout sequences by implicitly inferring object size and projection\nstatus from a single reference timestamp or significantly obstructed sequences.\nAchieving SOTA results in randomly obstructed experiments and extremely short\ninput experiments, our model illustrates the effectiveness of leveraging noisy\nmobile data. In summary, our approach offers a promising solution to the\nchallenges faced by layout sequence and trajectory prediction models in\nreal-world settings, paving the way for utilizing sensor data from mobile\nphones to accurately predict pedestrian bounding box trajectories. To the best\nof our knowledge, this is the first work that addresses severely obstructed and\nextremely short layout sequences by combining vision with noisy mobile\nmodality, making it the pioneering work in the field of layout sequence\ntrajectory prediction.\n","authors":["Haichao Zhang","Yi Xu","Hongsheng Lu","Takayuki Shimizu","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2310.06138v1.pdf","comment":"In Proceedings of the 31st ACM International Conference on Multimedia\n  2023 (MM 23)"},{"id":"http://arxiv.org/abs/2307.16368v2","updated":"2023-10-09T20:12:08Z","published":"2023-07-31T02:14:19Z","title":"AntGPT: Can Large Language Models Help Long-term Action Anticipation\n  from Videos?","summary":"  Can we better anticipate an actor's future actions (e.g. mix eggs) by knowing\nwhat commonly happens after his/her current action (e.g. crack eggs)? What if\nwe also know the longer-term goal of the actor (e.g. making egg fried rice)?\nThe long-term action anticipation (LTA) task aims to predict an actor's future\nbehavior from video observations in the form of verb and noun sequences, and it\nis crucial for human-machine interaction. We propose to formulate the LTA task\nfrom two perspectives: a bottom-up approach that predicts the next actions\nautoregressively by modeling temporal dynamics; and a top-down approach that\ninfers the goal of the actor and plans the needed procedure to accomplish the\ngoal. We hypothesize that large language models (LLMs), which have been\npretrained on procedure text data (e.g. recipes, how-tos), have the potential\nto help LTA from both perspectives. It can help provide the prior knowledge on\nthe possible next actions, and infer the goal given the observed part of a\nprocedure, respectively. To leverage the LLMs, we propose a two-stage\nframework, AntGPT. It first recognizes the actions already performed in the\nobserved videos and then asks an LLM to predict the future actions via\nconditioned generation, or to infer the goal and plan the whole procedure by\nchain-of-thought prompting. Empirical results on the Ego4D LTA v1 and v2\nbenchmarks, EPIC-Kitchens-55, as well as EGTEA GAZE+ demonstrate the\neffectiveness of our proposed approach. AntGPT achieves state-of-the-art\nperformance on all above benchmarks, and can successfully infer the goal and\nthus perform goal-conditioned \"counterfactual\" prediction via qualitative\nanalysis. Code and model will be released at\nhttps://brown-palm.github.io/AntGPT\n","authors":["Qi Zhao","Shijie Wang","Ce Zhang","Changcheng Fu","Minh Quan Do","Nakul Agarwal","Kwonjoon Lee","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2307.16368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06124v1","updated":"2023-10-09T19:59:59Z","published":"2023-10-09T19:59:59Z","title":"Factorized Tensor Networks for Multi-Task and Multi-Domain Learning","summary":"  Multi-task and multi-domain learning methods seek to learn multiple\ntasks/domains, jointly or one after another, using a single unified network.\nThe key challenge and opportunity is to exploit shared information across tasks\nand domains to improve the efficiency of the unified network. The efficiency\ncan be in terms of accuracy, storage cost, computation, or sample complexity.\nIn this paper, we propose a factorized tensor network (FTN) that can achieve\naccuracy comparable to independent single-task/domain networks with a small\nnumber of additional parameters. FTN uses a frozen backbone network from a\nsource model and incrementally adds task/domain-specific low-rank tensor\nfactors to the shared frozen network. This approach can adapt to a large number\nof target domains and tasks without catastrophic forgetting. Furthermore, FTN\nrequires a significantly smaller number of task-specific parameters compared to\nexisting methods. We performed experiments on widely used multi-domain and\nmulti-task datasets. We show the experiments on convolutional-based\narchitecture with different backbones and on transformer-based architecture. We\nobserved that FTN achieves similar accuracy as single-task/domain methods while\nusing only a fraction of additional parameters per task.\n","authors":["Yash Garg","Nebiyou Yismaw","Rakib Hyder","Ashley Prater-Bennette","M. Salman Asif"],"pdf_url":"https://arxiv.org/pdf/2310.06124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06123v1","updated":"2023-10-09T19:57:24Z","published":"2023-10-09T19:57:24Z","title":"Text-driven Prompt Generation for Vision-Language Models in Federated\n  Learning","summary":"  Prompt learning for vision-language models, e.g., CoOp, has shown great\nsuccess in adapting CLIP to different downstream tasks, making it a promising\nsolution for federated learning due to computational reasons. Existing prompt\nlearning techniques replace hand-crafted text prompts with learned vectors that\noffer improvements on seen classes, but struggle to generalize to unseen\nclasses. Our work addresses this challenge by proposing Federated Text-driven\nPrompt Generation (FedTPG), which learns a unified prompt generation network\nacross multiple remote clients in a scalable manner. The prompt generation\nnetwork is conditioned on task-related text input, thus is context-aware,\nmaking it suitable to generalize for both seen and unseen classes. Our\ncomprehensive empirical evaluations on nine diverse image classification\ndatasets show that our method is superior to existing federated prompt learning\nmethods, that achieve overall better generalization on both seen and unseen\nclasses and is also generalizable to unseen datasets.\n","authors":["Chen Qiu","Xingyu Li","Chaithanya Kumar Mummadi","Madan Ravi Ganesh","Zhenzhen Li","Lu Peng","Wan-Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2310.06123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06109v1","updated":"2023-10-09T19:33:06Z","published":"2023-10-09T19:33:06Z","title":"QR-Tag: Angular Measurement and Tracking with a QR-Design Marker","summary":"  Directional information measurement has many applications in domains such as\nrobotics, virtual and augmented reality, and industrial computer vision.\nConventional methods either require pre-calibration or necessitate controlled\nenvironments. The state-of-the-art MoireTag approach exploits the Moire effect\nand QR-design to continuously track the angular shift precisely. However, it is\nstill not a fully QR code design. To overcome the above challenges, we propose\na novel snapshot method for discrete angular measurement and tracking with\nscannable QR-design patterns that are generated by binary structures printed on\nboth sides of a glass plate. The QR codes, resulting from the parallax effect\ndue to the geometry alignment between two layers, can be readily measured as\nangular information using a phone camera. The simulation results show that the\nproposed non-contact object tracking framework is computationally efficient\nwith high accuracy.\n","authors":["Simeng Qiu","Hadi Amata","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2310.06109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06107v1","updated":"2023-10-09T19:27:02Z","published":"2023-10-09T19:27:02Z","title":"Developing and Refining a Multifunctional Facial Recognition System for\n  Older Adults with Cognitive Impairments: A Journey Towards Enhanced Quality\n  of Life","summary":"  In an era where the global population is aging significantly, cognitive\nimpairments among the elderly have become a major health concern. The need for\neffective assistive technologies is clear, and facial recognition systems are\nemerging as promising tools to address this issue. This document discusses the\ndevelopment and evaluation of a new Multifunctional Facial Recognition System\n(MFRS), designed specifically to assist older adults with cognitive\nimpairments. The MFRS leverages face_recognition [1], a powerful open-source\nlibrary capable of extracting, identifying, and manipulating facial features.\nOur system integrates the face recognition and retrieval capabilities of\nface_recognition, along with additional functionalities to capture images and\nrecord voice memos. This combination of features notably enhances the system's\nusability and versatility, making it a more user-friendly and universally\napplicable tool for end-users. The source code for this project can be accessed\nat https://github.com/Li-8023/Multi-function-face-recognition.git.\n","authors":["Li He"],"pdf_url":"https://arxiv.org/pdf/2310.06107v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2305.00087v2","updated":"2023-10-09T19:21:35Z","published":"2023-04-28T20:38:58Z","title":"Inverse Consistency by Construction for Multistep Deep Registration","summary":"  Inverse consistency is a desirable property for image registration. We\npropose a simple technique to make a neural registration network inverse\nconsistent by construction, as a consequence of its structure, as long as it\nparameterizes its output transform by a Lie group. We extend this technique to\nmulti-step neural registration by composing many such networks in a way that\npreserves inverse consistency. This multi-step approach also allows for\ninverse-consistent coarse to fine registration. We evaluate our technique on\nsynthetic 2-D data and four 3-D medical image registration tasks and obtain\nexcellent registration accuracy while assuring inverse consistency.\n","authors":["Hastings Greer","Lin Tian","Francois-Xavier Vialard","Roland Kwitt","Sylvain Bouix","Raul San Jose Estepar","Richard Rushmore","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2305.00087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.04297v2","updated":"2023-10-09T19:09:12Z","published":"2022-04-08T21:12:13Z","title":"Learning to Modulate Random Weights: Neuromodulation-inspired Neural\n  Networks For Efficient Continual Learning","summary":"  Existing Continual Learning (CL) approaches have focused on addressing\ncatastrophic forgetting by leveraging regularization methods, replay buffers,\nand task-specific components. However, realistic CL solutions must be shaped\nnot only by metrics of catastrophic forgetting but also by computational\nefficiency and running time. Here, we introduce a novel neural network\narchitecture inspired by neuromodulation in biological nervous systems to\neconomically and efficiently address catastrophic forgetting and provide new\navenues for interpreting learned representations. Neuromodulation is a\nbiological mechanism that has received limited attention in machine learning;\nit dynamically controls and fine tunes synaptic dynamics in real time to track\nthe demands of different behavioral contexts. Inspired by this, our proposed\narchitecture learns a relatively small set of parameters per task context that\n\\emph{neuromodulates} the activity of unchanging, randomized weights that\ntransform the input. We show that this approach has strong learning performance\nper task despite the very small number of learnable parameters. Furthermore,\nbecause context vectors are so compact, multiple networks can be stored\nconcurrently with no interference and little spatial footprint, thus completely\neliminating catastrophic forgetting and accelerating the training process.\n","authors":["Jinyung Hong","Theodore P. Pavlic"],"pdf_url":"https://arxiv.org/pdf/2204.04297v2.pdf","comment":"13 pages, 13 figures, 5 tables"},{"id":"http://arxiv.org/abs/2309.08021v2","updated":"2023-10-09T19:00:13Z","published":"2023-09-14T20:34:30Z","title":"Vision-based Analysis of Driver Activity and Driving Performance Under\n  the Influence of Alcohol","summary":"  About 30% of all traffic crash fatalities in the United States involve drunk\ndrivers, making the prevention of drunk driving paramount to vehicle safety in\nthe US and other locations which have a high prevalence of driving while under\nthe influence of alcohol. Driving impairment can be monitored through active\nuse of sensors (when drivers are asked to engage in providing breath samples to\na vehicle instrument or when pulled over by a police officer), but a more\npassive and robust mechanism of sensing may allow for wider adoption and\nbenefit of intelligent systems that reduce drunk driving accidents. This could\nassist in identifying impaired drivers before they drive, or early in the\ndriving process (before a crash or detection by law enforcement). In this\nresearch, we introduce a study which adopts a multi-modal ensemble of visual,\nthermal, audio, and chemical sensors to (1) examine the impact of acute alcohol\nadministration on driving performance in a driving simulator, and (2) identify\ndata-driven methods for detecting driving under the influence of alcohol. We\ndescribe computer vision and machine learning models for analyzing the driver's\nface in thermal imagery, and introduce a pipeline for training models on data\ncollected from drivers with a range of breath-alcohol content levels, including\ndiscussion of relevant machine learning phenomena which can help in future\nexperiment design for related studies.\n","authors":["Ross Greer","Akshay Gopalkrishnan","Sumega Mandadi","Pujitha Gunaratne","Mohan M. Trivedi","Thomas D. Marcotte"],"pdf_url":"https://arxiv.org/pdf/2309.08021v2.pdf","comment":"Withdrawn at the request of industry research collaborators, per\n  contract agreement"},{"id":"http://arxiv.org/abs/2306.04642v2","updated":"2023-10-09T18:58:24Z","published":"2023-05-25T11:59:28Z","title":"DiffusionShield: A Watermark for Copyright Protection against Generative\n  Diffusion Models","summary":"  Recently, Generative Diffusion Models (GDMs) have showcased their remarkable\ncapabilities in learning and generating images. A large community of GDMs has\nnaturally emerged, further promoting the diversified applications of GDMs in\nvarious fields. However, this unrestricted proliferation has raised serious\nconcerns about copyright protection. For example, artists including painters\nand photographers are becoming increasingly concerned that GDMs could\neffortlessly replicate their unique creative works without authorization. In\nresponse to these challenges, we introduce a novel watermarking scheme,\nDiffusionShield, tailored for GDMs. DiffusionShield protects images from\ncopyright infringement by GDMs through encoding the ownership information into\nan imperceptible watermark and injecting it into the images. Its watermark can\nbe easily learned by GDMs and will be reproduced in their generated images. By\ndetecting the watermark from generated images, copyright infringement can be\nexposed with evidence. Benefiting from the uniformity of the watermarks and the\njoint optimization method, DiffusionShield ensures low distortion of the\noriginal image, high watermark detection performance, and the ability to embed\nlengthy messages. We conduct rigorous and comprehensive experiments to show the\neffectiveness of DiffusionShield in defending against infringement by GDMs and\nits superiority over traditional watermarking methods.\n","authors":["Yingqian Cui","Jie Ren","Han Xu","Pengfei He","Hui Liu","Lichao Sun","Yue Xing","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2306.04642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06080v1","updated":"2023-10-09T18:38:49Z","published":"2023-10-09T18:38:49Z","title":"Advancing Diagnostic Precision: Leveraging Machine Learning Techniques\n  for Accurate Detection of Covid-19, Pneumonia, and Tuberculosis in Chest\n  X-Ray Images","summary":"  Lung diseases such as COVID-19, tuberculosis (TB), and pneumonia continue to\nbe serious global health concerns that affect millions of people worldwide. In\nmedical practice, chest X-ray examinations have emerged as the norm for\ndiagnosing diseases, particularly chest infections such as COVID-19. Paramedics\nand scientists are working intensively to create a reliable and precise\napproach for early-stage COVID-19 diagnosis in order to save lives. But with a\nvariety of symptoms, medical diagnosis of these disorders poses special\ndifficulties. It is essential to address their identification and timely\ndiagnosis in order to successfully treat and prevent these illnesses. In this\nresearch, a multiclass classification approach using state-of-the-art methods\nfor deep learning and image processing is proposed. This method takes into\naccount the robustness and efficiency of the system in order to increase\ndiagnostic precision of chest diseases. A comparison between a brand-new\nconvolution neural network (CNN) and several transfer learning pre-trained\nmodels including VGG19, ResNet, DenseNet, EfficientNet, and InceptionNet is\nrecommended. Publicly available and widely used research datasets like Shenzen,\nMontogomery, the multiclass Kaggle dataset and the NIH dataset were used to\nrigorously test the model. Recall, precision, F1-score, and Area Under Curve\n(AUC) score are used to evaluate and compare the performance of the proposed\nmodel. An AUC value of 0.95 for COVID-19, 0.99 for TB, and 0.98 for pneumonia\nis obtained using the proposed network. Recall and precision ratings of 0.95,\n0.98, and 0.97, respectively, likewise met high standards.\n","authors":["Aditya Kulkarni","Guruprasad Parasnis","Harish Balasubramanian","Vansh Jain","Anmol Chokshi","Reena Sonkusare"],"pdf_url":"https://arxiv.org/pdf/2310.06080v1.pdf","comment":"11 pages, 18 figures, Under review in Discover Artificial\n  Intelligence Journal by Springer Nature"},{"id":"http://arxiv.org/abs/2310.06068v1","updated":"2023-10-09T18:19:51Z","published":"2023-10-09T18:19:51Z","title":"Augmenting Vision-Based Human Pose Estimation with Rotation Matrix","summary":"  Fitness applications are commonly used to monitor activities within the gym,\nbut they often fail to automatically track indoor activities inside the gym.\nThis study proposes a model that utilizes pose estimation combined with a novel\ndata augmentation method, i.e., rotation matrix. We aim to enhance the\nclassification accuracy of activity recognition based on pose estimation data.\nThrough our experiments, we experiment with different classification algorithms\nalong with image augmentation approaches. Our findings demonstrate that the SVM\nwith SGD optimization, using data augmentation with the Rotation Matrix, yields\nthe most accurate results, achieving a 96% accuracy rate in classifying five\nphysical activities. Conversely, without implementing the data augmentation\ntechniques, the baseline accuracy remains at a modest 64%.\n","authors":["Milad Vazan","Fatemeh Sadat Masoumi","Ruizhi Ou","Reza Rawassizadeh"],"pdf_url":"https://arxiv.org/pdf/2310.06068v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2310.06020v1","updated":"2023-10-09T18:00:01Z","published":"2023-10-09T18:00:01Z","title":"DyST: Towards Dynamic Neural Scene Representations on Real-World Videos","summary":"  Visual understanding of the world goes beyond the semantics and flat\nstructure of individual images. In this work, we aim to capture both the 3D\nstructure and dynamics of real-world scenes from monocular real-world videos.\nOur Dynamic Scene Transformer (DyST) model leverages recent work in neural\nscene representation to learn a latent decomposition of monocular real-world\nvideos into scene content, per-view scene dynamics, and camera pose. This\nseparation is achieved through a novel co-training scheme on monocular videos\nand our new synthetic dataset DySO. DyST learns tangible latent representations\nfor dynamic scenes that enable view generation with separate control over the\ncamera and the content of the scene.\n","authors":["Maximilian Seitzer","Sjoerd van Steenkiste","Thomas Kipf","Klaus Greff","Mehdi S. M. Sajjadi"],"pdf_url":"https://arxiv.org/pdf/2310.06020v1.pdf","comment":"Project website: https://dyst-paper.github.io/"},{"id":"http://arxiv.org/abs/2310.06008v1","updated":"2023-10-09T17:52:26Z","published":"2023-10-09T17:52:26Z","title":"CoBEVFusion: Cooperative Perception with LiDAR-Camera Bird's-Eye View\n  Fusion","summary":"  Autonomous Vehicles (AVs) use multiple sensors to gather information about\ntheir surroundings. By sharing sensor data between Connected Autonomous\nVehicles (CAVs), the safety and reliability of these vehicles can be improved\nthrough a concept known as cooperative perception. However, recent approaches\nin cooperative perception only share single sensor information such as cameras\nor LiDAR. In this research, we explore the fusion of multiple sensor data\nsources and present a framework, called CoBEVFusion, that fuses LiDAR and\ncamera data to create a Bird's-Eye View (BEV) representation. The CAVs process\nthe multi-modal data locally and utilize a Dual Window-based Cross-Attention\n(DWCA) module to fuse the LiDAR and camera features into a unified BEV\nrepresentation. The fused BEV feature maps are shared among the CAVs, and a 3D\nConvolutional Neural Network is applied to aggregate the features from the\nCAVs. Our CoBEVFusion framework was evaluated on the cooperative perception\ndataset OPV2V for two perception tasks: BEV semantic segmentation and 3D object\ndetection. The results show that our DWCA LiDAR-camera fusion model outperforms\nperception models with single-modal data and state-of-the-art BEV fusion\nmodels. Our overall cooperative perception architecture, CoBEVFusion, also\nachieves comparable performance with other cooperative perception models.\n","authors":["Donghao Qiao","Farhana Zulkernine"],"pdf_url":"https://arxiv.org/pdf/2310.06008v1.pdf","comment":null}]},"2023-10-08T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2309.13224v2","updated":"2023-10-08T23:22:33Z","published":"2023-09-23T00:26:49Z","title":"Pick Planning Strategies for Large-Scale Package Manipulation","summary":"  Automating warehouse operations can reduce logistics overhead costs,\nultimately driving down the final price for consumers, increasing the speed of\ndelivery, and enhancing the resiliency to market fluctuations.\n  This extended abstract showcases a large-scale package manipulation from\nunstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which is\nused for picking and singulating up to 6 million packages per day and so far\nhas manipulated over 2 billion packages. It describes the various heuristic\nmethods developed over time and their successor, which utilizes a pick success\npredictor trained on real production data.\n  To the best of the authors' knowledge, this work is the first large-scale\ndeployment of learned pick quality estimation methods in a real production\nsystem.\n","authors":["Shuai Li","Azarakhsh Keipour","Kevin Jamieson","Nicolas Hudson","Sicong Zhao","Charles Swan","Kostas Bekris"],"pdf_url":"https://arxiv.org/pdf/2309.13224v2.pdf","comment":"2023 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), Learning Meets Model-based Methods for Manipulation and\n  Grasping Workshop"},{"id":"http://arxiv.org/abs/2310.05290v1","updated":"2023-10-08T21:32:30Z","published":"2023-10-08T21:32:30Z","title":"MSight: An Edge-Cloud Infrastructure-based Perception System for\n  Connected Automated Vehicles","summary":"  As vehicular communication and networking technologies continue to advance,\ninfrastructure-based roadside perception emerges as a pivotal tool for\nconnected automated vehicle (CAV) applications. Due to their elevated\npositioning, roadside sensors, including cameras and lidars, often enjoy\nunobstructed views with diminished object occlusion. This provides them a\ndistinct advantage over onboard perception, enabling more robust and accurate\ndetection of road objects. This paper presents MSight, a cutting-edge roadside\nperception system specifically designed for CAVs. MSight offers real-time\nvehicle detection, localization, tracking, and short-term trajectory\nprediction. Evaluations underscore the system's capability to uphold lane-level\naccuracy with minimal latency, revealing a range of potential applications to\nenhance CAV safety and efficiency. Presently, MSight operates 24/7 at a\ntwo-lane roundabout in the City of Ann Arbor, Michigan.\n","authors":["Rusheng Zhang","Depu Meng","Shengyin Shen","Zhengxia Zou","Houqiang Li","Henry X. Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05290v1.pdf","comment":"Submitted to IEEE T-ITS"},{"id":"http://arxiv.org/abs/2309.16118v2","updated":"2023-10-08T21:17:51Z","published":"2023-09-28T02:50:16Z","title":"D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable\n  Robotic Manipulation","summary":"  Scene representation has been a crucial design choice in robotic manipulation\nsystems. An ideal representation should be 3D, dynamic, and semantic to meet\nthe demands of diverse manipulation tasks. However, previous works often lack\nall three properties simultaneously. In this work, we introduce D$^3$Fields -\ndynamic 3D descriptor fields. These fields capture the dynamics of the\nunderlying 3D environment and encode both semantic features and instance masks.\nSpecifically, we project arbitrary 3D points in the workspace onto multi-view\n2D visual observations and interpolate features derived from foundational\nmodels. The resulting fused descriptor fields allow for flexible goal\nspecifications using 2D images with varied contexts, styles, and instances. To\nevaluate the effectiveness of these descriptor fields, we apply our\nrepresentation to a wide range of robotic manipulation tasks in a zero-shot\nmanner. Through extensive evaluation in both real-world scenarios and\nsimulations, we demonstrate that D$^3$Fields are both generalizable and\neffective for zero-shot robotic manipulation tasks. In quantitative comparisons\nwith state-of-the-art dense descriptors, such as Dense Object Nets and DINO,\nD$^3$Fields exhibit significantly better generalization abilities and\nmanipulation accuracy.\n","authors":["Yixuan Wang","Zhuoran Li","Mingtong Zhang","Katherine Driggs-Campbell","Jiajun Wu","Li Fei-Fei","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2309.16118v2.pdf","comment":"Project Page: https://robopil.github.io/d3fields/"},{"id":"http://arxiv.org/abs/2310.05266v1","updated":"2023-10-08T19:33:01Z","published":"2023-10-08T19:33:01Z","title":"DELTAHANDS: A Synergistic Dexterous Hand Framework Based on Delta Robots","summary":"  Dexterous robotic manipulation in unstructured environments can aid in\neveryday tasks such as cleaning and caretaking. Anthropomorphic robotic hands\nare highly dexterous and theoretically well-suited for working in human\ndomains, but their complex designs and dynamics often make them difficult to\ncontrol. By contrast, parallel-jaw grippers are easy to control and are used\nextensively in industrial applications, but they lack the dexterity for various\nkinds of grasps and in-hand manipulations. In this work, we present DELTAHANDS,\na synergistic dexterous hand framework with Delta robots. The DELTAHANDS are\nsoft, easy to reconfigure, simple to manufacture with low-cost off-the-shelf\nmaterials, and possess high degrees of freedom that can be easily controlled.\nDELTAHANDS' dexterity can be adjusted for different applications by leveraging\nactuation synergies, which can further reduce the control complexity, overall\ncost, and energy consumption. We characterize the Delta robots' kinematics\naccuracy, force profiles, and workspace range to assist with hand design.\nFinally, we evaluate the versatility of DELTAHANDS by grasping a diverse set of\nobjects and by using teleoperation to complete three dexterous manipulation\ntasks: cloth folding, cap opening, and cable arrangement.\n","authors":["Zilin Si","Kevin Zhang","Oliver Kroemer","F. Zeynep Temel"],"pdf_url":"https://arxiv.org/pdf/2310.05266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05245v1","updated":"2023-10-08T17:37:32Z","published":"2023-10-08T17:37:32Z","title":"Influence of Camera-LiDAR Configuration on 3D Object Detection for\n  Autonomous Driving","summary":"  Cameras and LiDARs are both important sensors for autonomous driving, playing\ncritical roles for 3D object detection. Camera-LiDAR Fusion has been a\nprevalent solution for robust and accurate autonomous driving perception. In\ncontrast to the vast majority of existing arts that focus on how to improve the\nperformance of 3D target detection through cross-modal schemes, deep learning\nalgorithms, and training tricks, we devote attention to the impact of sensor\nconfigurations on the performance of learning-based methods. To achieve this,\nwe propose a unified information-theoretic surrogate metric for camera and\nLiDAR evaluation based on the proposed sensor perception model. We also design\nan accelerated high-quality framework for data acquisition, model training, and\nperformance evaluation that functions with the CARLA simulator. To show the\ncorrelation between detection performance and our surrogate metrics, We conduct\nexperiments using several camera-LiDAR placements and parameters inspired by\nself-driving companies and research institutions. Extensive experimental\nresults of representative algorithms on NuScenes dataset validate the\neffectiveness of our surrogate metric, demonstrating that sensor configurations\nsignificantly impact point-cloud-image fusion based detection models, which\ncontribute up to 30% discrepancy in terms of average precision.\n","authors":["Ye Li","Hanjiang Hu","Zuxin Liu","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.05245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05239v1","updated":"2023-10-08T17:13:01Z","published":"2023-10-08T17:13:01Z","title":"LAN-grasp: Using Large Language Models for Semantic Object Grasping","summary":"  In this paper, we propose LAN-grasp, a novel approach towards more\nappropriate semantic grasping. We use foundation models to provide the robot\nwith a deeper understanding of the objects, the right place to grasp an object,\nor even the parts to avoid. This allows our robot to grasp and utilize objects\nin a more meaningful and safe manner. We leverage the combination of a Large\nLanguage Model, a Vision Language Model, and a traditional grasp planner to\ngenerate grasps demonstrating a deeper semantic understanding of the objects.\nWe first prompt the Large Language Model about which object part is appropriate\nfor grasping. Next, the Vision Language Model identifies the corresponding part\nin the object image. Finally, we generate grasp proposals in the region\nproposed by the Vision Language Model. Building on foundation models provides\nus with a zero-shot grasp method that can handle a wide range of objects\nwithout the need for further training or fine-tuning. We evaluated our method\nin real-world experiments on a custom object data set. We present the results\nof a survey that asks the participants to choose an object part appropriate for\ngrasping. The results show that the grasps generated by our method are\nconsistently ranked higher by the participants than those generated by a\nconventional grasping planner and a recent semantic grasping approach.\n","authors":["Reihaneh Mirjalili","Michael Krawez","Simone Silenzi","Yannik Blei","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2310.05239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16772v3","updated":"2023-10-08T16:32:45Z","published":"2023-09-28T18:09:40Z","title":"XVO: Generalized Visual Odometry via Cross-Modal Self-Training","summary":"  We propose XVO, a semi-supervised learning method for training generalized\nmonocular Visual Odometry (VO) models with robust off-the-self operation across\ndiverse datasets and settings. In contrast to standard monocular VO approaches\nwhich often study a known calibration within a single dataset, XVO efficiently\nlearns to recover relative pose with real-world scale from visual scene\nsemantics, i.e., without relying on any known camera parameters. We optimize\nthe motion estimation model via self-training from large amounts of\nunconstrained and heterogeneous dash camera videos available on YouTube. Our\nkey contribution is twofold. First, we empirically demonstrate the benefits of\nsemi-supervised training for learning a general-purpose direct VO regression\nnetwork. Second, we demonstrate multi-modal supervision, including\nsegmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate\ngeneralized representations for the VO task. Specifically, we find audio\nprediction task to significantly enhance the semi-supervised learning process\nwhile alleviating noisy pseudo-labels, particularly in highly dynamic and\nout-of-domain video data. Our proposed teacher network achieves\nstate-of-the-art performance on the commonly used KITTI benchmark despite no\nmulti-frame optimization or knowledge of camera parameters. Combined with the\nproposed semi-supervised step, XVO demonstrates off-the-shelf knowledge\ntransfer across diverse conditions on KITTI, nuScenes, and Argoverse without\nfine-tuning.\n","authors":["Lei Lai","Zhongkai Shangguan","Jimuyang Zhang","Eshed Ohn-Bar"],"pdf_url":"https://arxiv.org/pdf/2309.16772v3.pdf","comment":"ICCV 2023, Paris https://genxvo.github.io/"},{"id":"http://arxiv.org/abs/2302.04823v3","updated":"2023-10-08T15:09:29Z","published":"2023-02-09T18:21:29Z","title":"Hierarchical Generative Adversarial Imitation Learning with Mid-level\n  Input Generation for Autonomous Driving on Urban Environments","summary":"  Deriving robust control policies for realistic urban navigation scenarios is\nnot a trivial task. In an end-to-end approach, these policies must map\nhigh-dimensional images from the vehicle's cameras to low-level actions such as\nsteering and throttle. While pure Reinforcement Learning (RL) approaches are\nbased exclusively on rewards,Generative Adversarial Imitation Learning (GAIL)\nagents learn from expert demonstrations while interacting with the environment,\nwhich favors GAIL on tasks for which a reward signal is difficult to derive. In\nthis work, the hGAIL architecture was proposed to solve the autonomous\nnavigation of a vehicle in an end-to-end approach, mapping sensory perceptions\ndirectly to low-level actions, while simultaneously learning mid-level input\nrepresentations of the agent's environment. The proposed hGAIL consists of an\nhierarchical Adversarial Imitation Learning architecture composed of two main\nmodules: the GAN (Generative Adversarial Nets) which generates the Bird's-Eye\nView (BEV) representation mainly from the images of three frontal cameras of\nthe vehicle, and the GAIL which learns to control the vehicle based mainly on\nthe BEV predictions from the GAN as input.Our experiments have shown that GAIL\nexclusively from cameras (without BEV) fails to even learn the task, while\nhGAIL, after training, was able to autonomously navigate successfully in all\nintersections of the city.\n","authors":["Gustavo Claudio Karl Couto","Eric Aislan Antonelo"],"pdf_url":"https://arxiv.org/pdf/2302.04823v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05198v1","updated":"2023-10-08T15:08:28Z","published":"2023-10-08T15:08:28Z","title":"Indoor Localization for an Autonomous Model Car: A Marker-Based\n  Multi-Sensor Fusion Framework","summary":"  Global navigation satellite systems readily provide accurate position\ninformation when localizing a robot outdoors. However, an analogous standard\nsolution does not exist yet for mobile robots operating indoors. This paper\npresents an integrated framework for indoor localization and experimental\nvalidation of an autonomous driving system based on an advanced\ndriver-assistance system (ADAS) model car. The global pose of the model car is\nobtained by fusing information from fiducial markers, inertial sensors and\nwheel odometry. In order to achieve robust localization, we investigate and\ncompare two extensions to the Extended Kalman Filter; first with adaptive noise\ntuning and second with Chi-squared test for measurement outlier detection. An\nefficient and low-cost ground truth measurement method using a single LiDAR\nsensor is also proposed to validate the results. The performance of the\nlocalization algorithms is tested on a complete autonomous driving system with\ntrajectory planning and model predictive control.\n","authors":["Xibo Li","Shruti Patel","David Stronzek-Pfeifer","Christof Büskens"],"pdf_url":"https://arxiv.org/pdf/2310.05198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05170v1","updated":"2023-10-08T13:59:43Z","published":"2023-10-08T13:59:43Z","title":"DeepQTest: Testing Autonomous Driving Systems with Reinforcement\n  Learning and Real-world Weather Data","summary":"  Autonomous driving systems (ADSs) are capable of sensing the environment and\nmaking driving decisions autonomously. These systems are safety-critical, and\ntesting them is one of the important approaches to ensure their safety.\nHowever, due to the inherent complexity of ADSs and the high dimensionality of\ntheir operating environment, the number of possible test scenarios for ADSs is\ninfinite. Besides, the operating environment of ADSs is dynamic, continuously\nevolving, and full of uncertainties, which requires a testing approach adaptive\nto the environment. In addition, existing ADS testing techniques have limited\neffectiveness in ensuring the realism of test scenarios, especially the realism\nof weather conditions and their changes over time. Recently, reinforcement\nlearning (RL) has demonstrated great potential in addressing challenging\nproblems, especially those requiring constant adaptations to dynamic\nenvironments. To this end, we present DeepQTest, a novel ADS testing approach\nthat uses RL to learn environment configurations with a high chance of\nrevealing abnormal ADS behaviors. Specifically, DeepQTest employs Deep\nQ-Learning and adopts three safety and comfort measures to construct the reward\nfunctions. To ensure the realism of generated scenarios, DeepQTest defines a\nset of realistic constraints and introduces real-world weather conditions into\nthe simulated environment. We employed three comparison baselines, i.e.,\nrandom, greedy, and a state-of-the-art RL-based approach DeepCOllision, for\nevaluating DeepQTest on an industrial-scale ADS. Evaluation results show that\nDeepQTest demonstrated significantly better effectiveness in terms of\ngenerating scenarios leading to collisions and ensuring scenario realism\ncompared with the baselines. In addition, among the three reward functions\nimplemented in DeepQTest, Time-To-Collision is recommended as the best design\naccording to our study.\n","authors":["Chengjie Lu","Tao Yue","Man Zhang","Shaukat Ali"],"pdf_url":"https://arxiv.org/pdf/2310.05170v1.pdf","comment":"40 pages, 7 figures, 13 tables"},{"id":"http://arxiv.org/abs/2310.01412v2","updated":"2023-10-08T13:47:23Z","published":"2023-10-02T17:59:52Z","title":"DriveGPT4: Interpretable End-to-end Autonomous Driving via Large\n  Language Model","summary":"  In the past decade, autonomous driving has experienced rapid development in\nboth academia and industry. However, its limited interpretability remains a\nsignificant unsolved problem, severely hindering autonomous vehicle\ncommercialization and further development. Previous approaches utilizing small\nlanguage models have failed to address this issue due to their lack of\nflexibility, generalization ability, and robustness. Recently, multimodal large\nlanguage models (LLMs) have gained considerable attention from the research\ncommunity for their capability to process and reason non-text data (e.g.,\nimages and videos) by text. In this paper, we present DriveGPT4, an\ninterpretable end-to-end autonomous driving system utilizing LLMs. DriveGPT4 is\ncapable of interpreting vehicle actions and providing corresponding reasoning,\nas well as answering diverse questions posed by human users for enhanced\ninteraction. Additionally, DriveGPT4 predicts vehicle low-level control signals\nin an end-to-end fashion. These capabilities stem from a customized visual\ninstruction tuning dataset specifically designed for autonomous driving. To the\nbest of our knowledge, DriveGPT4 is the first work focusing on interpretable\nend-to-end autonomous driving. When evaluated on multiple tasks alongside\nconventional methods and video understanding LLMs, DriveGPT4 demonstrates\nsuperior qualitative and quantitative performance. Additionally, DriveGPT4 can\nbe generalized in a zero-shot fashion to accommodate more unseen scenarios. The\nproject page is available at https://tonyxuqaq.github.io/projects/DriveGPT4/ .\n","authors":["Zhenhua Xu","Yujia Zhang","Enze Xie","Zhen Zhao","Yong Guo","Kwan-Yee. K. Wong","Zhenguo Li","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.01412v2.pdf","comment":"The project page is available at\n  https://tonyxuqaq.github.io/projects/DriveGPT4/"},{"id":"http://arxiv.org/abs/2305.15729v3","updated":"2023-10-08T13:46:05Z","published":"2023-05-25T05:24:04Z","title":"Accelerated K-Serial Stable Coalition for Dynamic Capture and Resource\n  Defense","summary":"  Coalition is an important mean of multi-robot systems to collaborate on\ncommon tasks. An adaptive coalition strategy is essential for the online\nperformance in dynamic and unknown environments. In this work, the problem of\nterritory defense by large-scale heterogeneous robotic teams is considered. The\ntasks include exploration, capture of dynamic targets, and perimeter defense\nover valuable resources. Since each robot can choose among many tasks, it\nremains a challenging problem to coordinate jointly these robots such that the\noverall utility is maximized. This work proposes a generic coalition strategy\ncalled K-serial stable coalition algorithm. Different from centralized\napproaches, it is distributed and complete, meaning that only local\ncommunication is required and a K-serial Stable solution is ensured.\nFurthermore, to accelerate adaptation to dynamic targets and resource\ndistribution that are only perceived online, a heterogeneous graph attention\nnetwork based heuristic is learned to select more appropriate parameters and\npromising initial solutions during local optimization. Compared with manual\nheuristics or end-to-end predictors, it is shown to both improve online\nadaptability and retain the quality guarantee. The proposed methods are\nvalidated via large-scale simulations with 170 robots and hardware experiments\nof 13 robots, against several strong baselines such as GreedyNE and FastMaxSum.\n","authors":["Junfeng Chen","Zili Tang","Meng Guo"],"pdf_url":"https://arxiv.org/pdf/2305.15729v3.pdf","comment":"8 pages, 10 figures, 1 table"},{"id":"http://arxiv.org/abs/2308.08543v3","updated":"2023-10-08T13:33:58Z","published":"2023-08-16T17:58:28Z","title":"InsightMapper: A Closer Look at Inner-instance Information for\n  Vectorized High-Definition Mapping","summary":"  Vectorized high-definition (HD) maps contain detailed information about\nsurrounding road elements, which are crucial for various downstream tasks in\nmodern autonomous driving vehicles, such as vehicle planning and control.\nRecent works have attempted to directly detect the vectorized HD map as a point\nset prediction task, resulting in significant improvements in detection\nperformance. However, these approaches fail to analyze and exploit the\ninner-instance correlations between predicted points, impeding further\nadvancements. To address these challenges, we investigate the utilization of\ninner-$\\textbf{INS}$tance information for vectorized h$\\textbf{IGH}$-definition\nmapping through $\\textbf{T}$ransformers and introduce InsightMapper. This paper\npresents three novel designs within InsightMapper that leverage inner-instance\ninformation in distinct ways, including hybrid query generation, inner-instance\nquery fusion, and inner-instance feature aggregation. Comparative experiments\nare conducted on the NuScenes dataset, showcasing the superiority of our\nproposed method. InsightMapper surpasses previous state-of-the-art (SOTA)\nmethods by 5.78 mAP and 5.12 TOPO, which assess topology correctness.\nSimultaneously, InsightMapper maintains high efficiency during both training\nand inference phases, resulting in remarkable comprehensive performance. The\nproject page for this work is available at\nhttps://tonyxuqaq.github.io/InsightMapper/ .\n","authors":["Zhenhua Xu","Kwan-Yee. K. Wong","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.08543v3.pdf","comment":"Code and demo will be available at\n  https://tonyxuqaq.github.io/InsightMapper/"},{"id":"http://arxiv.org/abs/2310.05134v1","updated":"2023-10-08T11:54:25Z","published":"2023-10-08T11:54:25Z","title":"LocoNeRF: A NeRF-based Approach for Local Structure from Motion for\n  Precise Localization","summary":"  Visual localization is a critical task in mobile robotics, and researchers\nare continuously developing new approaches to enhance its efficiency. In this\narticle, we propose a novel approach to improve the accuracy of visual\nlocalization using Structure from Motion (SfM) techniques. We highlight the\nlimitations of global SfM, which suffers from high latency, and the challenges\nof local SfM, which requires large image databases for accurate reconstruction.\nTo address these issues, we propose utilizing Neural Radiance Fields (NeRF), as\nopposed to image databases, to cut down on the space required for storage. We\nsuggest that sampling reference images around the prior query position can lead\nto further improvements. We evaluate the accuracy of our proposed method\nagainst ground truth obtained using LIDAR and Advanced Lidar Odometry and\nMapping in Real-time (A-LOAM), and compare its storage usage against local SfM\nwith COLMAP in the conducted experiments. Our proposed method achieves an\naccuracy of 0.068 meters compared to the ground truth, which is slightly lower\nthan the most advanced method COLMAP, which has an accuracy of 0.022 meters.\nHowever, the size of the database required for COLMAP is 400 megabytes, whereas\nthe size of our NeRF model is only 160 megabytes. Finally, we perform an\nablation study to assess the impact of using reference images from the NeRF\nreconstruction.\n","authors":["Artem Nenashev","Mikhail Kurenkov","Andrei Potapov","Iana Zhura","Maksim Katerishich","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2310.05134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05041v1","updated":"2023-10-08T06:51:05Z","published":"2023-10-08T06:51:05Z","title":"An Anomaly Behavior Analysis Framework for Securing Autonomous Vehicle\n  Perception","summary":"  As a rapidly growing cyber-physical platform, Autonomous Vehicles (AVs) are\nencountering more security challenges as their capabilities continue to expand.\nIn recent years, adversaries are actively targeting the perception sensors of\nautonomous vehicles with sophisticated attacks that are not easily detected by\nthe vehicles' control systems. This work proposes an Anomaly Behavior Analysis\napproach to detect a perception sensor attack against an autonomous vehicle.\nThe framework relies on temporal features extracted from a physics-based\nautonomous vehicle behavior model to capture the normal behavior of vehicular\nperception in autonomous driving. By employing a combination of model-based\ntechniques and machine learning algorithms, the proposed framework\ndistinguishes between normal and abnormal vehicular perception behavior. To\ndemonstrate the application of the framework in practice, we performed a depth\ncamera attack experiment on an autonomous vehicle testbed and generated an\nextensive dataset. We validated the effectiveness of the proposed framework\nusing this real-world data and released the dataset for public access. To our\nknowledge, this dataset is the first of its kind and will serve as a valuable\nresource for the research community in evaluating their intrusion detection\ntechniques effectively.\n","authors":["Murad Mehrab Abrar","Salim Hariri"],"pdf_url":"https://arxiv.org/pdf/2310.05041v1.pdf","comment":"20th ACS/IEEE International Conference on Computer Systems and\n  Applications (Accepted for publication)"},{"id":"http://arxiv.org/abs/2309.16426v2","updated":"2023-10-08T05:54:47Z","published":"2023-09-28T13:23:23Z","title":"QwenGrasp: A Usage of Large Vision-Language Model for Target-Oriented\n  Grasping","summary":"  Target-oriented grasping in unstructured scenes with language control is\nessential for intelligent robot arm grasping. The ability for the robot arm to\nunderstand the human language and execute corresponding grasping actions is a\npivotal challenge. In this paper, we propose a combination model called\nQwenGrasp which combines a large vision-language model with a 6-DoF grasp\nneural network. QwenGrasp is able to conduct a 6-DoF grasping task on the\ntarget object with textual language instruction. We design a complete\nexperiment with six-dimension instructions to test the QwenGrasp when facing\nwith different cases. The results show that QwenGrasp has a superior ability to\ncomprehend the human intention. Even in the face of vague instructions with\ndescriptive words or instructions with direction information, the target object\ncan be grasped accurately. When QwenGrasp accepts the instruction which is not\nfeasible or not relevant to the grasping task, our approach has the ability to\nsuspend the task execution and provide a proper feedback to humans, improving\nthe safety. In conclusion, with the great power of large vision-language model,\nQwenGrasp can be applied in the open language environment to conduct the\ntarget-oriented grasping task with freely input instructions.\n","authors":["Xinyu Chen","Jian Yang","Zonghan He","Haobin Yang","Qi Zhao","Yuhui Shi"],"pdf_url":"https://arxiv.org/pdf/2309.16426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05022v1","updated":"2023-10-08T05:48:30Z","published":"2023-10-08T05:48:30Z","title":"Fully Spiking Neural Network for Legged Robots","summary":"  In recent years, legged robots based on deep reinforcement learning have made\nremarkable progress. Quadruped robots have demonstrated the ability to complete\nchallenging tasks in complex environments and have been deployed in real-world\nscenarios to assist humans. Simultaneously, bipedal and humanoid robots have\nachieved breakthroughs in various demanding tasks. Current reinforcement\nlearning methods can utilize diverse robot bodies and historical information to\nperform actions. However, prior research has not emphasized the speed and\nenergy consumption of network inference, as well as the biological significance\nof the neural networks themselves. Most of the networks employed are\ntraditional artificial neural networks that utilize multilayer perceptrons\n(MLP). In this paper, we successfully apply a novel Spiking Neural Network\n(SNN) to process legged robots, achieving outstanding results across a range of\nsimulated terrains. SNN holds a natural advantage over traditional neural\nnetworks in terms of inference speed and energy consumption, and their\npulse-form processing of body perception signals offers improved biological\ninterpretability. To the best of our knowledge, this is the first work to\nimplement SNN in legged robots.\n","authors":["Xiaoyang Jiang","Qiang Zhang","Jingkai Sun","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2310.05022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06305v2","updated":"2023-10-08T04:39:27Z","published":"2023-03-11T04:42:28Z","title":"Reducing Non-IID Effects in Federated Autonomous Driving with\n  Contrastive Divergence Loss","summary":"  Federated learning has been widely applied in autonomous driving since it\nenables training a learning model among vehicles without sharing users' data.\nHowever, data from autonomous vehicles usually suffer from the\nnon-independent-and-identically-distributed (non-IID) problem, which may cause\nnegative effects on the convergence of the learning process. In this paper, we\npropose a new contrastive divergence loss to address the non-IID problem in\nautonomous driving by reducing the impact of divergence factors from\ntransmitted models during the local learning process of each silo. We also\nanalyze the effects of contrastive divergence in various autonomous driving\nscenarios, under multiple network infrastructures, and with different\ncentralized/distributed learning schemes. Our intensive experiments on three\ndatasets demonstrate that our proposed contrastive divergence loss\nsignificantly improves the performance over current state-of-the-art\napproaches.\n","authors":["Tuong Do","Binh X. Nguyen","Hien Nguyen","Erman Tjiputra","Quang D. Tran","Te-Chuan Chiu","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2303.06305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04979v1","updated":"2023-10-08T03:03:59Z","published":"2023-10-08T03:03:59Z","title":"Initial Task Assignment in Multi-Human Multi-Robot Teams: An\n  Attention-enhanced Hierarchical Reinforcement Learning Approach","summary":"  Multi-human multi-robot teams (MH-MR) obtain tremendous potential in tackling\nintricate and massive missions by merging distinct strengths and expertise of\nindividual members. The inherent heterogeneity of these teams necessitates\nadvanced initial task assignment (ITA) methods that align tasks with the\nintrinsic capabilities of team members from the outset. While existing\nreinforcement learning approaches show encouraging results, they might fall\nshort in addressing the nuances of long-horizon ITA problems, particularly in\nsettings with large-scale MH-MR teams or multifaceted tasks. To bridge this\ngap, we propose an attention-enhanced hierarchical reinforcement learning\napproach that decomposes the complex ITA problem into structured sub-problems,\nfacilitating more efficient allocations. To bolster sub-policy learning, we\nintroduce a hierarchical cross-attribute attention (HCA) mechanism, encouraging\neach sub-policy within the hierarchy to discern and leverage the specific\nnuances in the state space that are crucial for its respective decision-making\nphase. Through an extensive environmental surveillance case study, we\ndemonstrate the benefits of our model and the HCA inside.\n","authors":["Ruiqi Wang","Dezhong Zhao","Arjun Gupte","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2310.04979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13821v3","updated":"2023-10-08T01:28:41Z","published":"2022-09-28T04:06:45Z","title":"Online Multi Camera-IMU Calibration","summary":"  Visual-inertial navigation systems are powerful in their ability to\naccurately estimate localization of mobile systems within complex environments\nthat preclude the use of global navigation satellite systems. However, these\nnavigation systems are reliant on accurate and up-to-date temporospatial\ncalibrations of the sensors being used. As such, online estimators for these\nparameters are useful in resilient systems. This paper presents an extension to\nexisting Kalman Filter based frameworks for estimating and calibrating the\nextrinsic parameters of multi-camera IMU systems. In addition to extending the\nfilter framework to include multiple camera sensors, the measurement model was\nreformulated to make use of measurement data that is typically made available\nin fiducial detection software. A secondary filter layer was used to estimate\ntime translation parameters without closed-loop feedback of sensor data.\nExperimental calibration results, including the use of cameras with\nnon-overlapping fields of view, were used to validate the stability and\naccuracy of the filter formulation when compared to offline methods. Finally\nthe generalized filter code has been open-sourced and is available online.\n","authors":["Jacob Hartzer","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2209.13821v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.14106v2","updated":"2023-10-08T01:21:32Z","published":"2021-04-29T04:53:33Z","title":"Vehicular Teamwork: Collaborative localization of Autonomous Vehicles","summary":"  This paper develops a distributed collaborative localization algorithm based\non an extended kalman filter. This algorithm incorporates Ultra-Wideband (UWB)\nmeasurements for vehicle to vehicle ranging, and shows improvements in\nlocalization accuracy where GPS typically falls short. The algorithm was first\ntested in a newly created open-source simulation environment that emulates\nvarious numbers of vehicles and sensors while simultaneously testing multiple\nlocalization algorithms. Predicted error distributions for various algorithms\nare quickly producible using the Monte-Carlo method and optimization techniques\nwithin MatLab. The simulation results were validated experimentally in an\noutdoor, urban environment. Improvements of localization accuracy over a\ntypical extended kalman filter ranged from 2.9% to 9.3% over 180 meter test\nruns. When GPS was denied, these improvements increased up to 83.3% over a\nstandard kalman filter. In both simulation and experimentally, the DCL\nalgorithm was shown to be a good approximation of a full state filter, while\nreducing required communication between vehicles. These results are promising\nin showing the efficacy of adding UWB ranging sensors to cars for collaborative\nand landmark localization, especially in GPS-denied environments. In the\nfuture, additional moving vehicles with additional tags will be tested in other\nchallenging GPS denied environments.\n","authors":["Jacob Hartzer","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2104.14106v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2304.05538v4","updated":"2023-10-08T23:03:33Z","published":"2023-04-11T23:55:50Z","title":"ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of\n  Zoom and Spatial Biases in Image Classification","summary":"  Image classifiers are information-discarding machines, by design. Yet, how\nthese models discard information remains mysterious. We hypothesize that one\nway for image classifiers to reach high accuracy is to first zoom to the most\ndiscriminative region in the image and then extract features from there to\npredict image labels, discarding the rest of the image. Studying six popular\nnetworks ranging from AlexNet to CLIP, we find that proper framing of the input\nimage can lead to the correct classification of 98.91% of ImageNet images.\nFurthermore, we uncover positional biases in various datasets, especially a\nstrong center bias in two popular datasets: ImageNet-A and ObjectNet. Finally,\nleveraging our insights into the potential of zooming, we propose a test-time\naugmentation (TTA) technique that improves classification accuracy by forcing\nmodels to explicitly perform zoom-in operations before making predictions. Our\nmethod is more interpretable, accurate, and faster than MEMO, a\nstate-of-the-art (SOTA) TTA method. We introduce ImageNet-Hard, a new benchmark\nthat challenges SOTA classifiers including large vision-language models even\nwhen optimal zooming is allowed.\n","authors":["Mohammad Reza Taesiri","Giang Nguyen","Sarra Habchi","Cor-Paul Bezemer","Anh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2304.05538v4.pdf","comment":"NeurIPS 2023 Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2310.05306v1","updated":"2023-10-08T22:58:31Z","published":"2023-10-08T22:58:31Z","title":"Progressive Neural Compression for Adaptive Image Offloading under\n  Timing Constraints","summary":"  IoT devices are increasingly the source of data for machine learning (ML)\napplications running on edge servers. Data transmissions from devices to\nservers are often over local wireless networks whose bandwidth is not just\nlimited but, more importantly, variable. Furthermore, in cyber-physical systems\ninteracting with the physical environment, image offloading is also commonly\nsubject to timing constraints. It is, therefore, important to develop an\nadaptive approach that maximizes the inference performance of ML applications\nunder timing constraints and the resource constraints of IoT devices. In this\npaper, we use image classification as our target application and propose\nprogressive neural compression (PNC) as an efficient solution to this problem.\nAlthough neural compression has been used to compress images for different ML\napplications, existing solutions often produce fixed-size outputs that are\nunsuitable for timing-constrained offloading over variable bandwidth. To\naddress this limitation, we train a multi-objective rateless autoencoder that\noptimizes for multiple compression rates via stochastic taildrop to create a\ncompression solution that produces features ordered according to their\nimportance to inference performance. Features are then transmitted in that\norder based on available bandwidth, with classification ultimately performed\nusing the (sub)set of features received by the deadline. We demonstrate the\nbenefits of PNC over state-of-the-art neural compression approaches and\ntraditional compression methods on a testbed comprising an IoT device and an\nedge server connected over a wireless network with varying bandwidth.\n","authors":["Ruiqi Wang","Hanyang Liu","Jiaming Qiu","Moran Xu","Roch Guerin","Chenyang Lu"],"pdf_url":"https://arxiv.org/pdf/2310.05306v1.pdf","comment":"IEEE the 44th Real-Time System Symposium (RTSS), 2023"},{"id":"http://arxiv.org/abs/2310.05304v1","updated":"2023-10-08T22:48:30Z","published":"2023-10-08T22:48:30Z","title":"GestSync: Determining who is speaking without a talking head","summary":"  In this paper we introduce a new synchronisation task, Gesture-Sync:\ndetermining if a person's gestures are correlated with their speech or not. In\ncomparison to Lip-Sync, Gesture-Sync is far more challenging as there is a far\nlooser relationship between the voice and body movement than there is between\nvoice and lip motion. We introduce a dual-encoder model for this task, and\ncompare a number of input representations including RGB frames, keypoint\nimages, and keypoint vectors, assessing their performance and advantages. We\nshow that the model can be trained using self-supervised learning alone, and\nevaluate its performance on the LRS3 dataset. Finally, we demonstrate\napplications of Gesture-Sync for audio-visual synchronisation, and in\ndetermining who is the speaker in a crowd, without seeing their faces. The\ncode, datasets and pre-trained models can be found at:\n\\url{https://www.robots.ox.ac.uk/~vgg/research/gestsync}.\n","authors":["Sindhu B Hegde","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2310.05304v1.pdf","comment":"Accepted in BMVC 2023, 10 pages paper, 7 pages supplementary, 7\n  Figures"},{"id":"http://arxiv.org/abs/2012.13633v3","updated":"2023-10-08T22:30:49Z","published":"2020-12-25T21:56:36Z","title":"Detecting Road Obstacles by Erasing Them","summary":"  Vehicles can encounter a myriad of obstacles on the road, and it is\nimpossible to record them all beforehand to train a detector. Instead, we\nselect image patches and inpaint them with the surrounding road texture, which\ntends to remove obstacles from those patches. We then use a network trained to\nrecognize discrepancies between the original patch and the inpainted one, which\nsignals an erased obstacle.\n","authors":["Krzysztof Lis","Sina Honari","Pascal Fua","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2012.13633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05299v1","updated":"2023-10-08T22:08:59Z","published":"2023-10-08T22:08:59Z","title":"Image Compression and Decompression Framework Based on Latent Diffusion\n  Model for Breast Mammography","summary":"  This research presents a novel framework for the compression and\ndecompression of medical images utilizing the Latent Diffusion Model (LDM). The\nLDM represents advancement over the denoising diffusion probabilistic model\n(DDPM) with a potential to yield superior image quality while requiring fewer\ncomputational resources in the image decompression process. A possible\napplication of LDM and Torchvision for image upscaling has been explored using\nmedical image data, serving as an alternative to traditional image compression\nand decompression algorithms. The experimental outcomes demonstrate that this\napproach surpasses a conventional file compression algorithm, and convolutional\nneural network (CNN) models trained with decompressed files perform comparably\nto those trained with original image files. This approach also significantly\nreduces dataset size so that it can be distributed with a smaller size, and\nmedical images take up much less space in medical devices. The research\nimplications extend to noise reduction in lossy compression algorithms and\nsubstitute for complex wavelet-based lossless algorithms.\n","authors":["InChan Hwang","MinJae Woo"],"pdf_url":"https://arxiv.org/pdf/2310.05299v1.pdf","comment":"6 pages IEEE conference"},{"id":"http://arxiv.org/abs/2301.10750v3","updated":"2023-10-08T22:01:52Z","published":"2023-01-25T18:14:49Z","title":"Out of Distribution Performance of State of Art Vision Model","summary":"  The vision transformer (ViT) has advanced to the cutting edge in the visual\nrecognition task. Transformers are more robust than CNN, according to the\nlatest research. ViT's self-attention mechanism, according to the claim, makes\nit more robust than CNN. Even with this, we discover that these conclusions are\nbased on unfair experimental conditions and just comparing a few models, which\ndid not allow us to depict the entire scenario of robustness performance. In\nthis study, we investigate the performance of 58 state-of-the-art computer\nvision models in a unified training setup based not only on attention and\nconvolution mechanisms but also on neural networks based on a combination of\nconvolution and attention mechanisms, sequence-based model, complementary\nsearch, and network-based method. Our research demonstrates that robustness\ndepends on the training setup and model types, and performance varies based on\nout-of-distribution type. Our research will aid the community in better\nunderstanding and benchmarking the robustness of computer vision models.\n","authors":["Salman Rahman","Wonkwon Lee"],"pdf_url":"https://arxiv.org/pdf/2301.10750v3.pdf","comment":"incomplete work - need to complete it"},{"id":"http://arxiv.org/abs/2308.01313v2","updated":"2023-10-08T21:56:53Z","published":"2023-08-02T17:57:25Z","title":"More Context, Less Distraction: Zero-shot Visual Classification by\n  Inferring and Conditioning on Contextual Attributes","summary":"  Vision-language models like CLIP are widely used in zero-shot image\nclassification due to their ability to understand various visual concepts and\nnatural language descriptions. However, how to fully leverage CLIP's\nunprecedented human-like understanding capabilities to achieve better\nperformance is still an open question. This paper draws inspiration from the\nhuman visual perception process: when classifying an object, humans first infer\ncontextual attributes (e.g., background and orientation) which help separate\nthe foreground object from the background, and then classify the object based\non this information. Inspired by it, we observe that providing CLIP with\ncontextual attributes improves zero-shot image classification and mitigates\nreliance on spurious features. We also observe that CLIP itself can reasonably\ninfer the attributes from an image. With these observations, we propose a\ntraining-free, two-step zero-shot classification method PerceptionCLIP. Given\nan image, it first infers contextual attributes (e.g., background) and then\nperforms object classification conditioning on them. Our experiments show that\nPerceptionCLIP achieves better generalization, group robustness, and\ninterpretability. For example, PerceptionCLIP with ViT-L/14 improves the worst\ngroup accuracy by 16.5% on the Waterbirds dataset and by 3.5% on CelebA.\n","authors":["Bang An","Sicheng Zhu","Michael-Andrei Panaitescu-Liess","Chaithanya Kumar Mummadi","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2308.01313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.01508v2","updated":"2023-10-08T21:51:46Z","published":"2023-08-03T02:34:01Z","title":"Circumventing Concept Erasure Methods For Text-to-Image Generative\n  Models","summary":"  Text-to-image generative models can produce photo-realistic images for an\nextremely broad range of concepts, and their usage has proliferated widely\namong the general public. On the flip side, these models have numerous\ndrawbacks, including their potential to generate images featuring sexually\nexplicit content, mirror artistic styles without permission, or even\nhallucinate (or deepfake) the likenesses of celebrities. Consequently, various\nmethods have been proposed in order to \"erase\" sensitive concepts from\ntext-to-image models. In this work, we examine five recently proposed concept\nerasure methods, and show that targeted concepts are not fully excised from any\nof these methods. Specifically, we leverage the existence of special learned\nword embeddings that can retrieve \"erased\" concepts from the sanitized models\nwith no alterations to their weights. Our results highlight the brittleness of\npost hoc concept erasure methods, and call into question their use in the\nalgorithmic toolkit for AI safety.\n","authors":["Minh Pham","Kelly O. Marshall","Niv Cohen","Govind Mittal","Chinmay Hegde"],"pdf_url":"https://arxiv.org/pdf/2308.01508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05290v1","updated":"2023-10-08T21:32:30Z","published":"2023-10-08T21:32:30Z","title":"MSight: An Edge-Cloud Infrastructure-based Perception System for\n  Connected Automated Vehicles","summary":"  As vehicular communication and networking technologies continue to advance,\ninfrastructure-based roadside perception emerges as a pivotal tool for\nconnected automated vehicle (CAV) applications. Due to their elevated\npositioning, roadside sensors, including cameras and lidars, often enjoy\nunobstructed views with diminished object occlusion. This provides them a\ndistinct advantage over onboard perception, enabling more robust and accurate\ndetection of road objects. This paper presents MSight, a cutting-edge roadside\nperception system specifically designed for CAVs. MSight offers real-time\nvehicle detection, localization, tracking, and short-term trajectory\nprediction. Evaluations underscore the system's capability to uphold lane-level\naccuracy with minimal latency, revealing a range of potential applications to\nenhance CAV safety and efficiency. Presently, MSight operates 24/7 at a\ntwo-lane roundabout in the City of Ann Arbor, Michigan.\n","authors":["Rusheng Zhang","Depu Meng","Shengyin Shen","Zhengxia Zou","Houqiang Li","Henry X. Liu"],"pdf_url":"https://arxiv.org/pdf/2310.05290v1.pdf","comment":"Submitted to IEEE T-ITS"},{"id":"http://arxiv.org/abs/2309.16118v2","updated":"2023-10-08T21:17:51Z","published":"2023-09-28T02:50:16Z","title":"D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable\n  Robotic Manipulation","summary":"  Scene representation has been a crucial design choice in robotic manipulation\nsystems. An ideal representation should be 3D, dynamic, and semantic to meet\nthe demands of diverse manipulation tasks. However, previous works often lack\nall three properties simultaneously. In this work, we introduce D$^3$Fields -\ndynamic 3D descriptor fields. These fields capture the dynamics of the\nunderlying 3D environment and encode both semantic features and instance masks.\nSpecifically, we project arbitrary 3D points in the workspace onto multi-view\n2D visual observations and interpolate features derived from foundational\nmodels. The resulting fused descriptor fields allow for flexible goal\nspecifications using 2D images with varied contexts, styles, and instances. To\nevaluate the effectiveness of these descriptor fields, we apply our\nrepresentation to a wide range of robotic manipulation tasks in a zero-shot\nmanner. Through extensive evaluation in both real-world scenarios and\nsimulations, we demonstrate that D$^3$Fields are both generalizable and\neffective for zero-shot robotic manipulation tasks. In quantitative comparisons\nwith state-of-the-art dense descriptors, such as Dense Object Nets and DINO,\nD$^3$Fields exhibit significantly better generalization abilities and\nmanipulation accuracy.\n","authors":["Yixuan Wang","Zhuoran Li","Mingtong Zhang","Katherine Driggs-Campbell","Jiajun Wu","Li Fei-Fei","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2309.16118v2.pdf","comment":"Project Page: https://robopil.github.io/d3fields/"},{"id":"http://arxiv.org/abs/2310.05270v1","updated":"2023-10-08T19:59:42Z","published":"2023-10-08T19:59:42Z","title":"Transforming Pixels into a Masterpiece: AI-Powered Art Restoration using\n  a Novel Distributed Denoising CNN (DDCNN)","summary":"  Art restoration is crucial for preserving cultural heritage, but traditional\nmethods have limitations in faithfully reproducing original artworks while\naddressing issues like fading, staining, and damage. We present an innovative\napproach using deep learning, specifically Convolutional Neural Networks\n(CNNs), and Computer Vision techniques to revolutionize art restoration. We\nstart by creating a diverse dataset of deteriorated art images with various\ndistortions and degradation levels. This dataset trains a Distributed Denoising\nCNN (DDCNN) to remove distortions while preserving intricate details. Our\nmethod is adaptable to different distortion types and levels, making it\nsuitable for various deteriorated artworks, including paintings, sketches, and\nphotographs. Extensive experiments demonstrate our approach's efficiency and\neffectiveness compared to other Denoising CNN models. We achieve a substantial\nreduction in distortion, transforming deteriorated artworks into masterpieces.\nQuantitative evaluations confirm our method's superiority over traditional\ntechniques, reshaping the art restoration field and preserving cultural\nheritage. In summary, our paper introduces an AI-powered solution that combines\nComputer Vision and deep learning with DDCNN to restore artworks accurately,\novercoming limitations and paving the way for future advancements in art\nrestoration.\n","authors":["Sankar B.","Mukil Saravanan","Kalaivanan Kumar","Siri Dubbaka"],"pdf_url":"https://arxiv.org/pdf/2310.05270v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.05264v1","updated":"2023-10-08T19:02:46Z","published":"2023-10-08T19:02:46Z","title":"The Emergence of Reproducibility and Consistency in Diffusion Models","summary":"  Recently, diffusion models have emerged as powerful deep generative models,\nshowcasing cutting-edge performance across various applications such as image\ngeneration, solving inverse problems, and text-to-image synthesis. These models\ngenerate new data (e.g., images) by transforming random noise inputs through a\nreverse diffusion process. In this work, we uncover a distinct and prevalent\nphenomenon within diffusion models in contrast to most other generative models,\nwhich we refer to as ``consistent model reproducibility''. To elaborate, our\nextensive experiments have consistently shown that when starting with the same\ninitial noise input and sampling with a deterministic solver, diffusion models\ntend to produce nearly identical output content. This consistency holds true\nregardless of the choices of model architectures and training procedures.\nAdditionally, our research has unveiled that this exceptional model\nreproducibility manifests in two distinct training regimes: (i) ``memorization\nregime,'' characterized by a significantly overparameterized model which\nattains reproducibility mainly by memorizing the training data; (ii)\n``generalization regime,'' in which the model is trained on an extensive\ndataset, and its reproducibility emerges with the model's generalization\ncapabilities. Our analysis provides theoretical justification for the model\nreproducibility in ``memorization regime''. Moreover, our research reveals that\nthis valuable property generalizes to many variants of diffusion models,\nincluding conditional diffusion models, diffusion models for solving inverse\nproblems, and fine-tuned diffusion models. A deeper understanding of this\nphenomenon has the potential to yield more interpretable and controllable data\ngenerative processes based on diffusion models.\n","authors":["Huijie Zhang","Jinfan Zhou","Yifu Lu","Minzhe Guo","Liyue Shen","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2310.05264v1.pdf","comment":"41 pages, 21 figures"},{"id":"http://arxiv.org/abs/2310.05262v1","updated":"2023-10-08T18:51:33Z","published":"2023-10-08T18:51:33Z","title":"Structure-Preserving Instance Segmentation via Skeleton-Aware Distance\n  Transform","summary":"  Objects with complex structures pose significant challenges to existing\ninstance segmentation methods that rely on boundary or affinity maps, which are\nvulnerable to small errors around contacting pixels that cause noticeable\nconnectivity change. While the distance transform (DT) makes instance interiors\nand boundaries more distinguishable, it tends to overlook the intra-object\nconnectivity for instances with varying width and result in over-segmentation.\nTo address these challenges, we propose a skeleton-aware distance transform\n(SDT) that combines the merits of object skeleton in preserving connectivity\nand DT in modeling geometric arrangement to represent instances with arbitrary\nstructures. Comprehensive experiments on histopathology image segmentation\ndemonstrate that SDT achieves state-of-the-art performance.\n","authors":["Zudi Lin","Donglai Wei","Aarush Gupta","Xingyu Liu","Deqing Sun","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2310.05262v1.pdf","comment":"MICCAI 2023 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2305.02034v3","updated":"2023-10-08T18:15:18Z","published":"2023-05-03T10:58:07Z","title":"SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment\n  Anything Model","summary":"  The success of the Segment Anything Model (SAM) demonstrates the significance\nof data-centric machine learning. However, due to the difficulties and high\ncosts associated with annotating Remote Sensing (RS) images, a large amount of\nvaluable RS data remains unlabeled, particularly at the pixel level. In this\nstudy, we leverage SAM and existing RS object detection datasets to develop an\nefficient pipeline for generating a large-scale RS segmentation dataset, dubbed\nSAMRS. SAMRS totally possesses 105,090 images and 1,668,241 instances,\nsurpassing existing high-resolution RS segmentation datasets in size by several\norders of magnitude. It provides object category, location, and instance\ninformation that can be used for semantic segmentation, instance segmentation,\nand object detection, either individually or in combination. We also provide a\ncomprehensive analysis of SAMRS from various aspects. Moreover, preliminary\nexperiments highlight the importance of conducting segmentation pre-training\nwith SAMRS to address task discrepancies and alleviate the limitations posed by\nlimited training data during fine-tuning. The code and dataset will be\navailable at https://github.com/ViTAE-Transformer/SAMRS.\n","authors":["Di Wang","Jing Zhang","Bo Du","Minqiang Xu","Lin Liu","Dacheng Tao","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.02034v3.pdf","comment":"Accepted by NeurIPS 2023 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2310.05255v1","updated":"2023-10-08T18:07:15Z","published":"2023-10-08T18:07:15Z","title":"Persis: A Persian Font Recognition Pipeline Using Convolutional Neural\n  Networks","summary":"  What happens if we encounter a suitable font for our design work but do not\nknow its name? Visual Font Recognition (VFR) systems are used to identify the\nfont typeface in an image. These systems can assist graphic designers in\nidentifying fonts used in images. A VFR system also aids in improving the speed\nand accuracy of Optical Character Recognition (OCR) systems. In this paper, we\nintroduce the first publicly available datasets in the field of Persian font\nrecognition and employ Convolutional Neural Networks (CNN) to address this\nproblem. The results show that the proposed pipeline obtained 78.0% top-1\naccuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the\nKAFD dataset. Furthermore, the average time spent in the entire pipeline for\none sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU,\nrespectively. We conclude that CNN methods can be used to recognize Persian\nfonts without the need for additional pre-processing steps such as feature\nextraction, binarization, normalization, etc.\n","authors":["Mehrdad Mohammadian","Neda Maleki","Tobias Olsson","Fredrik Ahlgren"],"pdf_url":"https://arxiv.org/pdf/2310.05255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05241v1","updated":"2023-10-08T17:19:58Z","published":"2023-10-08T17:19:58Z","title":"SCANet: Scene Complexity Aware Network for Weakly-Supervised Video\n  Moment Retrieval","summary":"  Video moment retrieval aims to localize moments in video corresponding to a\ngiven language query. To avoid the expensive cost of annotating the temporal\nmoments, weakly-supervised VMR (wsVMR) systems have been studied. For such\nsystems, generating a number of proposals as moment candidates and then\nselecting the most appropriate proposal has been a popular approach. These\nproposals are assumed to contain many distinguishable scenes in a video as\ncandidates. However, existing proposals of wsVMR systems do not respect the\nvarying numbers of scenes in each video, where the proposals are heuristically\ndetermined irrespective of the video. We argue that the retrieval system should\nbe able to counter the complexities caused by varying numbers of scenes in each\nvideo. To this end, we present a novel concept of a retrieval system referred\nto as Scene Complexity Aware Network (SCANet), which measures the `scene\ncomplexity' of multiple scenes in each video and generates adaptive proposals\nresponding to variable complexities of scenes in each video. Experimental\nresults on three retrieval benchmarks (i.e., Charades-STA, ActivityNet, TVR)\nachieve state-of-the-art performances and demonstrate the effectiveness of\nincorporating the scene complexity.\n","authors":["Sunjae Yoon","Gwanhyeong Koo","Dahyun Kim","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2310.05241v1.pdf","comment":"11 pages, Accepted in ICCV 2023"},{"id":"http://arxiv.org/abs/2310.05237v1","updated":"2023-10-08T17:11:14Z","published":"2023-10-08T17:11:14Z","title":"Latent Diffusion Model for Medical Image Standardization and Enhancement","summary":"  Computed tomography (CT) serves as an effective tool for lung cancer\nscreening, diagnosis, treatment, and prognosis, providing a rich source of\nfeatures to quantify temporal and spatial tumor changes. Nonetheless, the\ndiversity of CT scanners and customized acquisition protocols can introduce\nsignificant inconsistencies in texture features, even when assessing the same\npatient. This variability poses a fundamental challenge for subsequent research\nthat relies on consistent image features. Existing CT image standardization\nmodels predominantly utilize GAN-based supervised or semi-supervised learning,\nbut their performance remains limited. We present DiffusionCT, an innovative\nscore-based DDPM model that operates in the latent space to transform disparate\nnon-standard distributions into a standardized form. The architecture comprises\na U-Net-based encoder-decoder, augmented by a DDPM model integrated at the\nbottleneck position. First, the encoder-decoder is trained independently,\nwithout embedding DDPM, to capture the latent representation of the input data.\nSecond, the latent DDPM model is trained while keeping the encoder-decoder\nparameters fixed. Finally, the decoder uses the transformed latent\nrepresentation to generate a standardized CT image, providing a more consistent\nbasis for downstream analysis. Empirical tests on patient CT images indicate\nnotable improvements in image standardization using DiffusionCT. Additionally,\nthe model significantly reduces image noise in SPAD images, further validating\nthe effectiveness of DiffusionCT for advanced imaging tasks.\n","authors":["Md Selim","Jie Zhang","Faraneh Fathi","Michael A. Brooks","Ge Wang","Guoqiang Yu","Jin Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16772v3","updated":"2023-10-08T16:32:45Z","published":"2023-09-28T18:09:40Z","title":"XVO: Generalized Visual Odometry via Cross-Modal Self-Training","summary":"  We propose XVO, a semi-supervised learning method for training generalized\nmonocular Visual Odometry (VO) models with robust off-the-self operation across\ndiverse datasets and settings. In contrast to standard monocular VO approaches\nwhich often study a known calibration within a single dataset, XVO efficiently\nlearns to recover relative pose with real-world scale from visual scene\nsemantics, i.e., without relying on any known camera parameters. We optimize\nthe motion estimation model via self-training from large amounts of\nunconstrained and heterogeneous dash camera videos available on YouTube. Our\nkey contribution is twofold. First, we empirically demonstrate the benefits of\nsemi-supervised training for learning a general-purpose direct VO regression\nnetwork. Second, we demonstrate multi-modal supervision, including\nsegmentation, flow, depth, and audio auxiliary prediction tasks, to facilitate\ngeneralized representations for the VO task. Specifically, we find audio\nprediction task to significantly enhance the semi-supervised learning process\nwhile alleviating noisy pseudo-labels, particularly in highly dynamic and\nout-of-domain video data. Our proposed teacher network achieves\nstate-of-the-art performance on the commonly used KITTI benchmark despite no\nmulti-frame optimization or knowledge of camera parameters. Combined with the\nproposed semi-supervised step, XVO demonstrates off-the-shelf knowledge\ntransfer across diverse conditions on KITTI, nuScenes, and Argoverse without\nfine-tuning.\n","authors":["Lei Lai","Zhongkai Shangguan","Jimuyang Zhang","Eshed Ohn-Bar"],"pdf_url":"https://arxiv.org/pdf/2309.16772v3.pdf","comment":"ICCV 2023, Paris https://genxvo.github.io/"},{"id":"http://arxiv.org/abs/2310.05212v1","updated":"2023-10-08T16:05:17Z","published":"2023-10-08T16:05:17Z","title":"Interpretable Semiotics Networks Representing Awareness","summary":"  Humans perceive objects daily and communicate their perceptions using various\nchannels. Here, we describe a computational model that track and simulate\nobjects' perception, and their representations as they pass in communication.\n  We describe two key components of our internal representation ('observed' and\n'seen') and relate them to familiar computer vision terms (encoding and\ndecoding). These elements joined together to form semiotic networks, which\nsimulate awareness in object perception and human communication.\n  Nowadays, most neural networks are uninterpretable. On the other hand, our\nmodel is free from this disadvantages. We performed several experiments and\ndemonstrated the visibility of our model.\n  We describe how our network may be used as preprocessing unit to any\nclassification network. In our experiments the compound network overperforms in\naverage the classification network at datasets with small training data.\n  Future work would leverage our model to gain better understanding of human\ncommunications and personal representations.\n","authors":["David Kupeev","Eyal Nitcany"],"pdf_url":"https://arxiv.org/pdf/2310.05212v1.pdf","comment":"58 pages"},{"id":"http://arxiv.org/abs/2208.04173v2","updated":"2023-10-08T15:55:35Z","published":"2022-08-08T14:26:35Z","title":"SIAD: Self-supervised Image Anomaly Detection System","summary":"  Recent trends in AIGC effectively boosted the application of visual\ninspection. However, most of the available systems work in a human-in-the-loop\nmanner and can not provide long-term support to the online application. To make\na step forward, this paper outlines an automatic annotation system called SsaA,\nworking in a self-supervised learning manner, for continuously making the\nonline visual inspection in the manufacturing automation scenarios. Benefit\nfrom the self-supervised learning, SsaA is effective to establish a visual\ninspection application for the whole life-cycle of manufacturing. In the early\nstage, with only the anomaly-free data, the unsupervised algorithms are adopted\nto process the pretext task and generate coarse labels for the following data.\nThen supervised algorithms are trained for the downstream task. With\nuser-friendly web-based interfaces, SsaA is very convenient to integrate and\ndeploy both of the unsupervised and supervised algorithms. So far, the SsaA\nsystem has been adopted for some real-life industrial applications.\n","authors":["Jiawei Li","Chenxi Lan","Xinyi Zhang","Bolin Jiang","Yuqiu Xie","Naiqi Li","Yan Liu","Yaowei Li","Enze Huo","Bin Chen"],"pdf_url":"https://arxiv.org/pdf/2208.04173v2.pdf","comment":"4 pages, 3 figures, ICCV 2023 Demo Track"},{"id":"http://arxiv.org/abs/2310.05207v1","updated":"2023-10-08T15:49:26Z","published":"2023-10-08T15:49:26Z","title":"Boosting Facial Action Unit Detection Through Jointly Learning Facial\n  Landmark Detection and Domain Separation and Reconstruction","summary":"  Recently how to introduce large amounts of unlabeled facial images in the\nwild into supervised Facial Action Unit (AU) detection frameworks has become a\nchallenging problem. In this paper, we propose a new AU detection framework\nwhere multi-task learning is introduced to jointly learn AU domain separation\nand reconstruction and facial landmark detection by sharing the parameters of\nhomostructural facial extraction modules. In addition, we propose a new feature\nalignment scheme based on contrastive learning by simple projectors and an\nimproved contrastive loss, which adds four additional intermediate supervisors\nto promote the feature reconstruction process. Experimental results on two\nbenchmarks demonstrate our superiority against the state-of-the-art methods for\nAU detection in the wild.\n","authors":["Ziqiao Shang","Li Yu"],"pdf_url":"https://arxiv.org/pdf/2310.05207v1.pdf","comment":"5 pages, 1 figure, published to ICASSP 2024"},{"id":"http://arxiv.org/abs/2310.05202v1","updated":"2023-10-08T15:28:01Z","published":"2023-10-08T15:28:01Z","title":"Enhancing Cross-Dataset Performance of Distracted Driving Detection With\n  Score-Softmax Classifier","summary":"  Deep neural networks enable real-time monitoring of in-vehicle driver,\nfacilitating the timely prediction of distractions, fatigue, and potential\nhazards. This technology is now integral to intelligent transportation systems.\nRecent research has exposed unreliable cross-dataset end-to-end driver behavior\nrecognition due to overfitting, often referred to as ``shortcut learning\",\nresulting from limited data samples. In this paper, we introduce the\nScore-Softmax classifier, which addresses this issue by enhancing inter-class\nindependence and Intra-class uncertainty. Motivated by human rating patterns,\nwe designed a two-dimensional supervisory matrix based on marginal Gaussian\ndistributions to train the classifier. Gaussian distributions help amplify\nintra-class uncertainty while ensuring the Score-Softmax classifier learns\naccurate knowledge. Furthermore, leveraging the summation of independent\nGaussian distributed random variables, we introduced a multi-channel\ninformation fusion method. This strategy effectively resolves the\nmulti-information fusion challenge for the Score-Softmax classifier.\nConcurrently, we substantiate the necessity of transfer learning and\nmulti-dataset combination. We conducted cross-dataset experiments using the\nSFD, AUCDD-V1, and 100-Driver datasets, demonstrating that Score-Softmax\nimproves cross-dataset performance without modifying the model architecture.\nThis provides a new approach for enhancing neural network generalization.\nAdditionally, our information fusion approach outperforms traditional methods.\n","authors":["Cong Duan","Zixuan Liu","Jiahao Xia","Minghai Zhang","Jiacai Liao","Libo Cao"],"pdf_url":"https://arxiv.org/pdf/2310.05202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.11642v3","updated":"2023-10-08T15:19:18Z","published":"2022-12-22T12:15:37Z","title":"Predictive Coding Based Multiscale Network with Encoder-Decoder LSTM for\n  Video Prediction","summary":"  We present a multi-scale predictive coding model for future video frames\nprediction. Drawing inspiration on the ``Predictive Coding\" theories in\ncognitive science, it is updated by a combination of bottom-up and top-down\ninformation flows, which can enhance the interaction between different network\nlevels. However, traditional predictive coding models only predict what is\nhappening hierarchically rather than predicting the future. To address the\nproblem, our model employs a multi-scale approach (Coarse to Fine), where the\nhigher level neurons generate coarser predictions (lower resolution), while the\nlower level generate finer predictions (higher resolution). In terms of network\narchitecture, we directly incorporate the encoder-decoder network within the\nLSTM module and share the final encoded high-level semantic information across\ndifferent network levels. This enables comprehensive interaction between the\ncurrent input and the historical states of LSTM compared with the traditional\nEncoder-LSTM-Decoder architecture, thus learning more believable temporal and\nspatial dependencies. Furthermore, to tackle the instability in adversarial\ntraining and mitigate the accumulation of prediction errors in long-term\nprediction, we propose several improvements to the training strategy. Our\napproach achieves good performance on datasets such as KTH, Moving MNIST and\nCaltech Pedestrian. Code is available at https://github.com/Ling-CF/MSPN.\n","authors":["Chaofan Ling","Junpei Zhong","Weihua Li"],"pdf_url":"https://arxiv.org/pdf/2212.11642v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05195v1","updated":"2023-10-08T15:04:50Z","published":"2023-10-08T15:04:50Z","title":"GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient\n  Partially Relevant Video Retrieval","summary":"  Given a text query, partially relevant video retrieval (PRVR) seeks to find\nuntrimmed videos containing pertinent moments in a database. For PRVR, clip\nmodeling is essential to capture the partial relationship between texts and\nvideos. Current PRVR methods adopt scanning-based clip construction to achieve\nexplicit clip modeling, which is information-redundant and requires a large\nstorage overhead. To solve the efficiency problem of PRVR methods, this paper\nproposes GMMFormer, a \\textbf{G}aussian-\\textbf{M}ixture-\\textbf{M}odel based\nTrans\\textbf{former} which models clip representations implicitly. During frame\ninteractions, we incorporate Gaussian-Mixture-Model constraints to focus each\nframe on its adjacent frames instead of the whole video. Then generated\nrepresentations will contain multi-scale clip information, achieving implicit\nclip modeling. In addition, PRVR methods ignore semantic differences between\ntext queries relevant to the same video, leading to a sparse embedding space.\nWe propose a query diverse loss to distinguish these text queries, making the\nembedding space more intensive and contain more semantic information. Extensive\nexperiments on three large-scale video datasets (\\ie, TVR, ActivityNet\nCaptions, and Charades-STA) demonstrate the superiority and efficiency of\nGMMFormer.\n","authors":["Yuting Wang","Jinpeng Wang","Bin Chen","Ziyun Zeng","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2310.05195v1.pdf","comment":"Work in progress. The code will be released"},{"id":"http://arxiv.org/abs/2310.05193v1","updated":"2023-10-08T15:01:54Z","published":"2023-10-08T15:01:54Z","title":"Improving Discriminative Multi-Modal Learning with Large-Scale\n  Pre-Trained Models","summary":"  This paper investigates how to better leverage large-scale pre-trained\nuni-modal models to further enhance discriminative multi-modal learning. Even\nwhen fine-tuned with only uni-modal data, these models can outperform previous\nmulti-modal models in certain tasks. It's clear that their incorporation into\nmulti-modal learning would significantly improve performance. However,\nmulti-modal learning with these models still suffers from insufficient learning\nof uni-modal features, which weakens the resulting multi-modal model's\ngeneralization ability. While fine-tuning uni-modal models separately and then\naggregating their predictions is straightforward, it doesn't allow for adequate\nadaptation between modalities, also leading to sub-optimal results. To this\nend, we introduce Multi-Modal Low-Rank Adaptation learning (MMLoRA). By\nfreezing the weights of uni-modal fine-tuned models, adding extra trainable\nrank decomposition matrices to them, and subsequently performing multi-modal\njoint training, our method enhances adaptation between modalities and boosts\noverall performance. We demonstrate the effectiveness of MMLoRA on three\ndataset categories: audio-visual (e.g., AVE, Kinetics-Sound, CREMA-D),\nvision-language (e.g., MM-IMDB, UPMC Food101), and RGB-Optical Flow (UCF101).\n","authors":["Chenzhuang Du","Yue Zhao","Chonghua Liao","Jiacheng You","Jie Fu","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.05193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05192v1","updated":"2023-10-08T15:00:38Z","published":"2023-10-08T15:00:38Z","title":"HOD: A Benchmark Dataset for Harmful Object Detection","summary":"  Recent multi-media data such as images and videos have been rapidly spread\nout on various online services such as social network services (SNS). With the\nexplosive growth of online media services, the number of image content that may\nharm users is also growing exponentially. Thus, most recent online platforms\nsuch as Facebook and Instagram have adopted content filtering systems to\nprevent the prevalence of harmful content and reduce the possible risk of\nadverse effects on users. Unfortunately, computer vision research on detecting\nharmful content has not yet attracted attention enough. Users of each platform\nstill manually click the report button to recognize patterns of harmful content\nthey dislike when exposed to harmful content. However, the problem with manual\nreporting is that users are already exposed to harmful content. To address\nthese issues, our research goal in this work is to develop automatic harmful\nobject detection systems for online services. We present a new benchmark\ndataset for harmful object detection. Unlike most related studies focusing on a\nsmall subset of object categories, our dataset addresses various categories.\nSpecifically, our proposed dataset contains more than 10,000 images across 6\ncategories that might be harmful, consisting of not only normal cases but also\nhard cases that are difficult to detect. Moreover, we have conducted extensive\nexperiments to evaluate the effectiveness of our proposed dataset. We have\nutilized the recently proposed state-of-the-art (SOTA) object detection\narchitectures and demonstrated our proposed dataset can be greatly useful for\nthe real-time harmful object detection task. The whole source codes and\ndatasets are publicly accessible at\nhttps://github.com/poori-nuna/HOD-Benchmark-Dataset.\n","authors":["Eungyeom Ha","Heemook Kim","Sung Chul Hong","Dongbin Na"],"pdf_url":"https://arxiv.org/pdf/2310.05192v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2304.14530v2","updated":"2023-10-08T14:51:57Z","published":"2023-04-27T20:55:38Z","title":"Generating images of rare concepts using pre-trained diffusion models","summary":"  Text-to-image diffusion models can synthesize high-quality images, but they\nhave various limitations. Here we highlight a common failure mode of these\nmodels, namely, generating uncommon concepts and structured concepts like hand\npalms. We show that their limitation is partly due to the long-tail nature of\ntheir training data: web-crawled data sets are strongly unbalanced, causing\nmodels to under-represent concepts from the tail of the distribution. We\ncharacterize the effect of unbalanced training data on text-to-image models and\noffer a remedy. We show that rare concepts can be correctly generated by\ncarefully selecting suitable generation seeds in the noise space, using a small\nreference set of images, a technique that we call SeedSelect. SeedSelect does\nnot require retraining or finetuning the diffusion model. We assess the\nfaithfulness, quality and diversity of SeedSelect in creating rare objects and\ngenerating complex formations like hand images, and find it consistently\nachieves superior performance. We further show the advantage of SeedSelect in\nsemantic data augmentation. Generating semantically appropriate images can\nsuccessfully improve performance in few-shot recognition benchmarks, for\nclasses from the head and from the tail of the training data of diffusion\nmodels\n","authors":["Dvir Samuel","Rami Ben-Ari","Simon Raviv","Nir Darshan","Gal Chechik"],"pdf_url":"https://arxiv.org/pdf/2304.14530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05184v1","updated":"2023-10-08T14:46:11Z","published":"2023-10-08T14:46:11Z","title":"AANet: Aggregation and Alignment Network with Semi-hard Positive Sample\n  Mining for Hierarchical Place Recognition","summary":"  Visual place recognition (VPR) is one of the research hotspots in robotics,\nwhich uses visual information to locate robots. Recently, the hierarchical\ntwo-stage VPR methods have become popular in this field due to the trade-off\nbetween accuracy and efficiency. These methods retrieve the top-k candidate\nimages using the global features in the first stage, then re-rank the\ncandidates by matching the local features in the second stage. However, they\nusually require additional algorithms (e.g. RANSAC) for geometric consistency\nverification in re-ranking, which is time-consuming. Here we propose a\nDynamically Aligning Local Features (DALF) algorithm to align the local\nfeatures under spatial constraints. It is significantly more efficient than the\nmethods that need geometric consistency verification. We present a unified\nnetwork capable of extracting global features for retrieving candidates via an\naggregation module and aligning local features for re-ranking via the DALF\nalignment module. We call this network AANet. Meanwhile, many works use the\nsimplest positive samples in triplet for weakly supervised training, which\nlimits the ability of the network to recognize harder positive pairs. To\naddress this issue, we propose a Semi-hard Positive Sample Mining (ShPSM)\nstrategy to select appropriate hard positive images for training more robust\nVPR networks. Extensive experiments on four benchmark VPR datasets show that\nthe proposed AANet can outperform several state-of-the-art methods with less\ntime consumption. The code is released at https://github.com/Lu-Feng/AANet.\n","authors":["Feng Lu","Lijun Zhang","Shuting Dong","Baifan Chen","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2310.05184v1.pdf","comment":"ICRA2023"},{"id":"http://arxiv.org/abs/2207.05225v5","updated":"2023-10-08T14:41:18Z","published":"2022-07-11T23:45:12Z","title":"Susceptibility of Continual Learning Against Adversarial Attacks","summary":"  Recent continual learning approaches have primarily focused on mitigating\ncatastrophic forgetting. Nevertheless, two critical areas have remained\nrelatively unexplored: 1) evaluating the robustness of proposed methods and 2)\nensuring the security of learned tasks. This paper investigates the\nsusceptibility of continually learned tasks, including current and previously\nacquired tasks, to adversarial attacks. Specifically, we have observed that any\nclass belonging to any task can be easily targeted and misclassified as the\ndesired target class of any other task. Such susceptibility or vulnerability of\nlearned tasks to adversarial attacks raises profound concerns regarding data\nintegrity and privacy. To assess the robustness of continual learning\napproaches, we consider continual learning approaches in all three scenarios,\ni.e., task-incremental learning, domain-incremental learning, and\nclass-incremental learning. In this regard, we explore the robustness of three\nregularization-based methods, three replay-based approaches, and one hybrid\ntechnique that combines replay and exemplar approaches. We empirically\ndemonstrated that in any setting of continual learning, any class, whether\nbelonging to the current or previously learned tasks, is susceptible to\nmisclassification. Our observations identify potential limitations of continual\nlearning approaches against adversarial attacks and highlight that current\ncontinual learning algorithms could not be suitable for deployment in\nreal-world settings.\n","authors":["Hikmat Khan","Pir Masoom Shah","Syed Farhan Alam Zaidi","Saif ul Islam","Qasim Zia"],"pdf_url":"https://arxiv.org/pdf/2207.05225v5.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2310.05171v1","updated":"2023-10-08T14:05:10Z","published":"2023-10-08T14:05:10Z","title":"Multi-Ship Tracking by Robust Similarity metric","summary":"  Multi-ship tracking (MST) as a core technology has been proven to be applied\nto situational awareness at sea and the development of a navigational system\nfor autonomous ships. Despite impressive tracking outcomes achieved by\nmulti-object tracking (MOT) algorithms for pedestrian and vehicle datasets,\nthese models and techniques exhibit poor performance when applied to ship\ndatasets. Intersection of Union (IoU) is the most popular metric for computing\nsimilarity used in object tracking. The low frame rates and severe image shake\ncaused by wave turbulence in ship datasets often result in minimal, or even\nzero, Intersection of Union (IoU) between the predicted and detected bounding\nboxes. This issue contributes to frequent identity switches of tracked objects,\nundermining the tracking performance. In this paper, we address the weaknesses\nof IoU by incorporating the smallest convex shapes that enclose both the\npredicted and detected bounding boxes. The calculation of the tracking version\nof IoU (TIoU) metric considers not only the size of the overlapping area\nbetween the detection bounding box and the prediction box, but also the\nsimilarity of their shapes. Through the integration of the TIoU into\nstate-of-the-art object tracking frameworks, such as DeepSort and ByteTrack, we\nconsistently achieve improvements in the tracking performance of these\nframeworks.\n","authors":["Hongyu Zhao","Gongming Wei","Yang Xiao","Xianglei Xing"],"pdf_url":"https://arxiv.org/pdf/2310.05171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15019v2","updated":"2023-10-08T13:56:18Z","published":"2023-09-26T15:38:52Z","title":"IFT: Image Fusion Transformer for Ghost-free High Dynamic Range Imaging","summary":"  Multi-frame high dynamic range (HDR) imaging aims to reconstruct ghost-free\nimages with photo-realistic details from content-complementary but spatially\nmisaligned low dynamic range (LDR) images. Existing HDR algorithms are prone to\nproducing ghosting artifacts as their methods fail to capture long-range\ndependencies between LDR frames with large motion in dynamic scenes. To address\nthis issue, we propose a novel image fusion transformer, referred to as IFT,\nwhich presents a fast global patch searching (FGPS) module followed by a\nself-cross fusion module (SCF) for ghost-free HDR imaging. The FGPS searches\nthe patches from supporting frames that have the closest dependency to each\npatch of the reference frame for long-range dependency modeling, while the SCF\nconducts intra-frame and inter-frame feature fusion on the patches obtained by\nthe FGPS with linear complexity to input resolution. By matching similar\npatches between frames, objects with large motion ranges in dynamic scenes can\nbe aligned, which can effectively alleviate the generation of artifacts. In\naddition, the proposed FGPS and SCF can be integrated into various deep HDR\nmethods as efficient plug-in modules. Extensive experiments on multiple\nbenchmarks show that our method achieves state-of-the-art performance both\nquantitatively and qualitatively.\n","authors":["Hailing Wang","Wei Li","Yuanyuan Xi","Jie Hu","Hanting Chen","Longyu Li","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2309.15019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01412v2","updated":"2023-10-08T13:47:23Z","published":"2023-10-02T17:59:52Z","title":"DriveGPT4: Interpretable End-to-end Autonomous Driving via Large\n  Language Model","summary":"  In the past decade, autonomous driving has experienced rapid development in\nboth academia and industry. However, its limited interpretability remains a\nsignificant unsolved problem, severely hindering autonomous vehicle\ncommercialization and further development. Previous approaches utilizing small\nlanguage models have failed to address this issue due to their lack of\nflexibility, generalization ability, and robustness. Recently, multimodal large\nlanguage models (LLMs) have gained considerable attention from the research\ncommunity for their capability to process and reason non-text data (e.g.,\nimages and videos) by text. In this paper, we present DriveGPT4, an\ninterpretable end-to-end autonomous driving system utilizing LLMs. DriveGPT4 is\ncapable of interpreting vehicle actions and providing corresponding reasoning,\nas well as answering diverse questions posed by human users for enhanced\ninteraction. Additionally, DriveGPT4 predicts vehicle low-level control signals\nin an end-to-end fashion. These capabilities stem from a customized visual\ninstruction tuning dataset specifically designed for autonomous driving. To the\nbest of our knowledge, DriveGPT4 is the first work focusing on interpretable\nend-to-end autonomous driving. When evaluated on multiple tasks alongside\nconventional methods and video understanding LLMs, DriveGPT4 demonstrates\nsuperior qualitative and quantitative performance. Additionally, DriveGPT4 can\nbe generalized in a zero-shot fashion to accommodate more unseen scenarios. The\nproject page is available at https://tonyxuqaq.github.io/projects/DriveGPT4/ .\n","authors":["Zhenhua Xu","Yujia Zhang","Enze Xie","Zhen Zhao","Yong Guo","Kwan-Yee. K. Wong","Zhenguo Li","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.01412v2.pdf","comment":"The project page is available at\n  https://tonyxuqaq.github.io/projects/DriveGPT4/"},{"id":"http://arxiv.org/abs/2305.11577v2","updated":"2023-10-08T13:40:34Z","published":"2023-05-19T10:29:42Z","title":"Harnessing Text-to-Image Attention Prior for Reference-based Multi-view\n  Image Synthesis","summary":"  This paper explores the domain of multi-view image synthesis, aiming to\ncreate specific image elements or entire scenes while ensuring visual\nconsistency with reference images. We categorize this task into two approaches:\nlocal synthesis, guided by structural cues from reference images\n(Reference-based inpainting, Ref-inpainting), and global synthesis, which\ngenerates entirely new images based solely on reference examples (Novel View\nSynthesis, NVS). In recent years, Text-to-Image (T2I) generative models have\ngained attention in various domains. However, adapting them for multi-view\nsynthesis is challenging due to the intricate correlations between reference\nand target images. To address these challenges efficiently, we introduce\nAttention Reactivated Contextual Inpainting (ARCI), a unified approach that\nreformulates both local and global reference-based multi-view synthesis as\ncontextual inpainting, which is enhanced with pre-existing attention mechanisms\nin T2I models. Formally, self-attention is leveraged to learn feature\ncorrelations across different reference views, while cross-attention is\nutilized to control the generation through prompt tuning. Our contributions of\nARCI, built upon the StableDiffusion fine-tuned for text-guided inpainting,\ninclude skillfully handling difficult multi-view synthesis tasks with\noff-the-shelf T2I models, introducing task and view-specific prompt tuning for\ngenerative control, achieving end-to-end Ref-inpainting, and implementing block\ncausal masking for autoregressive NVS. We also show the versatility of ARCI by\nextending it to multi-view generation for superior consistency with the same\narchitecture, which has also been validated through extensive experiments.\nCodes and models will be released in \\url{https://github.com/ewrfcas/ARCI}.\n","authors":["Chenjie Cao","Yunuo Cai","Qiaole Dong","Yikai Wang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2305.11577v2.pdf","comment":"We further improved our methods for multi-view synthesis. The project\n  page is https://ewrfcas.github.io/ARCI/"},{"id":"http://arxiv.org/abs/2308.08543v3","updated":"2023-10-08T13:33:58Z","published":"2023-08-16T17:58:28Z","title":"InsightMapper: A Closer Look at Inner-instance Information for\n  Vectorized High-Definition Mapping","summary":"  Vectorized high-definition (HD) maps contain detailed information about\nsurrounding road elements, which are crucial for various downstream tasks in\nmodern autonomous driving vehicles, such as vehicle planning and control.\nRecent works have attempted to directly detect the vectorized HD map as a point\nset prediction task, resulting in significant improvements in detection\nperformance. However, these approaches fail to analyze and exploit the\ninner-instance correlations between predicted points, impeding further\nadvancements. To address these challenges, we investigate the utilization of\ninner-$\\textbf{INS}$tance information for vectorized h$\\textbf{IGH}$-definition\nmapping through $\\textbf{T}$ransformers and introduce InsightMapper. This paper\npresents three novel designs within InsightMapper that leverage inner-instance\ninformation in distinct ways, including hybrid query generation, inner-instance\nquery fusion, and inner-instance feature aggregation. Comparative experiments\nare conducted on the NuScenes dataset, showcasing the superiority of our\nproposed method. InsightMapper surpasses previous state-of-the-art (SOTA)\nmethods by 5.78 mAP and 5.12 TOPO, which assess topology correctness.\nSimultaneously, InsightMapper maintains high efficiency during both training\nand inference phases, resulting in remarkable comprehensive performance. The\nproject page for this work is available at\nhttps://tonyxuqaq.github.io/InsightMapper/ .\n","authors":["Zhenhua Xu","Kwan-Yee. K. Wong","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.08543v3.pdf","comment":"Code and demo will be available at\n  https://tonyxuqaq.github.io/InsightMapper/"},{"id":"http://arxiv.org/abs/2310.05158v1","updated":"2023-10-08T13:22:20Z","published":"2023-10-08T13:22:20Z","title":"ITRE: Low-light Image Enhancement Based on Illumination Transmission\n  Ratio Estimation","summary":"  Noise, artifacts, and over-exposure are significant challenges in the field\nof low-light image enhancement. Existing methods often struggle to address\nthese issues simultaneously. In this paper, we propose a novel Retinex-based\nmethod, called ITRE, which suppresses noise and artifacts from the origin of\nthe model, prevents over-exposure throughout the enhancement process.\nSpecifically, we assume that there must exist a pixel which is least disturbed\nby low light within pixels of same color. First, clustering the pixels on the\nRGB color space to find the Illumination Transmission Ratio (ITR) matrix of the\nwhole image, which determines that noise is not over-amplified easily. Next, we\nconsider ITR of the image as the initial illumination transmission map to\nconstruct a base model for refined transmission map, which prevents artifacts.\nAdditionally, we design an over-exposure module that captures the fundamental\ncharacteristics of pixel over-exposure and seamlessly integrate it into the\nbase model. Finally, there is a possibility of weak enhancement when\ninter-class distance of pixels with same color is too small. To counteract\nthis, we design a Robust-Guard module that safeguards the robustness of the\nimage enhancement process. Extensive experiments demonstrate the effectiveness\nof our approach in suppressing noise, preventing artifacts, and controlling\nover-exposure level simultaneously. Our method performs superiority in\nqualitative and quantitative performance evaluations by comparing with\nstate-of-the-art methods.\n","authors":["Yu Wang","Yihong Wang","Tong Liu","Xiubao Sui","Qian Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.13117v2","updated":"2023-10-08T12:31:28Z","published":"2022-06-27T08:49:11Z","title":"SARNet: Semantic Augmented Registration of Large-Scale Urban Point\n  Clouds","summary":"  Registering urban point clouds is a quite challenging task due to the\nlarge-scale, noise and data incompleteness of LiDAR scanning data. In this\npaper, we propose SARNet, a novel semantic augmented registration network aimed\nat achieving efficient registration of urban point clouds at city scale.\nDifferent from previous methods that construct correspondences only in the\npoint-level space, our approach fully exploits semantic features as assistance\nto improve registration accuracy. Specifically, we extract per-point semantic\nlabels with advanced semantic segmentation networks and build a prior semantic\npart-to-part correspondence. Then we incorporate the semantic information into\na learning-based registration pipeline, consisting of three core modules: a\nsemantic-based farthest point sampling module to efficiently filter out\noutliers and dynamic objects; a semantic-augmented feature extraction module\nfor learning more discriminative point descriptors; a semantic-refined\ntransformation estimation module that utilizes prior semantic matching as a\nmask to refine point correspondences by reducing false matching for better\nconvergence. We evaluate the proposed SARNet extensively by using real-world\ndata from large regions of urban scenes and comparing it with alternative\nmethods. The code is available at\nhttps://github.com/WinterCodeForEverything/SARNet.\n","authors":["Chao Liu","Jianwei Guo","Dong-Ming Yan","Zhirong Liang","Xiaopeng Zhang","Zhanglin Cheng"],"pdf_url":"https://arxiv.org/pdf/2206.13117v2.pdf","comment":"Author information changes"},{"id":"http://arxiv.org/abs/2309.17389v2","updated":"2023-10-08T12:17:04Z","published":"2023-09-29T16:50:38Z","title":"Prompt-based test-time real image dehazing: a novel pipeline","summary":"  Existing methods attempt to improve models' generalization ability on\nreal-world hazy images by exploring well-designed training schemes (e.g.,\ncycleGAN, prior loss). However, most of them need very complicated training\nprocedures to achieve satisfactory results. In this work, we present a totally\nnovel testing pipeline called Prompt-based Test-Time Dehazing (PTTD) to help\ngenerate visually pleasing results of real-captured hazy images during the\ninference phase. We experimentally find that given a dehazing model trained on\nsynthetic data, by fine-tuning the statistics (i.e., mean and standard\ndeviation) of encoding features, PTTD is able to narrow the domain gap,\nboosting the performance of real image dehazing. Accordingly, we first apply a\nprompt generation module (PGM) to generate a visual prompt, which is the source\nof appropriate statistical perturbations for mean and standard deviation. And\nthen, we employ the feature adaptation module (FAM) into the existing dehazing\nmodels for adjusting the original statistics with the guidance of the generated\nprompt. Note that, PTTD is model-agnostic and can be equipped with various\nstate-of-the-art dehazing models trained on synthetic hazy-clean pairs.\nExtensive experimental results demonstrate that our PTTD is flexible meanwhile\nachieves superior performance against state-of-the-art dehazing methods in\nreal-world scenarios. The source code of our PTTD will be made available at\nhttps://github.com/cecret3350/PTTD-Dehazing.\n","authors":["Zixuan Chen","Zewei He","Ziqian Lu","Zhe-Ming Lu"],"pdf_url":"https://arxiv.org/pdf/2309.17389v2.pdf","comment":"update github link (https://github.com/cecret3350/PTTD-Dehazing)"},{"id":"http://arxiv.org/abs/2310.05136v1","updated":"2023-10-08T12:10:44Z","published":"2023-10-08T12:10:44Z","title":"InstructDET: Diversifying Referring Object Detection with Generalized\n  Instructions","summary":"  We propose InstructDET, a data-centric method for referring object detection\n(ROD) that localizes target objects based on user instructions. While deriving\nfrom referring expressions (REC), the instructions we leverage are greatly\ndiversified to encompass common user intentions related to object detection.\nFor one image, we produce tremendous instructions that refer to every single\nobject and different combinations of multiple objects. Each instruction and its\ncorresponding object bounding boxes (bbxs) constitute one training data pair.\nIn order to encompass common detection expressions, we involve emerging\nvision-language model (VLM) and large language model (LLM) to generate\ninstructions guided by text prompts and object bbxs, as the generalizations of\nfoundation models are effective to produce human-like expressions (e.g.,\ndescribing object property, category, and relationship). We name our\nconstructed dataset as InDET. It contains images, bbxs and generalized\ninstructions that are from foundation models. Our InDET is developed from\nexisting REC datasets and object detection datasets, with the expanding\npotential that any image with object bbxs can be incorporated through using our\nInstructDET method. By using our InDET dataset, we show that a conventional ROD\nmodel surpasses existing methods on standard REC datasets and our InDET test\nset. Our data-centric method InstructDET, with automatic data expansion by\nleveraging foundation models, directs a promising field that ROD can be greatly\ndiversified to execute common object detection instructions.\n","authors":["Ronghao Dang","Jiangyan Feng","Haodong Zhang","Chongjian Ge","Lin Song","Lijun Gong","Chengju Liu","Qijun Chen","Feng Zhu","Rui Zhao","Yibing Song"],"pdf_url":"https://arxiv.org/pdf/2310.05136v1.pdf","comment":"27 pages (include appendix), under review"},{"id":"http://arxiv.org/abs/2310.05134v1","updated":"2023-10-08T11:54:25Z","published":"2023-10-08T11:54:25Z","title":"LocoNeRF: A NeRF-based Approach for Local Structure from Motion for\n  Precise Localization","summary":"  Visual localization is a critical task in mobile robotics, and researchers\nare continuously developing new approaches to enhance its efficiency. In this\narticle, we propose a novel approach to improve the accuracy of visual\nlocalization using Structure from Motion (SfM) techniques. We highlight the\nlimitations of global SfM, which suffers from high latency, and the challenges\nof local SfM, which requires large image databases for accurate reconstruction.\nTo address these issues, we propose utilizing Neural Radiance Fields (NeRF), as\nopposed to image databases, to cut down on the space required for storage. We\nsuggest that sampling reference images around the prior query position can lead\nto further improvements. We evaluate the accuracy of our proposed method\nagainst ground truth obtained using LIDAR and Advanced Lidar Odometry and\nMapping in Real-time (A-LOAM), and compare its storage usage against local SfM\nwith COLMAP in the conducted experiments. Our proposed method achieves an\naccuracy of 0.068 meters compared to the ground truth, which is slightly lower\nthan the most advanced method COLMAP, which has an accuracy of 0.022 meters.\nHowever, the size of the database required for COLMAP is 400 megabytes, whereas\nthe size of our NeRF model is only 160 megabytes. Finally, we perform an\nablation study to assess the impact of using reference images from the NeRF\nreconstruction.\n","authors":["Artem Nenashev","Mikhail Kurenkov","Andrei Potapov","Iana Zhura","Maksim Katerishich","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2310.05134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05133v1","updated":"2023-10-08T11:48:19Z","published":"2023-10-08T11:48:19Z","title":"Geometry Aware Field-to-field Transformations for 3D Semantic\n  Segmentation","summary":"  We present a novel approach to perform 3D semantic segmentation solely from\n2D supervision by leveraging Neural Radiance Fields (NeRFs). By extracting\nfeatures along a surface point cloud, we achieve a compact representation of\nthe scene which is sample-efficient and conducive to 3D reasoning. Learning\nthis feature space in an unsupervised manner via masked autoencoding enables\nfew-shot segmentation. Our method is agnostic to the scene parameterization,\nworking on scenes fit with any type of NeRF.\n","authors":["Dominik Hollidt","Clinton Wang","Polina Golland","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2310.05133v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2303.11516v2","updated":"2023-10-08T11:44:50Z","published":"2023-03-21T00:32:31Z","title":"Linear-Covariance Loss for End-to-End Learning of 6D Pose Estimation","summary":"  Most modern image-based 6D object pose estimation methods learn to predict\n2D-3D correspondences, from which the pose can be obtained using a PnP solver.\nBecause of the non-differentiable nature of common PnP solvers, these methods\nare supervised via the individual correspondences. To address this, several\nmethods have designed differentiable PnP strategies, thus imposing supervision\non the pose obtained after the PnP step. Here, we argue that this conflicts\nwith the averaging nature of the PnP problem, leading to gradients that may\nencourage the network to degrade the accuracy of individual correspondences. To\naddress this, we derive a loss function that exploits the ground truth pose\nbefore solving the PnP problem. Specifically, we linearize the PnP solver\naround the ground-truth pose and compute the covariance of the resulting pose\ndistribution. We then define our loss based on the diagonal covariance\nelements, which entails considering the final pose estimate yet not suffering\nfrom the PnP averaging issue. Our experiments show that our loss consistently\nimproves the pose estimation accuracy for both dense and sparse correspondence\nbased methods, achieving state-of-the-art results on both Linemod-Occluded and\nYCB-Video.\n","authors":["Fulin Liu","Yinlin Hu","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2303.11516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05126v1","updated":"2023-10-08T11:33:09Z","published":"2023-10-08T11:33:09Z","title":"UReader: Universal OCR-free Visually-situated Language Understanding\n  with Multimodal Large Language Model","summary":"  Text is ubiquitous in our visual world, conveying crucial information, such\nas in documents, websites, and everyday photographs. In this work, we propose\nUReader, a first exploration of universal OCR-free visually-situated language\nunderstanding based on the Multimodal Large Language Model (MLLM). By\nleveraging the shallow text recognition ability of the MLLM, we only finetuned\n1.2% parameters and the training cost is much lower than previous work\nfollowing domain-specific pretraining and finetuning paradigms. Concretely,\nUReader is jointly finetuned on a wide range of Visually-situated Language\nUnderstanding tasks via a unified instruction format. To enhance the visual\ntext and semantic understanding, we further apply two auxiliary tasks with the\nsame format, namely text reading and key points generation tasks. We design a\nshape-adaptive cropping module before the encoder-decoder architecture of MLLM\nto leverage the frozen low-resolution vision encoder for processing\nhigh-resolution images. Without downstream finetuning, our single model\nachieves state-of-the-art ocr-free performance in 8 out of 10 visually-situated\nlanguage understanding tasks, across 5 domains: documents, tables, charts,\nnatural images, and webpage screenshots. Codes and instruction-tuning datasets\nwill be released.\n","authors":["Jiabo Ye","Anwen Hu","Haiyang Xu","Qinghao Ye","Ming Yan","Guohai Xu","Chenliang Li","Junfeng Tian","Qi Qian","Ji Zhang","Qin Jin","Liang He","Xin Alex Lin","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2310.05126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05125v1","updated":"2023-10-08T11:32:50Z","published":"2023-10-08T11:32:50Z","title":"Bidirectional Knowledge Reconfiguration for Lightweight Point Cloud\n  Analysis","summary":"  Point cloud analysis faces computational system overhead, limiting its\napplication on mobile or edge devices. Directly employing small models may\nresult in a significant drop in performance since it is difficult for a small\nmodel to adequately capture local structure and global shape information\nsimultaneously, which are essential clues for point cloud analysis. This paper\nexplores feature distillation for lightweight point cloud models. To mitigate\nthe semantic gap between the lightweight student and the cumbersome teacher, we\npropose bidirectional knowledge reconfiguration (BKR) to distill informative\ncontextual knowledge from the teacher to the student. Specifically, a top-down\nknowledge reconfiguration and a bottom-up knowledge reconfiguration are\ndeveloped to inherit diverse local structure information and consistent global\nshape knowledge from the teacher, respectively. However, due to the farthest\npoint sampling in most point cloud models, the intermediate features between\nteacher and student are misaligned, deteriorating the feature distillation\nperformance. To eliminate it, we propose a feature mover's distance (FMD) loss\nbased on optimal transportation, which can measure the distance between\nunordered point cloud features effectively. Extensive experiments conducted on\nshape classification, part segmentation, and semantic segmentation benchmarks\ndemonstrate the universality and superiority of our method.\n","authors":["Peipei Li","Xing Cui","Yibo Hu","Man Zhang","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2310.05125v1.pdf","comment":"Accepted by IEEE Transactions on Multimedia (TMM)"},{"id":"http://arxiv.org/abs/2310.05124v1","updated":"2023-10-08T11:30:22Z","published":"2023-10-08T11:30:22Z","title":"Cross-domain Robust Deepfake Bias Expansion Network for Face Forgery\n  Detection","summary":"  The rapid advancement of deepfake technologies raises significant concerns\nabout the security of face recognition systems. While existing methods leverage\nthe clues left by deepfake techniques for face forgery detection, malicious\nusers may intentionally manipulate forged faces to obscure the traces of\ndeepfake clues and thereby deceive detection tools. Meanwhile, attaining\ncross-domain robustness for data-based methods poses a challenge due to\npotential gaps in the training data, which may not encompass samples from all\nrelevant domains. Therefore, in this paper, we introduce a solution - a\nCross-Domain Robust Bias Expansion Network (BENet) - designed to enhance face\nforgery detection. BENet employs an auto-encoder to reconstruct input faces,\nmaintaining the invariance of real faces while selectively enhancing the\ndifference between reconstructed fake faces and their original counterparts.\nThis enhanced bias forms a robust foundation upon which dependable forgery\ndetection can be built. To optimize the reconstruction results in BENet, we\nemploy a bias expansion loss infused with contrastive concepts to attain the\naforementioned objective. In addition, to further heighten the amplification of\nforged clues, BENet incorporates a Latent-Space Attention (LSA) module. This\nLSA module effectively captures variances in latent features between the\nauto-encoder's encoder and decoder, placing emphasis on inconsistent\nforgery-related information. Furthermore, BENet incorporates a cross-domain\ndetector with a threshold to determine whether the sample belongs to a known\ndistribution. The correction of classification results through the cross-domain\ndetector enables BENet to defend against unknown deepfake attacks from\ncross-domain. Extensive experiments demonstrate the superiority of BENet\ncompared with state-of-the-art methods in intra-database and cross-database\nevaluations.\n","authors":["Weihua Liu","Lin Li","Chaochao Lin","Said Boumaraf"],"pdf_url":"https://arxiv.org/pdf/2310.05124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05119v1","updated":"2023-10-08T11:20:02Z","published":"2023-10-08T11:20:02Z","title":"Dynamic Multi-Domain Knowledge Networks for Chest X-ray Report\n  Generation","summary":"  The automated generation of radiology diagnostic reports helps radiologists\nmake timely and accurate diagnostic decisions while also enhancing clinical\ndiagnostic efficiency. However, the significant imbalance in the distribution\nof data between normal and abnormal samples (including visual and textual\nbiases) poses significant challenges for a data-driven task like automatically\ngenerating diagnostic radiology reports. Therefore, we propose a Dynamic\nMulti-Domain Knowledge(DMDK) network for radiology diagnostic report\ngeneration. The DMDK network consists of four modules: Chest Feature\nExtractor(CFE), Dynamic Knowledge Extractor(DKE), Specific Knowledge\nExtractor(SKE), and Multi-knowledge Integrator(MKI) module. Specifically, the\nCFE module is primarily responsible for extracting the unprocessed visual\nmedical features of the images. The DKE module is responsible for extracting\ndynamic disease topic labels from the retrieved radiology diagnostic reports.\nWe then fuse the dynamic disease topic labels with the original visual features\nof the images to highlight the abnormal regions in the original visual features\nto alleviate the visual data bias problem. The SKE module expands upon the\nconventional static knowledge graph to mitigate textual data biases and amplify\nthe interpretability capabilities of the model via domain-specific dynamic\nknowledge graphs. The MKI distills all the knowledge and generates the final\ndiagnostic radiology report. We performed extensive experiments on two widely\nused datasets, IU X-Ray and MIMIC-CXR. The experimental results demonstrate the\neffectiveness of our method, with all evaluation metrics outperforming previous\nstate-of-the-art models.\n","authors":["Weihua Liu","Youyuan Xue","Chaochao Lin","Said Boumaraf"],"pdf_url":"https://arxiv.org/pdf/2310.05119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18980v2","updated":"2023-10-08T11:08:31Z","published":"2023-05-30T12:24:38Z","title":"Multi-modal Queried Object Detection in the Wild","summary":"  We introduce MQ-Det, an efficient architecture and pre-training strategy\ndesign to utilize both textual description with open-set generalization and\nvisual exemplars with rich description granularity as category queries, namely,\nMulti-modal Queried object Detection, for real-world detection with both\nopen-vocabulary categories and various granularity. MQ-Det incorporates vision\nqueries into existing well-established language-queried-only detectors. A\nplug-and-play gated class-scalable perceiver module upon the frozen detector is\nproposed to augment category text with class-wise visual information. To\naddress the learning inertia problem brought by the frozen detector, a vision\nconditioned masked language prediction strategy is proposed. MQ-Det's simple\nyet effective architecture and training strategy design is compatible with most\nlanguage-queried object detectors, thus yielding versatile applications.\nExperimental results demonstrate that multi-modal queries largely boost\nopen-world detection. For instance, MQ-Det significantly improves the\nstate-of-the-art open-set detector GLIP by +7.8% AP on the LVIS benchmark via\nmulti-modal queries without any downstream finetuning, and averagely +6.3% AP\non 13 few-shot downstream tasks, with merely additional 3% modulating time\nrequired by GLIP. Code is available at https://github.com/YifanXu74/MQ-Det.\n","authors":["Yifan Xu","Mengdan Zhang","Chaoyou Fu","Peixian Chen","Xiaoshan Yang","Ke Li","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2305.18980v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05109v1","updated":"2023-10-08T10:47:24Z","published":"2023-10-08T10:47:24Z","title":"Lightweight In-Context Tuning for Multimodal Unified Models","summary":"  In-context learning (ICL) involves reasoning from given contextual examples.\nAs more modalities comes, this procedure is becoming more challenging as the\ninterleaved input modalities convolutes the understanding process. This is\nexemplified by the observation that multimodal models often struggle to\neffectively extrapolate from contextual examples to perform ICL. To address\nthese challenges, we introduce MultiModal In-conteXt Tuning (M$^2$IXT), a\nlightweight module to enhance the ICL capabilities of multimodal unified\nmodels. The proposed M$^2$IXT module perceives an expandable context window to\nincorporate various labeled examples of multiple modalities (e.g., text, image,\nand coordinates). It can be prepended to various multimodal unified models\n(e.g., OFA, Unival, LLaVA) of different architectures and trained via a\nmixed-tasks strategy to enable rapid few-shot adaption on multiple tasks and\ndatasets. When tuned on as little as 50K multimodal data, M$^2$IXT can boost\nthe few-shot ICL performance significantly (e.g., 18\\% relative increase for\nOFA), and obtained state-of-the-art results across an array of tasks including\nvisual question answering, image captioning, visual grounding, and visual\nentailment, while being considerably small in terms of model parameters (e.g.,\n$\\sim$$20\\times$ smaller than Flamingo or MMICL), highlighting the flexibility\nand effectiveness of M$^2$IXT as a multimodal in-context learner.\n","authors":["Yixin Chen","Shuai Zhang","Boran Han","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2310.05109v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2310.05108v1","updated":"2023-10-08T10:44:05Z","published":"2023-10-08T10:44:05Z","title":"Enhancing Representations through Heterogeneous Self-Supervised Learning","summary":"  Incorporating heterogeneous representations from different architectures has\nfacilitated various vision tasks, e.g., some hybrid networks combine\ntransformers and convolutions. However, complementarity between such\nheterogeneous architectures has not been well exploited in self-supervised\nlearning. Thus, we propose Heterogeneous Self-Supervised Learning (HSSL), which\nenforces a base model to learn from an auxiliary head whose architecture is\nheterogeneous from the base model. In this process, HSSL endows the base model\nwith new characteristics in a representation learning way without structural\nchanges. To comprehensively understand the HSSL, we conduct experiments on\nvarious heterogeneous pairs containing a base model and an auxiliary head. We\ndiscover that the representation quality of the base model moves up as their\narchitecture discrepancy grows. This observation motivates us to propose a\nsearch strategy that quickly determines the most suitable auxiliary head for a\nspecific base model to learn and several simple but effective methods to\nenlarge the model discrepancy. The HSSL is compatible with various\nself-supervised methods, achieving superior performances on various downstream\ntasks, including image classification, semantic segmentation, instance\nsegmentation, and object detection. Our source code will be made publicly\navailable.\n","authors":["Zhong-Yu Li","Bo-Wen Yin","Shanghua Gao","Yongxiang Liu","Li Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.05108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05107v1","updated":"2023-10-08T10:28:42Z","published":"2023-10-08T10:28:42Z","title":"OV-PARTS: Towards Open-Vocabulary Part Segmentation","summary":"  Segmenting and recognizing diverse object parts is a crucial ability in\napplications spanning various computer vision and robotic tasks. While\nsignificant progress has been made in object-level Open-Vocabulary Semantic\nSegmentation (OVSS), i.e., segmenting objects with arbitrary text, the\ncorresponding part-level research poses additional challenges. Firstly, part\nsegmentation inherently involves intricate boundaries, while limited annotated\ndata compounds the challenge. Secondly, part segmentation introduces an open\ngranularity challenge due to the diverse and often ambiguous definitions of\nparts in the open world. Furthermore, the large-scale vision and language\nmodels, which play a key role in the open vocabulary setting, struggle to\nrecognize parts as effectively as objects. To comprehensively investigate and\ntackle these challenges, we propose an Open-Vocabulary Part Segmentation\n(OV-PARTS) benchmark. OV-PARTS includes refined versions of two publicly\navailable datasets: Pascal-Part-116 and ADE20K-Part-234. And it covers three\nspecific tasks: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part\nSegmentation, and Few-Shot Part Segmentation, providing insights into\nanalogical reasoning, open granularity and few-shot adapting abilities of\nmodels. Moreover, we analyze and adapt two prevailing paradigms of existing\nobject-level OVSS methods for OV-PARTS. Extensive experimental analysis is\nconducted to inspire future research in leveraging foundational models for\nOV-PARTS. The code and dataset are available at\nhttps://github.com/OpenRobotLab/OV_PARTS.\n","authors":["Meng Wei","Xiaoyu Yue","Wenwei Zhang","Shu Kong","Xihui Liu","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2310.05107v1.pdf","comment":"Accepted by NeurIPS Dataset and Benchmark Track 2023"},{"id":"http://arxiv.org/abs/2208.04441v2","updated":"2023-10-08T09:51:43Z","published":"2022-08-08T22:02:10Z","title":"Txt2Img-MHN: Remote Sensing Image Generation from Text Using Modern\n  Hopfield Networks","summary":"  The synthesis of high-resolution remote sensing images based on text\ndescriptions has great potential in many practical application scenarios.\nAlthough deep neural networks have achieved great success in many important\nremote sensing tasks, generating realistic remote sensing images from text\ndescriptions is still very difficult. To address this challenge, we propose a\nnovel text-to-image modern Hopfield network (Txt2Img-MHN). The main idea of\nTxt2Img-MHN is to conduct hierarchical prototype learning on both text and\nimage embeddings with modern Hopfield layers. Instead of directly learning\nconcrete but highly diverse text-image joint feature representations for\ndifferent semantics, Txt2Img-MHN aims to learn the most representative\nprototypes from text-image embeddings, achieving a coarse-to-fine learning\nstrategy. These learned prototypes can then be utilized to represent more\ncomplex semantics in the text-to-image generation task. To better evaluate the\nrealism and semantic consistency of the generated images, we further conduct\nzero-shot classification on real remote sensing data using the classification\nmodel trained on synthesized images. Despite its simplicity, we find that the\noverall accuracy in the zero-shot classification may serve as a good metric to\nevaluate the ability to generate an image from text. Extensive experiments on\nthe benchmark remote sensing text-image dataset demonstrate that the proposed\nTxt2Img-MHN can generate more realistic remote sensing images than existing\nmethods. Code and pre-trained models are available online\n(https://github.com/YonghaoXu/Txt2Img-MHN).\n","authors":["Yonghao Xu","Weikang Yu","Pedram Ghamisi","Michael Kopp","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2208.04441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07207v2","updated":"2023-10-08T09:49:53Z","published":"2023-06-12T16:11:10Z","title":"Valley: Video Assistant with Large Language model Enhanced abilitY","summary":"  Large language models (LLMs), with their remarkable conversational\ncapabilities, have demonstrated impressive performance across various\napplications and have emerged as formidable AI assistants. In view of this, it\nraises an intuitive question: Can we harness the power of LLMs to build\nmultimodal AI assistants for visual applications? Recently, several multi-modal\nmodels have been developed for this purpose. They typically pre-train an\nadaptation module to align the semantics of the vision encoder and language\nmodel, followed by fine-tuning on instruction-following data. However, despite\nthe success of this pipeline in image and language understanding, its\neffectiveness in joint video and language understanding has not been widely\nexplored. In this paper, we aim to develop a novel multi-modal foundation model\ncapable of comprehending video, image, and language within a general framework.\nTo achieve this goal, we introduce Valley, a Video Assistant with Large\nLanguage model Enhanced abilitY. The Valley consists of a LLM, a temporal\nmodeling module, a visual encoder, and a simple projection module designed to\nbridge visual and textual modes. To empower Valley with video comprehension and\ninstruction-following capabilities, we construct a video instruction dataset\nand adopt a two-stage tuning procedure to train it. Specifically, we employ\nChatGPT to facilitate the construction of task-oriented conversation data\nencompassing various tasks, including multi-shot captions, long video\ndescriptions, action recognition, causal relationship inference, etc.\nSubsequently, we adopt a pre-training-then-instructions-tuned pipeline to align\nvisual and textual modalities and improve the instruction-following capability\nof Valley. Qualitative experiments demonstrate that Valley has the potential to\nfunction as a highly effective video assistant that can make complex video\nunderstanding scenarios easy.\n","authors":["Ruipu Luo","Ziwang Zhao","Min Yang","Junwei Dong","Da Li","Pengcheng Lu","Tao Wang","Linmei Hu","Minghui Qiu","Zhongyu Wei"],"pdf_url":"https://arxiv.org/pdf/2306.07207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.09423v3","updated":"2023-10-08T09:16:28Z","published":"2023-04-19T04:55:28Z","title":"ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling","summary":"  The research fields of parametric face model and 3D face reconstruction have\nbeen extensively studied. However, a critical question remains unanswered: how\nto tailor the face model for specific reconstruction settings. We argue that\nreconstruction with multi-view uncalibrated images demands a new model with\nstronger capacity. Our study shifts attention from data-dependent 3D Morphable\nModels (3DMM) to an understudied human-designed skinning model. We propose\nAdaptive Skinning Model (ASM), which redefines the skinning model with more\ncompact and fully tunable parameters. With extensive experiments, we\ndemonstrate that ASM achieves significantly improved capacity than 3DMM, with\nthe additional advantage of model size and easy implementation for new\ntopology. We achieve state-of-the-art performance with ASM for multi-view\nreconstruction on the Florence MICC Coop benchmark. Our quantitative analysis\ndemonstrates the importance of a high-capacity model for fully exploiting\nabundant information from multi-view input in reconstruction. Furthermore, our\nmodel with physical-semantic parameters can be directly utilized for real-world\napplications, such as in-game avatar creation. As a result, our work opens up\nnew research direction for parametric face model and facilitates future\nresearch on multi-view reconstruction.\n","authors":["Kai Yang","Hong Shang","Tianyang Shi","Xinghan Chen","Jingkai Zhou","Zhongqian Sun","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2304.09423v3.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2310.05082v1","updated":"2023-10-08T09:13:04Z","published":"2023-10-08T09:13:04Z","title":"Cross-head mutual Mean-Teaching for semi-supervised medical image\n  segmentation","summary":"  Semi-supervised medical image segmentation (SSMIS) has witnessed substantial\nadvancements by leveraging limited labeled data and abundant unlabeled data.\nNevertheless, existing state-of-the-art methods encounter challenges in\naccurately predicting labels for the unlabeled data, resulting in disruptive\nnoise during training and susceptibility to erroneous information overfitting.\nAdditionally, applying perturbations to inaccurate predictions further reduces\nconsistent learning. To address these concerns, a novel \\textbf{C}ross-head\n\\textbf{m}utual \\textbf{m}ean-\\textbf{t}eaching Network (CMMT-Net) is proposed\nto address these issues. The CMMT-Net comprises teacher-student networks and\nincorporates strong-weak data augmentation within a shared encoder,\nfacilitating cross-head co-training by capitalizing on both self-training and\nconsistent learning. The consistent learning is enhanced by averaging teacher\nnetworks and mutual virtual adversarial training, leading to deterministic and\nhigher-quality predictions. The diversity of consistency training samples can\nbe enhanced through the use of Cross-Set CutMix, which also helps mitigate\nissues related to distribution mismatch. Notably, CMMT-Net simultaneously\nimplements data-level, feature-level, and network-level perturbations, boosting\nmodel diversity and generalization performance. The proposed method\nconsistently outperforms existing SSMIS methods on three publicly available\ndatasets across various semi-supervised settings. Code and logs will be\navailable at \\url{https://github.com/Leesoon1984/CMMT-Net}.\n","authors":["Wei Li","Ruifeng Bian","Wenyi Zhao","Huihua Yang"],"pdf_url":"https://arxiv.org/pdf/2310.05082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08730v4","updated":"2023-10-08T08:16:54Z","published":"2023-08-17T01:59:59Z","title":"Learning A Coarse-to-Fine Diffusion Transformer for Image Restoration","summary":"  Recent years have witnessed the remarkable performance of diffusion models in\nvarious vision tasks. However, for image restoration that aims to recover clear\nimages with sharper details from given degraded observations, diffusion-based\nmethods may fail to recover promising results due to inaccurate noise\nestimation. Moreover, simple constraining noises cannot effectively learn\ncomplex degradation information, which subsequently hinders the model capacity.\nTo solve the above problems, we propose a coarse-to-fine diffusion Transformer\n(C2F-DFT) for image restoration. Specifically, our C2F-DFT contains diffusion\nself-attention (DFSA) and diffusion feed-forward network (DFN) within a new\ncoarse-to-fine training scheme. The DFSA and DFN respectively capture the\nlong-range diffusion dependencies and learn hierarchy diffusion representation\nto facilitate better restoration. In the coarse training stage, our C2F-DFT\nestimates noises and then generates the final clean image by a sampling\nalgorithm. To further improve the restoration quality, we propose a simple yet\neffective fine training scheme. It first exploits the coarse-trained diffusion\nmodel with fixed steps to generate restoration results, which then would be\nconstrained with corresponding ground-truth ones to optimize the models to\nremedy the unsatisfactory results affected by inaccurate noise estimation.\nExtensive experiments show that C2F-DFT significantly outperforms\ndiffusion-based restoration method IR-SDE and achieves competitive performance\ncompared with Transformer-based state-of-the-art methods on $3$ tasks,\nincluding image deraining, image deblurring, and real image denoising. Code is\navailable at https://github.com/wlydlut/C2F-DFT.\n","authors":["Liyan Wang","Qinyu Yang","Cong Wang","Wei Wang","Jinshan Pan","Zhixun Su"],"pdf_url":"https://arxiv.org/pdf/2308.08730v4.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.05060v1","updated":"2023-10-08T08:02:43Z","published":"2023-10-08T08:02:43Z","title":"Video-CSR: Complex Video Digest Creation for Visual-Language Models","summary":"  We present a novel task and human annotated dataset for evaluating the\nability for visual-language models to generate captions and summaries for\nreal-world video clips, which we call Video-CSR (Captioning, Summarization and\nRetrieval). The dataset contains 4.8K YouTube video clips of 20-60 seconds in\nduration and covers a wide range of topics and interests. Each video clip\ncorresponds to 5 independently annotated captions (1 sentence) and summaries\n(3-10 sentences). Given any video selected from the dataset and its\ncorresponding ASR information, we evaluate visual-language models on either\ncaption or summary generation that is grounded in both the visual and auditory\ncontent of the video. Additionally, models are also evaluated on caption- and\nsummary-based retrieval tasks, where the summary-based retrieval task requires\nthe identification of a target video given excerpts of a corresponding summary.\nGiven the novel nature of the paragraph-length video summarization task, we\nperform extensive comparative analyses of different existing evaluation metrics\nand their alignment with human preferences. Finally, we propose a foundation\nmodel with competitive generation and retrieval capabilities that serves as a\nbaseline for the Video-CSR task. We aim for Video-CSR to serve as a useful\nevaluation set in the age of large language models and complex multi-modal\ntasks.\n","authors":["Tingkai Liu","Yunzhe Tao","Haogeng Liu","Qihang Fan","Ding Zhou","Huaibo Huang","Ran He","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2310.05060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05058v1","updated":"2023-10-08T07:48:25Z","published":"2023-10-08T07:48:25Z","title":"Learning Separable Hidden Unit Contributions for Speaker-Adaptive\n  Lip-Reading","summary":"  In this paper, we propose a novel method for speaker adaptation in lip\nreading, motivated by two observations. Firstly, a speaker's own\ncharacteristics can always be portrayed well by his/her few facial images or\neven a single image with shallow networks, while the fine-grained dynamic\nfeatures associated with speech content expressed by the talking face always\nneed deep sequential networks to represent accurately. Therefore, we treat the\nshallow and deep layers differently for speaker adaptive lip reading. Secondly,\nwe observe that a speaker's unique characteristics ( e.g. prominent oral cavity\nand mandible) have varied effects on lip reading performance for different\nwords and pronunciations, necessitating adaptive enhancement or suppression of\nthe features for robust lip reading. Based on these two observations, we\npropose to take advantage of the speaker's own characteristics to automatically\nlearn separable hidden unit contributions with different targets for shallow\nlayers and deep layers respectively. For shallow layers where features related\nto the speaker's characteristics are stronger than the speech content related\nfeatures, we introduce speaker-adaptive features to learn for enhancing the\nspeech content features. For deep layers where both the speaker's features and\nthe speech content features are all expressed well, we introduce the\nspeaker-adaptive features to learn for suppressing the speech content\nirrelevant noise for robust lip reading. Our approach consistently outperforms\nexisting methods, as confirmed by comprehensive analysis and comparison across\ndifferent settings. Besides the evaluation on the popular LRW-ID and GRID\ndatasets, we also release a new dataset for evaluation, CAS-VSR-S68h, to\nfurther assess the performance in an extreme setting where just a few speakers\nare available but the speech content covers a large and diversified range.\n","authors":["Songtao Luo","Shuang Yang","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2310.05058v1.pdf","comment":"Accepted to BMVC 2023 20pages"},{"id":"http://arxiv.org/abs/2310.05056v1","updated":"2023-10-08T07:42:41Z","published":"2023-10-08T07:42:41Z","title":"Language-driven Open-Vocabulary Keypoint Detection for Animal Body and\n  Face","summary":"  Current approaches for image-based keypoint detection on animal (including\nhuman) body and face are limited to specific keypoints and species. We address\nthe limitation by proposing the Open-Vocabulary Keypoint Detection (OVKD) task.\nIt aims to use text prompts to localize arbitrary keypoints of any species. To\naccomplish this objective, we propose Open-Vocabulary Keypoint Detection with\nSemantic-feature Matching (KDSM), which utilizes both vision and language\nmodels to harness the relationship between text and vision and thus achieve\nkeypoint detection through associating text prompt with relevant keypoint\nfeatures. Additionally, KDSM integrates domain distribution matrix matching and\nsome special designs to reinforce the relationship between language and vision,\nthereby improving the model's generalizability and performance. Extensive\nexperiments show that our proposed components bring significant performance\nimprovements, and our overall method achieves impressive results in OVKD.\nRemarkably, our method outperforms the state-of-the-art few-shot keypoint\ndetection methods using a zero-shot fashion. We will make the source code\npublicly accessible.\n","authors":["Hao Zhang","Kaipeng Zhang","Lumin Xu","Shenqi Lai","Wenqi Shao","Naning Zheng","Ping Luo","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2310.05056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09019v2","updated":"2023-10-08T07:41:58Z","published":"2023-07-18T07:15:26Z","title":"U-shaped Transformer: Retain High Frequency Context in Time Series\n  Analysis","summary":"  Time series prediction plays a crucial role in various industrial fields. In\nrecent years, neural networks with a transformer backbone have achieved\nremarkable success in many domains, including computer vision and NLP. In time\nseries analysis domain, some studies have suggested that even the simplest MLP\nnetworks outperform advanced transformer-based networks on time series forecast\ntasks. However, we believe these findings indicate there to be low-rank\nproperties in time series sequences. In this paper, we consider the low-pass\ncharacteristics of transformers and try to incorporate the advantages of MLP.\nWe adopt skip-layer connections inspired by Unet into traditional transformer\nbackbone, thus preserving high-frequency context from input to output, namely\nU-shaped Transformer. We introduce patch merge and split operation to extract\nfeatures with different scales and use larger datasets to fully make use of the\ntransformer backbone. Our experiments demonstrate that the model performs at an\nadvanced level across multiple datasets with relatively low cost.\n","authors":["Qingkui Chen","Yiqin Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.09019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05055v1","updated":"2023-10-08T07:41:15Z","published":"2023-10-08T07:41:15Z","title":"FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in\n  Medical Image Analysis","summary":"  Training models with robust group fairness properties is crucial in ethically\nsensitive application areas such as medical diagnosis. Despite the growing body\nof work aiming to minimise demographic bias in AI, this problem remains\nchallenging. A key reason for this challenge is the fairness generalisation\ngap: High-capacity deep learning models can fit all training data nearly\nperfectly, and thus also exhibit perfect fairness during training. In this\ncase, bias emerges only during testing when generalisation performance differs\nacross subgroups. This motivates us to take a bi-level optimisation perspective\non fair learning: Optimising the learning strategy based on validation\nfairness. Specifically, we consider the highly effective workflow of adapting\npre-trained models to downstream medical imaging tasks using\nparameter-efficient fine-tuning (PEFT) techniques. There is a trade-off between\nupdating more parameters, enabling a better fit to the task of interest vs.\nfewer parameters, potentially reducing the generalisation gap. To manage this\ntradeoff, we propose FairTune, a framework to optimise the choice of PEFT\nparameters with respect to fairness. We demonstrate empirically that FairTune\nleads to improved fairness on a range of medical imaging datasets.\n","authors":["Raman Dutt","Ondrej Bohdal","Sotirios A. Tsaftaris","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2310.05055v1.pdf","comment":"9 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2206.00311v2","updated":"2023-10-08T07:08:00Z","published":"2022-06-01T08:27:19Z","title":"MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining","summary":"  Text images contain both visual and linguistic information. However, existing\npre-training techniques for text recognition mainly focus on either visual\nrepresentation learning or linguistic knowledge learning. In this paper, we\npropose a novel approach MaskOCR to unify vision and language pre-training in\nthe classical encoder-decoder recognition framework. We adopt the masked image\nmodeling approach to pre-train the feature encoder using a large set of\nunlabeled real text images, which allows us to learn strong visual\nrepresentations. In contrast to introducing linguistic knowledge with an\nadditional language model, we directly pre-train the sequence decoder.\nSpecifically, we transform text data into synthesized text images to unify the\ndata modalities of vision and language, and enhance the language modeling\ncapability of the sequence decoder using a proposed masked image-language\nmodeling scheme. Significantly, the encoder is frozen during the pre-training\nphase of the sequence decoder. Experimental results demonstrate that our\nproposed method achieves superior performance on benchmark datasets, including\nChinese and English text images. The code for our approach will be made\navailable.\n","authors":["Pengyuan Lyu","Chengquan Zhang","Shanshan Liu","Meina Qiao","Yangliu Xu","Liang Wu","Kun Yao","Junyu Han","Errui Ding","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2206.00311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16254v2","updated":"2023-10-08T06:57:39Z","published":"2023-03-28T18:59:17Z","title":"CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using\n  Transformer-based Neural Representations","summary":"  Cryo-electron microscopy (cryo-EM) allows for the high-resolution\nreconstruction of 3D structures of proteins and other biomolecules. Successful\nreconstruction of both shape and movement greatly helps understand the\nfundamental processes of life. However, it is still challenging to reconstruct\nthe continuous motions of 3D structures from hundreds of thousands of noisy and\nrandomly oriented 2D cryo-EM images. Recent advancements use Fourier domain\ncoordinate-based neural networks to continuously model 3D conformations, yet\nthey often struggle to capture local flexible regions accurately. We propose\nCryoFormer, a new approach for continuous heterogeneous cryo-EM reconstruction.\nOur approach leverages an implicit feature volume directly in the real domain\nas the 3D representation. We further introduce a novel query-based deformation\ntransformer decoder to improve the reconstruction quality. Our approach is\ncapable of refining pre-computed pose estimations and locating flexible\nregions. In experiments, our method outperforms current approaches on three\npublic datasets (1 synthetic and 2 experimental) and a new synthetic dataset of\nPEDV spike protein. The code and new synthetic dataset will be released for\nbetter reproducibility of our results. Project page:\nhttps://cryoformer.github.io.\n","authors":["Xinhang Liu","Yan Zeng","Yifan Qin","Hao Li","Jiakai Zhang","Lan Xu","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2303.16254v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07684v2","updated":"2023-10-08T06:41:12Z","published":"2023-06-13T10:55:20Z","title":"Lookaround Optimizer: $k$ steps around, 1 step average","summary":"  Weight Average (WA) is an active research topic due to its simplicity in\nensembling deep networks and the effectiveness in promoting generalization.\nExisting weight average approaches, however, are often carried out along only\none training trajectory in a post-hoc manner (i.e., the weights are averaged\nafter the entire training process is finished), which significantly degrades\nthe diversity between networks and thus impairs the effectiveness. In this\npaper, inspired by weight average, we propose Lookaround, a straightforward yet\neffective SGD-based optimizer leading to flatter minima with better\ngeneralization. Specifically, Lookaround iterates two steps during the whole\ntraining period: the around step and the average step. In each iteration, 1)\nthe around step starts from a common point and trains multiple networks\nsimultaneously, each on transformed data by a different data augmentation, and\n2) the average step averages these trained networks to get the averaged\nnetwork, which serves as the starting point for the next iteration. The around\nstep improves the functionality diversity while the average step guarantees the\nweight locality of these networks during the whole training, which is essential\nfor WA to work. We theoretically explain the superiority of Lookaround by\nconvergence analysis, and make extensive experiments to evaluate Lookaround on\npopular benchmarks including CIFAR and ImageNet with both CNNs and ViTs,\ndemonstrating clear superiority over state-of-the-arts. Our code is available\nat https://github.com/Ardcy/Lookaround.\n","authors":["Jiangtao Zhang","Shunyu Liu","Jie Song","Tongtian Zhu","Zhengqi Xu","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2306.07684v2.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.05026v1","updated":"2023-10-08T06:10:09Z","published":"2023-10-08T06:10:09Z","title":"Low-Resolution Self-Attention for Semantic Segmentation","summary":"  Semantic segmentation tasks naturally require high-resolution information for\npixel-wise segmentation and global context information for class prediction.\nWhile existing vision transformers demonstrate promising performance, they\noften utilize high resolution context modeling, resulting in a computational\nbottleneck. In this work, we challenge conventional wisdom and introduce the\nLow-Resolution Self-Attention (LRSA) mechanism to capture global context at a\nsignificantly reduced computational cost. Our approach involves computing\nself-attention in a fixed low-resolution space regardless of the input image's\nresolution, with additional 3x3 depth-wise convolutions to capture fine details\nin the high-resolution space. We demonstrate the effectiveness of our LRSA\napproach by building the LRFormer, a vision transformer with an encoder-decoder\nstructure. Extensive experiments on the ADE20K, COCO-Stuff, and Cityscapes\ndatasets demonstrate that LRFormer outperforms state-of-the-art models. The\ncode will be made available at https://github.com/yuhuan-wu/LRFormer.\n","authors":["Yu-Huan Wu","Shi-Chen Zhang","Yun Liu","Le Zhang","Xin Zhan","Daquan Zhou","Jiashi Feng","Ming-Ming Cheng","Liangli Zhen"],"pdf_url":"https://arxiv.org/pdf/2310.05026v1.pdf","comment":"11 pages, 11 tables, 6 figures"},{"id":"http://arxiv.org/abs/2310.05024v1","updated":"2023-10-08T06:05:01Z","published":"2023-10-08T06:05:01Z","title":"Single Stage Warped Cloth Learning and Semantic-Contextual Attention\n  Feature Fusion for Virtual TryOn","summary":"  Image-based virtual try-on aims to fit an in-shop garment onto a clothed\nperson image. Garment warping, which aligns the target garment with the\ncorresponding body parts in the person image, is a crucial step in achieving\nthis goal. Existing methods often use multi-stage frameworks to handle clothes\nwarping, person body synthesis and tryon generation separately or rely on noisy\nintermediate parser-based labels. We propose a novel single-stage framework\nthat implicitly learns the same without explicit multi-stage learning. Our\napproach utilizes a novel semantic-contextual fusion attention module for\ngarment-person feature fusion, enabling efficient and realistic cloth warping\nand body synthesis from target pose keypoints. By introducing a lightweight\nlinear attention framework that attends to garment regions and fuses multiple\nsampled flow fields, we also address misalignment and artifacts present in\nprevious methods. To achieve simultaneous learning of warped garment and try-on\nresults, we introduce a Warped Cloth Learning Module. WCLM uses segmented\nwarped garments as ground truth, operating within a single-stage paradigm. Our\nproposed approach significantly improves the quality and efficiency of virtual\ntry-on methods, providing users with a more reliable and realistic virtual\ntry-on experience. We evaluate our method on the VITON dataset and demonstrate\nits state-of-the-art performance in terms of both qualitative and quantitative\nmetrics.\n","authors":["Sanhita Pathak","Vinay Kaushik","Brejesh Lall"],"pdf_url":"https://arxiv.org/pdf/2310.05024v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2309.14136v2","updated":"2023-10-08T05:48:18Z","published":"2023-09-25T13:45:28Z","title":"Masked Image Residual Learning for Scaling Deeper Vision Transformers","summary":"  Deeper Vision Transformers (ViTs) are more challenging to train. We expose a\ndegradation problem in deeper layers of ViT when using masked image modeling\n(MIM) for pre-training. To ease the training of deeper ViTs, we introduce a\nself-supervised learning framework called Masked Image Residual Learning\n(MIRL), which significantly alleviates the degradation problem, making scaling\nViT along depth a promising direction for performance upgrade. We reformulate\nthe pre-training objective for deeper layers of ViT as learning to recover the\nresidual of the masked image. We provide extensive empirical evidence showing\nthat deeper ViTs can be effectively optimized using MIRL and easily gain\naccuracy from increased depth. With the same level of computational complexity\nas ViT-Base and ViT-Large, we instantiate 4.5$\\times$ and 2$\\times$ deeper\nViTs, dubbed ViT-S-54 and ViT-B-48. The deeper ViT-S-54, costing 3$\\times$ less\nthan ViT-Large, achieves performance on par with ViT-Large. ViT-B-48 achieves\n86.2% top-1 accuracy on ImageNet. On one hand, deeper ViTs pre-trained with\nMIRL exhibit excellent generalization capabilities on downstream tasks, such as\nobject detection and semantic segmentation. On the other hand, MIRL\ndemonstrates high pre-training efficiency. With less pre-training time, MIRL\nyields competitive performance compared to other approaches.\n","authors":["Guoxi Huang","Hongtao Fu","Adrian G. Bors"],"pdf_url":"https://arxiv.org/pdf/2309.14136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03272v3","updated":"2023-10-08T05:19:04Z","published":"2023-08-07T03:27:04Z","title":"Feature-Suppressed Contrast for Self-Supervised Food Pre-training","summary":"  Most previous approaches for analyzing food images have relied on extensively\nannotated datasets, resulting in significant human labeling expenses due to the\nvaried and intricate nature of such images. Inspired by the effectiveness of\ncontrastive self-supervised methods in utilizing unlabelled data, we explore\nleveraging these techniques on unlabelled food images. In contrastive\nself-supervised methods, two views are randomly generated from an image by data\naugmentations. However, regarding food images, the two views tend to contain\nsimilar informative contents, causing large mutual information, which impedes\nthe efficacy of contrastive self-supervised learning. To address this problem,\nwe propose Feature Suppressed Contrast (FeaSC) to reduce mutual information\nbetween views. As the similar contents of the two views are salient or highly\nresponsive in the feature map, the proposed FeaSC uses a response-aware scheme\nto localize salient features in an unsupervised manner. By suppressing some\nsalient features in one view while leaving another contrast view unchanged, the\nmutual information between the two views is reduced, thereby enhancing the\neffectiveness of contrast learning for self-supervised food pre-training. As a\nplug-and-play module, the proposed method consistently improves BYOL and\nSimSiam by 1.70\\% $\\sim$ 6.69\\% classification accuracy on four publicly\navailable food recognition datasets. Superior results have also been achieved\non downstream segmentation tasks, demonstrating the effectiveness of the\nproposed method.\n","authors":["Xinda Liu","Yaohui Zhu","Linhu Liu","Jiang Tian","Lili Wang"],"pdf_url":"https://arxiv.org/pdf/2308.03272v3.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.05012v1","updated":"2023-10-08T05:03:35Z","published":"2023-10-08T05:03:35Z","title":"Detecting Abnormal Health Conditions in Smart Home Using a Drone","summary":"  Nowadays, detecting aberrant health issues is a difficult process. Falling,\nespecially among the elderly, is a severe concern worldwide. Falls can result\nin deadly consequences, including unconsciousness, internal bleeding, and often\ntimes, death. A practical and optimal, smart approach of detecting falling is\ncurrently a concern. The use of vision-based fall monitoring is becoming more\ncommon among scientists as it enables senior citizens and those with other\nhealth conditions to live independently. For tracking, surveillance, and\nrescue, unmanned aerial vehicles use video or image segmentation and object\ndetection methods. The Tello drone is equipped with a camera and with this\ndevice we determined normal and abnormal behaviors among our participants. The\nautonomous falling objects are classified using a convolutional neural network\n(CNN) classifier. The results demonstrate that the systems can identify falling\nobjects with a precision of 0.9948.\n","authors":["Pronob Kumar Barman"],"pdf_url":"https://arxiv.org/pdf/2310.05012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00451v2","updated":"2023-10-08T04:48:55Z","published":"2023-09-30T18:02:51Z","title":"On the Role of Neural Collapse in Meta Learning Models for Few-shot\n  Learning","summary":"  Meta-learning frameworks for few-shot learning aims to learn models that can\nlearn new skills or adapt to new environments rapidly with a few training\nexamples. This has led to the generalizability of the developed model towards\nnew classes with just a few labelled samples. However these networks are seen\nas black-box models and understanding the representations learnt under\ndifferent learning scenarios is crucial. Neural collapse ($\\mathcal{NC}$) is a\nrecently discovered phenomenon which showcases unique properties at the network\nproceeds towards zero loss. The input features collapse to their respective\nclass means, the class means form a Simplex equiangular tight frame (ETF) where\nthe class means are maximally distant and linearly separable, and the\nclassifier acts as a simple nearest neighbor classifier. While these phenomena\nhave been observed in simple classification networks, this study is the first\nto explore and understand the properties of neural collapse in meta learning\nframeworks for few-shot learning. We perform studies on the Omniglot dataset in\nthe few-shot setting and study the neural collapse phenomenon. We observe that\nthe learnt features indeed have the trend of neural collapse, especially as\nmodel size grows, but to do not necessarily showcase the complete collapse as\nmeasured by the $\\mathcal{NC}$ properties.\n","authors":["Saaketh Medepalli","Naren Doraiswamy"],"pdf_url":"https://arxiv.org/pdf/2310.00451v2.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.05010v1","updated":"2023-10-08T04:46:43Z","published":"2023-10-08T04:46:43Z","title":"Building an Open-Vocabulary Video CLIP Model with Better Architectures,\n  Optimization and Data","summary":"  Despite significant results achieved by Contrastive Language-Image\nPretraining (CLIP) in zero-shot image recognition, limited effort has been made\nexploring its potential for zero-shot video recognition. This paper presents\nOpen-VCLIP++, a simple yet effective framework that adapts CLIP to a strong\nzero-shot video classifier, capable of identifying novel actions and events\nduring testing. Open-VCLIP++ minimally modifies CLIP to capture\nspatial-temporal relationships in videos, thereby creating a specialized video\nclassifier while striving for generalization. We formally demonstrate that\ntraining Open-VCLIP++ is tantamount to continual learning with zero historical\ndata. To address this problem, we introduce Interpolated Weight Optimization, a\ntechnique that leverages the advantages of weight interpolation during both\ntraining and testing. Furthermore, we build upon large language models to\nproduce fine-grained video descriptions. These detailed descriptions are\nfurther aligned with video features, facilitating a better transfer of CLIP to\nthe video domain. Our approach is evaluated on three widely used action\nrecognition datasets, following a variety of zero-shot evaluation protocols.\nThe results demonstrate that our method surpasses existing state-of-the-art\ntechniques by significant margins. Specifically, we achieve zero-shot accuracy\nscores of 88.1%, 58.7%, and 81.2% on UCF, HMDB, and Kinetics-600 datasets\nrespectively, outpacing the best-performing alternative methods by 8.5%, 8.2%,\nand 12.3%. We also evaluate our approach on the MSR-VTT video-text retrieval\ndataset, where it delivers competitive video-to-text and text-to-video\nretrieval performance, while utilizing substantially less fine-tuning data\ncompared to other methods. Code is released at\nhttps://github.com/wengzejia1/Open-VCLIP.\n","authors":["Zuxuan Wu","Zejia Weng","Wujian Peng","Xitong Yang","Ang Li","Larry S. Davis","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.05010v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.00624"},{"id":"http://arxiv.org/abs/2306.05718v2","updated":"2023-10-08T04:30:12Z","published":"2023-06-09T07:30:10Z","title":"Learning Domain-Aware Detection Head with Prompt Tuning","summary":"  Domain adaptive object detection (DAOD) aims to generalize detectors trained\non an annotated source domain to an unlabelled target domain. However, existing\nmethods focus on reducing the domain bias of the detection backbone by\ninferring a discriminative visual encoder, while ignoring the domain bias in\nthe detection head. Inspired by the high generalization of vision-language\nmodels (VLMs), applying a VLM as the robust detection backbone following a\ndomain-aware detection head is a reasonable way to learn the discriminative\ndetector for each domain, rather than reducing the domain bias in traditional\nmethods. To achieve the above issue, we thus propose a novel DAOD framework\nnamed Domain-Aware detection head with Prompt tuning (DA-Pro), which applies\nthe learnable domain-adaptive prompt to generate the dynamic detection head for\neach domain. Formally, the domain-adaptive prompt consists of the\ndomain-invariant tokens, domain-specific tokens, and the domain-related textual\ndescription along with the class label. Furthermore, two constraints between\nthe source and target domains are applied to ensure that the domain-adaptive\nprompt can capture the domains-shared and domain-specific knowledge. A prompt\nensemble strategy is also proposed to reduce the effect of prompt disturbance.\nComprehensive experiments over multiple cross-domain adaptation tasks\ndemonstrate that using the domain-adaptive prompt can produce an effectively\ndomain-related detection head for boosting domain-adaptive object detection.\nOur code is available at https://github.com/Therock90421/DA-Pro.\n","authors":["Haochen Li","Rui Zhang","Hantao Yao","Xinkai Song","Yifan Hao","Yongwei Zhao","Ling Li","Yunji Chen"],"pdf_url":"https://arxiv.org/pdf/2306.05718v2.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.04999v1","updated":"2023-10-08T04:00:20Z","published":"2023-10-08T04:00:20Z","title":"Symmetrical Linguistic Feature Distillation with CLIP for Scene Text\n  Recognition","summary":"  In this paper, we explore the potential of the Contrastive Language-Image\nPretraining (CLIP) model in scene text recognition (STR), and establish a novel\nSymmetrical Linguistic Feature Distillation framework (named CLIP-OCR) to\nleverage both visual and linguistic knowledge in CLIP. Different from previous\nCLIP-based methods mainly considering feature generalization on visual\nencoding, we propose a symmetrical distillation strategy (SDS) that further\ncaptures the linguistic knowledge in the CLIP text encoder. By cascading the\nCLIP image encoder with the reversed CLIP text encoder, a symmetrical structure\nis built with an image-to-text feature flow that covers not only visual but\nalso linguistic information for distillation.Benefiting from the natural\nalignment in CLIP, such guidance flow provides a progressive optimization\nobjective from vision to language, which can supervise the STR feature\nforwarding process layer-by-layer.Besides, a new Linguistic Consistency Loss\n(LCL) is proposed to enhance the linguistic capability by considering\nsecond-order statistics during the optimization. Overall, CLIP-OCR is the first\nto design a smooth transition between image and text for the STR task.Extensive\nexperiments demonstrate the effectiveness of CLIP-OCR with 93.8% average\naccuracy on six popular STR benchmarks.Code will be available at\nhttps://github.com/wzx99/CLIPOCR.\n","authors":["Zixiao Wang","Hongtao Xie","Yuxin Wang","Jianjun Xu","Boqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04999v1.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.04995v1","updated":"2023-10-08T03:44:58Z","published":"2023-10-08T03:44:58Z","title":"SemST: Semantically Consistent Multi-Scale Image Translation via\n  Structure-Texture Alignment","summary":"  Unsupervised image-to-image (I2I) translation learns cross-domain image\nmapping that transfers input from the source domain to output in the target\ndomain while preserving its semantics. One challenge is that different semantic\nstatistics in source and target domains result in content discrepancy known as\nsemantic distortion. To address this problem, a novel I2I method that maintains\nsemantic consistency in translation is proposed and named SemST in this work.\nSemST reduces semantic distortion by employing contrastive learning and\naligning the structural and textural properties of input and output by\nmaximizing their mutual information. Furthermore, a multi-scale approach is\nintroduced to enhance translation performance, thereby enabling the\napplicability of SemST to domain adaptation in high-resolution images.\nExperiments show that SemST effectively mitigates semantic distortion and\nachieves state-of-the-art performance. Also, the application of SemST to domain\nadaptation (DA) is explored. It is demonstrated by preliminary experiments that\nSemST can be utilized as a beneficial pre-training for the semantic\nsegmentation task.\n","authors":["Ganning Zhao","Wenhui Cui","Suya You","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2310.04995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04992v1","updated":"2023-10-08T03:40:14Z","published":"2023-10-08T03:40:14Z","title":"VisionFM: a Multi-Modal Multi-Task Vision Foundation Model for\n  Generalist Ophthalmic Artificial Intelligence","summary":"  We present VisionFM, a foundation model pre-trained with 3.4 million\nophthalmic images from 560,457 individuals, covering a broad range of\nophthalmic diseases, modalities, imaging devices, and demography. After\npre-training, VisionFM provides a foundation to foster multiple ophthalmic\nartificial intelligence (AI) applications, such as disease screening and\ndiagnosis, disease prognosis, subclassification of disease phenotype, and\nsystemic biomarker and disease prediction, with each application enhanced with\nexpert-level intelligence and accuracy. The generalist intelligence of VisionFM\noutperformed ophthalmologists with basic and intermediate levels in jointly\ndiagnosing 12 common ophthalmic diseases. Evaluated on a new large-scale\nophthalmic disease diagnosis benchmark database, as well as a new large-scale\nsegmentation and detection benchmark database, VisionFM outperformed strong\nbaseline deep neural networks. The ophthalmic image representations learned by\nVisionFM exhibited noteworthy explainability, and demonstrated strong\ngeneralizability to new ophthalmic modalities, disease spectrum, and imaging\ndevices. As a foundation model, VisionFM has a large capacity to learn from\ndiverse ophthalmic imaging data and disparate datasets. To be commensurate with\nthis capacity, in addition to the real data used for pre-training, we also\ngenerated and leveraged synthetic ophthalmic imaging data. Experimental results\nrevealed that synthetic data that passed visual Turing tests, can also enhance\nthe representation learning capability of VisionFM, leading to substantial\nperformance gains on downstream ophthalmic AI tasks. Beyond the ophthalmic AI\napplications developed, validated, and demonstrated in this work, substantial\nfurther applications can be achieved in an efficient and cost-effective manner\nusing VisionFM as the foundation.\n","authors":["Jianing Qiu","Jian Wu","Hao Wei","Peilun Shi","Minqing Zhang","Yunyun Sun","Lin Li","Hanruo Liu","Hongyi Liu","Simeng Hou","Yuyang Zhao","Xuehui Shi","Junfang Xian","Xiaoxia Qu","Sirui Zhu","Lijie Pan","Xiaoniao Chen","Xiaojia Zhang","Shuai Jiang","Kebing Wang","Chenlong Yang","Mingqiang Chen","Sujie Fan","Jianhua Hu","Aiguo Lv","Hui Miao","Li Guo","Shujun Zhang","Cheng Pei","Xiaojuan Fan","Jianqin Lei","Ting Wei","Junguo Duan","Chun Liu","Xiaobo Xia","Siqi Xiong","Junhong Li","Benny Lo","Yih Chung Tham","Tien Yin Wong","Ningli Wang","Wu Yuan"],"pdf_url":"https://arxiv.org/pdf/2310.04992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04991v1","updated":"2023-10-08T03:35:27Z","published":"2023-10-08T03:35:27Z","title":"Video-Teller: Enhancing Cross-Modal Generation with Fusion and\n  Decoupling","summary":"  This paper proposes Video-Teller, a video-language foundation model that\nleverages multi-modal fusion and fine-grained modality alignment to\nsignificantly enhance the video-to-text generation task. Video-Teller boosts\nthe training efficiency by utilizing frozen pretrained vision and language\nmodules. It capitalizes on the robust linguistic capabilities of large language\nmodels, enabling the generation of both concise and elaborate video\ndescriptions. To effectively integrate visual and auditory information,\nVideo-Teller builds upon the image-based BLIP-2 model and introduces a cascaded\nQ-Former which fuses information across frames and ASR texts. To better guide\nvideo summarization, we introduce a fine-grained modality alignment objective,\nwhere the cascaded Q-Former's output embedding is trained to align with the\ncaption/summary embedding created by a pretrained text auto-encoder.\nExperimental results demonstrate the efficacy of our proposed video-language\nfoundation model in accurately comprehending videos and generating coherent and\nprecise language descriptions. It is worth noting that the fine-grained\nalignment enhances the model's capabilities (4% improvement of CIDEr score on\nMSR-VTT) with only 13% extra parameters in training and zero additional cost in\ninference.\n","authors":["Haogeng Liu","Qihang Fan","Tingkai Liu","Linjie Yang","Yunzhe Tao","Huaibo Huang","Ran He","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04981v1","updated":"2023-10-08T03:07:14Z","published":"2023-10-08T03:07:14Z","title":"Compositional Semantics for Open Vocabulary Spatio-semantic\n  Representations","summary":"  General-purpose mobile robots need to complete tasks without exact human\ninstructions. Large language models (LLMs) is a promising direction for\nrealizing commonsense world knowledge and reasoning-based planning.\nVision-language models (VLMs) transform environment percepts into\nvision-language semantics interpretable by LLMs. However, completing complex\ntasks often requires reasoning about information beyond what is currently\nperceived. We propose latent compositional semantic embeddings z* as a\nprincipled learning-based knowledge representation for queryable\nspatio-semantic memories. We mathematically prove that z* can always be found,\nand the optimal z* is the centroid for any set Z. We derive a probabilistic\nbound for estimating separability of related and unrelated semantics. We prove\nthat z* is discoverable by iterative optimization by gradient descent from\nvisual appearance and singular descriptions. We experimentally verify our\nfindings on four embedding spaces incl. CLIP and SBERT. Our results show that\nz* can represent up to 10 semantics encoded by SBERT, and up to 100 semantics\nfor ideal uniformly distributed high-dimensional embeddings. We demonstrate\nthat a simple dense VLM trained on the COCO-Stuff dataset can learn z* for 181\noverlapping semantics by 42.23 mIoU, while improving conventional\nnon-overlapping open-vocabulary segmentation performance by +3.48 mIoU compared\nwith a popular SOTA model.\n","authors":["Robin Karlsson","Francisco Lepe-Salazar","Kazuya Takeda"],"pdf_url":"https://arxiv.org/pdf/2310.04981v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2204.04145v2","updated":"2023-10-08T03:00:45Z","published":"2022-04-08T15:53:13Z","title":"Constrained Bundle Adjustment for Structure From Motion Using\n  Uncalibrated Multi-Camera Systems","summary":"  Structure from motion using uncalibrated multi-camera systems is a\nchallenging task. This paper proposes a bundle adjustment solution that\nimplements a baseline constraint respecting that these cameras are static to\neach other. We assume these cameras are mounted on a mobile platform,\nuncalibrated, and coarsely synchronized. To this end, we propose the baseline\nconstraint that is formulated for the scenario in which the cameras have\noverlapping views. The constraint is incorporated in the bundle adjustment\nsolution to keep the relative motion of different cameras static. Experiments\nwere conducted using video frames of two collocated GoPro cameras mounted on a\nvehicle with no system calibration. These two cameras were placed capturing\noverlapping contents. We performed our bundle adjustment using the proposed\nconstraint and then produced 3D dense point clouds. Evaluations were performed\nby comparing these dense point clouds against LiDAR reference data. We showed\nthat, as compared to traditional bundle adjustment, our proposed method\nachieved an improvement of 29.38%.\n","authors":["Debao Huang","Mostafa Elhashash","Rongjun Qin"],"pdf_url":"https://arxiv.org/pdf/2204.04145v2.pdf","comment":"to be published in ISPRS Congress 2022"}]},"2023-10-07T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2104.14103v2","updated":"2023-10-07T22:19:31Z","published":"2021-04-29T04:50:30Z","title":"AutoCone: An OmniDirectional Robot for Lane-Level Cone Placement","summary":"  This paper summarizes the progress in developing a rugged, low-cost,\nautomated ground cone robot network capable of traffic delineation at\nlane-level precision. A holonomic omnidirectional base with a traffic\ndelineator was developed to allow flexibility in initialization. RTK GPS was\nutilized to reduce minimum position error to 2 centimeters. Due to recent\ndevelopments, the cost of the platform is now less than $1,600. To minimize the\neffects of GPS-denied environments, wheel encoders and an Extended Kalman\nFilter were implemented to maintain lane-level accuracy during operation and a\nmaximum error of 1.97 meters through 50 meters with little to no GPS signal.\nFuture work includes increasing the operational speed of the platforms,\nincorporating lanelet information for path planning, and cross-platform\nestimation.\n","authors":["Jacob Hartzer","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2104.14103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04930v1","updated":"2023-10-07T22:01:49Z","published":"2023-10-07T22:01:49Z","title":"Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via\n  Differentiable Physics Simulation","summary":"  The capability to transfer mastered skills to accomplish a range of similar\nyet novel tasks is crucial for intelligent robots. In this work, we introduce\n$\\textit{Diff-Transfer}$, a novel framework leveraging differentiable physics\nsimulation to efficiently transfer robotic skills. Specifically,\n$\\textit{Diff-Transfer}$ discovers a feasible path within the task space that\nbrings the source task to the target task. At each pair of adjacent points\nalong this task path, which is two sub-tasks, $\\textit{Diff-Transfer}$ adapts\nknown actions from one sub-task to tackle the other sub-task successfully. The\nadaptation is guided by the gradient information from differentiable physics\nsimulations. We propose a novel path-planning method to generate sub-tasks,\nleveraging $Q$-learning with a task-level state and reward. We implement our\nframework in simulation experiments and execute four challenging transfer tasks\non robotic manipulation, demonstrating the efficacy of $\\textit{Diff-Transfer}$\nthrough comprehensive experiments. Supplementary and Videos are on the website\n$~\\href{https://sites.google.com/view/difftransfer}{https://sites.google.com/view/difftransfer}$\n","authors":["Yuqi Xiang","Feitong Chen","Qinsi Wang","Yang Gang","Xiang Zhang","Xinghao Zhu","Xingyu Liu","Lin Shao"],"pdf_url":"https://arxiv.org/pdf/2310.04930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00887v2","updated":"2023-10-07T17:38:20Z","published":"2023-10-02T04:09:27Z","title":"GRID: A Platform for General Robot Intelligence Development","summary":"  Developing machine intelligence abilities in robots and autonomous systems is\nan expensive and time consuming process. Existing solutions are tailored to\nspecific applications and are harder to generalize. Furthermore, scarcity of\ntraining data adds a layer of complexity in deploying deep machine learning\nmodels. We present a new platform for General Robot Intelligence Development\n(GRID) to address both of these issues. The platform enables robots to learn,\ncompose and adapt skills to their physical capabilities, environmental\nconstraints and goals. The platform addresses AI problems in robotics via\nfoundation models that know the physical world. GRID is designed from the\nground up to be extensible to accommodate new types of robots, vehicles,\nhardware platforms and software protocols. In addition, the modular design\nenables various deep ML components and existing foundation models to be easily\nusable in a wider variety of robot-centric problems. We demonstrate the\nplatform in various aerial robotics scenarios and demonstrate how the platform\ndramatically accelerates development of machine intelligent robots.\n","authors":["Sai Vemprala","Shuhang Chen","Abhinav Shukla","Dinesh Narayanan","Ashish Kapoor"],"pdf_url":"https://arxiv.org/pdf/2310.00887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04874v1","updated":"2023-10-07T17:08:22Z","published":"2023-10-07T17:08:22Z","title":"AirIMU: Learning Uncertainty Propagation for Inertial Odometry","summary":"  Accurate uncertainty estimation for inertial odometry is the foundation to\nachieve optimal fusion in multi-sensor systems, such as visual or LiDAR\ninertial odometry. Prior studies often simplify the assumptions regarding the\nuncertainty of inertial measurements, presuming fixed covariance parameters and\nempirical IMU sensor models. However, the inherent physical limitations and\nnon-linear characteristics of sensors are difficult to capture. Moreover,\nuncertainty may fluctuate based on sensor rates and motion modalities, leading\nto variations across different IMUs. To address these challenges, we formulate\na learning-based method that not only encapsulate the non-linearities inherent\nto IMUs but also ensure the accurate propagation of covariance in a data-driven\nmanner. We extend the PyPose library to enable differentiable batched IMU\nintegration with covariance propagation on manifolds, leading to significant\nruntime speedup. To demonstrate our method's adaptability, we evaluate it on\nseveral benchmarks as well as a large-scale helicopter dataset spanning over\n262 kilometers. The drift rate of the inertial odometry on these datasets is\nreduced by a factor of between 2.2 and 4 times. Our method lays the groundwork\nfor advanced developments in inertial odometry.\n","authors":["Yuheng Qiu","Chen Wang","Xunfei Zhou","Youjie Xia","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2310.04874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04846v1","updated":"2023-10-07T15:11:31Z","published":"2023-10-07T15:11:31Z","title":"Soft finger dynamic stability and slip by Coulomb friction and bulk\n  stiffness","summary":"  Soft robotic fingers can safely grasp fragile or non-uniform objects, but\ntheir force capacity is limited, especially with less contact area: objects\nwhich are smaller, not round, or where an enclosing grasp is not feasible. To\nimprove force capacity, this paper considers two types of grip failure, slip\nand dynamic rotational stability. For slip, a Coulomb model for soft fingers\nbased on total normal and tangential force is validated, identifying the effect\nof contact area, pressure, and grip position on effective Coulomb coefficient,\nnormal force and transverse stiffness. For rotational stability, bulk stiffness\nof the fingers is used to develop conditions for dynamic stability about the\ninitial grasp, and a condition for when the rotation leads to slip. Together,\nthese models suggest contact area improves grip by increasing transverse\nstiffness and normal force. The models are validated in a range of grasp\nconditions, shown to predict the influence of object radius and finger distance\non grip stability limits.\n","authors":["Hun Jang","Valentyn Petrichenko","Joonbum Bae","Kevin Haninger"],"pdf_url":"https://arxiv.org/pdf/2310.04846v1.pdf","comment":"Submitted ICRA24"},{"id":"http://arxiv.org/abs/2310.04828v1","updated":"2023-10-07T14:36:48Z","published":"2023-10-07T14:36:48Z","title":"Guardians as You Fall: Active Mode Transition for Safe Falling","summary":"  Recent advancements in optimal control and reinforcement learning have\nenabled quadrupedal robots to perform various agile locomotion tasks over\ndiverse terrains. During these agile motions, ensuring the stability and\nresiliency of the robot is a primary concern to prevent catastrophic falls and\nmitigate potential damages. Previous methods primarily focus on recovery\npolicies after the robot falls. There is no active safe falling solution to the\nbest of our knowledge. In this paper, we proposed Guardians as You Fall (GYF),\na safe falling/tumbling and recovery framework that can actively tumble and\nrecover to stable modes to reduce damage in highly dynamic scenarios. The key\nidea of GYF is to adaptively traverse different stable modes via active\ntumbling before the robot shifts to irrecoverable poses. Via comprehensive\nsimulation and real-world experiments, we show that GYF significantly reduces\nthe maximum acceleration and jerk of the robot base compared to the baselines.\nIn particular, GYF reduces the maximum acceleration and jerk by 20%~73% in\ndifferent scenarios in simulation and real-world experiments. GYF offers a new\nperspective on safe falling and recovery in locomotion tasks, potentially\nenabling much more aggressive explorations of existing agile locomotion skills.\n","authors":["Yikai Wang","Mengdi Xu","Guanya Shi","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.04828v1.pdf","comment":"website: https://sites.google.com/view/guardians-as-you-fall/"},{"id":"http://arxiv.org/abs/2303.17476v3","updated":"2023-10-07T14:22:19Z","published":"2023-03-30T15:48:51Z","title":"Differentiable Compliant Contact Primitives for Estimation and Model\n  Predictive Control","summary":"  Control techniques like MPC can realize contact-rich manipulation which\nexploits dynamic information, maintaining friction limits and safety\nconstraints. However, contact geometry and dynamics are required to be known.\nThis information is often extracted from CAD, limiting scalability and the\nability to handle tasks with varying geometry. To reduce the need for a priori\nmodels, we propose a framework for estimating contact models online based on\ntorque and position measurements. To do this, compliant contact models are\nused, connected in parallel to model multi-point contact and constraints such\nas a hinge. They are parameterized to be differentiable with respect to all of\ntheir parameters (rest position, stiffness, contact location), allowing the\ncoupled robot/environment dynamics to be linearized or efficiently used in\ngradient-based optimization. These models are then applied for: offline\ngradient-based parameter fitting, online estimation via an extended Kalman\nfilter, and online gradient-based MPC. The proposed approach is validated on\ntwo robots, showing the efficacy of sensorless contact estimation and the\neffects of online estimation on MPC performance.\n","authors":["Kevin Haninger","Kangwagye Samuel","Filippo Rozzi","Sehoon Oh","Loris Roveda"],"pdf_url":"https://arxiv.org/pdf/2303.17476v3.pdf","comment":"Submitted ICRA24. Video available at https://youtu.be/CuCTcmn3H-o\n  Code available at https://gitlab.cc-asp.fraunhofer.de/hanikevi/contact_mpc"},{"id":"http://arxiv.org/abs/2310.04822v1","updated":"2023-10-07T14:21:43Z","published":"2023-10-07T14:21:43Z","title":"Combining Sampling- and Gradient-based Planning for Contact-rich\n  Manipulation","summary":"  Planning over discontinuous dynamics is needed for robotics tasks like\ncontact-rich manipulation, which presents challenges in the numerical stability\nand speed of planning methods when either neural network or analytical models\nare used. On the one hand, sampling-based planners require higher sample\ncomplexity in high-dimensional problems and cannot describe safety constraints\nsuch as force limits. On the other hand, gradient-based solvers can suffer from\nlocal optima and convergence issues when the Hessian is poorly conditioned. We\npropose a planning method with both sampling- and gradient-based elements,\nusing the Cross-entropy Method to initialize a gradient-based solver, providing\nbetter search over local minima and the ability to handle explicit constraints.\nWe show the approach allows smooth, stable contact-rich planning for an\nimpedance-controlled robot making contact with a stiff environment,\nbenchmarking against gradient-only MPC and CEM.\n","authors":["Filippo Rozzi","Loris Roveda","Kevin Haninger"],"pdf_url":"https://arxiv.org/pdf/2310.04822v1.pdf","comment":"Submitted ICRA24. Video available at https://youtu.be/COqR90392Kw\n  Code available at https://gitlab.cc-asp.fraunhofer.de/hanikevi/contact_mpc"},{"id":"http://arxiv.org/abs/2310.04802v1","updated":"2023-10-07T13:40:52Z","published":"2023-10-07T13:40:52Z","title":"Hierarchical Unsupervised Topological SLAM","summary":"  In this paper we present a novel framework for unsupervised topological\nclustering resulting in improved loop. In this paper we present a novel\nframework for unsupervised topological clustering resulting in improved loop\ndetection and closure for SLAM. A navigating mobile robot clusters its\ntraversal into visually similar topologies where each cluster (topology)\ncontains a set of similar looking images typically observed from spatially\nadjacent locations. Each such set of spatially adjacent and visually similar\ngrouping of images constitutes a topology obtained without any supervision. We\nformulate a hierarchical loop discovery strategy that first detects loops at\nthe level of topologies and subsequently at the level of images between the\nlooped topologies. We show over a number of traversals across different Habitat\nenvironments that such a hierarchical pipeline significantly improves SOTA\nimage based loop detection and closure methods. Further, as a consequence of\nimproved loop detection, we enhance the loop closure and backend SLAM\nperformance. Such a rendering of a traversal into topological segments is\nbeneficial for downstream tasks such as navigation that can now build a\ntopological graph where spatially adjacent topological clusters are connected\nby an edge and navigate over such topological graphs.\n","authors":["Ayush Sharma","Yash Mehan","Pradyumna Dasu","Sourav Garg","Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2310.04802v1.pdf","comment":"Accepted to IEEE ITSC 2023"},{"id":"http://arxiv.org/abs/2310.04800v1","updated":"2023-10-07T13:39:46Z","published":"2023-10-07T13:39:46Z","title":"Fully Sparse Long Range 3D Object Detection Using Range Experts and\n  Multimodal Virtual Points","summary":"  3D object detection at long-range is crucial for ensuring the safety and\nefficiency of self-driving cars, allowing them to accurately perceive and react\nto objects, obstacles, and potential hazards from a distance. But most current\nstate-of-the-art LiDAR based methods are limited by the sparsity of range\nsensors, which generates a form of domain gap between points closer to and\nfarther away from the ego vehicle. Another related problem is the label\nimbalance for faraway objects, which inhibits the performance of Deep Neural\nNetworks at long-range. Although image features could be beneficial for\nlong-range detections, and some recently proposed multimodal methods\nincorporate image features, they do not scale well computationally at long\nranges or are limited by depth estimation accuracy. To address the above\nlimitations, we propose to combine two LiDAR based 3D detection networks, one\nspecializing at near to mid-range objects, and one at long-range 3D detection.\nTo train a detector at long range under a scarce label regime, we further\npropose to weigh the loss according to the labelled objects' distance from ego\nvehicle. To mitigate the LiDAR sparsity issue, we leverage Multimodal Virtual\nPoints (MVP), an image based depth completion algorithm, to enrich our data\nwith virtual points. Our method, combining two range experts trained with MVP,\nwhich we refer to as RangeFSD, achieves state-of-the-art performance on the\nArgoverse2 (AV2) dataset, with improvements at long range. The code will be\nreleased soon.\n","authors":["Ajinkya Khoche","Laura Pereira Sánchez","Nazre Batool","Sina Sharif Mansouri","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2310.04800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04787v1","updated":"2023-10-07T12:26:56Z","published":"2023-10-07T12:26:56Z","title":"HI-SLAM: Monocular Real-time Dense Mapping with Hybrid Implicit Fields","summary":"  In this letter, we present a neural field-based real-time monocular mapping\nframework for accurate and dense Simultaneous Localization and Mapping (SLAM).\nRecent neural mapping frameworks show promising results, but rely on RGB-D or\npose inputs, or cannot run in real-time. To address these limitations, our\napproach integrates dense-SLAM with neural implicit fields. Specifically, our\ndense SLAM approach runs parallel tracking and global optimization, while a\nneural field-based map is constructed incrementally based on the latest SLAM\nestimates. For the efficient construction of neural fields, we employ\nmulti-resolution grid encoding and signed distance function (SDF)\nrepresentation. This allows us to keep the map always up-to-date and adapt\ninstantly to global updates via loop closing. For global consistency, we\npropose an efficient Sim(3)-based pose graph bundle adjustment (PGBA) approach\nto run online loop closing and mitigate the pose and scale drift. To enhance\ndepth accuracy further, we incorporate learned monocular depth priors. We\npropose a novel joint depth and scale adjustment (JDSA) module to solve the\nscale ambiguity inherent in depth priors. Extensive evaluations across\nsynthetic and real-world datasets validate that our approach outperforms\nexisting methods in accuracy and map completeness while preserving real-time\nperformance.\n","authors":["Wei Zhang","Tiecheng Sun","Sen Wang","Qing Cheng","Norbert Haala"],"pdf_url":"https://arxiv.org/pdf/2310.04787v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2310.04781v1","updated":"2023-10-07T11:47:58Z","published":"2023-10-07T11:47:58Z","title":"Unifying Foundation Models with Quadrotor Control for Visual Tracking\n  Beyond Object Categories","summary":"  Visual control enables quadrotors to adaptively navigate using real-time\nsensory data, bridging perception with action. Yet, challenges persist,\nincluding generalization across scenarios, maintaining reliability, and\nensuring real-time responsiveness. This paper introduces a perception framework\ngrounded in foundation models for universal object detection and tracking,\nmoving beyond specific training categories. Integral to our approach is a\nmulti-layered tracker integrated with the foundation detector, ensuring\ncontinuous target visibility, even when faced with motion blur, abrupt light\nshifts, and occlusions. Complementing this, we introduce a model-free\ncontroller tailored for resilient quadrotor visual tracking. Our system\noperates efficiently on limited hardware, relying solely on an onboard camera\nand an inertial measurement unit. Through extensive validation in diverse\nchallenging indoor and outdoor environments, we demonstrate our system's\neffectiveness and adaptability. In conclusion, our research represents a step\nforward in quadrotor visual tracking, moving from task-specific methods to more\nversatile and adaptable operations.\n","authors":["Alessandro Saviolo","Pratyaksh Rao","Vivek Radhakrishnan","Jiuhong Xiao","Giuseppe Loianno"],"pdf_url":"https://arxiv.org/pdf/2310.04781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14235v2","updated":"2023-10-07T10:30:59Z","published":"2023-09-25T15:47:07Z","title":"Stackelberg Driver Model for Continual Policy Improvement in\n  Scenario-Based Closed-Loop Autonomous Driving","summary":"  The deployment of autonomous vehicles (AVs) has faced hurdles due to the\ndominance of rare but critical corner cases within the long-tail distribution\nof driving scenarios, which negatively affects their overall performance. To\naddress this challenge, adversarial generation methods have emerged as a class\nof efficient approaches to synthesize safety-critical scenarios for AV testing.\nHowever, these generated scenarios are often underutilized for AV training,\nresulting in the potential for continual AV policy improvement remaining\nuntapped, along with a deficiency in the closed-loop design needed to achieve\nit. Therefore, we tailor the Stackelberg Driver Model (SDM) to accurately\ncharacterize the hierarchical nature of vehicle interaction dynamics,\nfacilitating iterative improvement by engaging background vehicles (BVs) and AV\nin a sequential game-like interaction paradigm. With AV acting as the leader\nand BVs as followers, this leader-follower modeling ensures that AV would\nconsistently refine its policy, always taking into account the additional\ninformation that BVs play the best response to challenge AV. Extensive\nexperiments have shown that our algorithm exhibits superior performance\ncompared to several baselines especially in higher dimensional scenarios,\nleading to substantial advancements in AV capabilities while continually\ngenerating progressively challenging scenarios. Code is available at\nhttps://github.com/BlueCat-de/SDM.\n","authors":["Haoyi Niu","Qimao Chen","Yingyue Li","Jianming Hu"],"pdf_url":"https://arxiv.org/pdf/2309.14235v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2306.05716v3","updated":"2023-10-07T08:43:23Z","published":"2023-06-09T07:22:12Z","title":"Transferring Foundation Models for Generalizable Robotic Manipulation","summary":"  Improving the generalization capabilities of general-purpose robotic\nmanipulation agents in the real world has long been a significant challenge.\nExisting approaches often rely on collecting large-scale robotic data which is\ncostly and time-consuming, such as the RT-1 dataset. However, due to\ninsufficient diversity of data, these approaches typically suffer from limiting\ntheir capability in open-domain scenarios with new objects, and diverse\nenvironments. In this paper, we propose a novel paradigm that effectively\nleverages language grounded segmentation mask generated by Internet-scale\nfoundation models, to address a wide range of pick-and-place robot manipulation\ntasks. By integrating the mask modality, which incorporates semantic,\ngeometric, and temporal correlation priors derived from vision foundation\nmodels, into the end-to-end policy model, our approach can effectively and\nrobustly perceive object pose and enable sample-efficient generalization\nlearning, including new object instances, semantic categories, and unseen\nbackgrounds. We first introduce a series of foundation models to ground natural\nlanguage demands across multiple tasks. Secondly, we develop a two-stream 2D\npolicy model based on imitation learning, which utilizes raw images, object\nmasks, and robot proprioception to predict robot actions. Extensive real-world\nexperiments conducted on a Franka Emika robot arm demonstrate the effectiveness\nof our proposed paradigm. Demos are shown in YouTube\n(https://www.youtube.com/watch?v=MAcUPFBfRIw ).\n","authors":["Jiange Yang","Wenhui Tan","Chuhao Jin","Keling Yao","Bei Liu","Jianlong Fu","Ruihua Song","Gangshan Wu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2306.05716v3.pdf","comment":"In submission"},{"id":"http://arxiv.org/abs/2310.04729v1","updated":"2023-10-07T08:05:11Z","published":"2023-10-07T08:05:11Z","title":"Current Trends and Advances in Quantum Navigation for Maritime\n  Applications: A Comprehensive Review","summary":"  This paper presents a comprehensive review of the current state of the art in\nquantum navigation systems, with a specific focus on their application in\nmaritime navigation. Quantum technologies have the potential to revolutionise\nnavigation and positioning systems due to their ability to provide highly\naccurate and secure information. The review covers the principles of quantum\nnavigation and highlights the latest developments in quantum-enhanced sensors,\natomic clocks, and quantum communication protocols. The paper also discusses\nthe challenges and opportunities of using quantum technologies in maritime\nnavigation, including the effects that the maritime environment and the\nspecificity of marine applications can have on the performance of quantum\nsensors. Finally, the paper concludes with a discussion on the future of\nquantum navigation systems and their potential impact on the maritime industry.\nThis review aims at providing a valuable resource for researchers and engineers\ninterested in the development and deployment of quantum navigation systems.\n","authors":["Olga Sambataro","Riccardo Costanzi","Joao Alves","Andrea Caiti","Pietro Paglierani","Roberto Petroccia","Andrea Munafo"],"pdf_url":"https://arxiv.org/pdf/2310.04729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04712v1","updated":"2023-10-07T07:08:25Z","published":"2023-10-07T07:08:25Z","title":"UFD-PRiME: Unsupervised Joint Learning of Optical Flow and Stereo Depth\n  through Pixel-Level Rigid Motion Estimation","summary":"  Both optical flow and stereo disparities are image matches and can therefore\nbenefit from joint training. Depth and 3D motion provide geometric rather than\nphotometric information and can further improve optical flow. Accordingly, we\ndesign a first network that estimates flow and disparity jointly and is trained\nwithout supervision. A second network, trained with optical flow from the first\nas pseudo-labels, takes disparities from the first network, estimates 3D rigid\nmotion at every pixel, and reconstructs optical flow again. A final stage fuses\nthe outputs from the two networks. In contrast with previous methods that only\nconsider camera motion, our method also estimates the rigid motions of dynamic\nobjects, which are of key interest in applications. This leads to better\noptical flow with visibly more detailed occlusions and object boundaries as a\nresult. Our unsupervised pipeline achieves 7.36% optical flow error on the\nKITTI-2015 benchmark and outperforms the previous state-of-the-art 9.38% by a\nwide margin. It also achieves slightly better or comparable stereo depth\nresults. Code will be made available.\n","authors":["Shuai Yuan","Carlo Tomasi"],"pdf_url":"https://arxiv.org/pdf/2310.04712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13586v2","updated":"2023-10-07T06:23:42Z","published":"2023-09-24T09:01:19Z","title":"Task-Oriented Dexterous Grasp Synthesis via Differentiable Grasp Wrench\n  Boundary Estimator","summary":"  Analytical dexterous grasping synthesis is often driven by grasp quality\nmetrics. However, existing metrics possess many problems, such as being\ncomputationally expensive, physically inaccurate, and non-differentiable.\nMoreover, none of them can facilitate the synthesis of non-force-closure\ngrasps, which account for a significant portion of task-oriented grasping such\nas lid screwing and button pushing. The main challenge behind all the above\ndrawbacks is the difficulty in modeling the complex Grasp Wrench Space (GWS).\nIn this work, we overcome this challenge by proposing a novel GWS estimator,\nthus enabling gradient-based task-oriented dexterous grasp synthesis for the\nfirst time. Our key contribution is a fast, accurate, and differentiable\ntechnique to estimate the GWS boundary with good physical interpretability by\nparallel sampling and mapping, which does not require iterative optimization.\nSecond, based on our differentiable GWS estimator, we derive a task-oriented\nenergy function to enable gradient-based grasp synthesis and a metric to\nevaluate non-force-closure grasps. Finally, we improve the previous dexterous\ngrasp synthesis pipeline mainly by a novel technique to make nearest-point\ncalculation differentiable, even on mesh edges and vertices. Extensive\nexperiments are performed to verify the efficiency and effectiveness of our\nmethods. Our GWS estimator can run in several milliseconds on GPUs with minimal\nmemory cost, more than three orders of magnitude faster than the classic\ndiscretization-based method. Using this GWS estimator, we synthesize 0.1\nmillion dexterous grasps to show that our pipeline can significantly outperform\nthe SOTA method, even in task-unaware force-closure-grasp synthesis. For\ntask-oriented grasp synthesis, we provide some qualitative results. Our project\npage is https://pku-epic.github.io/TaskDexGrasp/.\n","authors":["Jiayi Chen","Yuxing Chen","Jialiang Zhang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2309.13586v2.pdf","comment":"In review. ICRA 2024 submission"},{"id":"http://arxiv.org/abs/2310.00498v2","updated":"2023-10-07T04:55:43Z","published":"2023-09-30T21:31:30Z","title":"Automated Gait Generation For Walking, Soft Robotic Quadrupeds","summary":"  Gait generation for soft robots is challenging due to the nonlinear dynamics\nand high dimensional input spaces of soft actuators. Limitations in soft\nrobotic control and perception force researchers to hand-craft open loop\ncontrollers for gait sequences, which is a non-trivial process. Moreover, short\nsoft actuator lifespans and natural variations in actuator behavior limit\nmachine learning techniques to settings that can be learned on the same time\nscales as robot deployment. Lastly, simulation is not always possible, due to\nheterogeneity and nonlinearity in soft robotic materials and their dynamics\nchange due to wear. We present a sample-efficient, simulation free, method for\nself-generating soft robot gaits, using very minimal computation. This\ntechnique is demonstrated on a motorized soft robotic quadruped that walks\nusing four legs constructed from 16 \"handed shearing auxetic\" (HSA) actuators.\nTo manage the dimension of the search space, gaits are composed of two\nsequential sets of leg motions selected from 7 possible primitives. Pairs of\nprimitives are executed on one leg at a time; we then select the\nbest-performing pair to execute while moving on to subsequent legs. This method\n-- which uses no simulation, sophisticated computation, or user input --\nconsistently generates good translation and rotation gaits in as low as 4\nminutes of hardware experimentation, outperforming hand-crafted gaits. This is\nthe first demonstration of completely autonomous gait generation in a soft\nrobot.\n","authors":["Jake Ketchum","Sophia Schiffer","Muchen Sun","Pranav Kaarthik","Ryan L. Truby","Todd D. Murphey"],"pdf_url":"https://arxiv.org/pdf/2310.00498v2.pdf","comment":"7 Pages, 6 Figures, Published at IROS 2023"},{"id":"http://arxiv.org/abs/2310.04676v1","updated":"2023-10-07T03:21:58Z","published":"2023-10-07T03:21:58Z","title":"Surgical Gym: A high-performance GPU-based platform for reinforcement\n  learning with surgical robots","summary":"  Recent advances in robot-assisted surgery have resulted in progressively more\nprecise, efficient, and minimally invasive procedures, sparking a new era of\nrobotic surgical intervention. This enables doctors, in collaborative\ninteraction with robots, to perform traditional or minimally invasive surgeries\nwith improved outcomes through smaller incisions. Recent efforts are working\ntoward making robotic surgery more autonomous which has the potential to reduce\nvariability of surgical outcomes and reduce complication rates. Deep\nreinforcement learning methodologies offer scalable solutions for surgical\nautomation, but their effectiveness relies on extensive data acquisition due to\nthe absence of prior knowledge in successfully accomplishing tasks. Due to the\nintensive nature of simulated data collection, previous works have focused on\nmaking existing algorithms more efficient. In this work, we focus on making the\nsimulator more efficient, making training data much more accessible than\npreviously possible. We introduce Surgical Gym, an open-source high performance\nplatform for surgical robot learning where both the physics simulation and\nreinforcement learning occur directly on the GPU. We demonstrate between\n100-5000x faster training times compared with previous surgical learning\nplatforms. The code is available at:\nhttps://github.com/SamuelSchmidgall/SurgicalGym.\n","authors":["Samuel Schmidgall","Axel Krieger","Jason Eshraghian"],"pdf_url":"https://arxiv.org/pdf/2310.04676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04675v1","updated":"2023-10-07T03:20:26Z","published":"2023-10-07T03:20:26Z","title":"Terrain-Aware Quadrupedal Locomotion via Reinforcement Learning","summary":"  In nature, legged animals have developed the ability to adapt to challenging\nterrains through perception, allowing them to plan safe body and foot\ntrajectories in advance, which leads to safe and energy-efficient locomotion.\nInspired by this observation, we present a novel approach to train a Deep\nNeural Network (DNN) policy that integrates proprioceptive and exteroceptive\nstates with a parameterized trajectory generator for quadruped robots to\ntraverse rough terrains. Our key idea is to use a DNN policy that can modify\nthe parameters of the trajectory generator, such as foot height and frequency,\nto adapt to different terrains. To encourage the robot to step on safe regions\nand save energy consumption, we propose foot terrain reward and lifting foot\nheight reward, respectively. By incorporating these rewards, our method can\nlearn a safer and more efficient terrain-aware locomotion policy that can move\na quadruped robot flexibly in any direction. To evaluate the effectiveness of\nour approach, we conduct simulation experiments on challenging terrains,\nincluding stairs, stepping stones, and poles. The simulation results\ndemonstrate that our approach can successfully direct the robot to traverse\nsuch tough terrains in any direction. Furthermore, we validate our method on a\nreal legged robot, which learns to traverse stepping stones with gaps over\n25.5cm.\n","authors":["Haojie Shi","Qingxu Zhu","Lei Han","Wanchao Chi","Tingguang Li","Max Q. -H. Meng"],"pdf_url":"https://arxiv.org/pdf/2310.04675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04637v1","updated":"2023-10-07T01:21:24Z","published":"2023-10-07T01:21:24Z","title":"Efficient State Estimation with Constrained Rao-Blackwellized Particle\n  Filter","summary":"  Due to the limitations of the robotic sensors, during a robotic manipulation\ntask, the acquisition of the object's state can be unreliable and noisy.\nCombining an accurate model of multi-body dynamic system with Bayesian\nfiltering methods has been shown to be able to filter out noise from the\nobject's observed states. However, efficiency of these filtering methods\nsuffers from samples that violate the physical constraints, e.g., no\npenetration constraint.\n  In this paper, we propose a Rao-Blackwellized Particle Filter (RBPF) that\nsamples the contact states and updates the object's poses using Kalman filters.\nThis RBPF also enforces the physical constraints on the samples by solving a\nquadratic programming problem. By comparing our method with methods that does\nnot consider physical constraints, we show that our proposed RBPF is not only\nable to estimate the object's states, e.g., poses, more accurately but also\nable to infer unobserved states, e.g., velocities, with higher precision.\n","authors":["Shuai Li","Siwei Lyu","Jeff Trinkle"],"pdf_url":"https://arxiv.org/pdf/2310.04637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16375v2","updated":"2023-10-07T00:58:01Z","published":"2023-09-28T12:22:39Z","title":"A Comprehensive Review on Tree Detection Methods Using Point Cloud and\n  Aerial Imagery from Unmanned Aerial Vehicles","summary":"  Unmanned Aerial Vehicles (UAVs) are considered cutting-edge technology with\nhighly cost-effective and flexible usage scenarios. Although many papers have\nreviewed the application of UAVs in agriculture, the review of the application\nfor tree detection is still insufficient. This paper focuses on tree detection\nmethods applied to UAV data collected by UAVs. There are two kinds of data, the\npoint cloud and the images, which are acquired by the Light Detection and\nRanging (LiDAR) sensor and camera, respectively. Among the detection methods\nusing point-cloud data, this paper mainly classifies these methods according to\nLiDAR and Digital Aerial Photography (DAP). For the detection methods using\nimages directly, this paper reviews these methods by whether or not to use the\nDeep Learning (DL) method. Our review concludes and analyses the comparison and\ncombination between the application of LiDAR-based and DAP-based point cloud\ndata. The performance, relative merits, and application fields of the methods\nare also introduced. Meanwhile, this review counts the number of tree detection\nstudies using different methods in recent years. From our statics, the\ndetection task using DL methods on the image has become a mainstream trend as\nthe number of DL-based detection researches increases to 45% of the total\nnumber of tree detection studies up to 2022. As a result, this review could\nhelp and guide researchers who want to carry out tree detection on specific\nforests and for farmers to use UAVs in managing agriculture production.\n","authors":["Weijie Kuang","Hann Woei Ho","Ye Zhou","Shahrel Azmin Suandi","Farzad Ismail"],"pdf_url":"https://arxiv.org/pdf/2309.16375v2.pdf","comment":"This paper has been submitted to Computers and Electronics in\n  Agriculture for review"}]},"2023-10-10T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2310.06828v1","updated":"2023-10-10T17:57:06Z","published":"2023-10-10T17:57:06Z","title":"RoboHive: A Unified Framework for Robot Learning","summary":"  We present RoboHive, a comprehensive software platform and ecosystem for\nresearch in the field of Robot Learning and Embodied Artificial Intelligence.\nOur platform encompasses a diverse range of pre-existing and novel\nenvironments, including dexterous manipulation with the Shadow Hand, whole-arm\nmanipulation tasks with Franka and Fetch robots, quadruped locomotion, among\nothers. Included environments are organized within and cover multiple domains\nsuch as hand manipulation, locomotion, multi-task, multi-agent, muscles, etc.\nIn comparison to prior works, RoboHive offers a streamlined and unified task\ninterface taking dependency on only a minimal set of well-maintained packages,\nfeatures tasks with high physics fidelity and rich visual diversity, and\nsupports common hardware drivers for real-world deployment. The unified\ninterface of RoboHive offers a convenient and accessible abstraction for\nalgorithmic research in imitation, reinforcement, multi-task, and hierarchical\nlearning. Furthermore, RoboHive includes expert demonstrations and baseline\nresults for most environments, providing a standard for benchmarking and\ncomparisons. Details: https://sites.google.com/view/robohive\n","authors":["Vikash Kumar","Rutav Shah","Gaoyue Zhou","Vincent Moens","Vittorio Caggiano","Jay Vakil","Abhishek Gupta","Aravind Rajeswaran"],"pdf_url":"https://arxiv.org/pdf/2310.06828v1.pdf","comment":"Accepted at 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023) Track on Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2310.06794v1","updated":"2023-10-10T17:07:05Z","published":"2023-10-10T17:07:05Z","title":"$f$-Policy Gradients: A General Framework for Goal Conditioned RL using\n  $f$-Divergences","summary":"  Goal-Conditioned Reinforcement Learning (RL) problems often have access to\nsparse rewards where the agent receives a reward signal only when it has\nachieved the goal, making policy optimization a difficult problem. Several\nworks augment this sparse reward with a learned dense reward function, but this\ncan lead to sub-optimal policies if the reward is misaligned. Moreover, recent\nworks have demonstrated that effective shaping rewards for a particular problem\ncan depend on the underlying learning algorithm. This paper introduces a novel\nway to encourage exploration called $f$-Policy Gradients, or $f$-PG. $f$-PG\nminimizes the f-divergence between the agent's state visitation distribution\nand the goal, which we show can lead to an optimal policy. We derive gradients\nfor various f-divergences to optimize this objective. Our learning paradigm\nprovides dense learning signals for exploration in sparse reward settings. We\nfurther introduce an entropy-regularized policy optimization objective, that we\ncall $state$-MaxEnt RL (or $s$-MaxEnt RL) as a special case of our objective.\nWe show that several metric-based shaping rewards like L2 can be used with\n$s$-MaxEnt RL, providing a common ground to study such metric-based shaping\nrewards with efficient exploration. We find that $f$-PG has better performance\ncompared to standard policy gradient methods on a challenging gridworld as well\nas the Point Maze and FetchReach environments. More information on our website\nhttps://agarwalsiddhant10.github.io/projects/fpg.html.\n","authors":["Siddhant Agarwal","Ishan Durugkar","Peter Stone","Amy Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06794v1.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06765v1","updated":"2023-10-10T16:40:38Z","published":"2023-10-10T16:40:38Z","title":"Efficient Graduated Non-Convexity for Pose Graph Optimization","summary":"  We propose a novel approach to Graduated Non-Convexity (GNC) and demonstrate\nits efficacy through its application in robust pose graph optimization, a key\ncomponent in SLAM backends. Traditional GNC methods often rely on heuristic\nmethods for GNC schedule, updating control parameter {\\mu} for escalating the\nnon-convexity. In contrast, our approach leverages the properties of convex\nfunctions and convex optimization to identify the boundary points beyond which\nconvexity is no longer guaranteed, thereby eliminating redundant optimization\nsteps in existing methodologies and enhancing both speed and robustness. We\nshow that our method outperforms the state-of-the-art method in terms of speed\nand accuracy when used for robust back-end pose graph optimization via GNC. Our\nwork builds upon and enhances the open-source riSAM framework. Our\nimplementation can be accessed from: https://github.com/SNU-DLLAB/EGNC-PGO\n","authors":["Wonseok Kang","Jaehyun Kim","Jiseong Chung","Seungwon Choi","Tae-wan Kim"],"pdf_url":"https://arxiv.org/pdf/2310.06765v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.06751v1","updated":"2023-10-10T16:23:34Z","published":"2023-10-10T16:23:34Z","title":"EARL: Eye-on-Hand Reinforcement Learner for Dynamic Grasping with Active\n  Pose Estimation","summary":"  In this paper, we explore the dynamic grasping of moving objects through\nactive pose tracking and reinforcement learning for hand-eye coordination\nsystems. Most existing vision-based robotic grasping methods implicitly assume\ntarget objects are stationary or moving predictably. Performing grasping of\nunpredictably moving objects presents a unique set of challenges. For example,\na pre-computed robust grasp can become unreachable or unstable as the target\nobject moves, and motion planning must also be adaptive. In this work, we\npresent a new approach, Eye-on-hAnd Reinforcement Learner (EARL), for enabling\ncoupled Eye-on-Hand (EoH) robotic manipulation systems to perform real-time\nactive pose tracking and dynamic grasping of novel objects without explicit\nmotion prediction. EARL readily addresses many thorny issues in automated\nhand-eye coordination, including fast-tracking of 6D object pose from vision,\nlearning control policy for a robotic arm to track a moving object while\nkeeping the object in the camera's field of view, and performing dynamic\ngrasping. We demonstrate the effectiveness of our approach in extensive\nexperiments validated on multiple commercial robotic arms in both simulations\nand complex real-world tasks.\n","authors":["Baichuan Huang","Jingjin Yu","Siddarth Jain"],"pdf_url":"https://arxiv.org/pdf/2310.06751v1.pdf","comment":"Presented on IROS 2023 Corresponding author Siddarth Jain"},{"id":"http://arxiv.org/abs/2307.08966v2","updated":"2023-10-10T16:22:35Z","published":"2023-07-18T04:34:43Z","title":"Multi-Robot Patrol Algorithm with Distributed Coordination and\n  Consciousness of the Base Station's Situation Awareness","summary":"  Multi-robot patrolling is the potential application for robotic systems to\nsurvey wide areas efficiently without human burdens and mistakes. However, such\nsystems have few examples of real-world applications due to their lack of human\npredictability. This paper proposes an algorithm: Local Reactive (LR) for\nmulti-robot patrolling to satisfy both needs: (i)patrol efficiently and\n(ii)provide humans with better situation awareness to enhance system\npredictability. Each robot operating according to the proposed algorithm\nselects its patrol target from the local areas around the robot's current\nlocation by two requirements: (i)patrol location with greater need, (ii)report\nits achievements to the base station. The algorithm is distributed and\ncoordinates the robots without centralized control by sharing their patrol\nachievements and degree of need to report to the base station. The proposed\nalgorithm performed better than existing algorithms in both patrolling and the\nbase station's situation awareness.\n","authors":["Kazuho Kobayashi","Seiya Ueno","Takehiro Higuchi"],"pdf_url":"https://arxiv.org/pdf/2307.08966v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2310.06654v1","updated":"2023-10-10T14:22:56Z","published":"2023-10-10T14:22:56Z","title":"Evaluating Explanation Methods for Vision-and-Language Navigation","summary":"  The ability to navigate robots with natural language instructions in an\nunknown environment is a crucial step for achieving embodied artificial\nintelligence (AI). With the improving performance of deep neural models\nproposed in the field of vision-and-language navigation (VLN), it is equally\ninteresting to know what information the models utilize for their\ndecision-making in the navigation tasks. To understand the inner workings of\ndeep neural models, various explanation methods have been developed for\npromoting explainable AI (XAI). But they are mostly applied to deep neural\nmodels for image or text classification tasks and little work has been done in\nexplaining deep neural models for VLN tasks. In this paper, we address these\nproblems by building quantitative benchmarks to evaluate explanation methods\nfor VLN models in terms of faithfulness. We propose a new erasure-based\nevaluation pipeline to measure the step-wise textual explanation in the\nsequential decision-making setting. We evaluate several explanation methods for\ntwo representative VLN models on two popular VLN datasets and reveal valuable\nfindings through our experiments.\n","authors":["Guanqi Chen","Lei Yang","Guanhua Chen","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2310.06654v1.pdf","comment":"Accepted by ECAI 2023"},{"id":"http://arxiv.org/abs/2310.06646v1","updated":"2023-10-10T14:10:39Z","published":"2023-10-10T14:10:39Z","title":"Forgetful Large Language Models: Lessons Learned from Using LLMs in\n  Robot Programming","summary":"  Large language models offer new ways of empowering people to program robot\napplications-namely, code generation via prompting. However, the code generated\nby LLMs is susceptible to errors. This work reports a preliminary exploration\nthat empirically characterizes common errors produced by LLMs in robot\nprogramming. We categorize these errors into two phases: interpretation and\nexecution. In this work, we focus on errors in execution and observe that they\nare caused by LLMs being \"forgetful\" of key information provided in user\nprompts. Based on this observation, we propose prompt engineering tactics\ndesigned to reduce errors in execution. We then demonstrate the effectiveness\nof these tactics with three language models: ChatGPT, Bard, and LLaMA-2.\nFinally, we discuss lessons learned from using LLMs in robot programming and\ncall for the benchmarking of LLM-powered end-user development of robot\napplications.\n","authors":["Juo-Tung Chen","Chien-Ming Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06646v1.pdf","comment":"9 pages ,8 figures, accepted by the AAAI 2023 Fall Symposium Series"},{"id":"http://arxiv.org/abs/2303.06222v3","updated":"2023-10-10T13:33:10Z","published":"2023-03-10T22:10:41Z","title":"Robust MADER: Decentralized Multiagent Trajectory Planner Robust to\n  Communication Delay in Dynamic Environments","summary":"  Communication delays can be catastrophic for multiagent systems. However,\nmost existing state-of-the-art multiagent trajectory planners assume perfect\ncommunication and therefore lack a strategy to rectify this issue in real-world\nenvironments. To address this challenge, we propose Robust MADER (RMADER), a\ndecentralized, asynchronous multiagent trajectory planner robust to\ncommunication delay. RMADER ensures safety by introducing (1) a Delay Check\nstep, (2) a two-step trajectory publication scheme, and (3) a novel\ntrajectory-storing-and-checking approach. Our primary contributions include:\nproving recursive feasibility for collision-free trajectory generation in\nasynchronous decentralized trajectory-sharing, simulation benchmark studies,\nand hardware experiments with different network topologies and dynamic\nobstacles. We show that RMADER outperforms existing approaches by achieving a\n100% success rate of collision-free trajectory generation, whereas the next\nbest asynchronous decentralized method only achieves 83% success.\n","authors":["Kota Kondo","Reinaldo Figueroa","Juan Rached","Jesus Tordesillas","Parker C. Lusk","Jonathan P. How"],"pdf_url":"https://arxiv.org/pdf/2303.06222v3.pdf","comment":"8 pagers, 10 figures,. arXiv admin note: substantial text overlap\n  with arXiv:2209.13667"},{"id":"http://arxiv.org/abs/2310.06606v1","updated":"2023-10-10T13:13:51Z","published":"2023-10-10T13:13:51Z","title":"SYNLOCO: Synthesizing Central Pattern Generator and Reinforcement\n  Learning for Quadruped Locomotion","summary":"  The Central Pattern Generator (CPG) is adept at generating rhythmic gait\npatterns characterized by consistent timing and adequate foot clearance. Yet,\nits open-loop configuration often compromises the system's control performance\nin response to environmental variations. On the other hand, Reinforcement\nLearning (RL), celebrated for its model-free properties, has gained significant\ntraction in robotics due to its inherent adaptability and robustness. However,\ninitiating traditional RL approaches from the ground up presents computational\nchallenges and a heightened risk of converging to suboptimal local minima. In\nthis paper, we propose an innovative quadruped locomotion framework, SYNLOCO,\nby synthesizing CPG and RL that can ingeniously integrate the strengths of both\nmethods, enabling the development of a locomotion controller that is both\nstable and natural. Furthermore, we introduce a set of performance-driven\nreward metrics that augment the learning of locomotion control. To optimize the\nlearning trajectory of SYNLOCO, a two-phased training strategy is presented.\nOur empirical evaluation, conducted on a Unitree GO1 robot under varied\nconditions--including distinct velocities, terrains, and payload\ncapacities--showcases SYNLOCO's ability to produce consistent and clear-footed\ngaits across diverse scenarios. The developed controller exhibits resilience\nagainst substantial parameter variations, underscoring its potential for robust\nreal-world applications.\n","authors":["Xinyu Zhang","Zhiyuan Xiao","Qingrui Zhang","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2310.06606v1.pdf","comment":"7 Pages"},{"id":"http://arxiv.org/abs/2310.06603v1","updated":"2023-10-10T13:12:03Z","published":"2023-10-10T13:12:03Z","title":"V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric\n  Heterogenous Distillation Network","summary":"  Object detection is the central issue of intelligent traffic systems, and\nrecent advancements in single-vehicle lidar-based 3D detection indicate that it\ncan provide accurate position information for intelligent agents to make\ndecisions and plan. Compared with single-vehicle perception, multi-view\nvehicle-road cooperation perception has fundamental advantages, such as the\nelimination of blind spots and a broader range of perception, and has become a\nresearch hotspot. However, the current perception of cooperation focuses on\nimproving the complexity of fusion while ignoring the fundamental problems\ncaused by the absence of single-view outlines. We propose a multi-view\nvehicle-road cooperation perception system, vehicle-to-everything cooperative\nperception (V2X-AHD), in order to enhance the identification capability,\nparticularly for predicting the vehicle's shape. At first, we propose an\nasymmetric heterogeneous distillation network fed with different training data\nto improve the accuracy of contour recognition, with multi-view teacher\nfeatures transferring to single-view student features. While the point cloud\ndata are sparse, we propose Spara Pillar, a spare convolutional-based plug-in\nfeature extraction backbone, to reduce the number of parameters and improve and\nenhance feature extraction capabilities. Moreover, we leverage the multi-head\nself-attention (MSA) to fuse the single-view feature, and the lightweight\ndesign makes the fusion feature a smooth expression. The results of applying\nour algorithm to the massive open dataset V2Xset demonstrate that our method\nachieves the state-of-the-art result. The V2X-AHD can effectively improve the\naccuracy of 3D object detection and reduce the number of network parameters,\naccording to this study, which serves as a benchmark for cooperative\nperception. The code for this article is available at\nhttps://github.com/feeling0414-lab/V2X-AHD.\n","authors":["Caizhen He","Hai Wang","Long Chen","Tong Luo","Yingfeng Cai"],"pdf_url":"https://arxiv.org/pdf/2310.06603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06585v1","updated":"2023-10-10T12:52:42Z","published":"2023-10-10T12:52:42Z","title":"A Black-Box Physics-Informed Estimator based on Gaussian Process\n  Regression for Robot Inverse Dynamics Identification","summary":"  In this paper, we propose a black-box model based on Gaussian process\nregression for the identification of the inverse dynamics of robotic\nmanipulators. The proposed model relies on a novel multidimensional kernel,\ncalled \\textit{Lagrangian Inspired Polynomial} (\\kernelInitials{}) kernel. The\n\\kernelInitials{} kernel is based on two main ideas. First, instead of directly\nmodeling the inverse dynamics components, we model as GPs the kinetic and\npotential energy of the system. The GP prior on the inverse dynamics components\nis derived from those on the energies by applying the properties of GPs under\nlinear operators. Second, as regards the energy prior definition, we prove a\npolynomial structure of the kinetic and potential energy, and we derive a\npolynomial kernel that encodes this property. As a consequence, the proposed\nmodel allows also to estimate the kinetic and potential energy without\nrequiring any label on these quantities. Results on simulation and on two real\nrobotic manipulators, namely a 7 DOF Franka Emika Panda and a 6 DOF MELFA\nRV4FL, show that the proposed model outperforms state-of-the-art black-box\nestimators based both on Gaussian Processes and Neural Networks in terms of\naccuracy, generality and data efficiency. The experiments on the MELFA robot\nalso demonstrate that our approach achieves performance comparable to\nfine-tuned model-based estimators, despite requiring less prior information.\n","authors":["Giulio Giacomuzzo","Alberto Dalla Libera","Diego Romeres","Ruggero Carli"],"pdf_url":"https://arxiv.org/pdf/2310.06585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07038v3","updated":"2023-10-10T11:55:30Z","published":"2023-09-13T15:46:40Z","title":"Efficient Reinforcement Learning for Jumping Monopods","summary":"  In this work, we consider the complex control problem of making a monopod\nreach a target with a jump. The monopod can jump in any direction and the\nterrain underneath its foot can be uneven. This is a template of a much larger\nclass of problems, which are extremely challenging and computationally\nexpensive to solve using standard optimisation-based techniques. Reinforcement\nLearning (RL) could be an interesting alternative, but the application of an\nend-to-end approach in which the controller must learn everything from scratch,\nis impractical. The solution advocated in this paper is to guide the learning\nprocess within an RL framework by injecting physical knowledge. This expedient\nbrings to widespread benefits, such as a drastic reduction of the learning\ntime, and the ability to learn and compensate for possible errors in the\nlow-level controller executing the motion. We demonstrate the advantage of our\napproach with respect to both optimization-based and end-to-end RL approaches.\n","authors":["Riccardo Bussola","Michele Focchi","Andrea Del Prete","Daniele Fontanelli","Luigi Palopoli"],"pdf_url":"https://arxiv.org/pdf/2309.07038v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14970v3","updated":"2023-10-10T11:54:37Z","published":"2023-09-26T14:42:28Z","title":"Recurrent Hypernetworks are Surprisingly Strong in Meta-RL","summary":"  Deep reinforcement learning (RL) is notoriously impractical to deploy due to\nsample inefficiency. Meta-RL directly addresses this sample inefficiency by\nlearning to perform few-shot learning when a distribution of related tasks is\navailable for meta-training. While many specialized meta-RL methods have been\nproposed, recent work suggests that end-to-end learning in conjunction with an\noff-the-shelf sequential model, such as a recurrent network, is a surprisingly\nstrong baseline. However, such claims have been controversial due to limited\nsupporting evidence, particularly in the face of prior work establishing\nprecisely the opposite. In this paper, we conduct an empirical investigation.\nWhile we likewise find that a recurrent network can achieve strong performance,\nwe demonstrate that the use of hypernetworks is crucial to maximizing their\npotential. Surprisingly, when combined with hypernetworks, the recurrent\nbaselines that are far simpler than existing specialized methods actually\nachieve the strongest performance of all methods evaluated.\n","authors":["Jacob Beck","Risto Vuorio","Zheng Xiong","Shimon Whiteson"],"pdf_url":"https://arxiv.org/pdf/2309.14970v3.pdf","comment":"Published at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06542v1","updated":"2023-10-10T11:41:16Z","published":"2023-10-10T11:41:16Z","title":"Data-driven mode shape selection and model-based vibration suppression\n  of 3-RRR parallel manipulator with flexible actuation links","summary":"  The mode shape function is difficult to determine in modeling manipulators\nwith flexible links using the assumed mode method. In this paper, for a planar\n3-RRR parallel manipulator with flexible actuation links, we provide a\ndata-driven method to identify the mode shape of the flexible links and propose\na model-based controller for the vibration suppression. By deriving the inverse\nkinematics of the studied mechanism in analytical form, the dynamic model is\nestablished by using the assumed mode method. To select the mode shape\nfunction, the software of multi-body system dynamics is used to simulate the\ndynamic behavior of the mechanism, and then the data-driven method which\ncombines the DMD and SINDy algorithms is employed to identify the reasonable\nmode shape functions for the flexible links. To suppress the vibration of the\nflexible links, a state observer for the end-effector is constructed by a\nneural network, and the model-based control law is designed on this basis. In\ncomparison with the model-free controller, the proposed controller with\ndeveloped dynamic model has promising performance in terms of tracking accuracy\nand vibration suppression.\n","authors":["Dingxu Guo","Jian Xu","Shu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07336v3","updated":"2023-10-10T09:25:33Z","published":"2023-05-12T09:28:09Z","title":"MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with\n  Bird's Eye View based Appearance and Motion Features","summary":"  Identifying moving objects is an essential capability for autonomous systems,\nas it provides critical information for pose estimation, navigation, collision\navoidance, and static map construction. In this paper, we present MotionBEV, a\nfast and accurate framework for LiDAR moving object segmentation, which\nsegments moving objects with appearance and motion features in the bird's eye\nview (BEV) domain. Our approach converts 3D LiDAR scans into a 2D polar BEV\nrepresentation to improve computational efficiency. Specifically, we learn\nappearance features with a simplified PointNet and compute motion features\nthrough the height differences of consecutive frames of point clouds projected\nonto vertical columns in the polar BEV coordinate system. We employ a\ndual-branch network bridged by the Appearance-Motion Co-attention Module (AMCM)\nto adaptively fuse the spatio-temporal information from appearance and motion\nfeatures. Our approach achieves state-of-the-art performance on the\nSemanticKITTI-MOS benchmark. Furthermore, to demonstrate the practical\neffectiveness of our method, we provide a LiDAR-MOS dataset recorded by a\nsolid-state LiDAR, which features non-repetitive scanning patterns and a small\nfield of view.\n","authors":["Bo Zhou","Jiapeng Xie","Yan Pan","Jiajie Wu","Chuanzhao Lu"],"pdf_url":"https://arxiv.org/pdf/2305.07336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06424v1","updated":"2023-10-10T08:48:49Z","published":"2023-10-10T08:48:49Z","title":"Feel the Tension: Manipulation of Deformable Linear Objects in\n  Environments with Fixtures using Force Information","summary":"  Humans are able to manipulate Deformable Linear Objects (DLOs) such as cables\nand wires, with little or no visual information, relying mostly on force\nsensing. In this work, we propose a reduced DLO model which enables such blind\nmanipulation by keeping the object under tension. Further, an online model\nestimation procedure is also proposed. A set of elementary sliding and clipping\nmanipulation primitives are defined based on our model. The combination of\nthese primitives allows for more complex motions such as winding of a DLO. The\nmodel estimation and manipulation primitives are tested individually but also\ntogether in a real-world cable harness production task, using a dual-arm YuMi,\nthus demonstrating that force-based perception can be sufficient even for such\na complex scenario.\n","authors":["Finn Süberkrüb","Rita Laezza","Yiannis Karayiannidis"],"pdf_url":"https://arxiv.org/pdf/2310.06424v1.pdf","comment":"2022 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)"},{"id":"http://arxiv.org/abs/2310.06414v1","updated":"2023-10-10T08:31:33Z","published":"2023-10-10T08:31:33Z","title":"Plane Constraints Aided Multi-Vehicle Cooperative Positioning Using\n  Factor Graph Optimization","summary":"  The development of vehicle-to-vehicle (V2V) communication facil-itates the\nstudy of cooperative positioning (CP) techniques for vehicular applications.\nThe CP methods can improve the posi-tioning availability and accuracy by\ninter-vehicle ranging and data exchange between vehicles. However, the\ninter-vehicle rang-ing can be easily interrupted due to many factors such as\nobsta-cles in-between two cars. Without inter-vehicle ranging, the other\ncooperative data such as vehicle positions will be wasted, leading to\nperformance degradation of range-based CP methods. To fully utilize the\ncooperative data and mitigate the impact of inter-vehicle ranging loss, a novel\ncooperative positioning method aided by plane constraints is proposed in this\npaper. The positioning results received from cooperative vehicles are used to\nconstruct the road plane for each vehicle. The plane parameters are then\nintroduced into CP scheme to impose constraints on positioning solutions. The\nstate-of-art factor graph optimization (FGO) algo-rithm is employed to\nintegrate the plane constraints with raw data of Global Navigation Satellite\nSystems (GNSS) as well as inter-vehicle ranging measurements. The proposed CP\nmethod has the ability to resist the interruptions of inter-vehicle ranging\nsince the plane constraints are computed by just using position-related data. A\nvehicle can still benefit from the position data of cooperative vehicles even\nif the inter-vehicle ranging is unavaila-ble. The experimental results indicate\nthe superiority of the pro-posed CP method in positioning performance over the\nexisting methods, especially when the inter-ranging interruptions occur.\n","authors":["Chen Zhuang","Hongbo Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.06414v1.pdf","comment":"14 pages, 16 figures, IEEE trans on ITS"},{"id":"http://arxiv.org/abs/2310.06385v1","updated":"2023-10-10T07:48:40Z","published":"2023-10-10T07:48:40Z","title":"3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic\n  Indoor Environments","summary":"  The existence of variable factors within the environment can cause a decline\nin camera localization accuracy, as it violates the fundamental assumption of a\nstatic environment in Simultaneous Localization and Mapping (SLAM) algorithms.\nRecent semantic SLAM systems towards dynamic environments either rely solely on\n2D semantic information, or solely on geometric information, or combine their\nresults in a loosely integrated manner. In this research paper, we introduce\n3DS-SLAM, 3D Semantic SLAM, tailored for dynamic scenes with visual 3D object\ndetection. The 3DS-SLAM is a tightly-coupled algorithm resolving both semantic\nand geometric constraints sequentially. We designed a 3D part-aware hybrid\ntransformer for point cloud-based object detection to identify dynamic objects.\nSubsequently, we propose a dynamic feature filter based on HDBSCAN clustering\nto extract objects with significant absolute depth differences. When compared\nagainst ORB-SLAM2, 3DS-SLAM exhibits an average improvement of 98.01% across\nthe dynamic sequences of the TUM RGB-D dataset. Furthermore, it surpasses the\nperformance of the other four leading SLAM systems designed for dynamic\nenvironments.\n","authors":["Ghanta Sai Krishna","Kundrapu Supriya","Sabur Baidya"],"pdf_url":"https://arxiv.org/pdf/2310.06385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06384v1","updated":"2023-10-10T07:48:23Z","published":"2023-10-10T07:48:23Z","title":"Redundant and Loosely Coupled LiDAR-Wi-Fi Integration for Robust Global\n  Localization in Autonomous Mobile Robotics","summary":"  This paper presents a framework addressing the challenge of global\nlocalization in autonomous mobile robotics by integrating LiDAR-based\ndescriptors and Wi-Fi fingerprinting in a pre-mapped environment. This is\nmotivated by the increasing demand for reliable localization in complex\nscenarios, such as urban areas or underground mines, requiring robust systems\nable to overcome limitations faced by traditional Global Navigation Satellite\nSystem (GNSS)-based localization methods. By leveraging the complementary\nstrengths of LiDAR and Wi-Fi sensors used to generate predictions and evaluate\nthe confidence of each prediction as an indicator of potential degradation, we\npropose a redundancy-based approach that enhances the system's overall\nrobustness and accuracy. The proposed framework allows independent operation of\nthe LiDAR and Wi-Fi sensors, ensuring system redundancy. By combining the\npredictions while considering their confidence levels, we achieve enhanced and\nconsistent performance in localization tasks.\n","authors":["Nikolaos Stathoulopoulos","Emanuele Pagliari","Luca Davoli","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2310.06384v1.pdf","comment":"7 pages, 5 figures. Accepted for publication in the 21st\n  International Conference on Advanced Robotics (ICAR 2023)"},{"id":"http://arxiv.org/abs/2309.05249v2","updated":"2023-10-10T05:57:01Z","published":"2023-09-11T05:55:01Z","title":"Evaluating Visual Odometry Methods for Autonomous Driving in Rain","summary":"  The increasing demand for autonomous vehicles has created a need for robust\nnavigation systems that can also operate effectively in adverse weather\nconditions. Visual odometry is a technique used in these navigation systems,\nenabling the estimation of vehicle position and motion using input from onboard\ncameras. However, visual odometry accuracy can be significantly impacted in\nchallenging weather conditions, such as heavy rain, snow, or fog. In this\npaper, we evaluate a range of visual odometry methods, including our DROID-SLAM\nbased heuristic approach. Specifically, these algorithms are tested on both\nclear and rainy weather urban driving data to evaluate their robustness. We\ncompiled a dataset comprising of a range of rainy weather conditions from\ndifferent cities. This includes, the Oxford Robotcar dataset from Oxford, the\n4Seasons dataset from Munich and an internal dataset collected in Singapore. We\nevaluated different visual odometry algorithms for both monocular and stereo\ncamera setups using the Absolute Trajectory Error (ATE). From the range of\napproaches evaluated, our findings suggest that the Depth and Flow for Visual\nOdometry (DF-VO) algorithm with monocular setup performed the best for short\nrange distances (< 500m) and our proposed DROID-SLAM based heuristic approach\nfor the stereo setup performed relatively well for long-term localization. Both\nVO algorithms suggested a need for a more robust sensor fusion based approach\nfor localization in rain.\n","authors":["Yu Xiang Tan","Marcel Bartholomeus Prasetyo","Mohammad Alif Daffa","Deshpande Sunny Nitin","Malika Meghjani"],"pdf_url":"https://arxiv.org/pdf/2309.05249v2.pdf","comment":"8 pages, 4 figures, Accepted at IEEE International Conference on\n  Automation Science and Engineering (CASE) 2023. Fixed grammar and phrasing to\n  improve clarity of the statements made. Emphasized on the need for a more\n  robust sensor fusion based approach for localization in rain for autonomous\n  driving"},{"id":"http://arxiv.org/abs/2310.06303v1","updated":"2023-10-10T04:34:00Z","published":"2023-10-10T04:34:00Z","title":"Dobby: A Conversational Service Robot Driven by GPT-4","summary":"  This work introduces a robotics platform which embeds a conversational AI\nagent in an embodied system for natural language understanding and intelligent\ndecision-making for service tasks; integrating task planning and human-like\nconversation. The agent is derived from a large language model, which has\nlearned from a vast corpus of general knowledge. In addition to generating\ndialogue, this agent can interface with the physical world by invoking commands\non the robot; seamlessly merging communication and behavior. This system is\ndemonstrated in a free-form tour-guide scenario, in an HRI study combining\nrobots with and without conversational AI capabilities. Performance is measured\nalong five dimensions: overall effectiveness, exploration abilities,\nscrutinization abilities, receptiveness to personification, and adaptability.\n","authors":["Carson Stark","Bohkyung Chun","Casey Charleston","Varsha Ravi","Luis Pabon","Surya Sunkari","Tarun Mohan","Peter Stone","Justin Hart"],"pdf_url":"https://arxiv.org/pdf/2310.06303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02635v2","updated":"2023-10-10T04:13:20Z","published":"2023-10-04T07:56:42Z","title":"Foundation Reinforcement Learning: towards Embodied Generalist Agents\n  with Foundation Prior Assistance","summary":"  Recently, people have shown that large-scale pre-training from internet-scale\ndata is the key to building generalist models, as witnessed in NLP. To build\nembodied generalist agents, we and many other researchers hypothesize that such\nfoundation prior is also an indispensable component. However, it is unclear\nwhat is the proper concrete form to represent those embodied foundation priors\nand how they should be used in the downstream task. In this paper, we propose\nan intuitive and effective set of embodied priors that consist of foundation\npolicy, value, and success reward. The proposed priors are based on the\ngoal-conditioned MDP. To verify their effectiveness, we instantiate an\nactor-critic method assisted by the priors, called Foundation Actor-Critic\n(FAC). We name our framework as Foundation Reinforcement Learning (FRL), since\nit completely relies on embodied foundation priors to explore, learn and\nreinforce. The benefits of FRL are threefold. (1) Sample efficient. With\nfoundation priors, FAC learns significantly faster than traditional RL. Our\nevaluation on the Meta-World has proved that FAC can achieve 100% success rates\nfor 7/8 tasks under less than 200k frames, which outperforms the baseline\nmethod with careful manual-designed rewards under 1M frames. (2) Robust to\nnoisy priors. Our method tolerates the unavoidable noise in embodied foundation\nmodels. We show that FAC works well even under heavy noise or quantization\nerrors. (3) Minimal human intervention: FAC completely learns from the\nfoundation priors, without the need of human-specified dense reward, or\nproviding teleoperated demos. Thus, FAC can be easily scaled up. We believe our\nFRL framework could enable the future robot to autonomously explore and learn\nwithout human intervention in the physical world. In summary, our proposed FRL\nis a novel and powerful learning paradigm, towards achieving embodied\ngeneralist agents.\n","authors":["Weirui Ye","Yunsheng Zhang","Mengchen Wang","Shengjie Wang","Xianfan Gu","Pieter Abbeel","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2310.02635v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05456v2","updated":"2023-10-10T02:14:09Z","published":"2023-05-04T06:17:25Z","title":"Language-Grounded Control for Coordinated Robot Motion and Speech","summary":"  Recent advancements have enabled human-robot collaboration through physical\nassistance and verbal guidance. However, limitations persist in coordinating\nrobots' physical motions and speech in response to real-time changes in human\nbehavior during collaborative contact tasks. We first derive principles from\nanalyzing physical therapists' movements and speech during patient exercises.\nThese principles are translated into control objectives to: 1) guide users\nthrough trajectories, 2) control motion and speech pace to align completion\ntimes with varying user cooperation, and 3) dynamically paraphrase speech along\nthe trajectory. We then propose a Language Controller that synchronizes motion\nand speech, modulating both based on user cooperation. Experiments with 12\nusers show the Language Controller successfully aligns motion and speech\ncompared to baselines. This provides a framework for fluent human-robot\ncollaboration.\n","authors":["Ravi Tejwani","Chengyuan Ma","Paco Gomez-Paz","Paolo Bonato","H. Harry Asada"],"pdf_url":"https://arxiv.org/pdf/2305.05456v2.pdf","comment":"Under review in ICRA 2024"},{"id":"http://arxiv.org/abs/2310.06249v1","updated":"2023-10-10T01:53:43Z","published":"2023-10-10T01:53:43Z","title":"l-dyno: framework to learn consistent visual features using robot's\n  motion","summary":"  Historically, feature-based approaches have been used extensively for\ncamera-based robot perception tasks such as localization, mapping, tracking,\nand others. Several of these approaches also combine other sensors (inertial\nsensing, for example) to perform combined state estimation. Our work rethinks\nthis approach; we present a representation learning mechanism that identifies\nvisual features that best correspond to robot motion as estimated by an\nexternal signal. Specifically, we utilize the robot's transformations through\nan external signal (inertial sensing, for example) and give attention to image\nspace that is most consistent with the external signal. We use a pairwise\nconsistency metric as a representation to keep the visual features consistent\nthrough a sequence with the robot's relative pose transformations. This\napproach enables us to incorporate information from the robot's perspective\ninstead of solely relying on the image attributes. We evaluate our approach on\nreal-world datasets such as KITTI & EuRoC and compare the refined features with\nexisting feature descriptors. We also evaluate our method using our real robot\nexperiment. We notice an average of 49% reduction in the image search space\nwithout compromising the trajectory estimation accuracy. Our method reduces the\nexecution time of visual odometry by 4.3% and also reduces reprojection errors.\nWe demonstrate the need to select only the most important features and show the\ncompetitiveness using various feature detection baselines.\n","authors":["Kartikeya Singh","Charuvaran Adhivarahan","Karthik Dantu"],"pdf_url":"https://arxiv.org/pdf/2310.06249v1.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.05921v2","updated":"2023-10-10T01:52:27Z","published":"2023-10-09T17:59:30Z","title":"Conformal Decision Theory: Safe Autonomous Decisions from Imperfect\n  Predictions","summary":"  We introduce Conformal Decision Theory, a framework for producing safe\nautonomous decisions despite imperfect machine learning predictions. Examples\nof such decisions are ubiquitous, from robot planning algorithms that rely on\npedestrian predictions, to calibrating autonomous manufacturing to exhibit high\nthroughput and low error, to the choice of trusting a nominal policy versus\nswitching to a safe backup policy at run-time. The decisions produced by our\nalgorithms are safe in the sense that they come with provable statistical\nguarantees of having low risk without any assumptions on the world model\nwhatsoever; the observations need not be I.I.D. and can even be adversarial.\nThe theory extends results from conformal prediction to calibrate decisions\ndirectly, without requiring the construction of prediction sets. Experiments\ndemonstrate the utility of our approach in robot motion planning around humans,\nautomated stock trading, and robot manufacturing.\n","authors":["Jordan Lekeufack","Anastasios N. Angelopoulos","Andrea Bajcsy","Michael I. Jordan","Jitendra Malik"],"pdf_url":"https://arxiv.org/pdf/2310.05921v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.04930v2","updated":"2023-10-10T01:36:56Z","published":"2023-10-07T22:01:49Z","title":"Diff-Transfer: Model-based Robotic Manipulation Skill Transfer via\n  Differentiable Physics Simulation","summary":"  The capability to transfer mastered skills to accomplish a range of similar\nyet novel tasks is crucial for intelligent robots. In this work, we introduce\n$\\textit{Diff-Transfer}$, a novel framework leveraging differentiable physics\nsimulation to efficiently transfer robotic skills. Specifically,\n$\\textit{Diff-Transfer}$ discovers a feasible path within the task space that\nbrings the source task to the target task. At each pair of adjacent points\nalong this task path, which is two sub-tasks, $\\textit{Diff-Transfer}$ adapts\nknown actions from one sub-task to tackle the other sub-task successfully. The\nadaptation is guided by the gradient information from differentiable physics\nsimulations. We propose a novel path-planning method to generate sub-tasks,\nleveraging $Q$-learning with a task-level state and reward. We implement our\nframework in simulation experiments and execute four challenging transfer tasks\non robotic manipulation, demonstrating the efficacy of $\\textit{Diff-Transfer}$\nthrough comprehensive experiments. Supplementary and Videos are on the website\nhttps://sites.google.com/view/difftransfer\n","authors":["Yuqi Xiang","Feitong Chen","Qinsi Wang","Yang Gang","Xiang Zhang","Xinghao Zhu","Xingyu Liu","Lin Shao"],"pdf_url":"https://arxiv.org/pdf/2310.04930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12369v2","updated":"2023-10-10T00:40:14Z","published":"2023-06-21T16:25:38Z","title":"Towards Efficient MPPI Trajectory Generation with Unscented Guidance:\n  U-MPPI Control Strategy","summary":"  The classical Model Predictive Path Integral (MPPI) control framework lacks\nreliable safety guarantees since it relies on a risk-neutral trajectory\nevaluation technique, which can present challenges for safety-critical\napplications such as autonomous driving. Additionally, if the majority of MPPI\nsampled trajectories concentrate in high-cost regions, it may generate an\ninfeasible control sequence. To address this challenge, we propose the U-MPPI\ncontrol strategy, a novel methodology that can effectively manage system\nuncertainties while integrating a more efficient trajectory sampling strategy.\nThe core concept is to leverage the Unscented Transform (UT) to propagate not\nonly the mean but also the covariance of the system dynamics, going beyond the\ntraditional MPPI method. As a result, it introduces a novel and more efficient\ntrajectory sampling strategy, significantly enhancing state-space exploration\nand ultimately reducing the risk of being trapped in local minima. Furthermore,\nby leveraging the uncertainty information provided by UT, we incorporate a\nrisk-sensitive cost function that explicitly accounts for risk or uncertainty\nthroughout the trajectory evaluation process, resulting in a more resilient\ncontrol system capable of handling uncertain conditions. By conducting\nextensive simulations of 2D aggressive autonomous navigation in both known and\nunknown cluttered environments, we verify the efficiency and robustness of our\nproposed U-MPPI control strategy compared to the baseline MPPI. We further\nvalidate the practicality of U-MPPI through real-world demonstrations in\nunknown cluttered environments, showcasing its superior ability to incorporate\nboth the UT and local costmap into the optimization problem without introducing\nadditional complexity.\n","authors":["Ihab S. Mohamed","Junhong Xu","Gaurav S Sukhatme","Lantao Liu"],"pdf_url":"https://arxiv.org/pdf/2306.12369v2.pdf","comment":"This paper has 15 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2310.06226v1","updated":"2023-10-10T00:39:37Z","published":"2023-10-10T00:39:37Z","title":"Words into Action: Learning Diverse Humanoid Robot Behaviors using\n  Language Guided Iterative Motion Refinement","summary":"  Humanoid robots are well suited for human habitats due to their morphological\nsimilarity, but developing controllers for them is a challenging task that\ninvolves multiple sub-problems, such as control, planning and perception. In\nthis paper, we introduce a method to simplify controller design by enabling\nusers to train and fine-tune robot control policies using natural language\ncommands. We first learn a neural network policy that generates behaviors given\na natural language command, such as \"walk forward\", by combining Large Language\nModels (LLMs), motion retargeting, and motion imitation. Based on the\nsynthesized motion, we iteratively fine-tune by updating the text prompt and\nquerying LLMs to find the best checkpoint associated with the closest motion in\nhistory. We validate our approach using a simulated Digit humanoid robot and\ndemonstrate learning of diverse motions, such as walking, hopping, and kicking,\nwithout the burden of complex reward engineering. In addition, we show that our\niterative refinement enables us to learn 3x times faster than a naive\nformulation that learns from scratch.\n","authors":["K. Niranjan Kumar","Irfan Essa","Sehoon Ha"],"pdf_url":"https://arxiv.org/pdf/2310.06226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03740v2","updated":"2023-10-10T00:03:04Z","published":"2023-06-06T14:57:49Z","title":"GMMap: Memory-Efficient Continuous Occupancy Map Using Gaussian Mixture\n  Model","summary":"  Energy consumption of memory accesses dominates the compute energy in\nenergy-constrained robots which require a compact 3D map of the environment to\nachieve autonomy. Recent mapping frameworks only focused on reducing the map\nsize while incurring significant memory usage during map construction due to\nmulti-pass processing of each depth image. In this work, we present a\nmemory-efficient continuous occupancy map, named GMMap, that accurately models\nthe 3D environment using a Gaussian Mixture Model (GMM). Memory-efficient GMMap\nconstruction is enabled by the single-pass compression of depth images into\nlocal GMMs which are directly fused together into a globally-consistent map. By\nextending Gaussian Mixture Regression to model unexplored regions, occupancy\nprobability is directly computed from Gaussians. Using a low-power ARM Cortex\nA57 CPU, GMMap can be constructed in real-time at up to 60 images per second.\nCompared with prior works, GMMap maintains high accuracy while reducing the map\nsize by at least 56%, memory overhead by at least 88%, DRAM access by at least\n78%, and energy consumption by at least 69%. Thus, GMMap enables real-time 3D\nmapping on energy-constrained robots.\n","authors":["Peter Zhi Xuan Li","Sertac Karaman","Vivienne Sze"],"pdf_url":"https://arxiv.org/pdf/2306.03740v2.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2011.00685v2","updated":"2023-10-10T23:42:33Z","published":"2020-11-02T02:15:20Z","title":"Fast Biconnectivity Restoration in Multi-Robot Systems for Robust\n  Communication Maintenance","summary":"  Maintaining a robust communication network plays an important role in the\nsuccess of a multi-robot team jointly performing an optimization task. A key\ncharacteristic of a robust multi-robot system is the ability to repair the\ncommunication topology itself in the case of robot failure. In this paper, we\nfocus on the Fast Biconnectivity Restoration (FBR) problem, which aims to\nrepair a connected network to make it biconnected as fast as possible, where a\nbiconnected network is a communication topology that cannot be disconnected by\nremoving one node. We develop a Quadratically Constrained Program (QCP)\nformulation of the FBR problem, which provides a way to optimally solve the\nproblem. We also propose an approximation algorithm for the FBR problem based\non graph theory. By conducting empirical studies, we demonstrate that our\nproposed approximation algorithm performs close to the optimal while\nsignificantly outperforming the existing solutions.\n","authors":["Md Ishat-E-Rabban","Guangyao Shi","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2011.00685v2.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2310.07070v1","updated":"2023-10-10T23:34:59Z","published":"2023-10-10T23:34:59Z","title":"D2M2N: Decentralized Differentiable Memory-Enabled Mapping and\n  Navigation for Multiple Robots","summary":"  Recently, a number of learning-based models have been proposed for\nmulti-robot navigation. However, these models lack memory and only rely on the\ncurrent observations of the robot to plan their actions. They are unable to\nleverage past observations to plan better paths, especially in complex\nenvironments. In this work, we propose a fully differentiable and decentralized\nmemory-enabled architecture for multi-robot navigation and mapping called\nD2M2N. D2M2N maintains a compact representation of the environment to remember\npast observations and uses Value Iteration Network for complex navigation. We\nconduct extensive experiments to show that D2M2N significantly outperforms the\nstate-of-the-art model in complex mapping and navigation task.\n","authors":["Md Ishat-E-Rabban","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2310.07070v1.pdf","comment":"7 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2310.07021v1","updated":"2023-10-10T21:16:29Z","published":"2023-10-10T21:16:29Z","title":"Pre-Trained Masked Image Model for Mobile Robot Navigation","summary":"  2D top-down maps are commonly used for the navigation and exploration of\nmobile robots through unknown areas. Typically, the robot builds the navigation\nmaps incrementally from local observations using onboard sensors. Recent works\nhave shown that predicting the structural patterns in the environment through\nlearning-based approaches can greatly enhance task efficiency. While many such\nworks build task-specific networks using limited datasets, we show that the\nexisting foundational vision networks can accomplish the same without any\nfine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street\nimages, to present novel applications for field-of-view expansion, single-agent\ntopological exploration, and multi-agent exploration for indoor mapping, across\ndifferent input modalities. Our work motivates the use of foundational vision\nmodels for generalized structure prediction-driven applications, especially in\nthe dearth of training data. For more qualitative results see\nhttps://raaslab.org/projects/MIM4Robots.\n","authors":["Vishnu Dutt Sharma","Anukriti Singh","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2310.07021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07018v1","updated":"2023-10-10T21:08:51Z","published":"2023-10-10T21:08:51Z","title":"NEWTON: Are Large Language Models Capable of Physical Reasoning?","summary":"  Large Language Models (LLMs), through their contextualized representations,\nhave been empirically proven to encapsulate syntactic, semantic, word sense,\nand common-sense knowledge. However, there has been limited exploration of\ntheir physical reasoning abilities, specifically concerning the crucial\nattributes for comprehending everyday objects. To address this gap, we\nintroduce NEWTON, a repository and benchmark for evaluating the physics\nreasoning skills of LLMs. Further, to enable domain-specific adaptation of this\nbenchmark, we present a pipeline to enable researchers to generate a variant of\nthis benchmark that has been customized to the objects and attributes relevant\nfor their application. The NEWTON repository comprises a collection of 2800\nobject-attribute pairs, providing the foundation for generating infinite-scale\nassessment templates. The NEWTON benchmark consists of 160K QA questions,\ncurated using the NEWTON repository to investigate the physical reasoning\ncapabilities of several mainstream language models across foundational,\nexplicit, and implicit reasoning tasks. Through extensive empirical analysis,\nour results highlight the capabilities of LLMs for physical reasoning. We find\nthat LLMs like GPT-4 demonstrate strong reasoning capabilities in\nscenario-based tasks but exhibit less consistency in object-attribute reasoning\ncompared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates\nits potential for evaluating and enhancing language models, paving the way for\ntheir integration into physically grounded settings, such as robotic\nmanipulation. Project site: https://newtonreasoning.github.io\n","authors":["Yi Ru Wang","Jiafei Duan","Dieter Fox","Siddhartha Srinivasa"],"pdf_url":"https://arxiv.org/pdf/2310.07018v1.pdf","comment":"EMNLP 2023 Findings; 8 pages, 3 figures, 7 tables; Project page:\n  https://newtonreasoning.github.io"},{"id":"http://arxiv.org/abs/2310.06984v1","updated":"2023-10-10T20:11:13Z","published":"2023-10-10T20:11:13Z","title":"Leveraging Neural Radiance Fields for Uncertainty-Aware Visual\n  Localization","summary":"  As a promising fashion for visual localization, scene coordinate regression\n(SCR) has seen tremendous progress in the past decade. Most recent methods\nusually adopt neural networks to learn the mapping from image pixels to 3D\nscene coordinates, which requires a vast amount of annotated training data. We\npropose to leverage Neural Radiance Fields (NeRF) to generate training samples\nfor SCR. Despite NeRF's efficiency in rendering, many of the rendered data are\npolluted by artifacts or only contain minimal information gain, which can\nhinder the regression accuracy or bring unnecessary computational costs with\nredundant data. These challenges are addressed in three folds in this paper:\n(1) A NeRF is designed to separately predict uncertainties for the rendered\ncolor and depth images, which reveal data reliability at the pixel level. (2)\nSCR is formulated as deep evidential learning with epistemic uncertainty, which\nis used to evaluate information gain and scene coordinate quality. (3) Based on\nthe three arts of uncertainties, a novel view selection policy is formed that\nsignificantly improves data efficiency. Experiments on public datasets\ndemonstrate that our method could select the samples that bring the most\ninformation gain and promote the performance with the highest efficiency.\n","authors":["Le Chen","Weirong Chen","Rui Wang","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2310.06984v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2307.05491v2","updated":"2023-10-10T20:05:09Z","published":"2023-06-16T18:11:00Z","title":"Parametric roll oscillations of a hydrodynamic Chaplygin sleigh","summary":"  Biomimetic underwater robots use lateral periodic oscillatory motion to\npropel forward, which is seen in most fishes known as body caudal fin (BCF)\npropulsion. The lateral oscillatory motion makes slender-bodied fish-like\nrobots roll unstable. Unlike the case of human-engineered aquatic robots, many\nspecies of fish can stabilize their roll motion to perturbations arising from\nthe periodic motions of propulsors. To first understand the origin of the roll\ninstability, the objective of this paper is to analyze the parameters affecting\nthe roll-angle stability of an autonomous fish-like underwater swimmer.\nEschewing complex models of fluid-structure interaction, we instead consider\nthe roll motion of a nonholonomic system inspired by the Chaplygin sleigh,\nwhose center of mass is above the ground. In past work, the dynamics of a\nfish-like periodic swimmer have been shown to be similar to that of a Chaplygin\nsleigh. The Chaplygin sleigh is propelled by periodic torque in the yaw\ndirection. The roll dynamics of the Chaplygin sleigh are linearized and around\na nominal limit cycle solution of the planar hydrodynamic Chaplygin sleigh in\nthe reduced velocity space. It is shown that the roll dynamics are then\ndescribed as a nonhomogeneous Mathieu equation where the periodic yaw motion\nprovides the parametric excitation. We study the added mass effects on the\nsleigh's linear dynamics and use the Floquet theory to investigate the roll\nstability due to parametric excitation. We show that fast motions of the model\nfor swimming are frequently associated with roll instability. The paper thus\nsheds light on the fundamental mechanics that present trade-offs between speed,\nefficiency, and stability of motion of fish-like robots.\n","authors":["Kartik Loya","Phanindra Tallapragada"],"pdf_url":"https://arxiv.org/pdf/2307.05491v2.pdf","comment":"25 pages, 9 figures, submitted to Nonlinear Dynamics journal by\n  Springer"},{"id":"http://arxiv.org/abs/2310.03344v2","updated":"2023-10-10T19:57:18Z","published":"2023-10-05T06:50:11Z","title":"Generalized Benders Decomposition with Continual Learning for Hybrid\n  Model Predictive Control in Dynamic Environment","summary":"  Hybrid model predictive control (MPC) with both continuous and discrete\nvariables is widely applicable to robotic control tasks, especially those\ninvolving contact with the environment. Due to the combinatorial complexity,\nthe solving speed of hybrid MPC can be insufficient for real-time applications.\nIn this paper, we proposed a hybrid MPC solver based on Generalized Benders\nDecomposition (GBD) with continual learning. The algorithm accumulates cutting\nplanes from the invariant dual space of the subproblems. After a short\ncold-start phase, the accumulated cuts provide warm-starts for the new problem\ninstances to increase the solving speed. Despite the randomly changing\nenvironment that the control is unprepared for, the solving speed maintains. We\nverified our solver on controlling a cart-pole system with randomly moving soft\ncontact walls and show that the solving speed is 2-3 times faster than the\noff-the-shelf solver Gurobi.\n","authors":["Xuan Lin"],"pdf_url":"https://arxiv.org/pdf/2310.03344v2.pdf","comment":"A correction of the author name in the metadata"},{"id":"http://arxiv.org/abs/2310.06974v1","updated":"2023-10-10T19:53:21Z","published":"2023-10-10T19:53:21Z","title":"Efficient Path Planning in Large Unknown Environments with Switchable\n  System Models for Automated Vehicles","summary":"  Large environments are challenging for path planning algorithms as the size\nof the configuration space increases. Furthermore, if the environment is mainly\nunexplored, large amounts of the path are planned through unknown areas. Hence,\na complete replanning of the entire path occurs whenever the path collides with\nnewly discovered obstacles. We propose a novel method that stops the path\nplanning algorithm after a certain distance. It is used to navigate the\nalgorithm in large environments and is not prone to problems of existing\nnavigation approaches. Furthermore, we developed a method to detect significant\nenvironment changes to allow a more efficient replanning. At last, we extend\nthe path planner to be used in the U-Shift concept vehicle. It can switch to\nanother system model and rotate around the center of its rear axis. The results\nshow that the proposed methods generate nearly identical paths compared to the\nstandard Hybrid A* while drastically reducing the execution time. Furthermore,\nwe show that the extended path planning algorithm enables the efficient use of\nthe maneuvering capabilities of the concept vehicle to plan concise paths in\nnarrow environments.\n","authors":["Oliver Schumann","Michael Buchholz","Klaus Dietmayer"],"pdf_url":"https://arxiv.org/pdf/2310.06974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06964v1","updated":"2023-10-10T19:28:10Z","published":"2023-10-10T19:28:10Z","title":"Multi-Robot Cooperative Navigation in Crowds: A Game-Theoretic\n  Learning-Based Model Predictive Control Approach","summary":"  In this paper, we develop a control framework for the coordination of\nmultiple robots as they navigate through crowded environments. Our framework\ncomprises of a local model predictive control (MPC) for each robot and a social\nlong short-term memory model that forecasts pedestrians' trajectories. We\nformulate the local MPC formulation for each individual robot that includes\nboth individual and shared objectives, in which the latter encourages the\nemergence of coordination among robots. Next, we consider the multi-robot\nnavigation and human-robot interaction, respectively, as a potential game and a\ntwo-player game, then employ an iterative best response approach to solve the\nresulting optimization problems in a centralized and distributed fashion.\nFinally, we demonstrate the effectiveness of coordination among robots in\nsimulated crowd navigation.\n","authors":["Viet-Anh Le","Vaishnav Tadiparthi","Behdad Chalaki","Hossein Nourkhiz Mahjoub","Jovin D'sa","Ehsan Moradi-Pari","Andreas A. Malikopoulos"],"pdf_url":"https://arxiv.org/pdf/2310.06964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04874v2","updated":"2023-10-10T18:53:38Z","published":"2023-10-07T17:08:22Z","title":"AirIMU: Learning Uncertainty Propagation for Inertial Odometry","summary":"  Accurate uncertainty estimation for inertial odometry is the foundation to\nachieve optimal fusion in multi-sensor systems, such as visual or LiDAR\ninertial odometry. Prior studies often simplify the assumptions regarding the\nuncertainty of inertial measurements, presuming fixed covariance parameters and\nempirical IMU sensor models. However, the inherent physical limitations and\nnon-linear characteristics of sensors are difficult to capture. Moreover,\nuncertainty may fluctuate based on sensor rates and motion modalities, leading\nto variations across different IMUs. To address these challenges, we formulate\na learning-based method that not only encapsulate the non-linearities inherent\nto IMUs but also ensure the accurate propagation of covariance in a data-driven\nmanner. We extend the PyPose library to enable differentiable batched IMU\nintegration with covariance propagation on manifolds, leading to significant\nruntime speedup. To demonstrate our method's adaptability, we evaluate it on\nseveral benchmarks as well as a large-scale helicopter dataset spanning over\n262 kilometers. The drift rate of the inertial odometry on these datasets is\nreduced by a factor of between 2.2 and 4 times. Our method lays the groundwork\nfor advanced developments in inertial odometry.\n","authors":["Yuheng Qiu","Chen Wang","Xunfei Zhou","Youjie Xia","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2310.04874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.07758v2","updated":"2023-10-10T18:53:29Z","published":"2022-09-16T07:35:20Z","title":"Game-theoretic Objective Space Planning","summary":"  Generating competitive strategies and performing continuous motion planning\nsimultaneously in an adversarial setting is a challenging problem. In addition,\nunderstanding the intent of other agents is crucial to deploying autonomous\nsystems in adversarial multi-agent environments. Existing approaches either\ndiscretize agent action by grouping similar control inputs, sacrificing\nperformance in motion planning, or plan in uninterpretable latent spaces,\nproducing hard-to-understand agent behaviors. Furthermore, the most popular\npolicy optimization frameworks do not recognize the long-term effect of actions\nand become myopic. This paper proposes an agent action discretization method\nvia abstraction that provides clear intentions of agent actions, an efficient\noffline pipeline of agent population synthesis, and a planning strategy using\ncounterfactual regret minimization with function approximation. Finally, we\nexperimentally validate our findings on scaled autonomous vehicles in a\nhead-to-head racing setting. We demonstrate that using the proposed framework\nsignificantly improves learning, improves the win rate against different\nopponents, and the improvements can be transferred to unseen opponents in an\nunseen environment.\n","authors":["Hongrui Zheng","Zhijun Zhuang","Johannes Betz","Rahul Mangharam"],"pdf_url":"https://arxiv.org/pdf/2209.07758v2.pdf","comment":"Submitted to 2024 International Conference on Autonomous Agents and\n  Multi-Agent Systems (AAMAS 2024)"},{"id":"http://arxiv.org/abs/2310.06933v1","updated":"2023-10-10T18:42:55Z","published":"2023-10-10T18:42:55Z","title":"Eclares: Energy-Aware Clarity-Driven Ergodic Search","summary":"  Planning informative trajectories while considering the spatial distribution\nof the information over the environment, as well as constraints such as the\nrobot's limited battery capacity, makes the long-time horizon persistent\ncoverage problem complex. Ergodic search methods consider the spatial\ndistribution of environmental information while optimizing robot trajectories;\nhowever, current methods lack the ability to construct the target information\nspatial distribution for environments that vary stochastically across space and\ntime. Moreover, current coverage methods dealing with battery capacity\nconstraints either assume simple robot and battery models, or are\ncomputationally expensive. To address these problems, we propose a framework\ncalled Eclares, in which our contribution is two-fold. 1) First, we propose a\nmethod to construct the target information spatial distribution for ergodic\ntrajectory optimization using clarity, an information measure bounded between\n[0,1]. The clarity dynamics allows us to capture information decay due to lack\nof measurements and to quantify the maximum attainable information in\nstochastic spatiotemporal environments. 2) Second, instead of directly tracking\nthe ergodic trajectory, we introduce the energy-aware (eware) filter, which\niteratively validates the ergodic trajectory to ensure that the robot has\nenough energy to return to the charging station when needed. The proposed eware\nfilter is applicable to nonlinear robot models and is computationally\nlightweight. We demonstrate the working of the framework through a simulation\ncase study.\n","authors":["Kaleb Ben Naveed","Devansh Agrawal","Christopher Vermillion","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2310.06933v1.pdf","comment":"Submitted to International Conference of Robotics and Automation\n  (ICRA) 2024"},{"id":"http://arxiv.org/abs/2310.06931v1","updated":"2023-10-10T18:38:52Z","published":"2023-10-10T18:38:52Z","title":"SAILing CAVs: Speed-Adaptive Infrastructure-Linked Connected and\n  Automated Vehicles","summary":"  This work demonstrates a new capability in roadway control: Speed-adaptive,\ninfrastructure-linked connected and automated vehicles. We develop and deploy a\nlightly modified vehicle that is able to dynamically adjust the vehicle speed\nin response to posted variable speed limit messages generated by the\ninfrastructure using LTE connectivity. This work describes the open source\nhardware and software platform that enables integration between\ninfrastructure-based variable posted speed limits, and existing vehicle\nplatforms for automated control. The vehicle is deployed in heavy morning\ntraffic on I-24 in Nashville, TN. The control vehicle follows the posted\nvariable speed limits, resulting in as much as a 25% reduction in speed\nvariability compared to a human-piloted vehicle in the same traffic stream.\n","authors":["Matthew Nice","Matthew Bunting","George Gunter","William Barbour","Jonathan Sprinkle","Dan Work"],"pdf_url":"https://arxiv.org/pdf/2310.06931v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2310.05765v2","updated":"2023-10-10T18:12:46Z","published":"2023-10-09T14:46:21Z","title":"Examining the simulation-to-reality gap of a wheel loader digging in\n  deformable terrain","summary":"  We investigate how well a physics-based simulator can replicate a real wheel\nloader performing bucket filling in a pile of soil. The comparison is made\nusing field test time series of the vehicle motion and actuation forces, loaded\nmass, and total work. The vehicle was modeled as a rigid multibody system with\nfrictional contacts, driveline, and linear actuators. For the soil, we tested\ndiscrete element models of different resolutions, with and without multiscale\nacceleration. The spatio-temporal resolution ranged between 50-400 mm and 2-500\nms, and the computational speed was between 1/10,000 to 5 times faster than\nreal-time. The simulation-to-reality gap was found to be around 10% and\nexhibited a weak dependence on the level of fidelity, e.g., compatible with\nreal-time simulation. Furthermore, the sensitivity of an optimized force\nfeedback controller under transfer between different simulation domains was\ninvestigated. The domain bias was observed to cause a performance reduction of\n5% despite the domain gap being about 15%.\n","authors":["Koji Aoshima","Martin Servin"],"pdf_url":"https://arxiv.org/pdf/2310.05765v2.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2309.10620v2","updated":"2023-10-10T18:05:16Z","published":"2023-09-19T13:53:07Z","title":"Perceptual Factors for Environmental Modeling in Robotic Active\n  Perception","summary":"  Accurately assessing the potential value of new sensor observations is a\ncritical aspect of planning for active perception. This task is particularly\nchallenging when reasoning about high-level scene understanding using\nmeasurements from vision-based neural networks. Due to appearance-based\nreasoning, the measurements are susceptible to several environmental effects\nsuch as the presence of occluders, variations in lighting conditions, and\nredundancy of information due to similarity in appearance between nearby\nviewpoints. To address this, we propose a new active perception framework\nincorporating an arbitrary number of perceptual effects in planning and fusion.\nOur method models the correlation with the environment by a set of general\nfunctions termed perceptual factors to construct a perceptual map, which\nquantifies the aggregated influence of the environment on candidate viewpoints.\nThis information is seamlessly incorporated into the planning and fusion\nprocesses by adjusting the uncertainty associated with measurements to weigh\ntheir contributions. We evaluate our perceptual maps in a simulated environment\nthat reproduces environmental conditions common in robotics applications. Our\nresults show that, by accounting for environmental effects within our\nperceptual maps, we improve in the state estimation by correctly selecting the\nviewpoints and considering the measurement noise correctly when affected by\nenvironmental factors. We furthermore deploy our approach on a ground robot to\nshowcase its applicability for real-world active perception missions.\n","authors":["David Morilla-Cabello","Jonas Westheider","Marija Popovic","Eduardo Montijano"],"pdf_url":"https://arxiv.org/pdf/2309.10620v2.pdf","comment":"7 pages, 9 figures, under review for IEEE ICRA 2023"},{"id":"http://arxiv.org/abs/2310.06903v1","updated":"2023-10-10T18:01:16Z","published":"2023-10-10T18:01:16Z","title":"Reinforcement Learning in a Safety-Embedded MDP with Trajectory\n  Optimization","summary":"  Safe Reinforcement Learning (RL) plays an important role in applying RL\nalgorithms to safety-critical real-world applications, addressing the trade-off\nbetween maximizing rewards and adhering to safety constraints. This work\nintroduces a novel approach that combines RL with trajectory optimization to\nmanage this trade-off effectively. Our approach embeds safety constraints\nwithin the action space of a modified Markov Decision Process (MDP). The RL\nagent produces a sequence of actions that are transformed into safe\ntrajectories by a trajectory optimizer, thereby effectively ensuring safety and\nincreasing training stability. This novel approach excels in its performance on\nchallenging Safety Gym tasks, achieving significantly higher rewards and\nnear-zero safety violations during inference. The method's real-world\napplicability is demonstrated through a safe and effective deployment in a real\nrobot task of box-pushing around obstacles.\n","authors":["Fan Yang","Wenxuan Zhou","Zuxin Liu","Ding Zhao","David Held"],"pdf_url":"https://arxiv.org/pdf/2310.06903v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.06838v1","updated":"2023-10-10T17:59:53Z","published":"2023-10-10T17:59:53Z","title":"AutoAD II: The Sequel -- Who, When, and What in Movie Audio Description","summary":"  Audio Description (AD) is the task of generating descriptions of visual\ncontent, at suitable time intervals, for the benefit of visually impaired\naudiences. For movies, this presents notable challenges -- AD must occur only\nduring existing pauses in dialogue, should refer to characters by name, and\nought to aid understanding of the storyline as a whole. To this end, we develop\na new model for automatically generating movie AD, given CLIP visual features\nof the frames, the cast list, and the temporal locations of the speech;\naddressing all three of the 'who', 'when', and 'what' questions: (i) who -- we\nintroduce a character bank consisting of the character's name, the actor that\nplayed the part, and a CLIP feature of their face, for the principal cast of\neach movie, and demonstrate how this can be used to improve naming in the\ngenerated AD; (ii) when -- we investigate several models for determining\nwhether an AD should be generated for a time interval or not, based on the\nvisual content of the interval and its neighbours; and (iii) what -- we\nimplement a new vision-language model for this task, that can ingest the\nproposals from the character bank, whilst conditioning on the visual features\nusing cross-attention, and demonstrate how this improves over previous\narchitectures for AD text generation in an apples-to-apples comparison.\n","authors":["Tengda Han","Max Bain","Arsha Nagrani","Gül Varol","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2310.06838v1.pdf","comment":"ICCV2023. Project page:\n  https://www.robots.ox.ac.uk/vgg/research/autoad/"},{"id":"http://arxiv.org/abs/2310.06836v1","updated":"2023-10-10T17:59:28Z","published":"2023-10-10T17:59:28Z","title":"What Does Stable Diffusion Know about the 3D Scene?","summary":"  Recent advances in generative models like Stable Diffusion enable the\ngeneration of highly photo-realistic images. Our objective in this paper is to\nprobe the diffusion network to determine to what extent it 'understands'\ndifferent properties of the 3D scene depicted in an image. To this end, we make\nthe following contributions: (i) We introduce a protocol to evaluate whether a\nnetwork models a number of physical 'properties' of the 3D scene by probing for\nexplicit features that represent these properties. The probes are applied on\ndatasets of real images with annotations for the property. (ii) We apply this\nprotocol to properties covering scene geometry, scene material, support\nrelations, lighting, and view dependent measures. (iii) We find that Stable\nDiffusion is good at a number of properties including scene geometry, support\nrelations, shadows and depth, but less performant for occlusion. (iv) We also\napply the probes to other models trained at large-scale, including DINO and\nCLIP, and find their performance inferior to that of Stable Diffusion.\n","authors":["Guanqi Zhan","Chuanxia Zheng","Weidi Xie","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2310.06836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06823v1","updated":"2023-10-10T17:53:36Z","published":"2023-10-10T17:53:36Z","title":"NECO: NEural Collapse Based Out-of-distribution detection","summary":"  Detecting out-of-distribution (OOD) data is a critical challenge in machine\nlearning due to model overconfidence, often without awareness of their\nepistemological limits. We hypothesize that ``neural collapse'', a phenomenon\naffecting in-distribution data for models trained beyond loss convergence, also\ninfluences OOD data. To benefit from this interplay, we introduce NECO, a novel\npost-hoc method for OOD detection, which leverages the geometric properties of\n``neural collapse'' and of principal component spaces to identify OOD data. Our\nextensive experiments demonstrate that NECO achieves state-of-the-art results\non both small and large-scale OOD detection tasks while exhibiting strong\ngeneralization capabilities across different network architectures.\nFurthermore, we provide a theoretical explanation for the effectiveness of our\nmethod in OOD detection. We plan to release the code after the anonymity\nperiod.\n","authors":["Mouïn Ben Ammar","Nacim Belkhir","Sebastian Popescu","Antoine Manzanera","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2310.06823v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2310.06822v1","updated":"2023-10-10T17:50:09Z","published":"2023-10-10T17:50:09Z","title":"Neural Bounding","summary":"  Bounding volumes are an established concept in computer graphics and vision\ntasks but have seen little change since their early inception. In this work, we\nstudy the use of neural networks as bounding volumes. Our key observation is\nthat bounding, which so far has primarily been considered a problem of\ncomputational geometry, can be redefined as a problem of learning to classify\nspace into free and empty. This learning-based approach is particularly\nadvantageous in high-dimensional spaces, such as animated scenes with complex\nqueries, where neural networks are known to excel. However, unlocking neural\nbounding requires a twist: allowing -- but also limiting -- false positives,\nwhile ensuring that the number of false negatives is strictly zero. We enable\nsuch tight and conservative results using a dynamically-weighted asymmetric\nloss function. Our results show that our neural bounding produces up to an\norder of magnitude fewer false positives than traditional methods.\n","authors":["Wenxin Liu","Michael Fischer","Paul D. Yoo","Tobias Ritschel"],"pdf_url":"https://arxiv.org/pdf/2310.06822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13655v2","updated":"2023-10-10T17:46:49Z","published":"2023-05-23T03:59:06Z","title":"LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image\n  Diffusion Models with Large Language Models","summary":"  Recent advancements in text-to-image diffusion models have yielded impressive\nresults in generating realistic and diverse images. However, these models still\nstruggle with complex prompts, such as those that involve numeracy and spatial\nreasoning. This work proposes to enhance prompt understanding capabilities in\ndiffusion models. Our method leverages a pretrained large language model (LLM)\nfor grounded generation in a novel two-stage process. In the first stage, the\nLLM generates a scene layout that comprises captioned bounding boxes from a\ngiven prompt describing the desired image. In the second stage, a novel\ncontroller guides an off-the-shelf diffusion model for layout-grounded image\ngeneration. Both stages utilize existing pretrained models without additional\nmodel parameter optimization. Our method significantly outperforms the base\ndiffusion model and several strong baselines in accurately generating images\naccording to prompts that require various capabilities, doubling the generation\naccuracy across four tasks on average. Furthermore, our method enables\ninstruction-based multi-round scene specification and can handle prompts in\nlanguages not supported by the underlying diffusion model. We anticipate that\nour method will unleash users' creativity by accurately following more complex\nprompts.\n","authors":["Long Lian","Boyi Li","Adam Yala","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2305.13655v2.pdf","comment":"Project page: https://llm-grounded-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2202.06095v2","updated":"2023-10-10T17:27:05Z","published":"2022-02-12T16:22:46Z","title":"A Review of Deep Learning-based Approaches for Deepfake Content\n  Detection","summary":"  Recent advancements in deep learning generative models have raised concerns\nas they can create highly convincing counterfeit images and videos. This poses\na threat to people's integrity and can lead to social instability. To address\nthis issue, there is a pressing need to develop new computational models that\ncan efficiently detect forged content and alert users to potential image and\nvideo manipulations. This paper presents a comprehensive review of recent\nstudies for deepfake content detection using deep learning-based approaches. We\naim to broaden the state-of-the-art research by systematically reviewing the\ndifferent categories of fake content detection. Furthermore, we report the\nadvantages and drawbacks of the examined works and future directions towards\nthe issues and shortcomings still unsolved on deepfake detection.\n","authors":["Leandro A. Passos","Danilo Jodas","Kelton A. P. da Costa","Luis A. Souza Júnior","Douglas Rodrigues","Javier Del Ser","David Camacho","João Paulo Papa"],"pdf_url":"https://arxiv.org/pdf/2202.06095v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05803v2","updated":"2023-10-10T17:13:03Z","published":"2023-05-09T23:24:09Z","title":"Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly\n  Supervised Semantic Segmentation","summary":"  Weakly supervised semantic segmentation (WSSS) aims to bypass the need for\nlaborious pixel-level annotation by using only image-level annotation. Most\nexisting methods rely on Class Activation Maps (CAM) to derive pixel-level\npseudo-labels and use them to train a fully supervised semantic segmentation\nmodel. Although these pseudo-labels are class-aware, indicating the coarse\nregions for particular classes, they are not object-aware and fail to delineate\naccurate object boundaries. To address this, we introduce a simple yet\neffective method harnessing the Segment Anything Model (SAM), a class-agnostic\nfoundation model capable of producing fine-grained instance masks of objects,\nparts, and subparts. We use CAM pseudo-labels as cues to select and combine SAM\nmasks, resulting in high-quality pseudo-labels that are both class-aware and\nobject-aware. Our approach is highly versatile and can be easily integrated\ninto existing WSSS methods without any modification. Despite its simplicity,\nour approach shows consistent gain over the state-of-the-art WSSS methods on\nboth PASCAL VOC and MS-COCO datasets.\n","authors":["Tianle Chen","Zheda Mai","Ruiwen Li","Wei-lun Chao"],"pdf_url":"https://arxiv.org/pdf/2305.05803v2.pdf","comment":"Tianle Chen and Zheda Mai contributed equally to this work. Our code\n  is available at \\url{https://github.com/cskyl/SAM_WSSS}"},{"id":"http://arxiv.org/abs/2310.06773v1","updated":"2023-10-10T16:49:21Z","published":"2023-10-10T16:49:21Z","title":"Uni3D: Exploring Unified 3D Representation at Scale","summary":"  Scaling up representations for images or text has been extensively\ninvestigated in the past few years and has led to revolutions in learning\nvision and language. However, scalable representation for 3D objects and scenes\nis relatively unexplored. In this work, we present Uni3D, a 3D foundation model\nto explore the unified 3D representation at scale. Uni3D uses a 2D initialized\nViT end-to-end pretrained to align the 3D point cloud features with the\nimage-text aligned features. Via the simple architecture and pretext task,\nUni3D can leverage abundant 2D pretrained models as initialization and\nimage-text aligned models as the target, unlocking the great potential of 2D\nmodels and scaling-up strategies to the 3D world. We efficiently scale up Uni3D\nto one billion parameters, and set new records on a broad range of 3D tasks,\nsuch as zero-shot classification, few-shot classification, open-world\nunderstanding and part segmentation. We show that the strong Uni3D\nrepresentation also enables applications such as 3D painting and retrieval in\nthe wild. We believe that Uni3D provides a new direction for exploring both\nscaling up and efficiency of the representation in 3D domain.\n","authors":["Junsheng Zhou","Jinsheng Wang","Baorui Ma","Yu-Shen Liu","Tiejun Huang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06773v1.pdf","comment":"Code and Demo: https://github.com/baaivision/Uni3D"},{"id":"http://arxiv.org/abs/2309.10791v2","updated":"2023-10-10T16:44:34Z","published":"2023-09-19T17:40:23Z","title":"Multi-spectral Entropy Constrained Neural Compression of Solar Imagery","summary":"  Missions studying the dynamic behaviour of the Sun are defined to capture\nmulti-spectral images of the sun and transmit them to the ground station in a\ndaily basis. To make transmission efficient and feasible, image compression\nsystems need to be exploited. Recently successful end-to-end optimized neural\nnetwork-based image compression systems have shown great potential to be used\nin an ad-hoc manner. In this work we have proposed a transformer-based\nmulti-spectral neural image compressor to efficiently capture redundancies both\nintra/inter-wavelength. To unleash the locality of window-based self attention\nmechanism, we propose an inter-window aggregated token multi head self\nattention. Additionally to make the neural compressor autoencoder shift\ninvariant, a randomly shifted window attention mechanism is used which makes\nthe transformer blocks insensitive to translations in their input domain. We\ndemonstrate that the proposed approach not only outperforms the conventional\ncompression algorithms but also it is able to better decorrelates images along\nthe multiple wavelengths compared to single spectral compression.\n","authors":["Ali Zafari","Atefeh Khoshkhahtinat","Piyush M. Mehta","Nasser M. Nasrabadi","Barbara J. Thompson","Michael S. F. Kirk","Daniel da Silva"],"pdf_url":"https://arxiv.org/pdf/2309.10791v2.pdf","comment":"Accepted to IEEE 22$^{nd}$ International Conference on Machine\n  Learning and Applications 2023 (ICMLA)"},{"id":"http://arxiv.org/abs/2304.05292v3","updated":"2023-10-10T16:40:41Z","published":"2023-04-11T15:42:20Z","title":"MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive\n  Impairment in older adults using facial videos","summary":"  Deep machine learning models including Convolutional Neural Networks (CNN)\nhave been successful in the detection of Mild Cognitive Impairment (MCI) using\nmedical images, questionnaires, and videos. This paper proposes a novel\nMulti-branch Classifier-Video Vision Transformer (MC-ViViT) model to\ndistinguish MCI from those with normal cognition by analyzing facial features.\nThe data comes from the I-CONECT, a behavioral intervention trial aimed at\nimproving cognitive function by providing frequent video chats. MC-ViViT\nextracts spatiotemporal features of videos in one branch and augments\nrepresentations by the MC module. The I-CONECT dataset is challenging as the\ndataset is imbalanced containing Hard-Easy and Positive-Negative samples, which\nimpedes the performance of MC-ViViT. We propose a loss function for Hard-Easy\nand Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE\nloss to address the imbalanced problem. Our experimental results on the\nI-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a\nhigh accuracy of 90.63% accuracy on some of the interview videos.\n","authors":["Jian Sun","Hiroko H. Dodge","Mohammad H. Mahoor"],"pdf_url":"https://arxiv.org/pdf/2304.05292v3.pdf","comment":"13 pages, 7 tables, 7 figures, 9 equations"},{"id":"http://arxiv.org/abs/2310.06753v1","updated":"2023-10-10T16:24:51Z","published":"2023-10-10T16:24:51Z","title":"TopoMLP: An Simple yet Strong Pipeline for Driving Topology Reasoning","summary":"  Topology reasoning aims to comprehensively understand road scenes and present\ndrivable routes in autonomous driving. It requires detecting road centerlines\n(lane) and traffic elements, further reasoning their topology relationship,\ni.e., lane-lane topology, and lane-traffic topology. In this work, we first\npresent that the topology score relies heavily on detection performance on lane\nand traffic elements. Therefore, we introduce a powerful 3D lane detector and\nan improved 2D traffic element detector to extend the upper limit of topology\nperformance. Further, we propose TopoMLP, a simple yet high-performance\npipeline for driving topology reasoning. Based on the impressive detection\nperformance, we develop two simple MLP-based heads for topology generation.\nTopoMLP achieves state-of-the-art performance on OpenLane-V2 benchmark, i.e.,\n41.2% OLS with ResNet-50 backbone. It is also the 1st solution for 1st OpenLane\nTopology in Autonomous Driving Challenge. We hope such simple and strong\npipeline can provide some new insights to the community. Code is at\nhttps://github.com/wudongming97/TopoMLP.\n","authors":["Dongming Wu","Jiahao Chang","Fan Jia","Yingfei Liu","Tiancai Wang","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2310.06753v1.pdf","comment":"The 1st solution for 1st OpenLane Topology in Autonomous Driving\n  Challenge. Code is at https://github.com/wudongming97/TopoMLP"},{"id":"http://arxiv.org/abs/2305.10983v3","updated":"2023-10-10T16:23:34Z","published":"2023-05-18T13:55:28Z","title":"Assessor360: Multi-sequence Network for Blind Omnidirectional Image\n  Quality Assessment","summary":"  Blind Omnidirectional Image Quality Assessment (BOIQA) aims to objectively\nassess the human perceptual quality of omnidirectional images (ODIs) without\nrelying on pristine-quality image information. It is becoming more significant\nwith the increasing advancement of virtual reality (VR) technology. However,\nthe quality assessment of ODIs is severely hampered by the fact that the\nexisting BOIQA pipeline lacks the modeling of the observer's browsing process.\nTo tackle this issue, we propose a novel multi-sequence network for BOIQA\ncalled Assessor360, which is derived from the realistic multi-assessor ODI\nquality assessment procedure. Specifically, we propose a generalized Recursive\nProbability Sampling (RPS) method for the BOIQA task, combining content and\ndetails information to generate multiple pseudo-viewport sequences from a given\nstarting point. Additionally, we design a Multi-scale Feature Aggregation (MFA)\nmodule with a Distortion-aware Block (DAB) to fuse distorted and semantic\nfeatures of each viewport. We also devise Temporal Modeling Module (TMM) to\nlearn the viewport transition in the temporal domain. Extensive experimental\nresults demonstrate that Assessor360 outperforms state-of-the-art methods on\nmultiple OIQA datasets. The code and models are available at\nhttps://github.com/TianheWu/Assessor360.\n","authors":["Tianhe Wu","Shuwei Shi","Haoming Cai","Mingdeng Cao","Jing Xiao","Yinqiang Zheng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2305.10983v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06744v1","updated":"2023-10-10T16:14:20Z","published":"2023-10-10T16:14:20Z","title":"HiFi-123: Towards High-fidelity One Image to 3D Content Generation","summary":"  Recent advances in text-to-image diffusion models have enabled 3D generation\nfrom a single image. However, current image-to-3D methods often produce\nsuboptimal results for novel views, with blurred textures and deviations from\nthe reference image, limiting their practical applications. In this paper, we\nintroduce HiFi-123, a method designed for high-fidelity and multi-view\nconsistent 3D generation. Our contributions are twofold: First, we propose a\nreference-guided novel view enhancement technique that substantially reduces\nthe quality gap between synthesized and reference views. Second, capitalizing\non the novel view enhancement, we present a novel reference-guided state\ndistillation loss. When incorporated into the optimization-based image-to-3D\npipeline, our method significantly improves 3D generation quality, achieving\nstate-of-the-art performance. Comprehensive evaluations demonstrate the\neffectiveness of our approach over existing methods, both qualitatively and\nquantitatively.\n","authors":["Wangbo Yu","Li Yuan","Yan-Pei Cao","Xiangjun Gao","Xiaoyu Li","Long Quan","Ying Shan","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2310.06744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06737v1","updated":"2023-10-10T16:07:23Z","published":"2023-10-10T16:07:23Z","title":"Multi-domain improves out-of-distribution and data-limited scenarios for\n  medical image analysis","summary":"  Current machine learning methods for medical image analysis primarily focus\non developing models tailored for their specific tasks, utilizing data within\ntheir target domain. These specialized models tend to be data-hungry and often\nexhibit limitations in generalizing to out-of-distribution samples. Recently,\nfoundation models have been proposed, which combine data from various domains\nand demonstrate excellent generalization capabilities. Building upon this, this\nwork introduces the incorporation of diverse medical image domains, including\ndifferent imaging modalities like X-ray, MRI, CT, and ultrasound images, as\nwell as various viewpoints such as axial, coronal, and sagittal views. We refer\nto this approach as multi-domain model and compare its performance to that of\nspecialized models. Our findings underscore the superior generalization\ncapabilities of multi-domain models, particularly in scenarios characterized by\nlimited data availability and out-of-distribution, frequently encountered in\nhealthcare applications. The integration of diverse data allows multi-domain\nmodels to utilize shared information across domains, enhancing the overall\noutcomes significantly. To illustrate, for organ recognition, multi-domain\nmodel can enhance accuracy by up to 10% compared to conventional specialized\nmodels.\n","authors":["Ece Ozkan","Xavier Boix"],"pdf_url":"https://arxiv.org/pdf/2310.06737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.08559v2","updated":"2023-10-10T15:49:45Z","published":"2022-11-15T23:04:15Z","title":"Robust Alzheimer's Progression Modeling using Cross-Domain\n  Self-Supervised Deep Learning","summary":"  Developing successful artificial intelligence systems in practice depends on\nboth robust deep learning models and large, high-quality data. However,\nacquiring and labeling data can be prohibitively expensive and time-consuming\nin many real-world applications, such as clinical disease models.\nSelf-supervised learning has demonstrated great potential in increasing model\naccuracy and robustness in small data regimes. In addition, many clinical\nimaging and disease modeling applications rely heavily on regression of\ncontinuous quantities. However, the applicability of self-supervised learning\nfor these medical-imaging regression tasks has not been extensively studied. In\nthis study, we develop a cross-domain self-supervised learning approach for\ndisease prognostic modeling as a regression problem using medical images as\ninput. We demonstrate that self-supervised pretraining can improve the\nprediction of Alzheimer's Disease progression from brain MRI. We also show that\npretraining on extended (but not labeled) brain MRI data outperforms\npretraining on natural images. We further observe that the highest performance\nis achieved when both natural images and extended brain-MRI data are used for\npretraining.\n","authors":["Saba Dadsetan","Mohsen Hejrati","Shandong Wu","Somaye Hashemifar"],"pdf_url":"https://arxiv.org/pdf/2211.08559v2.pdf","comment":"This work has been published at the Transactions on Machine Learning\n  Research (TMLR) journal"},{"id":"http://arxiv.org/abs/2306.00042v2","updated":"2023-10-10T14:48:59Z","published":"2023-05-31T13:21:54Z","title":"Graph-based methods coupled with specific distributional distances for\n  adversarial attack detection","summary":"  Artificial neural networks are prone to being fooled by carefully perturbed\ninputs which cause an egregious misclassification. These \\textit{adversarial}\nattacks have been the focus of extensive research. Likewise, there has been an\nabundance of research in ways to detect and defend against them. We introduce a\nnovel approach of detection and interpretation of adversarial attacks from a\ngraph perspective. For an input image, we compute an associated sparse graph\nusing the layer-wise relevance propagation algorithm \\cite{bach15}.\nSpecifically, we only keep edges of the neural network with the highest\nrelevance values. Three quantities are then computed from the graph which are\nthen compared against those computed from the training set. The result of the\ncomparison is a classification of the image as benign or adversarial. To make\nthe comparison, two classification methods are introduced: 1) an explicit\nformula based on Wasserstein distance applied to the degree of node and 2) a\nlogistic regression. Both classification methods produce strong results which\nlead us to believe that a graph-based interpretation of adversarial attacks is\nvaluable.\n","authors":["Dwight Nwaigwe","Lucrezia Carboni","Martial Mermillod","Sophie Achard","Michel Dojat"],"pdf_url":"https://arxiv.org/pdf/2306.00042v2.pdf","comment":"published in Neural Networks"},{"id":"http://arxiv.org/abs/2310.06670v1","updated":"2023-10-10T14:46:22Z","published":"2023-10-10T14:46:22Z","title":"Domain Generalization by Rejecting Extreme Augmentations","summary":"  Data augmentation is one of the most effective techniques for regularizing\ndeep learning models and improving their recognition performance in a variety\nof tasks and domains. However, this holds for standard in-domain settings, in\nwhich the training and test data follow the same distribution. For the\nout-of-domain case, where the test data follow a different and unknown\ndistribution, the best recipe for data augmentation is unclear. In this paper,\nwe show that for out-of-domain and domain generalization settings, data\naugmentation can provide a conspicuous and robust improvement in performance.\nTo do that, we propose a simple training procedure: (i) use uniform sampling on\nstandard data augmentation transformations; (ii) increase the strength\ntransformations to account for the higher data variance expected when working\nout-of-domain, and (iii) devise a new reward function to reject extreme\ntransformations that can harm the training. With this procedure, our data\naugmentation scheme achieves a level of accuracy that is comparable to or\nbetter than state-of-the-art methods on benchmark domain generalization\ndatasets. Code: \\url{https://github.com/Masseeh/DCAug}\n","authors":["Masih Aminbeidokhti","Fidel A. Guerrero Peña","Heitor Rapela Medeiros","Thomas Dubail","Eric Granger","Marco Pedersoli"],"pdf_url":"https://arxiv.org/pdf/2310.06670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06668v1","updated":"2023-10-10T14:42:34Z","published":"2023-10-10T14:42:34Z","title":"Latent Diffusion Counterfactual Explanations","summary":"  Counterfactual explanations have emerged as a promising method for\nelucidating the behavior of opaque black-box models. Recently, several works\nleveraged pixel-space diffusion models for counterfactual generation. To handle\nnoisy, adversarial gradients during counterfactual generation -- causing\nunrealistic artifacts or mere adversarial perturbations -- they required either\nauxiliary adversarially robust models or computationally intensive guidance\nschemes. However, such requirements limit their applicability, e.g., in\nscenarios with restricted access to the model's training data. To address these\nlimitations, we introduce Latent Diffusion Counterfactual Explanations (LDCE).\nLDCE harnesses the capabilities of recent class- or text-conditional foundation\nlatent diffusion models to expedite counterfactual generation and focus on the\nimportant, semantic parts of the data. Furthermore, we propose a novel\nconsensus guidance mechanism to filter out noisy, adversarial gradients that\nare misaligned with the diffusion model's implicit classifier. We demonstrate\nthe versatility of LDCE across a wide spectrum of models trained on diverse\ndatasets with different learning paradigms. Finally, we showcase how LDCE can\nprovide insights into model errors, enhancing our understanding of black-box\nmodel behavior.\n","authors":["Karim Farid","Simon Schrodi","Max Argus","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2310.06668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06667v1","updated":"2023-10-10T14:42:32Z","published":"2023-10-10T14:42:32Z","title":"SC2GAN: Rethinking Entanglement by Self-correcting Correlated GAN Space","summary":"  Generative Adversarial Networks (GANs) can synthesize realistic images, with\nthe learned latent space shown to encode rich semantic information with various\ninterpretable directions. However, due to the unstructured nature of the\nlearned latent space, it inherits the bias from the training data where\nspecific groups of visual attributes that are not causally related tend to\nappear together, a phenomenon also known as spurious correlations, e.g., age\nand eyeglasses or women and lipsticks. Consequently, the learned distribution\noften lacks the proper modelling of the missing examples. The interpolation\nfollowing editing directions for one attribute could result in entangled\nchanges with other attributes. To address this problem, previous works\ntypically adjust the learned directions to minimize the changes in other\nattributes, yet they still fail on strongly correlated features. In this work,\nwe study the entanglement issue in both the training data and the learned\nlatent space for the StyleGAN2-FFHQ model. We propose a novel framework\nSC$^2$GAN that achieves disentanglement by re-projecting low-density latent\ncode samples in the original latent space and correcting the editing directions\nbased on both the high-density and low-density regions. By leveraging the\noriginal meaningful directions and semantic region-specific layers, our\nframework interpolates the original latent codes to generate images with\nattribute combination that appears infrequently, then inverts these samples\nback to the original latent space. We apply our framework to pre-existing\nmethods that learn meaningful latent directions and showcase its strong\ncapability to disentangle the attributes with small amounts of low-density\nregion samples added.\n","authors":["Zikun Chen","Han Zhao","Parham Aarabi","Ruowei Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.06667v1.pdf","comment":"Accepted to the Out Of Distribution Generalization in Computer Vision\n  workshop at ICCV2023"},{"id":"http://arxiv.org/abs/2310.06654v1","updated":"2023-10-10T14:22:56Z","published":"2023-10-10T14:22:56Z","title":"Evaluating Explanation Methods for Vision-and-Language Navigation","summary":"  The ability to navigate robots with natural language instructions in an\nunknown environment is a crucial step for achieving embodied artificial\nintelligence (AI). With the improving performance of deep neural models\nproposed in the field of vision-and-language navigation (VLN), it is equally\ninteresting to know what information the models utilize for their\ndecision-making in the navigation tasks. To understand the inner workings of\ndeep neural models, various explanation methods have been developed for\npromoting explainable AI (XAI). But they are mostly applied to deep neural\nmodels for image or text classification tasks and little work has been done in\nexplaining deep neural models for VLN tasks. In this paper, we address these\nproblems by building quantitative benchmarks to evaluate explanation methods\nfor VLN models in terms of faithfulness. We propose a new erasure-based\nevaluation pipeline to measure the step-wise textual explanation in the\nsequential decision-making setting. We evaluate several explanation methods for\ntwo representative VLN models on two popular VLN datasets and reveal valuable\nfindings through our experiments.\n","authors":["Guanqi Chen","Lei Yang","Guanhua Chen","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2310.06654v1.pdf","comment":"Accepted by ECAI 2023"},{"id":"http://arxiv.org/abs/2202.10115v4","updated":"2023-10-10T14:12:43Z","published":"2022-02-21T10:57:16Z","title":"An Efficient Smoothing and Thresholding Image Segmentation Framework\n  with Weighted Anisotropic-Isotropic Total Variation","summary":"  In this paper, we design an efficient, multi-stage image segmentation\nframework that incorporates a weighted difference of anisotropic and isotropic\ntotal variation (AITV). The segmentation framework generally consists of two\nstages: smoothing and thresholding, thus referred to as SaT. In the first\nstage, a smoothed image is obtained by an AITV-regularized Mumford-Shah (MS)\nmodel, which can be solved efficiently by the alternating direction method of\nmultipliers (ADMM) with a closed-form solution of a proximal operator of the\n$\\ell_1 -\\alpha \\ell_2$ regularizer. Convergence of the ADMM algorithm is\nanalyzed. In the second stage, we threshold the smoothed image by $K$-means\nclustering to obtain the final segmentation result. Numerical experiments\ndemonstrate that the proposed segmentation framework is versatile for both\ngrayscale and color images, efficient in producing high-quality segmentation\nresults within a few seconds, and robust to input images that are corrupted\nwith noise, blur, or both. We compare the AITV method with its original convex\nTV and nonconvex TV$^p (0<p<1)$ counterparts, showcasing the qualitative and\nquantitative advantages of our proposed method.\n","authors":["Kevin Bui","Yifei Lou","Fredrick Park","Jack Xin"],"pdf_url":"https://arxiv.org/pdf/2202.10115v4.pdf","comment":"accepted to Springer CAMC"},{"id":"http://arxiv.org/abs/2309.08066v2","updated":"2023-10-10T14:10:05Z","published":"2023-09-14T23:28:58Z","title":"Morphologically-Aware Consensus Computation via Heuristics-based\n  IterATive Optimization (MACCHIatO)","summary":"  The extraction of consensus segmentations from several binary or\nprobabilistic masks is important to solve various tasks such as the analysis of\ninter-rater variability or the fusion of several neural network outputs. One of\nthe most widely used methods to obtain such a consensus segmentation is the\nSTAPLE algorithm. In this paper, we first demonstrate that the output of that\nalgorithm is heavily impacted by the background size of images and the choice\nof the prior. We then propose a new method to construct a binary or a\nprobabilistic consensus segmentation based on the Fr\\'{e}chet means of\ncarefully chosen distances which makes it totally independent of the image\nbackground size. We provide a heuristic approach to optimize this criterion\nsuch that a voxel's class is fully determined by its voxel-wise distance to the\ndifferent masks, the connected component it belongs to and the group of raters\nwho segmented it. We compared extensively our method on several datasets with\nthe STAPLE method and the naive segmentation averaging method, showing that it\nleads to binary consensus masks of intermediate size between Majority Voting\nand STAPLE and to different posterior probabilities than Mask Averaging and\nSTAPLE methods. Our code is available at\nhttps://gitlab.inria.fr/dhamzaou/jaccardmap .\n","authors":["Dimitri Hamzaoui","Sarah Montagne","Raphaële Renard-Penna","Nicholas Ayache","Hervé Delingette"],"pdf_url":"https://arxiv.org/pdf/2309.08066v2.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2023:013"},{"id":"http://arxiv.org/abs/2310.06641v1","updated":"2023-10-10T14:04:32Z","published":"2023-10-10T14:04:32Z","title":"How (not) to ensemble LVLMs for VQA","summary":"  This paper studies ensembling in the era of Large Vision-Language Models\n(LVLMs). Ensembling is a classical method to combine different models to get\nincreased performance. In the recent work on Encyclopedic-VQA the authors\nexamine a wide variety of models to solve their task: from vanilla LVLMs, to\nmodels including the caption as extra context, to models augmented with\nLens-based retrieval of Wikipedia pages. Intuitively these models are highly\ncomplementary, which should make them ideal for ensembling. Indeed, an oracle\nexperiment shows potential gains from 48.8% accuracy (the best single model)\nall the way up to 67% (best possible ensemble). So it is a trivial exercise to\ncreate an ensemble with substantial real gains. Or is it?\n","authors":["Lisa Alazraki","Lluis Castrejon","Mostafa Dehghani","Fantine Huot","Jasper Uijlings","Thomas Mensink"],"pdf_url":"https://arxiv.org/pdf/2310.06641v1.pdf","comment":"Under submission"},{"id":"http://arxiv.org/abs/2310.06633v1","updated":"2023-10-10T13:51:24Z","published":"2023-10-10T13:51:24Z","title":"Blind Dates: Examining the Expression of Temporality in Historical\n  Photographs","summary":"  This paper explores the capacity of computer vision models to discern\ntemporal information in visual content, focusing specifically on historical\nphotographs. We investigate the dating of images using OpenCLIP, an open-source\nimplementation of CLIP, a multi-modal language and vision model. Our experiment\nconsists of three steps: zero-shot classification, fine-tuning, and analysis of\nvisual content. We use the \\textit{De Boer Scene Detection} dataset, containing\n39,866 gray-scale historical press photographs from 1950 to 1999. The results\nshow that zero-shot classification is relatively ineffective for image dating,\nwith a bias towards predicting dates in the past. Fine-tuning OpenCLIP with a\nlogistic classifier improves performance and eliminates the bias. Additionally,\nour analysis reveals that images featuring buses, cars, cats, dogs, and people\nare more accurately dated, suggesting the presence of temporal markers. The\nstudy highlights the potential of machine learning models like OpenCLIP in\ndating images and emphasizes the importance of fine-tuning for accurate\ntemporal analysis. Future research should explore the application of these\nfindings to color photographs and diverse datasets.\n","authors":["Alexandra Barancová","Melvin Wevers","Nanne van Noord"],"pdf_url":"https://arxiv.org/pdf/2310.06633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06629v1","updated":"2023-10-10T13:48:18Z","published":"2023-10-10T13:48:18Z","title":"EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention","summary":"  Because of the advancement of deep learning technology, vision transformer\nhas demonstrated competitive performance in various computer vision tasks.\nUnfortunately, vision transformer still faces some challenges such as high\ncomputational complexity and absence of desirable inductive bias. To alleviate\nthese problems, this study proposes a novel Bi-Fovea Self-Attention (BFSA)\ninspired by the physiological structure and characteristics of bi-fovea vision\nin eagle eyes. This BFSA can simulate the shallow fovea and deep fovea\nfunctions of eagle vision, enabling the network to extract feature\nrepresentations of targets from coarse to fine, facilitating the interaction of\nmulti-scale feature representations. Additionally, this study designs a Bionic\nEagle Vision (BEV) block based on BFSA and CNN. It combines CNN and Vision\nTransformer, to enhance the network's local and global representation ability\nfor targets. Furthermore, this study develops a unified and efficient general\npyramid backbone network family, named Eagle Vision Transformers (EViTs) by\nstacking the BEV blocks. Experimental results on various computer vision tasks\nincluding image classification, object detection, instance segmentation and\nother transfer learning tasks show that the proposed EViTs perform\nsignificantly better than the baselines under similar model sizes, which\nexhibits faster speed on graphics processing unit compared to other models.\nCode will be released at https://github.com/nkusyl.\n","authors":["Yulong Shi","Mingwei Sun","Yongshuai Wang","Rui Wang","Hui Sun","Zengqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2310.06629v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.06628v1","updated":"2023-10-10T13:46:11Z","published":"2023-10-10T13:46:11Z","title":"Deep Cardiac MRI Reconstruction with ADMM","summary":"  Cardiac magnetic resonance imaging is a valuable non-invasive tool for\nidentifying cardiovascular diseases. For instance, Cine MRI is the benchmark\nmodality for assessing the cardiac function and anatomy. On the other hand,\nmulti-contrast (T1 and T2) mapping has the potential to assess pathologies and\nabnormalities in the myocardium and interstitium. However, voluntary\nbreath-holding and often arrhythmia, in combination with MRI's slow imaging\nspeed, can lead to motion artifacts, hindering real-time acquisition image\nquality. Although performing accelerated acquisitions can facilitate dynamic\nimaging, it induces aliasing, causing low reconstructed image quality in Cine\nMRI and inaccurate T1 and T2 mapping estimation. In this work, inspired by\nrelated work in accelerated MRI reconstruction, we present a deep learning\n(DL)-based method for accelerated cine and multi-contrast reconstruction in the\ncontext of dynamic cardiac imaging. We formulate the reconstruction problem as\na least squares regularized optimization task, and employ vSHARP, a\nstate-of-the-art DL-based inverse problem solver, which incorporates\nhalf-quadratic variable splitting and the alternating direction method of\nmultipliers with neural networks. We treat the problem in two setups; a 2D\nreconstruction and a 2D dynamic reconstruction task, and employ 2D and 3D deep\nlearning networks, respectively. Our method optimizes in both the image and\nk-space domains, allowing for high reconstruction fidelity. Although the target\ndata is undersampled with a Cartesian equispaced scheme, we train our model\nusing both Cartesian and simulated non-Cartesian undersampling schemes to\nenhance generalization of the model to unseen data. Furthermore, our model\nadopts a deep neural network to learn and refine the sensitivity maps of\nmulti-coil k-space data. Lastly, our method is jointly trained on both,\nundersampled cine and multi-contrast data.\n","authors":["George Yiasemis","Nikita Moriakov","Jan-Jakob Sonke","Jonas Teuwen"],"pdf_url":"https://arxiv.org/pdf/2310.06628v1.pdf","comment":"12 pages, 3 figures, 2 tables. CMRxRecon Challenge, MICCAI 2023"},{"id":"http://arxiv.org/abs/2310.06627v1","updated":"2023-10-10T13:45:59Z","published":"2023-10-10T13:45:59Z","title":"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of\n  Multi-modal Language Models","summary":"  Counterfactual reasoning ability is one of the core abilities of human\nintelligence. This reasoning process involves the processing of alternatives to\nobserved states or past events, and this process can improve our ability for\nplanning and decision-making. In this work, we focus on benchmarking the\ncounterfactual reasoning ability of multi-modal large language models. We take\nthe question and answer pairs from the VQAv2 dataset and add one counterfactual\npresupposition to the questions, with the answer being modified accordingly.\nAfter generating counterfactual questions and answers using ChatGPT, we\nmanually examine all generated questions and answers to ensure correctness.\nOver 2k counterfactual question and answer pairs are collected this way. We\nevaluate recent vision language models on our newly collected test dataset and\nfound that all models exhibit a large performance drop compared to the results\ntested on questions without the counterfactual presupposition. This result\nindicates that there still exists space for developing vision language models.\nApart from the vision language models, our proposed dataset can also serves as\na benchmark for evaluating the ability of code generation LLMs, results\ndemonstrate a large gap between GPT-4 and current open-source models. Our code\nand dataset are available at \\url{https://github.com/Letian2003/C-VQA}.\n","authors":["Letian Zhang","Xiaotong Zhai","Zhongkai Zhao","Xin Wen","Yongshuo Zong","Bingchen Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.06627v1.pdf","comment":"Short paper accepted at ICCV 2023 VLAR workshop"},{"id":"http://arxiv.org/abs/2303.09756v2","updated":"2023-10-10T13:31:22Z","published":"2023-03-17T03:44:15Z","title":"Video Action Recognition with Attentive Semantic Units","summary":"  Visual-Language Models (VLMs) have significantly advanced action video\nrecognition. Supervised by the semantics of action labels, recent works adapt\nthe visual branch of VLMs to learn video representations. Despite the\neffectiveness proved by these works, we believe that the potential of VLMs has\nyet to be fully harnessed. In light of this, we exploit the semantic units (SU)\nhiding behind the action labels and leverage their correlations with\nfine-grained items in frames for more accurate action recognition. SUs are\nentities extracted from the language descriptions of the entire action set,\nincluding body parts, objects, scenes, and motions. To further enhance the\nalignments between visual contents and the SUs, we introduce a multi-region\nmodule (MRA) to the visual branch of the VLM. The MRA allows the perception of\nregion-aware visual features beyond the original global feature. Our method\nadaptively attends to and selects relevant SUs with visual features of frames.\nWith a cross-modal decoder, the selected SUs serve to decode spatiotemporal\nvideo representations. In summary, the SUs as the medium can boost\ndiscriminative ability and transferability. Specifically, in fully-supervised\nlearning, our method achieved 87.8% top-1 accuracy on Kinetics-400. In K=2\nfew-shot experiments, our method surpassed the previous state-of-the-art by\n+7.1% and +15.0% on HMDB-51 and UCF-101, respectively.\n","authors":["Yifei Chen","Dapeng Chen","Ruijin Liu","Hao Li","Wei Peng"],"pdf_url":"https://arxiv.org/pdf/2303.09756v2.pdf","comment":"Accepted at ICCV 2023"},{"id":"http://arxiv.org/abs/2310.06603v1","updated":"2023-10-10T13:12:03Z","published":"2023-10-10T13:12:03Z","title":"V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric\n  Heterogenous Distillation Network","summary":"  Object detection is the central issue of intelligent traffic systems, and\nrecent advancements in single-vehicle lidar-based 3D detection indicate that it\ncan provide accurate position information for intelligent agents to make\ndecisions and plan. Compared with single-vehicle perception, multi-view\nvehicle-road cooperation perception has fundamental advantages, such as the\nelimination of blind spots and a broader range of perception, and has become a\nresearch hotspot. However, the current perception of cooperation focuses on\nimproving the complexity of fusion while ignoring the fundamental problems\ncaused by the absence of single-view outlines. We propose a multi-view\nvehicle-road cooperation perception system, vehicle-to-everything cooperative\nperception (V2X-AHD), in order to enhance the identification capability,\nparticularly for predicting the vehicle's shape. At first, we propose an\nasymmetric heterogeneous distillation network fed with different training data\nto improve the accuracy of contour recognition, with multi-view teacher\nfeatures transferring to single-view student features. While the point cloud\ndata are sparse, we propose Spara Pillar, a spare convolutional-based plug-in\nfeature extraction backbone, to reduce the number of parameters and improve and\nenhance feature extraction capabilities. Moreover, we leverage the multi-head\nself-attention (MSA) to fuse the single-view feature, and the lightweight\ndesign makes the fusion feature a smooth expression. The results of applying\nour algorithm to the massive open dataset V2Xset demonstrate that our method\nachieves the state-of-the-art result. The V2X-AHD can effectively improve the\naccuracy of 3D object detection and reduce the number of network parameters,\naccording to this study, which serves as a benchmark for cooperative\nperception. The code for this article is available at\nhttps://github.com/feeling0414-lab/V2X-AHD.\n","authors":["Caizhen He","Hai Wang","Long Chen","Tong Luo","Yingfeng Cai"],"pdf_url":"https://arxiv.org/pdf/2310.06603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06600v1","updated":"2023-10-10T13:08:50Z","published":"2023-10-10T13:08:50Z","title":"Pi-DUAL: Using Privileged Information to Distinguish Clean from Noisy\n  Labels","summary":"  Label noise is a pervasive problem in deep learning that often compromises\nthe generalization performance of trained models. Recently, leveraging\nprivileged information (PI) -- information available only during training but\nnot at test time -- has emerged as an effective approach to mitigate this\nissue. Yet, existing PI-based methods have failed to consistently outperform\ntheir no-PI counterparts in terms of preventing overfitting to label noise. To\naddress this deficiency, we introduce Pi-DUAL, an architecture designed to\nharness PI to distinguish clean from wrong labels. Pi-DUAL decomposes the\noutput logits into a prediction term, based on conventional input features, and\na noise-fitting term influenced solely by PI. A gating mechanism steered by PI\nadaptively shifts focus between these terms, allowing the model to implicitly\nseparate the learning paths of clean and wrong labels. Empirically, Pi-DUAL\nachieves significant performance improvements on key PI benchmarks (e.g., +6.8%\non ImageNet-PI), establishing a new state-of-the-art test set accuracy.\nAdditionally, Pi-DUAL is a potent method for identifying noisy samples\npost-training, outperforming other strong methods at this task. Overall,\nPi-DUAL is a simple, scalable and practical approach for mitigating the effects\nof label noise in a variety of real-world scenarios with PI.\n","authors":["Ke Wang","Guillermo Ortiz-Jimenez","Rodolphe Jenatton","Mark Collier","Efi Kokiopoulou","Pascal Frossard"],"pdf_url":"https://arxiv.org/pdf/2310.06600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06594v1","updated":"2023-10-10T13:01:38Z","published":"2023-10-10T13:01:38Z","title":"REVO-LION: Evaluating and Refining Vision-Language Instruction Tuning\n  Datasets","summary":"  There is an emerging line of research on multimodal instruction tuning, and a\nline of benchmarks have been proposed for evaluating these models recently.\nInstead of evaluating the models directly, in this paper we try to evaluate the\nVision-Language Instruction-Tuning (VLIT) datasets themselves and further seek\nthe way of building a dataset for developing an all-powerful VLIT model, which\nwe believe could also be of utility for establishing a grounded protocol for\nbenchmarking VLIT models. For effective analysis of VLIT datasets that remains\nan open question, we propose a tune-cross-evaluation paradigm: tuning on one\ndataset and evaluating on the others in turn. For each single tune-evaluation\nexperiment set, we define the Meta Quality (MQ) as the mean score measured by a\nseries of caption metrics including BLEU, METEOR, and ROUGE-L to quantify the\nquality of a certain dataset or a sample. On this basis, to evaluate the\ncomprehensiveness of a dataset, we develop the Dataset Quality (DQ) covering\nall tune-evaluation sets. To lay the foundation for building a comprehensive\ndataset and developing an all-powerful model for practical applications, we\nfurther define the Sample Quality (SQ) to quantify the all-sided quality of\neach sample. Extensive experiments validate the rationality of the proposed\nevaluation paradigm. Based on the holistic evaluation, we build a new dataset,\nREVO-LION (REfining VisiOn-Language InstructiOn tuNing), by collecting samples\nwith higher SQ from each dataset. With only half of the full data, the model\ntrained on REVO-LION can achieve performance comparable to simply adding all\nVLIT datasets up. In addition to developing an all-powerful model, REVO-LION\nalso includes an evaluation set, which is expected to serve as a convenient\nevaluation benchmark for future research.\n","authors":["Ning Liao","Shaofeng Zhang","Renqiu Xia","Bo Zhang","Min Cao","Yu Qiao","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2310.06594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06582v1","updated":"2023-10-10T12:47:31Z","published":"2023-10-10T12:47:31Z","title":"Hierarchical Mask2Former: Panoptic Segmentation of Crops, Weeds and\n  Leaves","summary":"  Advancements in machine vision that enable detailed inferences to be made\nfrom images have the potential to transform many sectors including agriculture.\nPrecision agriculture, where data analysis enables interventions to be\nprecisely targeted, has many possible applications. Precision spraying, for\nexample, can limit the application of herbicide only to weeds, or limit the\napplication of fertiliser only to undernourished crops, instead of spraying the\nentire field. The approach promises to maximise yields, whilst minimising\nresource use and harms to the surrounding environment. To this end, we propose\na hierarchical panoptic segmentation method to simultaneously identify\nindicators of plant growth and locate weeds within an image. We adapt\nMask2Former, a state-of-the-art architecture for panoptic segmentation, to\npredict crop, weed and leaf masks. We achieve a PQ{\\dag} of 75.99.\nAdditionally, we explore approaches to make the architecture more compact and\ntherefore more suitable for time and compute constrained applications. With our\nmore compact architecture, inference is up to 60% faster and the reduction in\nPQ{\\dag} is less than 1%.\n","authors":["Madeleine Darbyshire","Elizabeth Sklar","Simon Parsons"],"pdf_url":"https://arxiv.org/pdf/2310.06582v1.pdf","comment":"6 pages, 5 figures, 2 tables, for code, see\n  https://github.com/madeleinedarbyshire/HierarchicalMask2Former"},{"id":"http://arxiv.org/abs/2310.06578v1","updated":"2023-10-10T12:39:10Z","published":"2023-10-10T12:39:10Z","title":"Energy-Efficient Visual Search by Eye Movement and Low-Latency Spiking\n  Neural Network","summary":"  Human vision incorporates non-uniform resolution retina, efficient eye\nmovement strategy, and spiking neural network (SNN) to balance the requirements\nin visual field size, visual resolution, energy cost, and inference latency.\nThese properties have inspired interest in developing human-like computer\nvision. However, existing models haven't fully incorporated the three features\nof human vision, and their learned eye movement strategies haven't been\ncompared with human's strategy, making the models' behavior difficult to\ninterpret. Here, we carry out experiments to examine human visual search\nbehaviors and establish the first SNN-based visual search model. The model\ncombines an artificial retina with spiking feature extraction, memory, and\nsaccade decision modules, and it employs population coding for fast and\nefficient saccade decisions. The model can learn either a human-like or a\nnear-optimal fixation strategy, outperform humans in search speed and accuracy,\nand achieve high energy efficiency through short saccade decision latency and\nsparse activation. It also suggests that the human search strategy is\nsuboptimal in terms of search speed. Our work connects modeling of vision in\nneuroscience and machine learning and sheds light on developing more\nenergy-efficient computer vision algorithms.\n","authors":["Yunhui Zhou","Dongqi Han","Yuguo Yu"],"pdf_url":"https://arxiv.org/pdf/2310.06578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06577v1","updated":"2023-10-10T12:38:34Z","published":"2023-10-10T12:38:34Z","title":"SketchBodyNet: A Sketch-Driven Multi-faceted Decoder Network for 3D\n  Human Reconstruction","summary":"  Reconstructing 3D human shapes from 2D images has received increasing\nattention recently due to its fundamental support for many high-level 3D\napplications. Compared with natural images, freehand sketches are much more\nflexible to depict various shapes, providing a high potential and valuable way\nfor 3D human reconstruction. However, such a task is highly challenging. The\nsparse abstract characteristics of sketches add severe difficulties, such as\narbitrariness, inaccuracy, and lacking image details, to the already badly\nill-posed problem of 2D-to-3D reconstruction. Although current methods have\nachieved great success in reconstructing 3D human bodies from a single-view\nimage, they do not work well on freehand sketches. In this paper, we propose a\nnovel sketch-driven multi-faceted decoder network termed SketchBodyNet to\naddress this task. Specifically, the network consists of a backbone and three\nseparate attention decoder branches, where a multi-head self-attention module\nis exploited in each decoder to obtain enhanced features, followed by a\nmulti-layer perceptron. The multi-faceted decoders aim to predict the camera,\nshape, and pose parameters, respectively, which are then associated with the\nSMPL model to reconstruct the corresponding 3D human mesh. In learning,\nexisting 3D meshes are projected via the camera parameters into 2D synthetic\nsketches with joints, which are combined with the freehand sketches to optimize\nthe model. To verify our method, we collect a large-scale dataset of about 26k\nfreehand sketches and their corresponding 3D meshes containing various poses of\nhuman bodies from 14 different angles. Extensive experimental results\ndemonstrate our SketchBodyNet achieves superior performance in reconstructing\n3D human meshes from freehand sketches.\n","authors":["Fei Wang","Kongzhang Tang","Hefeng Wu","Baoquan Zhao","Hao Cai","Teng Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.06577v1.pdf","comment":"9 pages, to appear in Pacific Graphics 2023"},{"id":"http://arxiv.org/abs/2310.05873v2","updated":"2023-10-10T12:32:32Z","published":"2023-10-09T17:13:10Z","title":"Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion\n  Models","summary":"  Fine-tuning diffusion models through personalized datasets is an acknowledged\nmethod for improving generation quality across downstream tasks, which,\nhowever, often inadvertently generates unintended concepts such as watermarks\nand QR codes, attributed to the limitations in image sources and collecting\nmethods within specific downstream tasks. Existing solutions suffer from\neliminating these unintentionally learned implicit concepts, primarily due to\nthe dependency on the model's ability to recognize concepts that it actually\ncannot discern. In this work, we introduce Geom-Erasing, a novel approach that\nsuccessfully removes the implicit concepts with either an additional accessible\nclassifier or detector model to encode geometric information of these concepts\ninto text domain. Moreover, we propose Implicit Concept, a novel image-text\ndataset imbued with three implicit concepts (i.e., watermarks, QR codes, and\ntext) for training and evaluation. Experimental results demonstrate that\nGeom-Erasing not only identifies but also proficiently eradicates implicit\nconcepts, revealing a significant improvement over the existing methods. The\nintegration of geometric information marks a substantial progression in the\nprecise removal of implicit concepts in diffusion models.\n","authors":["Zhili Liu","Kai Chen","Yifan Zhang","Jianhua Han","Lanqing Hong","Hang Xu","Zhenguo Li","Dit-Yan Yeung","James Kwok"],"pdf_url":"https://arxiv.org/pdf/2310.05873v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06566v1","updated":"2023-10-10T12:25:52Z","published":"2023-10-10T12:25:52Z","title":"Efficient Retrieval of Images with Irregular Patterns using\n  Morphological Image Analysis: Applications to Industrial and Healthcare\n  datasets","summary":"  Image retrieval is the process of searching and retrieving images from a\ndatabase based on their visual content and features. Recently, much attention\nhas been directed towards the retrieval of irregular patterns within industrial\nor medical images by extracting features from the images, such as deep\nfeatures, colour-based features, shape-based features and local features. This\nhas applications across a spectrum of industries, including fault inspection,\ndisease diagnosis, and maintenance prediction. This paper proposes an image\nretrieval framework to search for images containing similar irregular patterns\nby extracting a set of morphological features (DefChars) from images; the\ndatasets employed in this paper contain wind turbine blade images with defects,\nchest computerised tomography scans with COVID-19 infection, heatsink images\nwith defects, and lake ice images. The proposed framework was evaluated with\ndifferent feature extraction methods (DefChars, resized raw image, local binary\npattern, and scale-invariant feature transforms) and distance metrics to\ndetermine the most efficient parameters in terms of retrieval performance\nacross datasets. The retrieval results show that the proposed framework using\nthe DefChars and the Manhattan distance metric achieves a mean average\nprecision of 80% and a low standard deviation of 0.09 across classes of\nirregular patterns, outperforming alternative feature-metric combinations\nacross all datasets. Furthermore, the low standard deviation between each class\nhighlights DefChars' capability for a reliable image retrieval task, even in\nthe presence of class imbalances or small-sized datasets.\n","authors":["Jiajun Zhang","Georgina Cosma","Sarah Bugby","Jason Watkins"],"pdf_url":"https://arxiv.org/pdf/2310.06566v1.pdf","comment":"35 pages, 5 figures, 19 tables (17 tables in appendix), submitted to\n  Special Issue: Advances and Challenges in Multimodal Machine Learning 2nd\n  Edition, Journal of Imaging, MDPI"},{"id":"http://arxiv.org/abs/2310.06562v1","updated":"2023-10-10T12:19:39Z","published":"2023-10-10T12:19:39Z","title":"Compositional Representation Learning for Brain Tumour Segmentation","summary":"  For brain tumour segmentation, deep learning models can achieve human\nexpert-level performance given a large amount of data and pixel-level\nannotations. However, the expensive exercise of obtaining pixel-level\nannotations for large amounts of data is not always feasible, and performance\nis often heavily reduced in a low-annotated data regime. To tackle this\nchallenge, we adapt a mixed supervision framework, vMFNet, to learn robust\ncompositional representations using unsupervised learning and weak supervision\nalongside non-exhaustive pixel-level pathology labels. In particular, we use\nthe BraTS dataset to simulate a collection of 2-point expert pathology\nannotations indicating the top and bottom slice of the tumour (or tumour\nsub-regions: peritumoural edema, GD-enhancing tumour, and the necrotic /\nnon-enhancing tumour) in each MRI volume, from which weak image-level labels\nthat indicate the presence or absence of the tumour (or the tumour sub-regions)\nin the image are constructed. Then, vMFNet models the encoded image features\nwith von-Mises-Fisher (vMF) distributions, via learnable and compositional vMF\nkernels which capture information about structures in the images. We show that\ngood tumour segmentation performance can be achieved with a large amount of\nweakly labelled data but only a small amount of fully-annotated data.\nInterestingly, emergent learning of anatomical structures occurs in the\ncompositional representation even given only supervision relating to pathology\n(tumour).\n","authors":["Xiao Liu","Antanas Kascenas","Hannah Watson","Sotirios A. Tsaftaris","Alison Q. O'Neil"],"pdf_url":"https://arxiv.org/pdf/2310.06562v1.pdf","comment":"Accepted by DART workshop, MICCAI 2023"},{"id":"http://arxiv.org/abs/2310.06557v1","updated":"2023-10-10T12:13:38Z","published":"2023-10-10T12:13:38Z","title":"Data efficient deep learning for medical image analysis: A survey","summary":"  The rapid evolution of deep learning has significantly advanced the field of\nmedical image analysis. However, despite these achievements, the further\nenhancement of deep learning models for medical image analysis faces a\nsignificant challenge due to the scarcity of large, well-annotated datasets. To\naddress this issue, recent years have witnessed a growing emphasis on the\ndevelopment of data-efficient deep learning methods. This paper conducts a\nthorough review of data-efficient deep learning methods for medical image\nanalysis. To this end, we categorize these methods based on the level of\nsupervision they rely on, encompassing categories such as no supervision,\ninexact supervision, incomplete supervision, inaccurate supervision, and only\nlimited supervision. We further divide these categories into finer\nsubcategories. For example, we categorize inexact supervision into multiple\ninstance learning and learning with weak annotations. Similarly, we categorize\nincomplete supervision into semi-supervised learning, active learning, and\ndomain-adaptive learning and so on. Furthermore, we systematically summarize\ncommonly used datasets for data efficient deep learning in medical image\nanalysis and investigate future research directions to conclude this survey.\n","authors":["Suruchi Kumari","Pravendra Singh"],"pdf_url":"https://arxiv.org/pdf/2310.06557v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2204.14240v2","updated":"2023-10-10T12:04:53Z","published":"2022-04-29T17:10:01Z","title":"EndoMapper dataset of complete calibrated endoscopy procedures","summary":"  Computer-assisted systems are becoming broadly used in medicine. In\nendoscopy, most research focuses on the automatic detection of polyps or other\npathologies, but localization and navigation of the endoscope are completely\nperformed manually by physicians. To broaden this research and bring spatial\nArtificial Intelligence to endoscopies, data from complete procedures is\nneeded. This paper introduces the Endomapper dataset, the first collection of\ncomplete endoscopy sequences acquired during regular medical practice, making\nsecondary use of medical data. Its main purpose is to facilitate the\ndevelopment and evaluation of Visual Simultaneous Localization and Mapping\n(VSLAM) methods in real endoscopy data. The dataset contains more than 24 hours\nof video. It is the first endoscopic dataset that includes endoscope\ncalibration as well as the original calibration videos. Meta-data and\nannotations associated with the dataset vary from the anatomical landmarks,\nprocedure labeling, segmentations, reconstructions, simulated sequences with\nground truth and same patient procedures. The software used in this paper is\npublicly available.\n","authors":["Pablo Azagra","Carlos Sostres","Ángel Ferrandez","Luis Riazuelo","Clara Tomasini","Oscar León Barbed","Javier Morlana","David Recasens","Victor M. Batlle","Juan J. Gómez-Rodríguez","Richard Elvira","Julia López","Cristina Oriol","Javier Civera","Juan D. Tardós","Ana Cristina Murillo","Angel Lanas","José M. M. Montiel"],"pdf_url":"https://arxiv.org/pdf/2204.14240v2.pdf","comment":"17 pages, 14 figures, 8 tables"},{"id":"http://arxiv.org/abs/2308.15126v3","updated":"2023-10-10T11:57:26Z","published":"2023-08-29T08:51:24Z","title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) have recently achieved remarkable\nsuccess. However, LVLMs are still plagued by the hallucination problem, which\nlimits the practicality in many scenarios. Hallucination refers to the\ninformation of LVLMs' responses that does not exist in the visual input, which\nposes potential risks of substantial consequences. There has been limited work\nstudying hallucination evaluation in LVLMs. In this paper, we propose\nHallucination Evaluation based on Large Language Models (HaELM), an LLM-based\nhallucination evaluation framework. HaELM achieves an approximate 95%\nperformance comparable to ChatGPT and has additional advantages including low\ncost, reproducibility, privacy preservation and local deployment. Leveraging\nthe HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we\nanalyze the factors contributing to hallucination in LVLMs and offer helpful\nsuggestions to mitigate the hallucination problem. Our training data and human\nannotation hallucination data will be made public soon.\n","authors":["Junyang Wang","Yiyang Zhou","Guohai Xu","Pengcheng Shi","Chenlin Zhao","Haiyang Xu","Qinghao Ye","Ming Yan","Ji Zhang","Jihua Zhu","Jitao Sang","Haoyu Tang"],"pdf_url":"https://arxiv.org/pdf/2308.15126v3.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.06549v1","updated":"2023-10-10T11:51:12Z","published":"2023-10-10T11:51:12Z","title":"Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield\n  but Also a Catalyst for Model Inversion Attacks","summary":"  Label smoothing -- using softened labels instead of hard ones -- is a widely\nadopted regularization method for deep learning, showing diverse benefits such\nas enhanced generalization and calibration. Its implications for preserving\nmodel privacy, however, have remained unexplored. To fill this gap, we\ninvestigate the impact of label smoothing on model inversion attacks (MIAs),\nwhich aim to generate class-representative samples by exploiting the knowledge\nencoded in a classifier, thereby inferring sensitive information about its\ntraining data. Through extensive analyses, we uncover that traditional label\nsmoothing fosters MIAs, thereby increasing a model's privacy leakage. Even\nmore, we reveal that smoothing with negative factors counters this trend,\nimpeding the extraction of class-related information and leading to privacy\npreservation, beating state-of-the-art defenses. This establishes a practical\nand powerful novel way for enhancing model resilience against MIAs.\n","authors":["Lukas Struppek","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.06549v1.pdf","comment":"23 pages, 8 tables, 8 figures"},{"id":"http://arxiv.org/abs/2310.05056v2","updated":"2023-10-10T11:18:28Z","published":"2023-10-08T07:42:41Z","title":"Language-driven Open-Vocabulary Keypoint Detection for Animal Body and\n  Face","summary":"  Current approaches for image-based keypoint detection on animal (including\nhuman) body and face are limited to specific keypoints and species. We address\nthe limitation by proposing the Open-Vocabulary Keypoint Detection (OVKD) task.\nIt aims to use text prompts to localize arbitrary keypoints of any species. To\naccomplish this objective, we propose Open-Vocabulary Keypoint Detection with\nSemantic-feature Matching (KDSM), which utilizes both vision and language\nmodels to harness the relationship between text and vision and thus achieve\nkeypoint detection through associating text prompt with relevant keypoint\nfeatures. Additionally, KDSM integrates domain distribution matrix matching and\nsome special designs to reinforce the relationship between language and vision,\nthereby improving the model's generalizability and performance. Extensive\nexperiments show that our proposed components bring significant performance\nimprovements, and our overall method achieves impressive results in OVKD.\nRemarkably, our method outperforms the state-of-the-art few-shot keypoint\ndetection methods using a zero-shot fashion. We will make the source code\npublicly accessible.\n","authors":["Hao Zhang","Kaipeng Zhang","Lumin Xu","Shenqi Lai","Wenqi Shao","Nanning Zheng","Ping Luo","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2310.05056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06525v1","updated":"2023-10-10T11:14:29Z","published":"2023-10-10T11:14:29Z","title":"Perceptual MAE for Image Manipulation Localization: A High-level Vision\n  Learner Focusing on Low-level Features","summary":"  Nowadays, multimedia forensics faces unprecedented challenges due to the\nrapid advancement of multimedia generation technology thereby making Image\nManipulation Localization (IML) crucial in the pursuit of truth. The key to IML\nlies in revealing the artifacts or inconsistencies between the tampered and\nauthentic areas, which are evident under pixel-level features. Consequently,\nexisting studies treat IML as a low-level vision task, focusing on allocating\ntampered masks by crafting pixel-level features such as image RGB noises, edge\nsignals, or high-frequency features. However, in practice, tampering commonly\noccurs at the object level, and different classes of objects have varying\nlikelihoods of becoming targets of tampering. Therefore, object semantics are\nalso vital in identifying the tampered areas in addition to pixel-level\nfeatures. This necessitates IML models to carry out a semantic understanding of\nthe entire image. In this paper, we reformulate the IML task as a high-level\nvision task that greatly benefits from low-level features. Based on such an\ninterpretation, we propose a method to enhance the Masked Autoencoder (MAE) by\nincorporating high-resolution inputs and a perceptual loss supervision module,\nwhich is termed Perceptual MAE (PMAE). While MAE has demonstrated an impressive\nunderstanding of object semantics, PMAE can also compensate for low-level\nsemantics with our proposed enhancements. Evidenced by extensive experiments,\nthis paradigm effectively unites the low-level and high-level features of the\nIML task and outperforms state-of-the-art tampering localization methods on all\nfive publicly available datasets.\n","authors":["Xiaochen Ma","Jizhe Zhou","Xiong Xu","Zhuohang Jiang","Chi-Man Pun"],"pdf_url":"https://arxiv.org/pdf/2310.06525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.04494v3","updated":"2023-10-10T11:10:59Z","published":"2023-01-11T14:42:47Z","title":"Multi-label Image Classification using Adaptive Graph Convolutional\n  Networks: from a Single Domain to Multiple Domains","summary":"  This paper proposes an adaptive graph-based approach for multi-label image\nclassification. Graph-based methods have been largely exploited in the field of\nmulti-label classification, given their ability to model label correlations.\nSpecifically, their effectiveness has been proven not only when considering a\nsingle domain but also when taking into account multiple domains. However, the\ntopology of the used graph is not optimal as it is pre-defined heuristically.\nIn addition, consecutive Graph Convolutional Network (GCN) aggregations tend to\ndestroy the feature similarity. To overcome these issues, an architecture for\nlearning the graph connectivity in an end-to-end fashion is introduced. This is\ndone by integrating an attention-based mechanism and a similarity-preserving\nstrategy. The proposed framework is then extended to multiple domains using an\nadversarial training scheme. Numerous experiments are reported on well-known\nsingle-domain and multi-domain benchmarks. The results demonstrate that our\napproach achieves competitive results in terms of mean Average Precision (mAP)\nand model size as compared to the state-of-the-art. The code will be made\npublicly available.\n","authors":["Indel Pal Singh","Enjie Ghorbel","Oyebade Oyedotun","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2301.04494v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06522v1","updated":"2023-10-10T11:08:31Z","published":"2023-10-10T11:08:31Z","title":"Watt For What: Rethinking Deep Learning's Energy-Performance\n  Relationship","summary":"  Deep learning models have revolutionized various fields, from image\nrecognition to natural language processing, by achieving unprecedented levels\nof accuracy. However, their increasing energy consumption has raised concerns\nabout their environmental impact, disadvantaging smaller entities in research\nand exacerbating global energy consumption. In this paper, we explore the\ntrade-off between model accuracy and electricity consumption, proposing a\nmetric that penalizes large consumption of electricity. We conduct a\ncomprehensive study on the electricity consumption of various deep learning\nmodels across different GPUs, presenting a detailed analysis of their\naccuracy-efficiency trade-offs. By evaluating accuracy per unit of electricity\nconsumed, we demonstrate how smaller, more energy-efficient models can\nsignificantly expedite research while mitigating environmental concerns. Our\nresults highlight the potential for a more sustainable approach to deep\nlearning, emphasizing the importance of optimizing models for efficiency. This\nresearch also contributes to a more equitable research landscape, where smaller\nentities can compete effectively with larger counterparts. This advocates for\nthe adoption of efficient deep learning practices to reduce electricity\nconsumption, safeguarding the environment for future generations whilst also\nhelping ensure a fairer competitive landscape.\n","authors":["Shreyank N Gowda","Xinyue Hao","Gen Li","Laura Sevilla-Lara","Shashank Narayana Gowda"],"pdf_url":"https://arxiv.org/pdf/2310.06522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05136v2","updated":"2023-10-10T11:04:26Z","published":"2023-10-08T12:10:44Z","title":"InstructDET: Diversifying Referring Object Detection with Generalized\n  Instructions","summary":"  We propose InstructDET, a data-centric method for referring object detection\n(ROD) that localizes target objects based on user instructions. While deriving\nfrom referring expressions (REC), the instructions we leverage are greatly\ndiversified to encompass common user intentions related to object detection.\nFor one image, we produce tremendous instructions that refer to every single\nobject and different combinations of multiple objects. Each instruction and its\ncorresponding object bounding boxes (bbxs) constitute one training data pair.\nIn order to encompass common detection expressions, we involve emerging\nvision-language model (VLM) and large language model (LLM) to generate\ninstructions guided by text prompts and object bbxs, as the generalizations of\nfoundation models are effective to produce human-like expressions (e.g.,\ndescribing object property, category, and relationship). We name our\nconstructed dataset as InDET. It contains images, bbxs and generalized\ninstructions that are from foundation models. Our InDET is developed from\nexisting REC datasets and object detection datasets, with the expanding\npotential that any image with object bbxs can be incorporated through using our\nInstructDET method. By using our InDET dataset, we show that a conventional ROD\nmodel surpasses existing methods on standard REC datasets and our InDET test\nset. Our data-centric method InstructDET, with automatic data expansion by\nleveraging foundation models, directs a promising field that ROD can be greatly\ndiversified to execute common object detection instructions.\n","authors":["Ronghao Dang","Jiangyan Feng","Haodong Zhang","Chongjian Ge","Lin Song","Lijun Gong","Chengju Liu","Qijun Chen","Feng Zhu","Rui Zhao","Yibing Song"],"pdf_url":"https://arxiv.org/pdf/2310.05136v2.pdf","comment":"27 pages (include appendix), technical report"},{"id":"http://arxiv.org/abs/2310.05371v2","updated":"2023-10-10T10:55:10Z","published":"2023-10-09T03:00:15Z","title":"Enhancing Prostate Cancer Diagnosis with Deep Learning: A Study using\n  mpMRI Segmentation and Classification","summary":"  Prostate cancer (PCa) is a severe disease among men globally. It is important\nto identify PCa early and make a precise diagnosis for effective treatment. For\nPCa diagnosis, Multi-parametric magnetic resonance imaging (mpMRI) emerged as\nan invaluable imaging modality that offers a precise anatomical view of the\nprostate gland and its tissue structure. Deep learning (DL) models can enhance\nexisting clinical systems and improve patient care by locating regions of\ninterest for physicians. Recently, DL techniques have been employed to develop\na pipeline for segmenting and classifying different cancer types. These studies\nshow that DL can be used to increase diagnostic precision and give objective\nresults without variability. This work uses well-known DL models for the\nclassification and segmentation of mpMRI images to detect PCa. Our\nimplementation involves four pipelines; Semantic DeepSegNet with ResNet50,\nDeepSegNet with recurrent neural network (RNN), U-Net with RNN, and U-Net with\na long short-term memory (LSTM). Each segmentation model is paired with a\ndifferent classifier to evaluate the performance using different metrics. The\nresults of our experiments show that the pipeline that uses the combination of\nU-Net and the LSTM model outperforms all other combinations, excelling in both\nsegmentation and classification tasks.\n","authors":["Anil B. Gavade","Neel Kanwal","Priyanka A. Gavade","Rajendra Nerli"],"pdf_url":"https://arxiv.org/pdf/2310.05371v2.pdf","comment":"Accepted at CISCON-2023"},{"id":"http://arxiv.org/abs/2304.08072v2","updated":"2023-10-10T10:53:29Z","published":"2023-04-17T08:34:41Z","title":"Two-stage MR Image Segmentation Method for Brain Tumors based on\n  Attention Mechanism","summary":"  Multimodal magnetic resonance imaging (MRI) can reveal different patterns of\nhuman tissue and is crucial for clinical diagnosis. However, limited by cost,\nnoise and manual labeling, obtaining diverse and reliable multimodal MR images\nremains a challenge. For the same lesion, different MRI manifestations have\ngreat differences in background information, coarse positioning and fine\nstructure. In order to obtain better generation and segmentation performance, a\ncoordination-spatial attention generation adversarial network (CASP-GAN) based\non the cycle-consistent generative adversarial network (CycleGAN) is proposed.\nThe performance of the generator is optimized by introducing the Coordinate\nAttention (CA) module and the Spatial Attention (SA) module. The two modules\ncan make full use of the captured location information, accurately locating the\ninterested region, and enhancing the generator model network structure. The\nability to extract the structure information and the detailed information of\nthe original medical image can help generate the desired image with higher\nquality. There exist some problems in the original CycleGAN that the training\ntime is long, the parameter amount is too large, and it is difficult to\nconverge. In response to this problem, we introduce the Coordinate Attention\n(CA) module to replace the Res Block to reduce the number of parameters, and\ncooperate with the spatial information extraction network above to strengthen\nthe information extraction ability. On the basis of CASP-GAN, an attentional\ngenerative cross-modality segmentation (AGCMS) method is further proposed. This\nmethod inputs the modalities generated by CASP-GAN and the real modalities into\nthe segmentation network for brain tumor segmentation. Experimental results\nshow that CASP-GAN outperforms CycleGAN and some state-of-the-art methods in\nPSNR, SSMI and RMSE in most tasks.\n","authors":["Li Zhu","Jiawei Jiang","Lin Lu","Jin Li"],"pdf_url":"https://arxiv.org/pdf/2304.08072v2.pdf","comment":"Some contributing authors are not signed"},{"id":"http://arxiv.org/abs/2309.12559v2","updated":"2023-10-10T10:14:28Z","published":"2023-09-22T01:06:16Z","title":"Invariant Learning via Probability of Sufficient and Necessary Causes","summary":"  Out-of-distribution (OOD) generalization is indispensable for learning models\nin the wild, where testing distribution typically unknown and different from\nthe training. Recent methods derived from causality have shown great potential\nin achieving OOD generalization. However, existing methods mainly focus on the\ninvariance property of causes, while largely overlooking the property of\n\\textit{sufficiency} and \\textit{necessity} conditions. Namely, a necessary but\ninsufficient cause (feature) is invariant to distribution shift, yet it may not\nhave required accuracy. By contrast, a sufficient yet unnecessary cause\n(feature) tends to fit specific data well but may have a risk of adapting to a\nnew domain. To capture the information of sufficient and necessary causes, we\nemploy a classical concept, the probability of sufficiency and necessary causes\n(PNS), which indicates the probability of whether one is the necessary and\nsufficient cause. To associate PNS with OOD generalization, we propose PNS risk\nand formulate an algorithm to learn representation with a high PNS value. We\ntheoretically analyze and prove the generalizability of the PNS risk.\nExperiments on both synthetic and real-world benchmarks demonstrate the\neffectiveness of the proposed method. The details of the implementation can be\nfound at the GitHub repository: https://github.com/ymy4323460/CaSN.\n","authors":["Mengyue Yang","Zhen Fang","Yonggang Zhang","Yali Du","Furui Liu","Jean-Francois Ton","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2309.12559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05314v2","updated":"2023-10-10T10:09:27Z","published":"2023-05-09T10:06:37Z","title":"CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and\n  Subtyping in Whole Slide Images","summary":"  The visual examination of tissue biopsy sections is fundamental for cancer\ndiagnosis, with pathologists analyzing sections at multiple magnifications to\ndiscern tumor cells and their subtypes. However, existing attention-based\nmultiple instance learning (MIL) models, used for analyzing Whole Slide Images\n(WSIs) in cancer diagnostics, often overlook the contextual information of\ntumor and neighboring tiles, leading to misclassifications. To address this, we\npropose the Context-Aware Multiple Instance Learning (CAMIL) architecture.\nCAMIL incorporates neighbor-constrained attention to consider dependencies\namong tiles within a WSI and integrates contextual constraints as prior\nknowledge into the MIL model. We evaluated CAMIL on subtyping non-small cell\nlung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16) metastasis,\nachieving test AUCs of 0.959\\% and 0.975\\%, respectively, outperforming other\nstate-of-the-art methods. Additionally, CAMIL enhances model interpretability\nby identifying regions of high diagnostic value.\n","authors":["Olga Fourkioti","Matt De Vries","Chris Bakal"],"pdf_url":"https://arxiv.org/pdf/2305.05314v2.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2212.02014v2","updated":"2023-10-10T10:03:24Z","published":"2022-12-05T04:04:21Z","title":"Med-Query: Steerable Parsing of 9-DoF Medical Anatomies with Query\n  Embedding","summary":"  Automatic parsing of human anatomies at instance-level from 3D computed\ntomography (CT) scans is a prerequisite step for many clinical applications.\nThe presence of pathologies, broken structures or limited field-of-view (FOV)\nall can make anatomy parsing algorithms vulnerable. In this work, we explore\nhow to exploit and conduct the prosperous detection-then-segmentation paradigm\nin 3D medical data, and propose a steerable, robust, and efficient computing\nframework for detection, identification, and segmentation of anatomies in CT\nscans. Considering complicated shapes, sizes and orientations of anatomies,\nwithout lose of generality, we present the nine degrees-of-freedom (9-DoF) pose\nestimation solution in full 3D space using a novel single-stage,\nnon-hierarchical forward representation. Our whole framework is executed in a\nsteerable manner where any anatomy of interest can be directly retrieved to\nfurther boost the inference efficiency. We have validated the proposed method\non three medical imaging parsing tasks of ribs, spine, and abdominal organs.\nFor rib parsing, CT scans have been annotated at the rib instance-level for\nquantitative evaluation, similarly for spine vertebrae and abdominal organs.\nExtensive experiments on 9-DoF box detection and rib instance segmentation\ndemonstrate the effectiveness of our framework (with the identification rate of\n97.0% and the segmentation Dice score of 90.9%) in high efficiency, compared\nfavorably against several strong baselines (e.g., CenterNet, FCOS, and\nnnU-Net). For spine identification and segmentation, our method achieves a new\nstate-of-the-art result on the public CTSpine1K dataset. Last, we report highly\ncompetitive results in multi-organ segmentation at FLARE22 competition. Our\nannotations, code and models will be made publicly available at:\nhttps://github.com/alibaba-damo-academy/Med_Query.\n","authors":["Heng Guo","Jianfeng Zhang","Ke Yan","Le Lu","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2212.02014v2.pdf","comment":"updated version"},{"id":"http://arxiv.org/abs/2310.06489v1","updated":"2023-10-10T09:57:19Z","published":"2023-10-10T09:57:19Z","title":"Deep Learning for Automatic Detection and Facial Recognition in Japanese\n  Macaques: Illuminating Social Networks","summary":"  Individual identification plays a pivotal role in ecology and ethology,\nnotably as a tool for complex social structures understanding. However,\ntraditional identification methods often involve invasive physical tags and can\nprove both disruptive for animals and time-intensive for researchers. In recent\nyears, the integration of deep learning in research offered new methodological\nperspectives through automatization of complex tasks. Harnessing object\ndetection and recognition technologies is increasingly used by researchers to\nachieve identification on video footage. This study represents a preliminary\nexploration into the development of a non-invasive tool for face detection and\nindividual identification of Japanese macaques (Macaca fuscata) through deep\nlearning. The ultimate goal of this research is, using identifications done on\nthe dataset, to automatically generate a social network representation of the\nstudied population. The current main results are promising: (i) the creation of\na Japanese macaques' face detector (Faster-RCNN model), reaching a 82.2%\naccuracy and (ii) the creation of an individual recognizer for K{\\=o}jima\nisland macaques population (YOLOv8n model), reaching a 83% accuracy. We also\ncreated a K{\\=o}jima population social network by traditional methods, based on\nco-occurrences on videos. Thus, we provide a benchmark against which the\nautomatically generated network will be assessed for reliability. These\npreliminary results are a testament to the potential of this innovative\napproach to provide the scientific community with a tool for tracking\nindividuals and social network studies in Japanese macaques.\n","authors":["Julien Paulet","Axel Molina","Benjamin Beltzung","Takafumi Suzumura","Shinya Yamamoto","Cédric Sueur"],"pdf_url":"https://arxiv.org/pdf/2310.06489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06488v1","updated":"2023-10-10T09:57:17Z","published":"2023-10-10T09:57:17Z","title":"SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network","summary":"  Spiking neural networks (SNNs) have demonstrated the capability to achieve\ncomparable performance to deep neural networks (DNNs) in both visual and\nlinguistic domains while offering the advantages of improved energy efficiency\nand adherence to biological plausibility. However, the extension of such\nsingle-modality SNNs into the realm of multimodal scenarios remains an\nunexplored territory. Drawing inspiration from the concept of contrastive\nlanguage-image pre-training (CLIP), we introduce a novel framework, named\nSpikeCLIP, to address the gap between two modalities within the context of\nspike-based computing through a two-step recipe involving ``Alignment\nPre-training + Dual-Loss Fine-tuning\". Extensive experiments demonstrate that\nSNNs achieve comparable results to their DNN counterparts while significantly\nreducing energy consumption across a variety of datasets commonly used for\nmultimodal model evaluation. Furthermore, SpikeCLIP maintains robust\nperformance in image classification tasks that involve class labels not\npredefined within specific categories.\n","authors":["Tianlong Li","Wenhao Liu","Changze Lv","Jianhan Xu","Cenyuan Zhang","Muling Wu","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06486v1","updated":"2023-10-10T09:53:59Z","published":"2023-10-10T09:53:59Z","title":"Topological RANSAC for instance verification and retrieval without\n  fine-tuning","summary":"  This paper presents an innovative approach to enhancing explainable image\nretrieval, particularly in situations where a fine-tuning set is unavailable.\nThe widely-used SPatial verification (SP) method, despite its efficacy, relies\non a spatial model and the hypothesis-testing strategy for instance\nrecognition, leading to inherent limitations, including the assumption of\nplanar structures and neglect of topological relations among features. To\naddress these shortcomings, we introduce a pioneering technique that replaces\nthe spatial model with a topological one within the RANSAC process. We propose\nbio-inspired saccade and fovea functions to verify the topological consistency\namong features, effectively circumventing the issues associated with SP's\nspatial model. Our experimental results demonstrate that our method\nsignificantly outperforms SP, achieving state-of-the-art performance in\nnon-fine-tuning retrieval. Furthermore, our approach can enhance performance\nwhen used in conjunction with fine-tuned features. Importantly, our method\nretains high explainability and is lightweight, offering a practical and\nadaptable solution for a variety of real-world applications.\n","authors":["Guoyuan An","Juhyung Seon","Inkyu An","Yuchi Huo","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2310.06486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11643v3","updated":"2023-10-10T09:45:30Z","published":"2023-07-21T15:22:32Z","title":"Morphological Image Analysis and Feature Extraction for Reasoning with\n  AI-based Defect Detection and Classification Models","summary":"  As the use of artificial intelligent (AI) models becomes more prevalent in\nindustries such as engineering and manufacturing, it is essential that these\nmodels provide transparent reasoning behind their predictions. This paper\nproposes the AI-Reasoner, which extracts the morphological characteristics of\ndefects (DefChars) from images and utilises decision trees to reason with the\nDefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e.\ncharts) and textual explanations to provide insights into outputs made by\nmasked-based defect detection and classification models. It also provides\neffective mitigation strategies to enhance data pre-processing and overall\nmodel performance. The AI-Reasoner was tested on explaining the outputs of an\nIE Mask R-CNN model using a set of 366 images containing defects. The results\ndemonstrated its effectiveness in explaining the IE Mask R-CNN model's\npredictions. Overall, the proposed AI-Reasoner provides a solution for\nimproving the performance of AI models in industrial applications that require\ndefect analysis.\n","authors":["Jiajun Zhang","Georgina Cosma","Sarah Bugby","Axel Finke","Jason Watkins"],"pdf_url":"https://arxiv.org/pdf/2307.11643v3.pdf","comment":"8 pages, 3 figures, 5 tables; accepted in 2023 IEEE symposium series\n  on computational intelligence (SSCI)"},{"id":"http://arxiv.org/abs/2308.08231v2","updated":"2023-10-10T09:41:24Z","published":"2023-08-16T09:06:32Z","title":"DDF-HO: Hand-Held Object Reconstruction via Conditional Directed\n  Distance Field","summary":"  Reconstructing hand-held objects from a single RGB image is an important and\nchallenging problem. Existing works utilizing Signed Distance Fields (SDF)\nreveal limitations in comprehensively capturing the complex hand-object\ninteractions, since SDF is only reliable within the proximity of the target,\nand hence, infeasible to simultaneously encode local hand and object cues. To\naddress this issue, we propose DDF-HO, a novel approach leveraging Directed\nDistance Field (DDF) as the shape representation. Unlike SDF, DDF maps a ray in\n3D space, consisting of an origin and a direction, to corresponding DDF values,\nincluding a binary visibility signal determining whether the ray intersects the\nobjects and a distance value measuring the distance from origin to target in\nthe given direction. We randomly sample multiple rays and collect local to\nglobal geometric features for them by introducing a novel 2D ray-based feature\naggregation scheme and a 3D intersection-aware hand pose embedding, combining\n2D-3D features to model hand-object interactions. Extensive experiments on\nsynthetic and real-world datasets demonstrate that DDF-HO consistently\noutperforms all baseline methods by a large margin, especially under Chamfer\nDistance, with about $80\\%$ leap forward. Codes are available at\n\\url{https://github.com/ZhangCYG/DDFHO}.\n","authors":["Chenyangguang Zhang","Yan Di","Ruida Zhang","Guangyao Zhai","Fabian Manhardt","Federico Tombari","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2308.08231v2.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06470v1","updated":"2023-10-10T09:41:13Z","published":"2023-10-10T09:41:13Z","title":"Focus on Local Regions for Query-based Object Detection","summary":"  Query-based methods have garnered significant attention in object detection\nsince the advent of DETR, the pioneering end-to-end query-based detector.\nHowever, these methods face challenges like slow convergence and suboptimal\nperformance. Notably, self-attention in object detection often hampers\nconvergence due to its global focus. To address these issues, we propose FoLR,\na transformer-like architecture with only decoders. We enhance the\nself-attention mechanism by isolating connections between irrelevant objects\nthat makes it focus on local regions but not global regions. We also design the\nadaptive sampling method to extract effective features based on queries' local\nregions from feature maps. Additionally, we employ a look-back strategy for\ndecoders to retain prior information, followed by the Feature Mixer module to\nfuse features and queries. Experimental results demonstrate FoLR's\nstate-of-the-art performance in query-based detectors, excelling in convergence\nspeed and computational efficiency.\n","authors":["Hongbin Xu","Yamei Xia","Shuai Zhao","Bo Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.06470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06468v1","updated":"2023-10-10T09:39:38Z","published":"2023-10-10T09:39:38Z","title":"A Geometrical Approach to Evaluate the Adversarial Robustness of Deep\n  Neural Networks","summary":"  Deep Neural Networks (DNNs) are widely used for computer vision tasks.\nHowever, it has been shown that deep models are vulnerable to adversarial\nattacks, i.e., their performances drop when imperceptible perturbations are\nmade to the original inputs, which may further degrade the following visual\ntasks or introduce new problems such as data and privacy security. Hence,\nmetrics for evaluating the robustness of deep models against adversarial\nattacks are desired. However, previous metrics are mainly proposed for\nevaluating the adversarial robustness of shallow networks on the small-scale\ndatasets. Although the Cross Lipschitz Extreme Value for nEtwork Robustness\n(CLEVER) metric has been proposed for large-scale datasets (e.g., the ImageNet\ndataset), it is computationally expensive and its performance relies on a\ntractable number of samples. In this paper, we propose the Adversarial\nConverging Time Score (ACTS), an attack-dependent metric that quantifies the\nadversarial robustness of a DNN on a specific input. Our key observation is\nthat local neighborhoods on a DNN's output surface would have different shapes\ngiven different inputs. Hence, given different inputs, it requires different\ntime for converging to an adversarial sample. Based on this geometry meaning,\nACTS measures the converging time as an adversarial robustness metric. We\nvalidate the effectiveness and generalization of the proposed ACTS metric\nagainst different adversarial attacks on the large-scale ImageNet dataset using\nstate-of-the-art deep networks. Extensive experiments show that our ACTS metric\nis an efficient and effective adversarial metric over the previous CLEVER\nmetric.\n","authors":["Yang Wang","Bo Dong","Ke Xu","Haiyin Piao","Yufei Ding","Baocai Yin","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2310.06468v1.pdf","comment":"ACM Transactions on Multimedia Computing, Communications, and\n  Applications (ACM TOMM)"},{"id":"http://arxiv.org/abs/2305.07336v3","updated":"2023-10-10T09:25:33Z","published":"2023-05-12T09:28:09Z","title":"MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with\n  Bird's Eye View based Appearance and Motion Features","summary":"  Identifying moving objects is an essential capability for autonomous systems,\nas it provides critical information for pose estimation, navigation, collision\navoidance, and static map construction. In this paper, we present MotionBEV, a\nfast and accurate framework for LiDAR moving object segmentation, which\nsegments moving objects with appearance and motion features in the bird's eye\nview (BEV) domain. Our approach converts 3D LiDAR scans into a 2D polar BEV\nrepresentation to improve computational efficiency. Specifically, we learn\nappearance features with a simplified PointNet and compute motion features\nthrough the height differences of consecutive frames of point clouds projected\nonto vertical columns in the polar BEV coordinate system. We employ a\ndual-branch network bridged by the Appearance-Motion Co-attention Module (AMCM)\nto adaptively fuse the spatio-temporal information from appearance and motion\nfeatures. Our approach achieves state-of-the-art performance on the\nSemanticKITTI-MOS benchmark. Furthermore, to demonstrate the practical\neffectiveness of our method, we provide a LiDAR-MOS dataset recorded by a\nsolid-state LiDAR, which features non-repetitive scanning patterns and a small\nfield of view.\n","authors":["Bo Zhou","Jiapeng Xie","Yan Pan","Jiajie Wu","Chuanzhao Lu"],"pdf_url":"https://arxiv.org/pdf/2305.07336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13150v3","updated":"2023-10-10T09:17:14Z","published":"2023-08-25T03:08:41Z","title":"Enhancing Breast Cancer Classification Using Transfer ResNet with\n  Lightweight Attention Mechanism","summary":"  Despite the remarkable results of deep learning in breast cancer image\nclassification, challenges such as data imbalance and interpretability still\nexist and require cross-domain knowledge and collaboration among medical\nexperts. In this study, we propose a dual-activated lightweight attention\nResNet50 module method-based breast cancer classification method that\neffectively addresses challenges such as data imbalance and interpretability.\nOur model fuses a pre-trained deep ResNet50 and a lightweight attention\nmechanism to accomplish classification by embedding an attention module in\nlayer 4 of ResNet50 and adding two fully connected layers. For the fully\nconnected network design, we employ both Leaky ReLU and ReLU activation\nfunctions. On medical histopathology datasets, our model outperforms\nconventional models, visual transformers, and large models in terms of\nprecision, accuracy, recall, F1 score, and GMean. In particular, the model\ndemonstrates significant robustness and broad applicability when dealing with\nthe unbalanced breast cancer dataset. Our model is tested on 40X, 100X, 200X,\nand 400X images and achieves accuracies of 98.5%, 98.7%, 97.9%, and 94.3%,\nrespectively. Through an in-depth analysis of loss and accuracy, as well as\nGrad-CAM analysis, we comprehensively assessed the model performance and gained\nperspective on its training process. In the later stages of training, the\nvalidated losses and accuracies change minimally, showing that the model avoids\noverfitting and exhibits good generalization ability. Overall, this study\nprovides an effective solution for breast cancer image classification with\npractical applica\n","authors":["Suxing Liu"],"pdf_url":"https://arxiv.org/pdf/2308.13150v3.pdf","comment":"13 pages, 8 figures,6 tables"},{"id":"http://arxiv.org/abs/2310.00369v2","updated":"2023-10-10T09:12:37Z","published":"2023-09-30T13:21:29Z","title":"Distilling Inductive Bias: Knowledge Distillation Beyond Model\n  Compression","summary":"  With the rapid development of computer vision, Vision Transformers (ViTs)\noffer the tantalizing prospect of unified information processing across visual\nand textual domains. But due to the lack of inherent inductive biases in ViTs,\nthey require enormous amount of data for training. To make their applications\npractical, we introduce an innovative ensemble-based distillation approach\ndistilling inductive bias from complementary lightweight teacher models. Prior\nsystems relied solely on convolution-based teaching. However, this method\nincorporates an ensemble of light teachers with different architectural\ntendencies, such as convolution and involution, to instruct the student\ntransformer jointly. Because of these unique inductive biases, instructors can\naccumulate a wide range of knowledge, even from readily identifiable stored\ndatasets, which leads to enhanced student performance. Our proposed framework\nalso involves precomputing and storing logits in advance, essentially the\nunnormalized predictions of the model. This optimization can accelerate the\ndistillation process by eliminating the need for repeated forward passes during\nknowledge distillation, significantly reducing the computational burden and\nenhancing efficiency.\n","authors":["Gousia Habib","Tausifa Jan Saleem","Brejesh Lall"],"pdf_url":"https://arxiv.org/pdf/2310.00369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06440v1","updated":"2023-10-10T09:12:27Z","published":"2023-10-10T09:12:27Z","title":"Solution for SMART-101 Challenge of ICCV Multi-modal Algorithmic\n  Reasoning Task 2023","summary":"  In this paper, we present our solution to a Multi-modal Algorithmic Reasoning\nTask: SMART-101 Challenge. Different from the traditional visual\nquestion-answering datasets, this challenge evaluates the abstraction,\ndeduction, and generalization abilities of neural networks in solving\nvisuolinguistic puzzles designed specifically for children in the 6-8 age\ngroup. We employed a divide-and-conquer approach. At the data level, inspired\nby the challenge paper, we categorized the whole questions into eight types and\nutilized the llama-2-chat model to directly generate the type for each question\nin a zero-shot manner. Additionally, we trained a yolov7 model on the icon45\ndataset for object detection and combined it with the OCR method to recognize\nand locate objects and text within the images. At the model level, we utilized\nthe BLIP-2 model and added eight adapters to the image encoder VIT-G to\nadaptively extract visual features for different question types. We fed the\npre-constructed question templates as input and generated answers using the\nflan-t5-xxl decoder. Under the puzzle splits configuration, we achieved an\naccuracy score of 26.5 on the validation set and 24.30 on the private test set.\n","authors":["Xiangyu Wu","Yang Yang","Shengdong Xu","Yifeng Wu","Qingguo Chen","Jianfeng Lu"],"pdf_url":"https://arxiv.org/pdf/2310.06440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12301v2","updated":"2023-10-10T09:08:21Z","published":"2023-09-21T17:58:26Z","title":"Environment-biased Feature Ranking for Novelty Detection Robustness","summary":"  We tackle the problem of robust novelty detection, where we aim to detect\nnovelties in terms of semantic content while being invariant to changes in\nother, irrelevant factors. Specifically, we operate in a setup with multiple\nenvironments, where we determine the set of features that are associated more\nwith the environments, rather than to the content relevant for the task. Thus,\nwe propose a method that starts with a pretrained embedding and a multi-env\nsetup and manages to rank the features based on their environment-focus. First,\nwe compute a per-feature score based on the feature distribution variance\nbetween envs. Next, we show that by dropping the highly scored ones, we manage\nto remove spurious correlations and improve the overall performance by up to\n6%, both in covariance and sub-population shift cases, both for a real and a\nsynthetic benchmark, that we introduce for this task.\n","authors":["Stefan Smeu","Elena Burceanu","Emanuela Haller","Andrei Liviu Nicolicioiu"],"pdf_url":"https://arxiv.org/pdf/2309.12301v2.pdf","comment":"The updated, long version of the paper is available at\n  arXiv:2310.03738"},{"id":"http://arxiv.org/abs/2310.06437v1","updated":"2023-10-10T09:06:39Z","published":"2023-10-10T09:06:39Z","title":"Skeleton Ground Truth Extraction: Methodology, Annotation Tool and\n  Benchmarks","summary":"  Skeleton Ground Truth (GT) is critical to the success of supervised skeleton\nextraction methods, especially with the popularity of deep learning techniques.\nFurthermore, we see skeleton GTs used not only for training skeleton detectors\nwith Convolutional Neural Networks (CNN) but also for evaluating\nskeleton-related pruning and matching algorithms. However, most existing shape\nand image datasets suffer from the lack of skeleton GT and inconsistency of GT\nstandards. As a result, it is difficult to evaluate and reproduce CNN-based\nskeleton detectors and algorithms on a fair basis. In this paper, we present a\nheuristic strategy for object skeleton GT extraction in binary shapes and\nnatural images. Our strategy is built on an extended theory of diagnosticity\nhypothesis, which enables encoding human-in-the-loop GT extraction based on\nclues from the target's context, simplicity, and completeness. Using this\nstrategy, we developed a tool, SkeView, to generate skeleton GT of 17 existing\nshape and image datasets. The GTs are then structurally evaluated with\nrepresentative methods to build viable baselines for fair comparisons.\nExperiments demonstrate that GTs generated by our strategy yield promising\nquality with respect to standard consistency, and also provide a balance\nbetween simplicity and completeness.\n","authors":["Cong Yang","Bipin Indurkhya","John See","Bo Gao","Yan Ke","Zeyd Boukhers","Zhenyu Yang","Marcin Grzegorzek"],"pdf_url":"https://arxiv.org/pdf/2310.06437v1.pdf","comment":"Accepted for publication in the International Journal of Computer\n  Vision (IJCV)"},{"id":"http://arxiv.org/abs/2310.06433v1","updated":"2023-10-10T09:03:01Z","published":"2023-10-10T09:03:01Z","title":"Retromorphic Testing: A New Approach to the Test Oracle Problem","summary":"  A test oracle serves as a criterion or mechanism to assess the correspondence\nbetween software output and the anticipated behavior for a given input set. In\nautomated testing, black-box techniques, known for their non-intrusive nature\nin test oracle construction, are widely used, including notable methodologies\nlike differential testing and metamorphic testing. Inspired by the mathematical\nconcept of inverse function, we present Retromorphic Testing, a novel black-box\ntesting methodology. It leverages an auxiliary program in conjunction with the\nprogram under test, which establishes a dual-program structure consisting of a\nforward program and a backward program. The input data is first processed by\nthe forward program and then its program output is reversed to its original\ninput format using the backward program. In particular, the auxiliary program\ncan operate as either the forward or backward program, leading to different\ntesting modes. The process concludes by examining the relationship between the\ninitial input and the transformed output within the input domain. For example,\nto test the implementation of the sine function $\\sin(x)$, we can employ its\ninverse function, $\\arcsin(x)$, and validate the equation $x =\n\\sin(\\arcsin(x)+2k\\pi), \\forall k \\in \\mathbb{Z}$. In addition to the\nhigh-level concept of Retromorphic Testing, this paper presents its three\ntesting modes with illustrative use cases across diverse programs, including\nalgorithms, traditional software, and AI applications.\n","authors":["Boxi Yu","Qiuyang Mang","Qingshuo Guo","Pinjia He"],"pdf_url":"https://arxiv.org/pdf/2310.06433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03504v2","updated":"2023-10-10T09:01:57Z","published":"2023-09-07T06:27:39Z","title":"Stroke-based Neural Painting and Stylization with Dynamically Predicted\n  Painting Region","summary":"  Stroke-based rendering aims to recreate an image with a set of strokes. Most\nexisting methods render complex images using an uniform-block-dividing\nstrategy, which leads to boundary inconsistency artifacts. To solve the\nproblem, we propose Compositional Neural Painter, a novel stroke-based\nrendering framework which dynamically predicts the next painting region based\non the current canvas, instead of dividing the image plane uniformly into\npainting regions. We start from an empty canvas and divide the painting process\ninto several steps. At each step, a compositor network trained with a phasic RL\nstrategy first predicts the next painting region, then a painter network\ntrained with a WGAN discriminator predicts stroke parameters, and a stroke\nrenderer paints the strokes onto the painting region of the current canvas.\nMoreover, we extend our method to stroke-based style transfer with a novel\ndifferentiable distance transform loss, which helps preserve the structure of\nthe input image during stroke-based stylization. Extensive experiments show our\nmodel outperforms the existing models in both stroke-based neural painting and\nstroke-based stylization. Code is available at\nhttps://github.com/sjtuplayer/Compositional_Neural_Painter\n","authors":["Teng Hu","Ran Yi","Haokun Zhu","Liang Liu","Jinlong Peng","Yabiao Wang","Chengjie Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2309.03504v2.pdf","comment":"ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.06430v1","updated":"2023-10-10T08:54:14Z","published":"2023-10-10T08:54:14Z","title":"Conformal Prediction for Deep Classifier via Label Ranking","summary":"  Conformal prediction is a statistical framework that generates prediction\nsets containing ground-truth labels with a desired coverage guarantee. The\npredicted probabilities produced by machine learning models are generally\nmiscalibrated, leading to large prediction sets in conformal prediction. In\nthis paper, we empirically and theoretically show that disregarding the\nprobabilities' value will mitigate the undesirable effect of miscalibrated\nprobability values. Then, we propose a novel algorithm named $\\textit{Sorted\nAdaptive prediction sets}$ (SAPS), which discards all the probability values\nexcept for the maximum softmax probability. The key idea behind SAPS is to\nminimize the dependence of the non-conformity score on the probability values\nwhile retaining the uncertainty information. In this manner, SAPS can produce\nsets of small size and communicate instance-wise uncertainty. Theoretically, we\nprovide a finite-sample coverage guarantee of SAPS and show that the expected\nvalue of set size from SAPS is always smaller than APS. Extensive experiments\nvalidate that SAPS not only lessens the prediction sets but also broadly\nenhances the conditional coverage rate and adaptation of prediction sets.\n","authors":["Jianguo Huang","Huajun Xi","Linjun Zhang","Huaxiu Yao","Yue Qiu","Hongxin Wei"],"pdf_url":"https://arxiv.org/pdf/2310.06430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06420v1","updated":"2023-10-10T08:44:47Z","published":"2023-10-10T08:44:47Z","title":"AnoDODE: Anomaly Detection with Diffusion ODE","summary":"  Anomaly detection is the process of identifying atypical data samples that\nsignificantly deviate from the majority of the dataset. In the realm of\nclinical screening and diagnosis, detecting abnormalities in medical images\nholds great importance. Typically, clinical practice provides access to a vast\ncollection of normal images, while abnormal images are relatively scarce. We\nhypothesize that abnormal images and their associated features tend to manifest\nin low-density regions of the data distribution. Following this assumption, we\nturn to diffusion ODEs for unsupervised anomaly detection, given their\ntractability and superior performance in density estimation tasks. More\nprecisely, we propose a new anomaly detection method based on diffusion ODEs by\nestimating the density of features extracted from multi-scale medical images.\nOur anomaly scoring mechanism depends on computing the negative log-likelihood\nof features extracted from medical images at different scales, quantified in\nbits per dimension. Furthermore, we propose a reconstruction-based anomaly\nlocalization suitable for our method. Our proposed method not only identifie\nanomalies but also provides interpretability at both the image and pixel\nlevels. Through experiments on the BraTS2021 medical dataset, our proposed\nmethod outperforms existing methods. These results confirm the effectiveness\nand robustness of our method.\n","authors":["Xianyao Hu","Congming Jin"],"pdf_url":"https://arxiv.org/pdf/2310.06420v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2304.04441v2","updated":"2023-10-10T08:33:24Z","published":"2023-04-10T07:57:24Z","title":"Self-training with dual uncertainty for semi-supervised medical image\n  segmentation","summary":"  In the field of semi-supervised medical image segmentation, the shortage of\nlabeled data is the fundamental problem. How to effectively learn image\nfeatures from unlabeled images to improve segmentation accuracy is the main\nresearch direction in this field. Traditional self-training methods can\npartially solve the problem of insufficient labeled data by generating pseudo\nlabels for iterative training. However, noise generated due to the model's\nuncertainty during training directly affects the segmentation results.\nTherefore, we added sample-level and pixel-level uncertainty to stabilize the\ntraining process based on the self-training framework. Specifically, we saved\nseveral moments of the model during pre-training, and used the difference\nbetween their predictions on unlabeled samples as the sample-level uncertainty\nestimate for that sample. Then, we gradually add unlabeled samples from easy to\nhard during training. At the same time, we added a decoder with different\nupsampling methods to the segmentation network and used the difference between\nthe outputs of the two decoders as pixel-level uncertainty. In short, we\nselectively retrained unlabeled samples and assigned pixel-level uncertainty to\npseudo labels to optimize the self-training process. We compared the\nsegmentation results of our model with five semi-supervised approaches on the\npublic 2017 ACDC dataset and 2018 Prostate dataset. Our proposed method\nachieves better segmentation performance on both datasets under the same\nsettings, demonstrating its effectiveness, robustness, and potential\ntransferability to other medical image segmentation tasks. Keywords: Medical\nimage segmentation, semi-supervised learning, self-training, uncertainty\nestimation\n","authors":["Zhanhong Qiu","Haitao Gan","Ming Shi","Zhongwei Huang","Zhi Yang"],"pdf_url":"https://arxiv.org/pdf/2304.04441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03063v2","updated":"2023-10-10T08:30:08Z","published":"2023-09-06T15:05:04Z","title":"Prompt-based Ingredient-Oriented All-in-One Image Restoration","summary":"  Image restoration aims to recover the high-quality images from their degraded\nobservations. Since most existing methods have been dedicated into single\ndegradation removal, they may not yield optimal results on other types of\ndegradations, which do not satisfy the applications in real world scenarios. In\nthis paper, we propose a novel data ingredient-oriented approach that leverages\nprompt-based learning to enable a single model to efficiently tackle multiple\nimage degradation tasks. Specifically, we utilize a encoder to capture features\nand introduce prompts with degradation-specific information to guide the\ndecoder in adaptively recovering images affected by various degradations. In\norder to model the local invariant properties and non-local information for\nhigh-quality image restoration, we combined CNNs operations and Transformers.\nSimultaneously, we made several key designs in the Transformer blocks\n(multi-head rearranged attention with prompts and simple-gate feed-forward\nnetwork) to reduce computational requirements and selectively determines what\ninformation should be persevered to facilitate efficient recovery of\npotentially sharp images. Furthermore, we incorporate a feature fusion\nmechanism further explores the multi-scale information to improve the\naggregated features. The resulting tightly interlinked hierarchy architecture,\nnamed as CAPTNet, extensive experiments demonstrate that our method performs\ncompetitively to the state-of-the-art.\n","authors":["Hu Gao","Depeng Dang"],"pdf_url":"https://arxiv.org/pdf/2309.03063v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06403v1","updated":"2023-10-10T08:14:24Z","published":"2023-10-10T08:14:24Z","title":"Boundary Discretization and Reliable Classification Network for Temporal\n  Action Detection","summary":"  Temporal action detection aims to recognize the action category and determine\nthe starting and ending time of each action instance in untrimmed videos. The\nmixed methods have achieved remarkable performance by simply merging\nanchor-based and anchor-free approaches. However, there are still two crucial\nissues in the mixed framework: (1) Brute-force merging and handcrafted anchors\ndesign affect the performance and practical application of the mixed methods.\n(2) A large number of false positives in action category predictions further\nimpact the detection performance. In this paper, we propose a novel Boundary\nDiscretization and Reliable Classification Network (BDRC-Net) that addresses\nthe above issues by introducing boundary discretization and reliable\nclassification modules. Specifically, the boundary discretization module (BDM)\nelegantly merges anchor-based and anchor-free approaches in the form of\nboundary discretization, avoiding the handcrafted anchors design required by\ntraditional mixed methods. Furthermore, the reliable classification module\n(RCM) predicts reliable action categories to reduce false positives in action\ncategory predictions. Extensive experiments conducted on different benchmarks\ndemonstrate that our proposed method achieves favorable performance compared\nwith the state-of-the-art. For example, BDRC-Net hits an average mAP of 68.6%\non THUMOS'14, outperforming the previous best by 1.5%. The code will be\nreleased at https://github.com/zhenyingfang/BDRC-Net.\n","authors":["Zhenying Fang"],"pdf_url":"https://arxiv.org/pdf/2310.06403v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2310.01405v3","updated":"2023-10-10T08:00:53Z","published":"2023-10-02T17:59:07Z","title":"Representation Engineering: A Top-Down Approach to AI Transparency","summary":"  In this paper, we identify and characterize the emerging area of\nrepresentation engineering (RepE), an approach to enhancing the transparency of\nAI systems that draws on insights from cognitive neuroscience. RepE places\npopulation-level representations, rather than neurons or circuits, at the\ncenter of analysis, equipping us with novel methods for monitoring and\nmanipulating high-level cognitive phenomena in deep neural networks (DNNs). We\nprovide baselines and an initial analysis of RepE techniques, showing that they\noffer simple yet effective solutions for improving our understanding and\ncontrol of large language models. We showcase how these methods can provide\ntraction on a wide range of safety-relevant problems, including honesty,\nharmlessness, power-seeking, and more, demonstrating the promise of top-down\ntransparency research. We hope that this work catalyzes further exploration of\nRepE and fosters advancements in the transparency and safety of AI systems.\n","authors":["Andy Zou","Long Phan","Sarah Chen","James Campbell","Phillip Guo","Richard Ren","Alexander Pan","Xuwang Yin","Mantas Mazeika","Ann-Kathrin Dombrowski","Shashwat Goel","Nathaniel Li","Michael J. Byun","Zifan Wang","Alex Mallen","Steven Basart","Sanmi Koyejo","Dawn Song","Matt Fredrikson","J. Zico Kolter","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2310.01405v3.pdf","comment":"Code is available at\n  https://github.com/andyzoujm/representation-engineering"},{"id":"http://arxiv.org/abs/2310.06389v1","updated":"2023-10-10T07:52:30Z","published":"2023-10-10T07:52:30Z","title":"Learning Stackable and Skippable LEGO Bricks for Efficient,\n  Reconfigurable, and Variable-Resolution Diffusion Modeling","summary":"  Diffusion models excel at generating photo-realistic images but come with\nsignificant computational costs in both training and sampling. While various\ntechniques address these computational challenges, a less-explored issue is\ndesigning an efficient and adaptable network backbone for iterative refinement.\nCurrent options like U-Net and Vision Transformer often rely on\nresource-intensive deep networks and lack the flexibility needed for generating\nimages at variable resolutions or with a smaller network than used in training.\nThis study introduces LEGO bricks, which seamlessly integrate Local-feature\nEnrichment and Global-content Orchestration. These bricks can be stacked to\ncreate a test-time reconfigurable diffusion backbone, allowing selective\nskipping of bricks to reduce sampling costs and generate higher-resolution\nimages than the training data. LEGO bricks enrich local regions with an MLP and\ntransform them using a Transformer block while maintaining a consistent\nfull-resolution image across all bricks. Experimental results demonstrate that\nLEGO bricks enhance training efficiency, expedite convergence, and facilitate\nvariable-resolution image generation while maintaining strong generative\nperformance. Moreover, LEGO significantly reduces sampling time compared to\nother methods, establishing it as a valuable enhancement for diffusion models.\n","authors":["Huangjie Zheng","Zhendong Wang","Jianbo Yuan","Guanghan Ning","Pengcheng He","Quanzeng You","Hongxia Yang","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.06389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06385v1","updated":"2023-10-10T07:48:40Z","published":"2023-10-10T07:48:40Z","title":"3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic\n  Indoor Environments","summary":"  The existence of variable factors within the environment can cause a decline\nin camera localization accuracy, as it violates the fundamental assumption of a\nstatic environment in Simultaneous Localization and Mapping (SLAM) algorithms.\nRecent semantic SLAM systems towards dynamic environments either rely solely on\n2D semantic information, or solely on geometric information, or combine their\nresults in a loosely integrated manner. In this research paper, we introduce\n3DS-SLAM, 3D Semantic SLAM, tailored for dynamic scenes with visual 3D object\ndetection. The 3DS-SLAM is a tightly-coupled algorithm resolving both semantic\nand geometric constraints sequentially. We designed a 3D part-aware hybrid\ntransformer for point cloud-based object detection to identify dynamic objects.\nSubsequently, we propose a dynamic feature filter based on HDBSCAN clustering\nto extract objects with significant absolute depth differences. When compared\nagainst ORB-SLAM2, 3DS-SLAM exhibits an average improvement of 98.01% across\nthe dynamic sequences of the TUM RGB-D dataset. Furthermore, it surpasses the\nperformance of the other four leading SLAM systems designed for dynamic\nenvironments.\n","authors":["Ghanta Sai Krishna","Kundrapu Supriya","Sabur Baidya"],"pdf_url":"https://arxiv.org/pdf/2310.06385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04086v2","updated":"2023-10-10T07:47:36Z","published":"2023-10-06T08:30:20Z","title":"End-to-End Chess Recognition","summary":"  Chess recognition refers to the task of identifying the chess pieces\nconfiguration from a chessboard image. Contrary to the predominant approach\nthat aims to solve this task through the pipeline of chessboard detection,\nsquare localization, and piece classification, we rely on the power of deep\nlearning models and introduce two novel methodologies to circumvent this\npipeline and directly predict the chessboard configuration from the entire\nimage. In doing so, we avoid the inherent error accumulation of the sequential\napproaches and the need for intermediate annotations. Furthermore, we introduce\na new dataset, Chess Recognition Dataset (ChessReD), specifically designed for\nchess recognition that consists of 10,800 images and their corresponding\nannotations. In contrast to existing synthetic datasets with limited angles,\nthis dataset comprises a diverse collection of real images of chess formations\ncaptured from various angles using smartphone cameras; a sensor choice made to\nensure real-world applicability. We use this dataset to both train our model\nand evaluate and compare its performance to that of the current\nstate-of-the-art. Our approach in chess recognition on this new benchmark\ndataset outperforms related approaches, achieving a board recognition accuracy\nof 15.26% ($\\approx$7x better than the current state-of-the-art).\n","authors":["Athanasios Masouris","Jan van Gemert"],"pdf_url":"https://arxiv.org/pdf/2310.04086v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2302.08861v2","updated":"2023-10-10T07:38:02Z","published":"2023-02-17T13:16:17Z","title":"AliasNet: Alias Artefact Suppression Network for Accelerated\n  Phase-Encode MRI","summary":"  Sparse reconstruction is an important aspect of MRI, helping to reduce\nacquisition time and improve spatial-temporal resolution. Popular methods are\nbased mostly on compressed sensing (CS), which relies on the random sampling of\nk-space to produce incoherent (noise-like) artefacts. Due to hardware\nconstraints, 1D Cartesian phase-encode under-sampling schemes are popular for\n2D CS-MRI. However, 1D under-sampling limits 2D incoherence between\nmeasurements, yielding structured aliasing artefacts (ghosts) that may be\ndifficult to remove assuming a 2D sparsity model. Reconstruction algorithms\ntypically deploy direction-insensitive 2D regularisation for these\ndirection-associated artefacts. Recognising that phase-encode artefacts can be\nseparated into contiguous 1D signals, we develop two decoupling techniques that\nenable explicit 1D regularisation and leverage the excellent 1D incoherence\ncharacteristics. We also derive a combined 1D + 2D reconstruction technique\nthat takes advantage of spatial relationships within the image. Experiments\nconducted on retrospectively under-sampled brain and knee data demonstrate that\ncombination of the proposed 1D AliasNet modules with existing 2D deep learned\n(DL) recovery techniques leads to an improvement in image quality. We also find\nAliasNet enables a superior scaling of performance compared to increasing the\nsize of the original 2D network layers. AliasNet therefore improves the\nregularisation of aliasing artefacts arising from phase-encode under-sampling,\nby tailoring the network architecture to account for their expected appearance.\nThe proposed 1D + 2D approach is compatible with any existing 2D DL recovery\ntechnique deployed for this application.\n","authors":["Marlon E. Bran Lorenzana","Shekhar S. Chandra","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2302.08861v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06372v1","updated":"2023-10-10T07:25:06Z","published":"2023-10-10T07:25:06Z","title":"Leveraging Diffusion-Based Image Variations for Robust Training on\n  Poisoned Data","summary":"  Backdoor attacks pose a serious security threat for training neural networks\nas they surreptitiously introduce hidden functionalities into a model. Such\nbackdoors remain silent during inference on clean inputs, evading detection due\nto inconspicuous behavior. However, once a specific trigger pattern appears in\nthe input data, the backdoor activates, causing the model to execute its\nconcealed function. Detecting such poisoned samples within vast datasets is\nvirtually impossible through manual inspection. To address this challenge, we\npropose a novel approach that enables model training on potentially poisoned\ndatasets by utilizing the power of recent diffusion models. Specifically, we\ncreate synthetic variations of all training samples, leveraging the inherent\nresilience of diffusion models to potential trigger patterns in the data. By\ncombining this generative approach with knowledge distillation, we produce\nstudent models that maintain their general performance on the task while\nexhibiting robust resistance to backdoor triggers.\n","authors":["Lukas Struppek","Martin B. Hentschel","Clifton Poth","Dominik Hintersdorf","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.06372v1.pdf","comment":"11 pages, 3 tables, 2 figures"},{"id":"http://arxiv.org/abs/2310.06370v1","updated":"2023-10-10T07:20:37Z","published":"2023-10-10T07:20:37Z","title":"Advanced Efficient Strategy for Detection of Dark Objects Based on\n  Spiking Network with Multi-Box Detection","summary":"  Several deep learning algorithms have shown amazing performance for existing\nobject detection tasks, but recognizing darker objects is the largest\nchallenge. Moreover, those techniques struggled to detect or had a slow\nrecognition rate, resulting in significant performance losses. As a result, an\nimproved and accurate detection approach is required to address the above\ndifficulty. The whole study proposes a combination of spiked and normal\nconvolution layers as an energy-efficient and reliable object detector model.\nThe proposed model is split into two sections. The first section is developed\nas a feature extractor, which utilizes pre-trained VGG16, and the second\nsection of the proposal structure is the combination of spiked and normal\nConvolutional layers to detect the bounding boxes of images. We drew a\npre-trained model for classifying detected objects. With state of the art\nPython libraries, spike layers can be trained efficiently. The proposed spike\nconvolutional object detector (SCOD) has been evaluated on VOC and Ex-Dark\ndatasets. SCOD reached 66.01% and 41.25% mAP for detecting 20 different objects\nin the VOC-12 and 12 objects in the Ex-Dark dataset. SCOD uses 14 Giga FLOPS\nfor its forward path calculations. Experimental results indicated superior\nperformance compared to Tiny YOLO, Spike YOLO, YOLO-LITE, Tinier YOLO and\nCenter of loc+Xception based on mAP for the VOC dataset.\n","authors":["Munawar Ali","Baoqun Yin","Hazrat Bilal","Aakash Kumar","Ali Muhammad","Avinash Rohra"],"pdf_url":"https://arxiv.org/pdf/2310.06370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.13976v6","updated":"2023-10-10T07:18:46Z","published":"2022-11-25T09:38:22Z","title":"Expanding Small-Scale Datasets with Guided Imagination","summary":"  The power of DNNs relies heavily on the quantity and quality of training\ndata. However, collecting and annotating data on a large scale is often\nexpensive and time-consuming. To address this issue, we explore a new task,\ntermed dataset expansion, aimed at expanding a ready-to-use small dataset by\nautomatically creating new labeled samples. To this end, we present a Guided\nImagination Framework (GIF) that leverages cutting-edge generative models like\nDALL-E2 and Stable Diffusion (SD) to \"imagine\" and create informative new data\nfrom the input seed data. Specifically, GIF conducts data imagination by\noptimizing the latent features of the seed data in the semantically meaningful\nspace of the prior model, resulting in the creation of photo-realistic images\nwith new content. To guide the imagination towards creating informative samples\nfor model training, we introduce two key criteria, i.e., class-maintained\ninformation boosting and sample diversity promotion. These criteria are\nverified to be essential for effective dataset expansion: GIF-SD obtains 13.5%\nhigher model accuracy on natural image datasets than unguided expansion with\nSD. With these essential criteria, GIF successfully expands small datasets in\nvarious scenarios, boosting model accuracy by 36.9% on average over six natural\nimage datasets and by 13.5% on average over three medical datasets. The source\ncode is available at https://github.com/Vanint/DatasetExpansion.\n","authors":["Yifan Zhang","Daquan Zhou","Bryan Hooi","Kai Wang","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2211.13976v6.pdf","comment":"NeurIPS 2023. Source code: https://github.com/Vanint/DatasetExpansion"},{"id":"http://arxiv.org/abs/2310.06368v1","updated":"2023-10-10T07:08:49Z","published":"2023-10-10T07:08:49Z","title":"CoinSeg: Contrast Inter- and Intra- Class Representations for\n  Incremental Segmentation","summary":"  Class incremental semantic segmentation aims to strike a balance between the\nmodel's stability and plasticity by maintaining old knowledge while adapting to\nnew concepts. However, most state-of-the-art methods use the freeze strategy\nfor stability, which compromises the model's plasticity.In contrast, releasing\nparameter training for plasticity could lead to the best performance for all\ncategories, but this requires discriminative feature representation.Therefore,\nwe prioritize the model's plasticity and propose the Contrast inter- and\nintra-class representations for Incremental Segmentation (CoinSeg), which\npursues discriminative representations for flexible parameter tuning. Inspired\nby the Gaussian mixture model that samples from a mixture of Gaussian\ndistributions, CoinSeg emphasizes intra-class diversity with multiple\ncontrastive representation centroids. Specifically, we use mask proposals to\nidentify regions with strong objectness that are likely to be diverse\ninstances/centroids of a category. These mask proposals are then used for\ncontrastive representations to reinforce intra-class diversity. Meanwhile, to\navoid bias from intra-class diversity, we also apply category-level\npseudo-labels to enhance category-level consistency and inter-category\ndiversity. Additionally, CoinSeg ensures the model's stability and alleviates\nforgetting through a specific flexible tuning strategy. We validate CoinSeg on\nPascal VOC 2012 and ADE20K datasets with multiple incremental scenarios and\nachieve superior results compared to previous state-of-the-art methods,\nespecially in more challenging and realistic long-term scenarios. Code is\navailable at https://github.com/zkzhang98/CoinSeg.\n","authors":["Zekang Zhang","Guangyu Gao","Jianbo Jiao","Chi Harold Liu","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2310.06368v1.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2305.14829v3","updated":"2023-10-10T06:38:00Z","published":"2023-05-24T07:34:49Z","title":"Deakin RF-Sensing: Experiments on Correlated Knowledge Distillation for\n  Monitoring Human Postures with Radios","summary":"  In this work, we propose and develop a simple experimental testbed to study\nthe feasibility of a novel idea by coupling radio frequency (RF) sensing\ntechnology with Correlated Knowledge Distillation (CKD) theory towards\ndesigning lightweight, near real-time and precise human pose monitoring\nsystems. The proposed CKD framework transfers and fuses pose knowledge from a\nrobust \"Teacher\" model to a parameterized \"Student\" model, which can be a\npromising technique for obtaining accurate yet lightweight pose estimates. To\nassure its efficacy, we implemented CKD for distilling logits in our integrated\nSoftware Defined Radio (SDR)-based experimental setup and investigated the\nRF-visual signal correlation. Our CKD-RF sensing technique is characterized by\ntwo modes - a camera-fed Teacher Class Network (e.g., images, videos) with an\nSDR-fed Student Class Network (e.g., RF signals). Specifically, our CKD model\ntrains a dual multi-branch teacher and student network by distilling and fusing\nknowledge bases. The resulting CKD models are then subsequently used to\nidentify the multimodal correlation and teach the student branch in reverse.\nInstead of simply aggregating their learnings, CKD training comprised multiple\nparallel transformations with the two domains, i.e., visual images and RF\nsignals. Once trained, our CKD model can efficiently preserve privacy and\nutilize the multimodal correlated logits from the two different neural networks\nfor estimating poses without using visual signals/video frames (by using only\nthe RF signals).\n","authors":["Shiva Raj Pokhrel","Jonathan Kua","Deol Satish","Philip Williams","Arkady Zaslavsky","Seng W. Loke","Jinho Choi"],"pdf_url":"https://arxiv.org/pdf/2305.14829v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06351v1","updated":"2023-10-10T06:37:03Z","published":"2023-10-10T06:37:03Z","title":"Fire Detection From Image and Video Using YOLOv5","summary":"  For the detection of fire-like targets in indoor, outdoor and forest fire\nimages, as well as fire detection under different natural lights, an improved\nYOLOv5 fire detection deep learning algorithm is proposed. The YOLOv5 detection\nmodel expands the feature extraction network from three dimensions, which\nenhances feature propagation of fire small targets identification, improves\nnetwork performance, and reduces model parameters. Furthermore, through the\npromotion of the feature pyramid, the top-performing prediction box is\nobtained. Fire-YOLOv5 attains excellent results compared to state-of-the-art\nobject detection networks, notably in the detection of small targets of fire\nand smoke with mAP 90.5% and f1 score 88%. Overall, the Fire-YOLOv5 detection\nmodel can effectively deal with the inspection of small fire targets, as well\nas fire-like and smoke-like objects with F1 score 0.88. When the input image\nsize is 416 x 416 resolution, the average detection time is 0.12 s per frame,\nwhich can provide real-time forest fire detection. Moreover, the algorithm\nproposed in this paper can also be applied to small target detection under\nother complicated situations. The proposed system shows an improved approach in\nall fire detection metrics such as precision, recall, and mean average\nprecision.\n","authors":["Arafat Islam","Md. Imtiaz Habib"],"pdf_url":"https://arxiv.org/pdf/2310.06351v1.pdf","comment":"6 pages, 6 sections, unpublished paper"},{"id":"http://arxiv.org/abs/2310.06347v1","updated":"2023-10-10T06:32:24Z","published":"2023-10-10T06:32:24Z","title":"JointNet: Extending Text-to-Image Diffusion for Dense Distribution\n  Modeling","summary":"  We introduce JointNet, a novel neural network architecture for modeling the\njoint distribution of images and an additional dense modality (e.g., depth\nmaps). JointNet is extended from a pre-trained text-to-image diffusion model,\nwhere a copy of the original network is created for the new dense modality\nbranch and is densely connected with the RGB branch. The RGB branch is locked\nduring network fine-tuning, which enables efficient learning of the new\nmodality distribution while maintaining the strong generalization ability of\nthe large-scale pre-trained diffusion model. We demonstrate the effectiveness\nof JointNet by using RGBD diffusion as an example and through extensive\nexperiments, showcasing its applicability in a variety of applications,\nincluding joint RGBD generation, dense depth prediction, depth-conditioned\nimage generation, and coherent tile-based 3D panorama generation.\n","authors":["Jingyang Zhang","Shiwei Li","Yuanxun Lu","Tian Fang","David McKinnon","Yanghai Tsin","Long Quan","Yao Yao"],"pdf_url":"https://arxiv.org/pdf/2310.06347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06344v1","updated":"2023-10-10T06:27:30Z","published":"2023-10-10T06:27:30Z","title":"Filter Pruning For CNN With Enhanced Linear Representation Redundancy","summary":"  Structured network pruning excels non-structured methods because they can\ntake advantage of the thriving developed parallel computing techniques. In this\npaper, we propose a new structured pruning method. Firstly, to create more\nstructured redundancy, we present a data-driven loss function term calculated\nfrom the correlation coefficient matrix of different feature maps in the same\nlayer, named CCM-loss. This loss term can encourage the neural network to learn\nstronger linear representation relations between feature maps during the\ntraining from the scratch so that more homogenous parts can be removed later in\npruning. CCM-loss provides us with another universal transcendental\nmathematical tool besides L*-norm regularization, which concentrates on\ngenerating zeros, to generate more redundancy but for the different genres.\nFurthermore, we design a matching channel selection strategy based on principal\ncomponents analysis to exploit the maximum potential ability of CCM-loss. In\nour new strategy, we mainly focus on the consistency and integrality of the\ninformation flow in the network. Instead of empirically hard-code the retain\nratio for each layer, our channel selection strategy can dynamically adjust\neach layer's retain ratio according to the specific circumstance of a\nper-trained model to push the prune ratio to the limit. Notably, on the\nCifar-10 dataset, our method brings 93.64% accuracy for pruned VGG-16 with only\n1.40M parameters and 49.60M FLOPs, the pruned ratios for parameters and FLOPs\nare 90.6% and 84.2%, respectively. For ResNet-50 trained on the ImageNet\ndataset, our approach achieves 42.8% and 47.3% storage and computation\nreductions, respectively, with an accuracy of 76.23%. Our code is available at\nhttps://github.com/Bojue-Wang/CCM-LRR.\n","authors":["Bojue Wang","Chunmei Ma","Bin Liu","Nianbo Liu","Jinqi Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.06344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04741v2","updated":"2023-10-10T06:22:45Z","published":"2023-10-07T08:54:43Z","title":"Balancing stability and plasticity in continual learning: the\n  readout-decomposition of activation change (RDAC) framework","summary":"  Continual learning (CL) algorithms strive to acquire new knowledge while\npreserving prior information. However, this stability-plasticity trade-off\nremains a central challenge. This paper introduces a framework that dissects\nthis trade-off, offering valuable insights into CL algorithms. The\nReadout-Decomposition of Activation Change (RDAC) framework first addresses the\nstability-plasticity dilemma and its relation to catastrophic forgetting. It\nrelates learning-induced activation changes in the range of prior readouts to\nthe degree of stability and changes in the null space to the degree of\nplasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the\nframework clarifies the stability-plasticity trade-offs of the popular\nregularization algorithms Synaptic intelligence (SI), Elastic-weight\nconsolidation (EWC), and learning without Forgetting (LwF), and replay-based\nalgorithms Gradient episodic memory (GEM), and data replay. GEM and data replay\npreserved stability and plasticity, while SI, EWC, and LwF traded off\nplasticity for stability. The inability of the regularization algorithms to\nmaintain plasticity was linked to them restricting the change of activations in\nthe null space of the prior readout. Additionally, for one-hidden-layer linear\nneural networks, we derived a gradient decomposition algorithm to restrict\nactivation change only in the range of the prior readouts, to maintain high\nstability while not further sacrificing plasticity. Results demonstrate that\nthe algorithm maintained stability without significant plasticity loss. The\nRDAC framework informs the behavior of existing CL algorithms and paves the way\nfor novel CL approaches. Finally, it sheds light on the connection between\nlearning-induced activation/representation changes and the stability-plasticity\ndilemma, also offering insights into representational drift in biological\nsystems.\n","authors":["Daniel Anthes","Sushrut Thorat","Peter König","Tim C. Kietzmann"],"pdf_url":"https://arxiv.org/pdf/2310.04741v2.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.06337v1","updated":"2023-10-10T06:13:09Z","published":"2023-10-10T06:13:09Z","title":"Local Style Awareness of Font Images","summary":"  When we compare fonts, we often pay attention to styles of local parts, such\nas serifs and curvatures. This paper proposes an attention mechanism to find\nimportant local parts. The local parts with larger attention are then\nconsidered important. The proposed mechanism can be trained in a\nquasi-self-supervised manner that requires no manual annotation other than\nknowing that a set of character images is from the same font, such as\nHelvetica. After confirming that the trained attention mechanism can find\nstyle-relevant local parts, we utilize the resulting attention for local\nstyle-aware font generation. Specifically, we design a new reconstruction loss\nfunction to put more weight on the local parts with larger attention for\ngenerating character images with more accurate style realization. This loss\nfunction has the merit of applicability to various font generation models. Our\nexperimental results show that the proposed loss function improves the quality\nof generated character images by several few-shot font generation models.\n","authors":["Daichi Haraguchi","Seiichi Uchida"],"pdf_url":"https://arxiv.org/pdf/2310.06337v1.pdf","comment":"Accepted at ICDAR WML 2023"},{"id":"http://arxiv.org/abs/2310.06332v1","updated":"2023-10-10T06:03:39Z","published":"2023-10-10T06:03:39Z","title":"CrowdRec: 3D Crowd Reconstruction from Single Color Images","summary":"  This is a technical report for the GigaCrowd challenge. Reconstructing 3D\ncrowds from monocular images is a challenging problem due to mutual occlusions,\nserver depth ambiguity, and complex spatial distribution. Since no large-scale\n3D crowd dataset can be used to train a robust model, the current multi-person\nmesh recovery methods can hardly achieve satisfactory performance in crowded\nscenes. In this paper, we exploit the crowd features and propose a\ncrowd-constrained optimization to improve the common single-person method on\ncrowd images. To avoid scale variations, we first detect human bounding-boxes\nand 2D poses from the original images with off-the-shelf detectors. Then, we\ntrain a single-person mesh recovery network using existing in-the-wild image\ndatasets. To promote a more reasonable spatial distribution, we further propose\na crowd constraint to refine the single-person network parameters. With the\noptimization, we can obtain accurate body poses and shapes with reasonable\nabsolute positions from a large-scale crowd image using a single-person\nbackbone. The code will be publicly available\nat~\\url{https://github.com/boycehbz/CrowdRec}.\n","authors":["Buzhen Huang","Jingyi Ju","Yangang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06332v1.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2309.09464v3","updated":"2023-10-10T05:59:43Z","published":"2023-09-18T03:55:41Z","title":"Reducing Adversarial Training Cost with Gradient Approximation","summary":"  Deep learning models have achieved state-of-the-art performances in various\ndomains, while they are vulnerable to the inputs with well-crafted but small\nperturbations, which are named after adversarial examples (AEs). Among many\nstrategies to improve the model robustness against AEs, Projected Gradient\nDescent (PGD) based adversarial training is one of the most effective methods.\nUnfortunately, the prohibitive computational overhead of generating strong\nenough AEs, due to the maximization of the loss function, sometimes makes the\nregular PGD adversarial training impractical when using larger and more\ncomplicated models. In this paper, we propose that the adversarial loss can be\napproximated by the partial sum of Taylor series. Furthermore, we approximate\nthe gradient of adversarial loss and propose a new and efficient adversarial\ntraining method, adversarial training with gradient approximation (GAAT), to\nreduce the cost of building up robust models. Additionally, extensive\nexperiments demonstrate that this efficiency improvement can be achieved\nwithout any or with very little loss in accuracy on natural and adversarial\nexamples, which show that our proposed method saves up to 60\\% of the training\ntime with comparable model test accuracy on MNIST, CIFAR-10 and CIFAR-100\ndatasets.\n","authors":["Huihui Gong"],"pdf_url":"https://arxiv.org/pdf/2309.09464v3.pdf","comment":"The experiments are insufficient, later will be updated. Withraw this\n  manuscript"},{"id":"http://arxiv.org/abs/2310.06329v1","updated":"2023-10-10T05:54:04Z","published":"2023-10-10T05:54:04Z","title":"Precise Payload Delivery via Unmanned Aerial Vehicles: An Approach Using\n  Object Detection Algorithms","summary":"  Recent years have seen tremendous advancements in the area of autonomous\npayload delivery via unmanned aerial vehicles, or drones. However, most of\nthese works involve delivering the payload at a predetermined location using\nits GPS coordinates. By relying on GPS coordinates for navigation, the\nprecision of payload delivery is restricted to the accuracy of the GPS network\nand the availability and strength of the GPS connection, which may be severely\nrestricted by the weather condition at the time and place of operation. In this\nwork we describe the development of a micro-class UAV and propose a novel\nnavigation method that improves the accuracy of conventional navigation methods\nby incorporating a deep-learning-based computer vision approach to identify and\nprecisely align the UAV with a target marked at the payload delivery position.\nThis proposed method achieves a 500% increase in average horizontal precision\nover conventional GPS-based approaches.\n","authors":["Aditya Vadduri","Anagh Benjwal","Abhishek Pai","Elkan Quadros","Aniruddh Kammar","Prajwal Uday"],"pdf_url":"https://arxiv.org/pdf/2310.06329v1.pdf","comment":"Second International Conference on Artificial Intelligence,\n  Computational Electronics and Communication System (AICECS 2023)"},{"id":"http://arxiv.org/abs/2310.05255v2","updated":"2023-10-10T05:48:25Z","published":"2023-10-08T18:07:15Z","title":"Persis: A Persian Font Recognition Pipeline Using Convolutional Neural\n  Networks","summary":"  What happens if we encounter a suitable font for our design work but do not\nknow its name? Visual Font Recognition (VFR) systems are used to identify the\nfont typeface in an image. These systems can assist graphic designers in\nidentifying fonts used in images. A VFR system also aids in improving the speed\nand accuracy of Optical Character Recognition (OCR) systems. In this paper, we\nintroduce the first publicly available datasets in the field of Persian font\nrecognition and employ Convolutional Neural Networks (CNN) to address this\nproblem. The results show that the proposed pipeline obtained 78.0% top-1\naccuracy on our new datasets, 89.1% on the IDPL-PFOD dataset, and 94.5% on the\nKAFD dataset. Furthermore, the average time spent in the entire pipeline for\none sample of our proposed datasets is 0.54 and 0.017 seconds for CPU and GPU,\nrespectively. We conclude that CNN methods can be used to recognize Persian\nfonts without the need for additional pre-processing steps such as feature\nextraction, binarization, normalization, etc.\n","authors":["Mehrdad Mohammadian","Neda Maleki","Tobias Olsson","Fredrik Ahlgren"],"pdf_url":"https://arxiv.org/pdf/2310.05255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09359v2","updated":"2023-10-10T05:31:01Z","published":"2022-11-17T06:18:45Z","title":"How to Fine-Tune Vision Models with SGD","summary":"  SGD and AdamW are the two most used optimizers for fine-tuning large neural\nnetworks in computer vision. When the two methods perform the same, SGD is\npreferable because it uses less memory (12 bytes/parameter with momentum and 8\nbytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite\nof downstream tasks, especially those with distribution shifts, we find that\nfine-tuning with AdamW performs substantially better than SGD on modern Vision\nTransformer and ConvNeXt models. We find that large gaps in performance between\nSGD and AdamW occur when the fine-tuning gradients in the first \"embedding\"\nlayer are much larger than in the rest of the model. Our analysis suggests an\neasy fix that works consistently across datasets and models: freezing the\nembedding layer (less than 1% of the parameters) leads to SGD with or without\nmomentum performing slightly better than AdamW while using less memory (e.g.,\non ViT-L, SGD uses 33% less GPU memory). Our insights result in\nstate-of-the-art accuracies on five popular distribution shift benchmarks:\nWILDS-FMoW, WILDS-Camelyon, BREEDS-Living-17, Waterbirds, and DomainNet.\n","authors":["Ananya Kumar","Ruoqi Shen","Sebastien Bubeck","Suriya Gunasekar"],"pdf_url":"https://arxiv.org/pdf/2211.09359v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05863v2","updated":"2023-10-10T05:30:49Z","published":"2023-10-09T17:00:20Z","title":"Fine-grained Audio-Visual Joint Representations for Multimodal Large\n  Language Models","summary":"  Audio-visual large language models (LLM) have drawn significant attention,\nyet the fine-grained combination of both input streams is rather\nunder-explored, which is challenging but necessary for LLMs to understand\ngeneral video inputs. To this end, a fine-grained audio-visual joint\nrepresentation (FAVOR) learning framework for multimodal LLMs is proposed in\nthis paper, which extends a text-based LLM to simultaneously perceive speech\nand audio events in the audio input stream and images or videos in the visual\ninput stream, at the frame level. To fuse the audio and visual feature streams\ninto joint representations and to align the joint space with the LLM input\nembedding space, we propose a causal Q-Former structure with a causal attention\nmodule to enhance the capture of causal relations of the audio-visual frames\nacross time. An audio-visual evaluation benchmark (AVEB) is also proposed which\ncomprises six representative single-modal tasks with five cross-modal tasks\nreflecting audio-visual co-reasoning abilities. While achieving competitive\nsingle-modal performance on audio, speech and image tasks in AVEB, FAVOR\nachieved over 20% accuracy improvements on the video question-answering task\nwhen fine-grained information or temporal causal reasoning is required. FAVOR,\nin addition, demonstrated remarkable video comprehension and reasoning\nabilities on tasks that are unprecedented by other multimodal LLMs. An\ninteractive demo of FAVOR is available at\nhttps://github.com/BriansIDP/AudioVisualLLM.git, and the training code and\nmodel checkpoints will be released soon.\n","authors":["Guangzhi Sun","Wenyi Yu","Changli Tang","Xianzhao Chen","Tian Tan","Wei Li","Lu Lu","Zejun Ma","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06318v1","updated":"2023-10-10T05:28:02Z","published":"2023-10-10T05:28:02Z","title":"Adversarial Masked Image Inpainting for Robust Detection of Mpox and\n  Non-Mpox","summary":"  Due to the lack of efficient mpox diagnostic technology, mpox cases continue\nto increase. Recently, the great potential of deep learning models in detecting\nmpox and non-mpox has been proven. However, existing models learn image\nrepresentations via image classification, which results in they may be easily\nsusceptible to interference from real-world noise, require diverse non-mpox\nimages, and fail to detect abnormal input. These drawbacks make classification\nmodels inapplicable in real-world settings. To address these challenges, we\npropose \"Mask, Inpainting, and Measure\" (MIM). In MIM's pipeline, a generative\nadversarial network only learns mpox image representations by inpainting the\nmasked mpox images. Then, MIM determines whether the input belongs to mpox by\nmeasuring the similarity between the inpainted image and the original image.\nThe underlying intuition is that since MIM solely models mpox images, it\nstruggles to accurately inpaint non-mpox images in real-world settings. Without\nutilizing any non-mpox images, MIM cleverly detects mpox and non-mpox and can\nhandle abnormal inputs. We used the recognized mpox dataset (MSLD) and images\nof eighteen non-mpox skin diseases to verify the effectiveness and robustness\nof MIM. Experimental results show that the average AUROC of MIM achieves\n0.8237. In addition, we demonstrated the drawbacks of classification models and\nbuttressed the potential of MIM through clinical validation. Finally, we\ndeveloped an online smartphone app to provide free testing to the public in\naffected areas. This work first employs generative models to improve mpox\ndetection and provides new insights into binary decision-making tasks in\nmedical images.\n","authors":["Yubiao Yue","Zhenzhang Li"],"pdf_url":"https://arxiv.org/pdf/2310.06318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06313v1","updated":"2023-10-10T05:13:17Z","published":"2023-10-10T05:13:17Z","title":"Advancing Pose-Guided Image Synthesis with Progressive Conditional\n  Diffusion Models","summary":"  Recent work has showcased the significant potential of diffusion models in\npose-guided person image synthesis. However, owing to the inconsistency in pose\nbetween the source and target images, synthesizing an image with a distinct\npose, relying exclusively on the source image and target pose information,\nremains a formidable challenge. This paper presents Progressive Conditional\nDiffusion Models (PCDMs) that incrementally bridge the gap between person\nimages under the target and source poses through three stages. Specifically, in\nthe first stage, we design a simple prior conditional diffusion model that\npredicts the global features of the target image by mining the global alignment\nrelationship between pose coordinates and image appearance. Then, the second\nstage establishes a dense correspondence between the source and target images\nusing the global features from the previous stage, and an inpainting\nconditional diffusion model is proposed to further align and enhance the\ncontextual features, generating a coarse-grained person image. In the third\nstage, we propose a refining conditional diffusion model to utilize the\ncoarsely generated image from the previous stage as a condition, achieving\ntexture restoration and enhancing fine-detail consistency. The three-stage\nPCDMs work progressively to generate the final high-quality and high-fidelity\nsynthesized image. Both qualitative and quantitative results demonstrate the\nconsistency and photorealism of our proposed PCDMs under challenging\nscenarios.The code and model will be available at\nhttps://github.com/muzishen/PCDMs.\n","authors":["Fei Shen","Hu Ye","Jun Zhang","Cong Wang","Xiao Han","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2310.06313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06311v1","updated":"2023-10-10T05:09:05Z","published":"2023-10-10T05:09:05Z","title":"Improving Compositional Text-to-image Generation with Large\n  Vision-Language Models","summary":"  Recent advancements in text-to-image models, particularly diffusion models,\nhave shown significant promise. However, compositional text-to-image models\nfrequently encounter difficulties in generating high-quality images that\naccurately align with input texts describing multiple objects, variable\nattributes, and intricate spatial relationships. To address this limitation, we\nemploy large vision-language models (LVLMs) for multi-dimensional assessment of\nthe alignment between generated images and their corresponding input texts.\nUtilizing this assessment, we fine-tune the diffusion model to enhance its\nalignment capabilities. During the inference phase, an initial image is\nproduced using the fine-tuned diffusion model. The LVLM is then employed to\npinpoint areas of misalignment in the initial image, which are subsequently\ncorrected using the image editing algorithm until no further misalignments are\ndetected by the LVLM. The resultant image is consequently more closely aligned\nwith the input text. Our experimental results validate that the proposed\nmethodology significantly improves text-image alignment in compositional image\ngeneration, particularly with respect to object number, attribute binding,\nspatial relationships, and aesthetic quality.\n","authors":["Song Wen","Guian Fang","Renrui Zhang","Peng Gao","Hao Dong","Dimitris Metaxas"],"pdf_url":"https://arxiv.org/pdf/2310.06311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05720v2","updated":"2023-10-10T05:00:48Z","published":"2023-10-09T13:45:21Z","title":"HyperLips: Hyper Control Lips with High Resolution Decoder for Talking\n  Face Generation","summary":"  Talking face generation has a wide range of potential applications in the\nfield of virtual digital humans. However, rendering high-fidelity facial video\nwhile ensuring lip synchronization is still a challenge for existing\naudio-driven talking face generation approaches. To address this issue, we\npropose HyperLips, a two-stage framework consisting of a hypernetwork for\ncontrolling lips and a high-resolution decoder for rendering high-fidelity\nfaces. In the first stage, we construct a base face generation network that\nuses the hypernetwork to control the encoding latent code of the visual face\ninformation over audio. First, FaceEncoder is used to obtain latent code by\nextracting features from the visual face information taken from the video\nsource containing the face frame.Then, HyperConv, which weighting parameters\nare updated by HyperNet with the audio features as input, will modify the\nlatent code to synchronize the lip movement with the audio. Finally,\nFaceDecoder will decode the modified and synchronized latent code into visual\nface content. In the second stage, we obtain higher quality face videos through\na high-resolution decoder. To further improve the quality of face generation,\nwe trained a high-resolution decoder, HRDecoder, using face images and detected\nsketches generated from the first stage as input.Extensive quantitative and\nqualitative experiments show that our method outperforms state-of-the-art work\nwith more realistic, high-fidelity, and lip synchronization. Project page:\nhttps://semchan.github.io/HyperLips/\n","authors":["Yaosen Chen","Yu Yao","Zhiqiang Li","Wei Wang","Yanru Zhang","Han Yang","Xuming Wen"],"pdf_url":"https://arxiv.org/pdf/2310.05720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12493v4","updated":"2023-10-10T04:23:11Z","published":"2023-07-24T02:50:44Z","title":"TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition","summary":"  Text-driven diffusion models have exhibited impressive generative\ncapabilities, enabling various image editing tasks. In this paper, we propose\nTF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the\npower of text-driven diffusion models for cross-domain image-guided\ncomposition. This task aims to seamlessly integrate user-provided objects into\na specific visual context. Current diffusion-based methods often involve costly\ninstance-based optimization or finetuning of pretrained models on customized\ndatasets, which can potentially undermine their rich prior. In contrast,\nTF-ICON can leverage off-the-shelf diffusion models to perform cross-domain\nimage-guided composition without requiring additional training, finetuning, or\noptimization. Moreover, we introduce the exceptional prompt, which contains no\ninformation, to facilitate text-driven diffusion models in accurately inverting\nreal images into latent representations, forming the basis for compositing. Our\nexperiments show that equipping Stable Diffusion with the exceptional prompt\noutperforms state-of-the-art inversion methods on various datasets (CelebA-HQ,\nCOCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile\nvisual domains. Code is available at https://github.com/Shilin-LU/TF-ICON\n","authors":["Shilin Lu","Yanzhu Liu","Adams Wai-Kin Kong"],"pdf_url":"https://arxiv.org/pdf/2307.12493v4.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2310.04991v2","updated":"2023-10-10T04:20:57Z","published":"2023-10-08T03:35:27Z","title":"Video-Teller: Enhancing Cross-Modal Generation with Fusion and\n  Decoupling","summary":"  This paper proposes Video-Teller, a video-language foundation model that\nleverages multi-modal fusion and fine-grained modality alignment to\nsignificantly enhance the video-to-text generation task. Video-Teller boosts\nthe training efficiency by utilizing frozen pretrained vision and language\nmodules. It capitalizes on the robust linguistic capabilities of large language\nmodels, enabling the generation of both concise and elaborate video\ndescriptions. To effectively integrate visual and auditory information,\nVideo-Teller builds upon the image-based BLIP-2 model and introduces a cascaded\nQ-Former which fuses information across frames and ASR texts. To better guide\nvideo summarization, we introduce a fine-grained modality alignment objective,\nwhere the cascaded Q-Former's output embedding is trained to align with the\ncaption/summary embedding created by a pretrained text auto-encoder.\nExperimental results demonstrate the efficacy of our proposed video-language\nfoundation model in accurately comprehending videos and generating coherent and\nprecise language descriptions. It is worth noting that the fine-grained\nalignment enhances the model's capabilities (4% improvement of CIDEr score on\nMSR-VTT) with only 13% extra parameters in training and zero additional cost in\ninference.\n","authors":["Haogeng Liu","Qihang Fan","Tingkai Liu","Linjie Yang","Yunzhe Tao","Huaibo Huang","Ran He","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04991v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06291v1","updated":"2023-10-10T04:10:56Z","published":"2023-10-10T04:10:56Z","title":"Three-Dimensional Medical Image Fusion with Deformable Cross-Attention","summary":"  Multimodal medical image fusion plays an instrumental role in several areas\nof medical image processing, particularly in disease recognition and tumor\ndetection. Traditional fusion methods tend to process each modality\nindependently before combining the features and reconstructing the fusion\nimage. However, this approach often neglects the fundamental commonalities and\ndisparities between multimodal information. Furthermore, the prevailing\nmethodologies are largely confined to fusing two-dimensional (2D) medical image\nslices, leading to a lack of contextual supervision in the fusion images and\nsubsequently, a decreased information yield for physicians relative to\nthree-dimensional (3D) images. In this study, we introduce an innovative\nunsupervised feature mutual learning fusion network designed to rectify these\nlimitations. Our approach incorporates a Deformable Cross Feature Blend (DCFB)\nmodule that facilitates the dual modalities in discerning their respective\nsimilarities and differences. We have applied our model to the fusion of 3D MRI\nand PET images obtained from 660 patients in the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) dataset. Through the application of the DCFB\nmodule, our network generates high-quality MRI-PET fusion images. Experimental\nresults demonstrate that our method surpasses traditional 2D image fusion\nmethods in performance metrics such as Peak Signal to Noise Ratio (PSNR) and\nStructural Similarity Index Measure (SSIM). Importantly, the capacity of our\nmethod to fuse 3D images enhances the information available to physicians and\nresearchers, thus marking a significant step forward in the field. The code\nwill soon be available online.\n","authors":["Lin Liu","Xinxin Fan","Chulong Zhang","Jingjing Dai","Yaoqin Xie","Xiaokun Liang"],"pdf_url":"https://arxiv.org/pdf/2310.06291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.00067v2","updated":"2023-10-10T04:01:38Z","published":"2023-04-28T19:37:17Z","title":"Unsupervised Discovery of 3D Hierarchical Structure with Generative\n  Diffusion Features","summary":"  Inspired by recent findings that generative diffusion models learn\nsemantically meaningful representations, we use them to discover the intrinsic\nhierarchical structure in biomedical 3D images using unsupervised segmentation.\nWe show that features of diffusion models from different stages of a\nU-Net-based ladder-like architecture capture different hierarchy levels in 3D\nbiomedical images. We design three losses to train a predictive unsupervised\nsegmentation network that encourages the decomposition of 3D volumes into\nmeaningful nested subvolumes that represent a hierarchy. First, we pretrain 3D\ndiffusion models and use the consistency of their features across subvolumes.\nSecond, we use the visual consistency between subvolumes. Third, we use the\ninvariance to photometric augmentations as a regularizer. Our models achieve\nbetter performance than prior unsupervised structure discovery approaches on\nchallenging biologically-inspired synthetic datasets and on a real-world brain\ntumor MRI dataset.\n","authors":["Nurislam Tursynbek","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2305.00067v2.pdf","comment":"MICCAI 2023"},{"id":"http://arxiv.org/abs/2308.06160v2","updated":"2023-10-10T03:59:41Z","published":"2023-08-11T14:38:11Z","title":"DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion\n  Models","summary":"  Current deep networks are very data-hungry and benefit from training on\nlargescale datasets, which are often time-consuming to collect and annotate. By\ncontrast, synthetic data can be generated infinitely using generative models\nsuch as DALL-E and diffusion models, with minimal effort and cost. In this\npaper, we present DatasetDM, a generic dataset generation model that can\nproduce diverse synthetic images and the corresponding high-quality perception\nannotations (e.g., segmentation masks, and depth). Our method builds upon the\npre-trained diffusion model and extends text-guided image synthesis to\nperception data generation. We show that the rich latent code of the diffusion\nmodel can be effectively decoded as accurate perception annotations using a\ndecoder module. Training the decoder only needs less than 1% (around 100\nimages) manually labeled images, enabling the generation of an infinitely large\nannotated dataset. Then these synthetic data can be used for training various\nperception models for downstream tasks. To showcase the power of the proposed\napproach, we generate datasets with rich dense pixel-wise labels for a wide\nrange of downstream tasks, including semantic segmentation, instance\nsegmentation, and depth estimation. Notably, it achieves 1) state-of-the-art\nresults on semantic segmentation and instance segmentation; 2) significantly\nmore robust on domain generalization than using the real data alone; and\nstate-of-the-art results in zero-shot segmentation setting; and 3) flexibility\nfor efficient application and novel task composition (e.g., image editing). The\nproject website and code can be found at\nhttps://weijiawu.github.io/DatasetDM_page/ and\nhttps://github.com/showlab/DatasetDM, respectively\n","authors":["Weijia Wu","Yuzhong Zhao","Hao Chen","Yuchao Gu","Rui Zhao","Yefei He","Hong Zhou","Mike Zheng Shou","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2308.06160v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09907v3","updated":"2023-10-10T03:55:14Z","published":"2023-02-20T11:08:07Z","title":"General Rotation Invariance Learning for Point Clouds via Weight-Feature\n  Alignment","summary":"  Compared to 2D images, 3D point clouds are much more sensitive to rotations.\nWe expect the point features describing certain patterns to keep invariant to\nthe rotation transformation. There are many recent SOTA works dedicated to\nrotation-invariant learning for 3D point clouds. However, current\nrotation-invariant methods lack generalizability on the point clouds in the\nopen scenes due to the reliance on the global distribution, \\ie the global\nscene and backgrounds. Considering that the output activation is a function of\nthe pattern and its orientation, we need to eliminate the effect of the\norientation.In this paper, inspired by the idea that the network weights can be\nconsidered a set of points distributed in the same 3D space as the input\npoints, we propose Weight-Feature Alignment (WFA) to construct a local\nInvariant Reference Frame (IRF) via aligning the features with the principal\naxes of the network weights. Our WFA algorithm provides a general solution for\nthe point clouds of all scenes. WFA ensures the model achieves the target that\nthe response activity is a necessary and sufficient condition of the pattern\nmatching degree. Practically, we perform experiments on the point clouds of\nboth single objects and open large-range scenes. The results suggest that our\nmethod almost bridges the gap between rotation invariance learning and normal\nmethods.\n","authors":["Liang Xie","Yibo Yang","Wenxiao Wang","Binbin Lin","Deng Cai","Xiaofei He","Ronghua Liang"],"pdf_url":"https://arxiv.org/pdf/2302.09907v3.pdf","comment":"4 figures"},{"id":"http://arxiv.org/abs/2310.01641v2","updated":"2023-10-10T03:50:28Z","published":"2023-10-02T21:09:43Z","title":"You Only Look at Once for Real-time and Generic Multi-Task","summary":"  High precision, lightweight, and real-time responsiveness are three essential\nrequirements for implementing autonomous driving. In this study, we present an\nadaptive, real-time, and lightweight multi-task model designed to concurrently\naddress object detection, drivable area segmentation, and lane line\nsegmentation tasks. Specifically, we developed an end-to-end multi-task model\nwith a unified and streamlined segmentation structure. We introduced a\nlearnable parameter that adaptively concatenate features in segmentation necks,\nusing the same loss function for all segmentation tasks. This eliminates the\nneed for customizations and enhances the model's generalization capabilities.\nWe also introduced a segmentation head composed only of a series of\nconvolutional layers, which reduces the inference time. We achieved competitive\nresults on the BDD100k dataset, particularly in visualization outcomes. The\nperformance results show a mAP50 of 81.1% for object detection, a mIoU of 91.0%\nfor drivable area segmentation, and an IoU of 28.8% for lane line segmentation.\nAdditionally, we introduced real-world scenarios to evaluate our model's\nperformance in a real scene, which significantly outperforms competitors. This\ndemonstrates that our model not only exhibits competitive performance but is\nalso more flexible and faster than existing multi-task models. The source codes\nand pre-trained models are released at\nhttps://github.com/JiayuanWang-JW/YOLOv8-multi-task\n","authors":["Jiayuan Wang","Q. M. Jonathan Wu","Ning Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.01641v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05718v3","updated":"2023-10-10T03:49:32Z","published":"2023-06-09T07:30:10Z","title":"Learning Domain-Aware Detection Head with Prompt Tuning","summary":"  Domain adaptive object detection (DAOD) aims to generalize detectors trained\non an annotated source domain to an unlabelled target domain. However, existing\nmethods focus on reducing the domain bias of the detection backbone by\ninferring a discriminative visual encoder, while ignoring the domain bias in\nthe detection head. Inspired by the high generalization of vision-language\nmodels (VLMs), applying a VLM as the robust detection backbone following a\ndomain-aware detection head is a reasonable way to learn the discriminative\ndetector for each domain, rather than reducing the domain bias in traditional\nmethods. To achieve the above issue, we thus propose a novel DAOD framework\nnamed Domain-Aware detection head with Prompt tuning (DA-Pro), which applies\nthe learnable domain-adaptive prompt to generate the dynamic detection head for\neach domain. Formally, the domain-adaptive prompt consists of the\ndomain-invariant tokens, domain-specific tokens, and the domain-related textual\ndescription along with the class label. Furthermore, two constraints between\nthe source and target domains are applied to ensure that the domain-adaptive\nprompt can capture the domains-shared and domain-specific knowledge. A prompt\nensemble strategy is also proposed to reduce the effect of prompt disturbance.\nComprehensive experiments over multiple cross-domain adaptation tasks\ndemonstrate that using the domain-adaptive prompt can produce an effectively\ndomain-related detection head for boosting domain-adaptive object detection.\nOur code is available at https://github.com/Therock90421/DA-Pro.\n","authors":["Haochen Li","Rui Zhang","Hantao Yao","Xinkai Song","Yifan Hao","Yongwei Zhao","Ling Li","Yunji Chen"],"pdf_url":"https://arxiv.org/pdf/2306.05718v3.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.04769v2","updated":"2023-10-10T03:48:32Z","published":"2023-10-07T10:25:50Z","title":"1st Place Solution of Egocentric 3D Hand Pose Estimation Challenge 2023\n  Technical Report:A Concise Pipeline for Egocentric Hand Pose Reconstruction","summary":"  This report introduce our work on Egocentric 3D Hand Pose Estimation\nworkshop. Using AssemblyHands, this challenge focuses on egocentric 3D hand\npose estimation from a single-view image. In the competition, we adopt ViT\nbased backbones and a simple regressor for 3D keypoints prediction, which\nprovides strong model baselines. We noticed that Hand-objects occlusions and\nself-occlusions lead to performance degradation, thus proposed a non-model\nmethod to merge multi-view results in the post-process stage. Moreover, We\nutilized test time augmentation and model ensemble to make further improvement.\nWe also found that public dataset and rational preprocess are beneficial. Our\nmethod achieved 12.21mm MPJPE on test dataset, achieve the first place in\nEgocentric 3D Hand Pose Estimation challenge.\n","authors":["Zhishan Zhou","Zhi Lv","Shihao Zhou","Minqiang Zou","Tong Wu","Mochen Yu","Yao Tang","Jiajun Liang"],"pdf_url":"https://arxiv.org/pdf/2310.04769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.05897v4","updated":"2023-10-10T03:42:57Z","published":"2022-06-13T04:03:49Z","title":"$\\texttt{GradICON}$: Approximate Diffeomorphisms via Gradient Inverse\n  Consistency","summary":"  We present an approach to learning regular spatial transformations between\nimage pairs in the context of medical image registration. Contrary to\noptimization-based registration techniques and many modern learning-based\nmethods, we do not directly penalize transformation irregularities but instead\npromote transformation regularity via an inverse consistency penalty. We use a\nneural network to predict a map between a source and a target image as well as\nthe map when swapping the source and target images. Different from existing\napproaches, we compose these two resulting maps and regularize deviations of\nthe $\\bf{Jacobian}$ of this composition from the identity matrix. This\nregularizer -- $\\texttt{GradICON}$ -- results in much better convergence when\ntraining registration models compared to promoting inverse consistency of the\ncomposition of maps directly while retaining the desirable implicit\nregularization effects of the latter. We achieve state-of-the-art registration\nperformance on a variety of real-world medical image datasets using a single\nset of hyperparameters and a single non-dataset-specific training protocol.\n","authors":["Lin Tian","Hastings Greer","François-Xavier Vialard","Roland Kwitt","Raúl San José Estépar","Richard Jarrett Rushmore","Nikolaos Makris","Sylvain Bouix","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2206.05897v4.pdf","comment":"29 pages, 16 figures, CVPR 2023"},{"id":"http://arxiv.org/abs/2310.05538v2","updated":"2023-10-10T03:41:51Z","published":"2023-10-09T09:01:53Z","title":"M3FPolypSegNet: Segmentation Network with Multi-frequency Feature Fusion\n  for Polyp Localization in Colonoscopy Images","summary":"  Polyp segmentation is crucial for preventing colorectal cancer a common type\nof cancer. Deep learning has been used to segment polyps automatically, which\nreduces the risk of misdiagnosis. Localizing small polyps in colonoscopy images\nis challenging because of its complex characteristics, such as color,\nocclusion, and various shapes of polyps. To address this challenge, a novel\nfrequency-based fully convolutional neural network, Multi-Frequency Feature\nFusion Polyp Segmentation Network (M3FPolypSegNet) was proposed to decompose\nthe input image into low/high/full-frequency components to use the\ncharacteristics of each component. We used three independent multi-frequency\nencoders to map multiple input images into a high-dimensional feature space. In\nthe Frequency-ASPP Scalable Attention Module (F-ASPP SAM), ASPP was applied\nbetween each frequency component to preserve scale information. Subsequently,\nscalable attention was applied to emphasize polyp regions in a high-dimensional\nfeature space. Finally, we designed three multi-task learning (i.e., region,\nedge, and distance) in four decoder blocks to learn the structural\ncharacteristics of the region. The proposed model outperformed various\nsegmentation models with performance gains of 6.92% and 7.52% on average for\nall metrics on CVC-ClinicDB and BKAI-IGH-NeoPolyp, respectively.\n","authors":["Ju-Hyeon Nam","Seo-Hyeong Park","Nur Suriza Syazwany","Yerim Jung","Yu-Han Im","Sang-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2310.05538v2.pdf","comment":"5pages. 2023 IEEE International Conference on Image Processing\n  (ICIP). IEEE, 2023"},{"id":"http://arxiv.org/abs/2310.06283v1","updated":"2023-10-10T03:34:31Z","published":"2023-10-10T03:34:31Z","title":"Towards More Efficient Depression Risk Recognition via Gait","summary":"  Depression, a highly prevalent mental illness, affects over 280 million\nindividuals worldwide. Early detection and timely intervention are crucial for\npromoting remission, preventing relapse, and alleviating the emotional and\nfinancial burdens associated with depression. However, patients with depression\noften go undiagnosed in the primary care setting. Unlike many physiological\nillnesses, depression lacks objective indicators for recognizing depression\nrisk, and existing methods for depression risk recognition are time-consuming\nand often encounter a shortage of trained medical professionals. The\ncorrelation between gait and depression risk has been empirically established.\nGait can serve as a promising objective biomarker, offering the advantage of\nefficient and convenient data collection. However, current methods for\nrecognizing depression risk based on gait have only been validated on small,\nprivate datasets, lacking large-scale publicly available datasets for research\npurposes. Additionally, these methods are primarily limited to hand-crafted\napproaches. Gait is a complex form of motion, and hand-crafted gait features\noften only capture a fraction of the intricate associations between gait and\ndepression risk. Therefore, this study first constructs a large-scale gait\ndatabase, encompassing over 1,200 individuals, 40,000 gait sequences, and\ncovering six perspectives and three types of attire. Two commonly used\npsychological scales are provided as depression risk annotations. Subsequently,\na deep learning-based depression risk recognition model is proposed, overcoming\nthe limitations of hand-crafted approaches. Through experiments conducted on\nthe constructed large-scale database, the effectiveness of the proposed method\nis validated, and numerous instructive insights are presented in the paper,\nhighlighting the significant potential of gait-based depression risk\nrecognition.\n","authors":["Min Ren","Muchan Tao","Xuecai Hu","Xiaotong Liu","Qiong Li","Yongzhen Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04999v2","updated":"2023-10-10T03:32:58Z","published":"2023-10-08T04:00:20Z","title":"Symmetrical Linguistic Feature Distillation with CLIP for Scene Text\n  Recognition","summary":"  In this paper, we explore the potential of the Contrastive Language-Image\nPretraining (CLIP) model in scene text recognition (STR), and establish a novel\nSymmetrical Linguistic Feature Distillation framework (named CLIP-OCR) to\nleverage both visual and linguistic knowledge in CLIP. Different from previous\nCLIP-based methods mainly considering feature generalization on visual\nencoding, we propose a symmetrical distillation strategy (SDS) that further\ncaptures the linguistic knowledge in the CLIP text encoder. By cascading the\nCLIP image encoder with the reversed CLIP text encoder, a symmetrical structure\nis built with an image-to-text feature flow that covers not only visual but\nalso linguistic information for distillation.Benefiting from the natural\nalignment in CLIP, such guidance flow provides a progressive optimization\nobjective from vision to language, which can supervise the STR feature\nforwarding process layer-by-layer.Besides, a new Linguistic Consistency Loss\n(LCL) is proposed to enhance the linguistic capability by considering\nsecond-order statistics during the optimization. Overall, CLIP-OCR is the first\nto design a smooth transition between image and text for the STR task.Extensive\nexperiments demonstrate the effectiveness of CLIP-OCR with 93.8% average\naccuracy on six popular STR benchmarks.Code will be available at\nhttps://github.com/wzx99/CLIPOCR.\n","authors":["Zixiao Wang","Hongtao Xie","Yuxin Wang","Jianjun Xu","Boqiang Zhang","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.04999v2.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.06282v1","updated":"2023-10-10T03:32:33Z","published":"2023-10-10T03:32:33Z","title":"MuseChat: A Conversational Music Recommendation System for Videos","summary":"  We introduce MuseChat, an innovative dialog-based music recommendation\nsystem. This unique platform not only offers interactive user engagement but\nalso suggests music tailored for input videos, so that users can refine and\npersonalize their music selections. In contrast, previous systems predominantly\nemphasized content compatibility, often overlooking the nuances of users'\nindividual preferences. For example, all the datasets only provide basic\nmusic-video pairings or such pairings with textual music descriptions. To\naddress this gap, our research offers three contributions. First, we devise a\nconversation-synthesis method that simulates a two-turn interaction between a\nuser and a recommendation system, which leverages pre-trained music tags and\nartist information. In this interaction, users submit a video to the system,\nwhich then suggests a suitable music piece with a rationale. Afterwards, users\ncommunicate their musical preferences, and the system presents a refined music\nrecommendation with reasoning. Second, we introduce a multi-modal\nrecommendation engine that matches music either by aligning it with visual cues\nfrom the video or by harmonizing visual information, feedback from previously\nrecommended music, and the user's textual input. Third, we bridge music\nrepresentations and textual data with a Large Language Model(Vicuna-7B). This\nalignment equips MuseChat to deliver music recommendations and their underlying\nreasoning in a manner resembling human communication. Our evaluations show that\nMuseChat surpasses existing state-of-the-art models in music retrieval tasks\nand pioneers the integration of the recommendation process within a natural\nlanguage framework.\n","authors":["Zhikang Dong","Bin Chen","Xiulong Liu","Pawel Polak","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04750v2","updated":"2023-10-10T03:22:31Z","published":"2023-10-07T09:10:28Z","title":"DiffNAS: Bootstrapping Diffusion Models by Prompting for Better\n  Architectures","summary":"  Diffusion models have recently exhibited remarkable performance on synthetic\ndata. After a diffusion path is selected, a base model, such as UNet, operates\nas a denoising autoencoder, primarily predicting noises that need to be\neliminated step by step. Consequently, it is crucial to employ a model that\naligns with the expected budgets to facilitate superior synthetic performance.\nIn this paper, we meticulously analyze the diffusion model and engineer a base\nmodel search approach, denoted \"DiffNAS\". Specifically, we leverage GPT-4 as a\nsupernet to expedite the search, supplemented with a search memory to enhance\nthe results. Moreover, we employ RFID as a proxy to promptly rank the\nexperimental outcomes produced by GPT-4. We also adopt a rapid-convergence\ntraining strategy to boost search efficiency. Rigorous experimentation\ncorroborates that our algorithm can augment the search efficiency by 2 times\nunder GPT-based scenarios, while also attaining a performance of 2.82 with 0.37\nimprovement in FID on CIFAR10 relative to the benchmark IDDPM algorithm.\n","authors":["Wenhao Li","Xiu Su","Shan You","Fei Wang","Chen Qian","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.04750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02736v3","updated":"2023-10-10T03:17:37Z","published":"2023-07-06T02:44:32Z","title":"An Uncertainty Aided Framework for Learning based Liver $T_1ρ$\n  Mapping and Analysis","summary":"  Objective: Quantitative $T_1\\rho$ imaging has potential for assessment of\nbiochemical alterations of liver pathologies. Deep learning methods have been\nemployed to accelerate quantitative $T_1\\rho$ imaging. To employ artificial\nintelligence-based quantitative imaging methods in complicated clinical\nenvironment, it is valuable to estimate the uncertainty of the predicated\n$T_1\\rho$ values to provide the confidence level of the quantification results.\nThe uncertainty should also be utilized to aid the post-hoc quantitative\nanalysis and model learning tasks. Approach: To address this need, we propose a\nparametric map refinement approach for learning-based $T_1\\rho$ mapping and\ntrain the model in a probabilistic way to model the uncertainty. We also\npropose to utilize the uncertainty map to spatially weight the training of an\nimproved $T_1\\rho$ mapping network to further improve the mapping performance\nand to remove pixels with unreliable $T_1\\rho$ values in the region of\ninterest. The framework was tested on a dataset of 51 patients with different\nliver fibrosis stages. Main results: Our results indicate that the\nlearning-based map refinement method leads to a relative mapping error of less\nthan 3% and provides uncertainty estimation simultaneously. The estimated\nuncertainty reflects the actual error level, and it can be used to further\nreduce relative $T_1\\rho$ mapping error to 2.60% as well as removing unreliable\npixels in the region of interest effectively. Significance: Our studies\ndemonstrate the proposed approach has potential to provide a learning-based\nquantitative MRI system for trustworthy $T_1\\rho$ mapping of the liver.\n","authors":["Chaoxing Huang","Vincent Wai Sun Wong","Queenie Chan","Winnie Chiu Wing Chu","Weitian Chen"],"pdf_url":"https://arxiv.org/pdf/2307.02736v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05916v2","updated":"2023-10-10T03:14:37Z","published":"2023-10-09T17:59:04Z","title":"Interpreting CLIP's Image Representation via Text-Based Decomposition","summary":"  We investigate the CLIP image encoder by analyzing how individual model\ncomponents affect the final representation. We decompose the image\nrepresentation as a sum across individual image patches, model layers, and\nattention heads, and use CLIP's text representation to interpret the summands.\nInterpreting the attention heads, we characterize each head's role by\nautomatically finding text representations that span its output space, which\nreveals property-specific roles for many heads (e.g. location or shape). Next,\ninterpreting the image patches, we uncover an emergent spatial localization\nwithin CLIP. Finally, we use this understanding to remove spurious features\nfrom CLIP and to create a strong zero-shot image segmenter. Our results\nindicate that a scalable understanding of transformer models is attainable and\ncan be used to repair and improve models.\n","authors":["Yossi Gandelsman","Alexei A. Efros","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2310.05916v2.pdf","comment":"Project page and code:\n  https://yossigandelsman.github.io/clip_decomposition/"},{"id":"http://arxiv.org/abs/2310.06275v1","updated":"2023-10-10T03:13:33Z","published":"2023-10-10T03:13:33Z","title":"High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying\n  Expression Conditioned Neural Radiance Field","summary":"  One crucial aspect of 3D head avatar reconstruction lies in the details of\nfacial expressions. Although recent NeRF-based photo-realistic 3D head avatar\nmethods achieve high-quality avatar rendering, they still encounter challenges\nretaining intricate facial expression details because they overlook the\npotential of specific expression variations at different spatial positions when\nconditioning the radiance field. Motivated by this observation, we introduce a\nnovel Spatially-Varying Expression (SVE) conditioning. The SVE can be obtained\nby a simple MLP-based generation network, encompassing both spatial positional\nfeatures and global expression information. Benefiting from rich and diverse\ninformation of the SVE at different positions, the proposed SVE-conditioned\nneural radiance field can deal with intricate facial expressions and achieve\nrealistic rendering and geometry details of high-fidelity 3D head avatars.\nAdditionally, to further elevate the geometric and rendering quality, we\nintroduce a new coarse-to-fine training strategy, including a geometry\ninitialization strategy at the coarse stage and an adaptive importance sampling\nstrategy at the fine stage. Extensive experiments indicate that our method\noutperforms other state-of-the-art (SOTA) methods in rendering and geometry\nquality on mobile phone-collected and public datasets.\n","authors":["Minghan Qin","Yifan Liu","Yuelang Xu","Xiaochen Zhao","Yebin Liu","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06275v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2206.00311v3","updated":"2023-10-10T03:06:45Z","published":"2022-06-01T08:27:19Z","title":"MaskOCR: Text Recognition with Masked Encoder-Decoder Pretraining","summary":"  Text images contain both visual and linguistic information. However, existing\npre-training techniques for text recognition mainly focus on either visual\nrepresentation learning or linguistic knowledge learning. In this paper, we\npropose a novel approach MaskOCR to unify vision and language pre-training in\nthe classical encoder-decoder recognition framework. We adopt the masked image\nmodeling approach to pre-train the feature encoder using a large set of\nunlabeled real text images, which allows us to learn strong visual\nrepresentations. In contrast to introducing linguistic knowledge with an\nadditional language model, we directly pre-train the sequence decoder.\nSpecifically, we transform text data into synthesized text images to unify the\ndata modalities of vision and language, and enhance the language modeling\ncapability of the sequence decoder using a proposed masked image-language\nmodeling scheme. Significantly, the encoder is frozen during the pre-training\nphase of the sequence decoder. Experimental results demonstrate that our\nproposed method achieves superior performance on benchmark datasets, including\nChinese and English text images.\n","authors":["Pengyuan Lyu","Chengquan Zhang","Shanshan Liu","Meina Qiao","Yangliu Xu","Liang Wu","Kun Yao","Junyu Han","Errui Ding","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2206.00311v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13570v3","updated":"2023-10-10T03:01:10Z","published":"2023-09-24T07:06:45Z","title":"Robust Digital-Twin Localization via An RGBD-based Transformer Network\n  and A Comprehensive Evaluation on a Mobile Dataset","summary":"  The potential of digital-twin technology, involving the creation of precise\ndigital replicas of physical objects, to reshape AR experiences in 3D object\ntracking and localization scenarios is significant. However, enabling robust 3D\nobject tracking in dynamic mobile AR environments remains a formidable\nchallenge. These scenarios often require a more robust pose estimator capable\nof handling the inherent sensor-level measurement noise. In this paper,\nrecognizing the challenges of comprehensive solutions in existing literature,\nwe propose a transformer-based 6DoF pose estimator designed to achieve\nstate-of-the-art accuracy under real-world noisy data. To systematically\nvalidate the new solution's performance against the prior art, we also\nintroduce a novel RGBD dataset called Digital Twin Tracking Dataset v2 (DTTD2),\nwhich is focused on digital-twin object tracking scenarios. Expanded from an\nexisting DTTD v1 (DTTD1), the new dataset adds digital-twin data captured using\na cutting-edge mobile RGBD sensor suite on Apple iPhone 14 Pro, expanding the\napplicability of our approach to iPhone sensor data. Through extensive\nexperimentation and in-depth analysis, we illustrate the effectiveness of our\nmethods under significant depth data errors, surpassing the performance of\nexisting baselines. Code and dataset are made publicly available at:\nhttps://github.com/augcog/DTTD2\n","authors":["Zixun Huang","Keling Yao","Seth Z. Zhao","Chuanyu Pan","Tianjian Xu","Weiyu Feng","Allen Y. Yang"],"pdf_url":"https://arxiv.org/pdf/2309.13570v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13808v3","updated":"2023-10-10T03:00:48Z","published":"2022-09-28T03:27:37Z","title":"Streaming Video Temporal Action Segmentation In Real Time","summary":"  Temporal action segmentation (TAS) is a critical step toward long-term video\nunderstanding. Recent studies follow a pattern that builds models based on\nfeatures instead of raw video picture information. However, we claim those\nmodels are trained complicatedly and limit application scenarios. It is hard\nfor them to segment human actions of video in real time because they must work\nafter the full video features are extracted. As the real-time action\nsegmentation task is different from TAS task, we define it as streaming video\nreal-time temporal action segmentation (SVTAS) task. In this paper, we propose\na real-time end-to-end multi-modality model for SVTAS task. More specifically,\nunder the circumstances that we cannot get any future information, we segment\nthe current human action of streaming video chunk in real time. Furthermore,\nthe model we propose combines the last steaming video chunk feature extracted\nby language model with the current image feature extracted by image model to\nimprove the quantity of real-time temporal action segmentation. To the best of\nour knowledge, it is the first multi-modality real-time temporal action\nsegmentation model. Under the same evaluation criteria as full video temporal\naction segmentation, our model segments human action in real time with less\nthan 40% of state-of-the-art model computation and achieves 90% of the accuracy\nof the full video state-of-the-art model.\n","authors":["Wujun Wen","Yunheng Li","Zhuben Dong","Lin Feng","Wanxiao Yang","Shenlan Liu"],"pdf_url":"https://arxiv.org/pdf/2209.13808v3.pdf","comment":"accepted by ISKE2023"},{"id":"http://arxiv.org/abs/2310.05202v2","updated":"2023-10-10T02:52:09Z","published":"2023-10-08T15:28:01Z","title":"Enhancing Cross-Dataset Performance of Distracted Driving Detection With\n  Score-Softmax Classifier","summary":"  Deep neural networks enable real-time monitoring of in-vehicle driver,\nfacilitating the timely prediction of distractions, fatigue, and potential\nhazards. This technology is now integral to intelligent transportation systems.\nRecent research has exposed unreliable cross-dataset end-to-end driver behavior\nrecognition due to overfitting, often referred to as ``shortcut learning\",\nresulting from limited data samples. In this paper, we introduce the\nScore-Softmax classifier, which addresses this issue by enhancing inter-class\nindependence and Intra-class uncertainty. Motivated by human rating patterns,\nwe designed a two-dimensional supervisory matrix based on marginal Gaussian\ndistributions to train the classifier. Gaussian distributions help amplify\nintra-class uncertainty while ensuring the Score-Softmax classifier learns\naccurate knowledge. Furthermore, leveraging the summation of independent\nGaussian distributed random variables, we introduced a multi-channel\ninformation fusion method. This strategy effectively resolves the\nmulti-information fusion challenge for the Score-Softmax classifier.\nConcurrently, we substantiate the necessity of transfer learning and\nmulti-dataset combination. We conducted cross-dataset experiments using the\nSFD, AUCDD-V1, and 100-Driver datasets, demonstrating that Score-Softmax\nimproves cross-dataset performance without modifying the model architecture.\nThis provides a new approach for enhancing neural network generalization.\nAdditionally, our information fusion approach outperforms traditional methods.\n","authors":["Cong Duan","Zixuan Liu","Jiahao Xia","Minghai Zhang","Jiacai Liao","Libo Cao"],"pdf_url":"https://arxiv.org/pdf/2310.05202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.13869v5","updated":"2023-10-10T02:49:40Z","published":"2022-10-25T09:45:49Z","title":"Jet tagging algorithm of graph network with HaarPooling message passing","summary":"  Recently methods of graph neural networks (GNNs) have been applied to solving\nthe problems in high energy physics (HEP) and have shown its great potential\nfor quark-gluon tagging with graph representation of jet events. In this paper,\nwe introduce an approach of GNNs combined with a HaarPooling operation to\nanalyze the events, called HaarPooling Message Passing neural network (HMPNet).\nIn HMPNet, HaarPooling not only extracts the features of graph, but embeds\nadditional information obtained by clustering of k-means of different particle\nfeatures. We construct Haarpooling from five different features: absolute\nenergy $\\log E$, transverse momentum $\\log p_T$, relative coordinates\n$(\\Delta\\eta,\\Delta\\phi)$, the mixed ones $(\\log E, \\log p_T)$ and $(\\log E,\n\\log p_T, \\Delta\\eta,\\Delta\\phi)$. The results show that an appropriate\nselection of information for HaarPooling enhances the accuracy of quark-gluon\ntagging, as adding extra information of $\\log P_T$ to the HMPNet outperforms\nall the others, whereas adding relative coordinates information\n$(\\Delta\\eta,\\Delta\\phi)$ is not very effective. This implies that by adding\neffective particle features from HaarPooling can achieve much better results\nthan solely pure message passing neutral network (MPNN) can do, which\ndemonstrates significant improvement of feature extraction via the pooling\nprocess. Finally we compare the HMPNet study, ordering by $p_T$, with other\nstudies and prove that the HMPNet is also a good choice of GNN algorithms for\njet tagging.\n","authors":["Fei Ma","Feiyi Liu","Wei Li"],"pdf_url":"https://arxiv.org/pdf/2210.13869v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10502v2","updated":"2023-10-10T02:46:01Z","published":"2023-06-18T08:51:14Z","title":"Online Map Vectorization for Autonomous Driving: A Rasterization\n  Perspective","summary":"  Vectorized high-definition (HD) map is essential for autonomous driving,\nproviding detailed and precise environmental information for advanced\nperception and planning. However, current map vectorization methods often\nexhibit deviations, and the existing evaluation metric for map vectorization\nlacks sufficient sensitivity to detect these deviations. To address these\nlimitations, we propose integrating the philosophy of rasterization into map\nvectorization. Specifically, we introduce a new rasterization-based evaluation\nmetric, which has superior sensitivity and is better suited to real-world\nautonomous driving scenarios. Furthermore, we propose MapVR (Map Vectorization\nvia Rasterization), a novel framework that applies differentiable rasterization\nto vectorized outputs and then performs precise and geometry-aware supervision\non rasterized HD maps. Notably, MapVR designs tailored rasterization strategies\nfor various geometric shapes, enabling effective adaptation to a wide range of\nmap elements. Experiments show that incorporating rasterization into map\nvectorization greatly enhances performance with no extra computational cost\nduring inference, leading to more accurate map perception and ultimately\npromoting safer autonomous driving.\n","authors":["Gongjie Zhang","Jiahao Lin","Shuang Wu","Yilin Song","Zhipeng Luo","Yang Xue","Shijian Lu","Zuoguan Wang"],"pdf_url":"https://arxiv.org/pdf/2306.10502v2.pdf","comment":"[NeurIPS 2023]"},{"id":"http://arxiv.org/abs/2306.17560v2","updated":"2023-10-10T02:33:18Z","published":"2023-06-30T11:23:49Z","title":"Class-Incremental Learning using Diffusion Model for Distillation and\n  Replay","summary":"  Class-incremental learning aims to learn new classes in an incremental\nfashion without forgetting the previously learned ones. Several research works\nhave shown how additional data can be used by incremental models to help\nmitigate catastrophic forgetting. In this work, following the recent\nbreakthrough in text-to-image generative models and their wide distribution, we\npropose the use of a pretrained Stable Diffusion model as a source of\nadditional data for class-incremental learning. Compared to competitive methods\nthat rely on external, often unlabeled, datasets of real images, our approach\ncan generate synthetic samples belonging to the same classes as the previously\nencountered images. This allows us to use those additional data samples not\nonly in the distillation loss but also for replay in the classification loss.\nExperiments on the competitive benchmarks CIFAR100, ImageNet-Subset, and\nImageNet demonstrate how this new approach can be used to further improve the\nperformance of state-of-the-art methods for class-incremental learning on large\nscale datasets.\n","authors":["Quentin Jodelet","Xin Liu","Yin Jun Phua","Tsuyoshi Murata"],"pdf_url":"https://arxiv.org/pdf/2306.17560v2.pdf","comment":"Best paper award at 1st Workshop on Visual Continual Learning, ICCV\n  2023"},{"id":"http://arxiv.org/abs/2310.04671v2","updated":"2023-10-10T02:31:24Z","published":"2023-10-07T03:16:30Z","title":"Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem\n  Formulation and Dataset","summary":"  This paper addresses the problem of predicting hazards that drivers may\nencounter while driving a car. We formulate it as a task of anticipating\nimpending accidents using a single input image captured by car dashcams. Unlike\nexisting approaches to driving hazard prediction that rely on computational\nsimulations or anomaly detection from videos, this study focuses on high-level\ninference from static images. The problem needs predicting and reasoning about\nfuture events based on uncertain observations, which falls under visual\nabductive reasoning. To enable research in this understudied area, a new\ndataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is\ncreated. The dataset consists of 15K dashcam images of street scenes, and each\nimage is associated with a tuple containing car speed, a hypothesized hazard\ndescription, and visual entities present in the scene. These are annotated by\nhuman annotators, who identify risky scenes and provide descriptions of\npotential accidents that could occur a few seconds later. We present several\nbaseline methods and evaluate their performance on our dataset, identifying\nremaining issues and discussing future directions. This study contributes to\nthe field by introducing a novel problem formulation and dataset, enabling\nresearchers to explore the potential of multi-modal AI for driving hazard\nprediction.\n","authors":["Korawat Charoenpitaks","Van-Quang Nguyen","Masanori Suganuma","Masahiro Takahashi","Ryoma Niihara","Takayuki Okatani"],"pdf_url":"https://arxiv.org/pdf/2310.04671v2.pdf","comment":"Main Paper: 10 pages, Supplementary Materials: 25 pages"},{"id":"http://arxiv.org/abs/2210.01189v2","updated":"2023-10-10T02:03:13Z","published":"2022-10-03T19:00:38Z","title":"Rank-N-Contrast: Learning Continuous Representations for Regression","summary":"  Deep regression models typically learn in an end-to-end fashion without\nexplicitly emphasizing a regression-aware representation. Consequently, the\nlearned representations exhibit fragmentation and fail to capture the\ncontinuous nature of sample orders, inducing suboptimal results across a wide\nrange of regression tasks. To fill the gap, we propose Rank-N-Contrast (RNC), a\nframework that learns continuous representations for regression by contrasting\nsamples against each other based on their rankings in the target space. We\ndemonstrate, theoretically and empirically, that RNC guarantees the desired\norder of learned representations in accordance with the target orders, enjoying\nnot only better performance but also significantly improved robustness,\nefficiency, and generalization. Extensive experiments using five real-world\nregression datasets that span computer vision, human-computer interaction, and\nhealthcare verify that RNC achieves state-of-the-art performance, highlighting\nits intriguing properties including better data efficiency, robustness to\nspurious targets and data corruptions, and generalization to distribution\nshifts. Code is available at: https://github.com/kaiwenzha/Rank-N-Contrast.\n","authors":["Kaiwen Zha","Peng Cao","Jeany Son","Yuzhe Yang","Dina Katabi"],"pdf_url":"https://arxiv.org/pdf/2210.01189v2.pdf","comment":"NeurIPS 2023 Spotlight. The first two authors contributed equally to\n  this paper"},{"id":"http://arxiv.org/abs/2310.05446v2","updated":"2023-10-10T02:00:14Z","published":"2023-10-09T06:43:38Z","title":"RetSeg: Retention-based Colorectal Polyps Segmentation Network","summary":"  Vision Transformers (ViTs) have revolutionized medical imaging analysis,\nshowcasing superior efficacy compared to conventional Convolutional Neural\nNetworks (CNNs) in vital tasks such as polyp classification, detection, and\nsegmentation. Leveraging attention mechanisms to focus on specific image\nregions, ViTs exhibit contextual awareness in processing visual data,\nculminating in robust and precise predictions, even for intricate medical\nimages. Moreover, the inherent self-attention mechanism in Transformers\naccommodates varying input sizes and resolutions, granting an unprecedented\nflexibility absent in traditional CNNs. However, Transformers grapple with\nchallenges like excessive memory usage and limited training parallelism due to\nself-attention, rendering them impractical for real-time disease detection on\nresource-constrained devices. In this study, we address these hurdles by\ninvestigating the integration of the recently introduced retention mechanism\ninto polyp segmentation, introducing RetSeg, an encoder-decoder network\nfeaturing multi-head retention blocks. Drawing inspiration from Retentive\nNetworks (RetNet), RetSeg is designed to bridge the gap between precise polyp\nsegmentation and resource utilization, particularly tailored for colonoscopy\nimages. We train and validate RetSeg for polyp segmentation employing two\npublicly available datasets: Kvasir-SEG and CVC-ClinicDB. Additionally, we\nshowcase RetSeg's promising performance across diverse public datasets,\nincluding CVC-ColonDB, ETIS-LaribPolypDB, CVC-300, and BKAI-IGH NeoPolyp. While\nour work represents an early-stage exploration, further in-depth studies are\nimperative to advance these promising findings.\n","authors":["Khaled ELKarazle","Valliappan Raman","Caslon Chua","Patrick Then"],"pdf_url":"https://arxiv.org/pdf/2310.05446v2.pdf","comment":"Updated version with a PDF"},{"id":"http://arxiv.org/abs/2310.05393v2","updated":"2023-10-10T01:59:36Z","published":"2023-10-09T04:16:35Z","title":"Hierarchical Side-Tuning for Vision Transformers","summary":"  Fine-tuning pre-trained Vision Transformers (ViT) has consistently\ndemonstrated promising performance in the realm of visual recognition. However,\nadapting large pre-trained models to various tasks poses a significant\nchallenge. This challenge arises from the need for each model to undergo an\nindependent and comprehensive fine-tuning process, leading to substantial\ncomputational and memory demands. While recent advancements in\nParameter-efficient Transfer Learning (PETL) have demonstrated their ability to\nachieve superior performance compared to full fine-tuning with a smaller subset\nof parameter updates, they tend to overlook dense prediction tasks such as\nobject detection and segmentation. In this paper, we introduce Hierarchical\nSide-Tuning (HST), a novel PETL approach that enables ViT transfer to various\ndownstream tasks effectively. Diverging from existing methods that exclusively\nfine-tune parameters within input spaces or certain modules connected to the\nbackbone, we tune a lightweight and hierarchical side network (HSN) that\nleverages intermediate activations extracted from the backbone and generates\nmulti-scale features to make predictions. To validate HST, we conducted\nextensive experiments encompassing diverse visual tasks, including\nclassification, object detection, instance segmentation, and semantic\nsegmentation. Notably, our method achieves state-of-the-art average Top-1\naccuracy of 76.0% on VTAB-1k, all while fine-tuning a mere 0.78M parameters.\nWhen applied to object detection tasks on COCO testdev benchmark, HST even\nsurpasses full fine-tuning and obtains better performance with 49.7 box AP and\n43.2 mask AP using Cascade Mask R-CNN.\n","authors":["Weifeng Lin","Ziheng Wu","Jiayu Chen","Wentao Yang","Mingxin Huang","Jun Huang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2310.05393v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05341v2","updated":"2023-10-10T01:31:06Z","published":"2023-10-09T01:59:49Z","title":"A Critical Look at Classic Test-Time Adaptation Methods in Semantic\n  Segmentation","summary":"  Test-time adaptation (TTA) aims to adapt a model, initially trained on\ntraining data, to potential distribution shifts in the test data. Most existing\nTTA studies, however, focus on classification tasks, leaving a notable gap in\nthe exploration of TTA for semantic segmentation. This pronounced emphasis on\nclassification might lead numerous newcomers and engineers to mistakenly assume\nthat classic TTA methods designed for classification can be directly applied to\nsegmentation. Nonetheless, this assumption remains unverified, posing an open\nquestion. To address this, we conduct a systematic, empirical study to disclose\nthe unique challenges of segmentation TTA, and to determine whether classic TTA\nstrategies can effectively address this task. Our comprehensive results have\nled to three key observations. First, the classic batch norm updating strategy,\ncommonly used in classification TTA, only brings slight performance\nimprovement, and in some cases it might even adversely affect the results. Even\nwith the application of advanced distribution estimation techniques like batch\nrenormalization, the problem remains unresolved. Second, the teacher-student\nscheme does enhance training stability for segmentation TTA in the presence of\nnoisy pseudo-labels. However, it cannot directly result in performance\nimprovement compared to the original model without TTA. Third, segmentation TTA\nsuffers a severe long-tailed imbalance problem, which is substantially more\ncomplex than that in TTA for classification. This long-tailed challenge\nsignificantly affects segmentation TTA performance, even when the accuracy of\npseudo-labels is high. In light of these observations, we conclude that TTA for\nsegmentation presents significant challenges, and simply using classic TTA\nmethods cannot address this problem well.\n","authors":["Chang'an Yi","Haotian Chen","Yifan Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04893v3","updated":"2023-10-10T01:24:45Z","published":"2023-06-08T02:45:15Z","title":"Coping with Change: Learning Invariant and Minimum Sufficient\n  Representations for Fine-Grained Visual Categorization","summary":"  Fine-grained visual categorization (FGVC) is a challenging task due to\nsimilar visual appearances between various species. Previous studies always\nimplicitly assume that the training and test data have the same underlying\ndistributions, and that features extracted by modern backbone architectures\nremain discriminative and generalize well to unseen test data. However, we\nempirically justify that these conditions are not always true on benchmark\ndatasets. To this end, we combine the merits of invariant risk minimization\n(IRM) and information bottleneck (IB) principle to learn invariant and minimum\nsufficient (IMS) representations for FGVC, such that the overall model can\nalways discover the most succinct and consistent fine-grained features. We\napply the matrix-based R{\\'e}nyi's $\\alpha$-order entropy to simplify and\nstabilize the training of IB; we also design a ``soft\" environment partition\nscheme to make IRM applicable to FGVC task. To the best of our knowledge, we\nare the first to address the problem of FGVC from a generalization perspective\nand develop a new information-theoretic solution accordingly. Extensive\nexperiments demonstrate the consistent performance gain offered by our IMS.\n","authors":["Shuo Ye","Shujian Yu","Wenjin Hou","Yu Wang","Xinge You"],"pdf_url":"https://arxiv.org/pdf/2306.04893v3.pdf","comment":"Manuscript accepted by CVIU, code is available at Github"},{"id":"http://arxiv.org/abs/2310.06238v1","updated":"2023-10-10T01:22:41Z","published":"2023-10-10T01:22:41Z","title":"Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for\n  Unbiased Question-Answering","summary":"  In recent years, there has been a growing emphasis on the intersection of\naudio, vision, and text modalities, driving forward the advancements in\nmultimodal research. However, strong bias that exists in any modality can lead\nto the model neglecting the others. Consequently, the model's ability to\neffectively reason across these diverse modalities is compromised, impeding\nfurther advancement. In this paper, we meticulously review each question type\nfrom the original dataset, selecting those with pronounced answer biases. To\ncounter these biases, we gather complementary videos and questions, ensuring\nthat no answers have outstanding skewed distribution. In particular, for binary\nquestions, we strive to ensure that both answers are almost uniformly spread\nwithin each question category. As a result, we construct a new dataset, named\nMUSIC-AVQA v2.0, which is more challenging and we believe could better foster\nthe progress of AVQA task. Furthermore, we present a novel baseline model that\ndelves deeper into the audio-visual-text interrelation. On MUSIC-AVQA v2.0,\nthis model surpasses all the existing benchmarks, improving accuracy by 2% on\nMUSIC-AVQA v2.0, setting a new state-of-the-art performance.\n","authors":["Xiulong Liu","Zhikang Dong","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06234v1","updated":"2023-10-10T01:04:15Z","published":"2023-10-10T01:04:15Z","title":"Efficient Adaptation of Large Vision Transformer via Adapter\n  Re-Composing","summary":"  The advent of high-capacity pre-trained models has revolutionized\nproblem-solving in computer vision, shifting the focus from training\ntask-specific models to adapting pre-trained models. Consequently, effectively\nadapting large pre-trained models to downstream tasks in an efficient manner\nhas become a prominent research area. Existing solutions primarily concentrate\non designing lightweight adapters and their interaction with pre-trained\nmodels, with the goal of minimizing the number of parameters requiring updates.\nIn this study, we propose a novel Adapter Re-Composing (ARC) strategy that\naddresses efficient pre-trained model adaptation from a fresh perspective. Our\napproach considers the reusability of adaptation parameters and introduces a\nparameter-sharing scheme. Specifically, we leverage symmetric\ndown-/up-projections to construct bottleneck operations, which are shared\nacross layers. By learning low-dimensional re-scaling coefficients, we can\neffectively re-compose layer-adaptive adapters. This parameter-sharing strategy\nin adapter design allows us to significantly reduce the number of new\nparameters while maintaining satisfactory performance, thereby offering a\npromising approach to compress the adaptation cost. We conduct experiments on\n24 downstream image classification tasks using various Vision Transformer\nvariants to evaluate our method. The results demonstrate that our approach\nachieves compelling transfer learning performance with a reduced parameter\ncount. Our code is available at\n\\href{https://github.com/DavidYanAnDe/ARC}{https://github.com/DavidYanAnDe/ARC}.\n","authors":["Wei Dong","Dawei Yan","Zhijun Lin","Peng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.06234v1.pdf","comment":"Paper is accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06232v1","updated":"2023-10-10T00:59:26Z","published":"2023-10-10T00:59:26Z","title":"Spiking PointNet: Spiking Neural Networks for Point Clouds","summary":"  Recently, Spiking Neural Networks (SNNs), enjoying extreme energy efficiency,\nhave drawn much research attention on 2D visual recognition and shown gradually\nincreasing application potential. However, it still remains underexplored\nwhether SNNs can be generalized to 3D recognition. To this end, we present\nSpiking PointNet in the paper, the first spiking neural model for efficient\ndeep learning on point clouds. We discover that the two huge obstacles limiting\nthe application of SNNs in point clouds are: the intrinsic optimization\nobstacle of SNNs that impedes the training of a big spiking model with large\ntime steps, and the expensive memory and computation cost of PointNet that\nmakes training a big spiking point model unrealistic. To solve the problems\nsimultaneously, we present a trained-less but learning-more paradigm for\nSpiking PointNet with theoretical justifications and in-depth experimental\nanalysis. In specific, our Spiking PointNet is trained with only a single time\nstep but can obtain better performance with multiple time steps inference,\ncompared to the one trained directly with multiple time steps. We conduct\nvarious experiments on ModelNet10, ModelNet40 to demonstrate the effectiveness\nof Spiking PointNet. Notably, our Spiking PointNet even can outperform its ANN\ncounterpart, which is rare in the SNN field thus providing a potential research\ndirection for the following work. Moreover, Spiking PointNet shows impressive\nspeedup and storage saving in the training phase.\n","authors":["Dayong Ren","Zhe Ma","Yuanpei Chen","Weihang Peng","Xiaode Liu","Yuhan Zhang","Yufei Guo"],"pdf_url":"https://arxiv.org/pdf/2310.06232v1.pdf","comment":"Accepted by NeurIPS"},{"id":"http://arxiv.org/abs/2310.06214v1","updated":"2023-10-10T00:07:25Z","published":"2023-10-10T00:07:25Z","title":"CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding","summary":"  3D visual grounding is the ability to localize objects in 3D scenes\nconditioned by utterances. Most existing methods devote the referring head to\nlocalize the referred object directly, causing failure in complex scenarios. In\naddition, it does not illustrate how and why the network reaches the final\ndecision. In this paper, we address this question Can we design an\ninterpretable 3D visual grounding framework that has the potential to mimic the\nhuman perception system?. To this end, we formulate the 3D visual grounding\nproblem as a sequence-to-sequence task by first predicting a chain of anchors\nand then the final target. Interpretability not only improves the overall\nperformance but also helps us identify failure cases. Following the chain of\nthoughts approach enables us to decompose the referring task into interpretable\nintermediate steps, boosting the performance and making our framework extremely\ndata-efficient. Moreover, our proposed framework can be easily integrated into\nany existing architecture. We validate our approach through comprehensive\nexperiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent\nperformance gains compared to existing methods without requiring manually\nannotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is\nsignificantly data-efficient, whereas on the Sr3D dataset, when trained only on\n10% of the data, we match the SOTA performance that trained on the entire data.\n","authors":["Eslam Mohamed Bakr","Mohamed Ayman","Mahmoud Ahmed","Habib Slim","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2310.06214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1903.06262v8","updated":"2023-10-10T23:31:32Z","published":"2019-03-08T18:20:36Z","title":"A Grid-based Method for Removing Overlaps of Dimensionality Reduction\n  Scatterplot Layouts","summary":"  Dimensionality Reduction (DR) scatterplot layouts have become a ubiquitous\nvisualization tool for analyzing multidimensional datasets. Despite their\npopularity, such scatterplots suffer from occlusion, especially when\ninformative glyphs are used to represent data instances, potentially\nobfuscating critical information for the analysis under execution. Different\nstrategies have been devised to address this issue, either producing\noverlap-free layouts that lack the powerful capabilities of contemporary DR\ntechniques in uncovering interesting data patterns or eliminating overlaps as a\npost-processing strategy. Despite the good results of post-processing\ntechniques, most of the best methods typically expand or distort the\nscatterplot area, thus reducing glyphs' size (sometimes) to unreadable\ndimensions, defeating the purpose of removing overlaps. This paper presents\nDistance Grid (DGrid), a novel post-processing strategy to remove overlaps from\nDR layouts that faithfully preserves the original layout's characteristics and\nbounds the minimum glyph sizes. We show that DGrid surpasses the\nstate-of-the-art in overlap removal (through an extensive comparative\nevaluation considering multiple different metrics) while also being one of the\nfastest techniques, especially for large datasets. A user study with 51\nparticipants also shows that DGrid is consistently ranked among the top\ntechniques for preserving the original scatterplots' visual characteristics and\nthe aesthetics of the final results.\n","authors":["Gladys M. Hilasaca","Wilson E. Marcílio-Jr","Danilo M. Eler","Rafael M. Martins","Fernando V. Paulovich"],"pdf_url":"https://arxiv.org/pdf/1903.06262v8.pdf","comment":"14 pages, 10 figures. A preprint version of a publication at IEEE\n  Transactions on Visualization and Computer Graphics (TVCG), 2023"},{"id":"http://arxiv.org/abs/2301.05174v2","updated":"2023-10-10T22:58:45Z","published":"2023-01-12T18:00:00Z","title":"Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A\n  Reproducibility Study","summary":"  Most approaches to cross-modal retrieval (CMR) focus either on object-centric\ndatasets, meaning that each document depicts or describes a single object, or\non scene-centric datasets, meaning that each image depicts or describes a\ncomplex scene that involves multiple objects and relations between them. We\nposit that a robust CMR model should generalize well across both dataset types.\nDespite recent advances in CMR, the reproducibility of the results and their\ngeneralizability across different dataset types has not been studied before. We\naddress this gap and focus on the reproducibility of the state-of-the-art CMR\nresults when evaluated on object-centric and scene-centric datasets. We select\ntwo state-of-the-art CMR models with different architectures: (i) CLIP; and\n(ii) X-VLM. Additionally, we select two scene-centric datasets, and three\nobject-centric datasets, and determine the relative performance of the selected\nmodels on these datasets. We focus on reproducibility, replicability, and\ngeneralizability of the outcomes of previously published CMR experiments. We\ndiscover that the experiments are not fully reproducible and replicable.\nBesides, the relative performance results partially generalize across\nobject-centric and scene-centric datasets. On top of that, the scores obtained\non object-centric datasets are much lower than the scores obtained on\nscene-centric datasets. For reproducibility and transparency we make our source\ncode and the trained models publicly available.\n","authors":["Mariya Hendriksen","Svitlana Vakulenko","Ernst Kuiper","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2301.05174v2.pdf","comment":"18 pages, accepted as a reproducibility paper at ECIR 2023"},{"id":"http://arxiv.org/abs/2306.04865v3","updated":"2023-10-10T22:56:41Z","published":"2023-06-08T01:35:43Z","title":"MyStyle++: A Controllable Personalized Generative Prior","summary":"  In this paper, we propose an approach to obtain a personalized generative\nprior with explicit control over a set of attributes. We build upon MyStyle, a\nrecently introduced method, that tunes the weights of a pre-trained StyleGAN\nface generator on a few images of an individual. This system allows\nsynthesizing, editing, and enhancing images of the target individual with high\nfidelity to their facial features. However, MyStyle does not demonstrate\nprecise control over the attributes of the generated images. We propose to\naddress this problem through a novel optimization system that organizes the\nlatent space in addition to tuning the generator. Our key contribution is to\nformulate a loss that arranges the latent codes, corresponding to the input\nimages, along a set of specific directions according to their attributes. We\ndemonstrate that our approach, dubbed MyStyle++, is able to synthesize, edit,\nand enhance images of an individual with great control over the attributes,\nwhile preserving the unique facial characteristics of that individual.\n","authors":["Libing Zeng","Lele Chen","Yi Xu","Nima Kalantari"],"pdf_url":"https://arxiv.org/pdf/2306.04865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07060v1","updated":"2023-10-10T22:54:01Z","published":"2023-10-10T22:54:01Z","title":"BeSt-LeS: Benchmarking Stroke Lesion Segmentation using Deep Supervision","summary":"  Brain stroke has become a significant burden on global health and thus we\nneed remedies and prevention strategies to overcome this challenge. For this,\nthe immediate identification of stroke and risk stratification is the primary\ntask for clinicians. To aid expert clinicians, automated segmentation models\nare crucial. In this work, we consider the publicly available dataset ATLAS\n$v2.0$ to benchmark various end-to-end supervised U-Net style models.\nSpecifically, we have benchmarked models on both 2D and 3D brain images and\nevaluated them using standard metrics. We have achieved the highest Dice score\nof 0.583 on the 2D transformer-based model and 0.504 on the 3D residual U-Net\nrespectively. We have conducted the Wilcoxon test for 3D models to correlate\nthe relationship between predicted and actual stroke volume. For\nreproducibility, the code and model weights are made publicly available:\nhttps://github.com/prantik-pdeb/BeSt-LeS.\n","authors":["Prantik Deb","Lalith Bharadwaj Baru","Kamalaker Dadi","Bapi Raju S"],"pdf_url":"https://arxiv.org/pdf/2310.07060v1.pdf","comment":"Accepted to MICCAI BrainLes 2023 (oral)"},{"id":"http://arxiv.org/abs/2310.07056v1","updated":"2023-10-10T22:36:15Z","published":"2023-10-10T22:36:15Z","title":"TextPSG: Panoptic Scene Graph Generation from Textual Descriptions","summary":"  Panoptic Scene Graph has recently been proposed for comprehensive scene\nunderstanding. However, previous works adopt a fully-supervised learning\nmanner, requiring large amounts of pixel-wise densely-annotated data, which is\nalways tedious and expensive to obtain. To address this limitation, we study a\nnew problem of Panoptic Scene Graph Generation from Purely Textual Descriptions\n(Caption-to-PSG). The key idea is to leverage the large collection of free\nimage-caption data on the Web alone to generate panoptic scene graphs. The\nproblem is very challenging for three constraints: 1) no location priors; 2) no\nexplicit links between visual regions and textual entities; and 3) no\npre-defined concept sets. To tackle this problem, we propose a new framework\nTextPSG consisting of four modules, i.e., a region grouper, an entity grounder,\na segment merger, and a label generator, with several novel techniques. The\nregion grouper first groups image pixels into different segments and the entity\ngrounder then aligns visual segments with language entities based on the\ntextual description of the segment being referred to. The grounding results can\nthus serve as pseudo labels enabling the segment merger to learn the segment\nsimilarity as well as guiding the label generator to learn object semantics and\nrelation predicates, resulting in a fine-grained structured scene\nunderstanding. Our framework is effective, significantly outperforming the\nbaselines and achieving strong out-of-distribution robustness. We perform\ncomprehensive ablation studies to corroborate the effectiveness of our design\nchoices and provide an in-depth analysis to highlight future directions. Our\ncode, data, and results are available on our project page:\nhttps://vis-www.cs.umass.edu/TextPSG.\n","authors":["Chengyang Zhao","Yikang Shen","Zhenfang Chen","Mingyu Ding","Chuang Gan"],"pdf_url":"https://arxiv.org/pdf/2310.07056v1.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2310.00772v2","updated":"2023-10-10T21:42:59Z","published":"2023-10-01T19:41:49Z","title":"SMOOT: Saliency Guided Mask Optimized Online Training","summary":"  Deep Neural Networks are powerful tools for understanding complex patterns\nand making decisions. However, their black-box nature impedes a complete\nunderstanding of their inner workings. Saliency-Guided Training (SGT) methods\ntry to highlight the prominent features in the model's training based on the\noutput to alleviate this problem. These methods use back-propagation and\nmodified gradients to guide the model toward the most relevant features while\nkeeping the impact on the prediction accuracy negligible. SGT makes the model's\nfinal result more interpretable by masking input partially. In this way,\nconsidering the model's output, we can infer how each segment of the input\naffects the output. In the particular case of image as the input, masking is\napplied to the input pixels. However, the masking strategy and number of pixels\nwhich we mask, are considered as a hyperparameter. Appropriate setting of\nmasking strategy can directly affect the model's training. In this paper, we\nfocus on this issue and present our contribution. We propose a novel method to\ndetermine the optimal number of masked images based on input, accuracy, and\nmodel loss during the training. The strategy prevents information loss which\nleads to better accuracy values. Also, by integrating the model's performance\nin the strategy formula, we show that our model represents the salient features\nmore meaningful. Our experimental results demonstrate a substantial improvement\nin both model accuracy and the prominence of saliency, thereby affirming the\neffectiveness of our proposed solution.\n","authors":["Ali Karkehabadi","Houman Homayoun","Avesta Sasan"],"pdf_url":"https://arxiv.org/pdf/2310.00772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07033v1","updated":"2023-10-10T21:40:19Z","published":"2023-10-10T21:40:19Z","title":"Computational Pathology at Health System Scale -- Self-Supervised\n  Foundation Models from Three Billion Images","summary":"  Recent breakthroughs in self-supervised learning have enabled the use of\nlarge unlabeled datasets to train visual foundation models that can generalize\nto a variety of downstream tasks. While this training paradigm is well suited\nfor the medical domain where annotations are scarce, large-scale pre-training\nin the medical domain, and in particular pathology, has not been extensively\nstudied. Previous work in self-supervised learning in pathology has leveraged\nsmaller datasets for both pre-training and evaluating downstream performance.\nThe aim of this project is to train the largest academic foundation model and\nbenchmark the most prominent self-supervised learning algorithms by\npre-training and evaluating downstream performance on large clinical pathology\ndatasets. We collected the largest pathology dataset to date, consisting of\nover 3 billion images from over 423 thousand microscopy slides. We compared\npre-training of visual transformer models using the masked autoencoder (MAE)\nand DINO algorithms. We evaluated performance on six clinically relevant tasks\nfrom three anatomic sites and two institutions: breast cancer detection,\ninflammatory bowel disease detection, breast cancer estrogen receptor\nprediction, lung adenocarcinoma EGFR mutation prediction, and lung cancer\nimmunotherapy response prediction. Our results demonstrate that pre-training on\npathology data is beneficial for downstream performance compared to\npre-training on natural images. Additionally, the DINO algorithm achieved\nbetter generalization performance across all tasks tested. The presented\nresults signify a phase change in computational pathology research, paving the\nway into a new era of more performant models based on large-scale, parallel\npre-training at the billion-image scale.\n","authors":["Gabriele Campanella","Ricky Kwan","Eugene Fluder","Jennifer Zeng","Aryeh Stock","Brandon Veremis","Alexandros D. Polydorides","Cyrus Hedvat","Adam Schoenfeld","Chad Vanderbilt","Patricia Kovatch","Carlos Cordon-Cardo","Thomas J. Fuchs"],"pdf_url":"https://arxiv.org/pdf/2310.07033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07028v1","updated":"2023-10-10T21:30:05Z","published":"2023-10-10T21:30:05Z","title":"Facial Forgery-based Deepfake Detection using Fine-Grained Features","summary":"  Facial forgery by deepfakes has caused major security risks and raised severe\nsocietal concerns. As a countermeasure, a number of deepfake detection methods\nhave been proposed. Most of them model deepfake detection as a binary\nclassification problem using a backbone convolutional neural network (CNN)\narchitecture pretrained for the task. These CNN-based methods have demonstrated\nvery high efficacy in deepfake detection with the Area under the Curve (AUC) as\nhigh as $0.99$. However, the performance of these methods degrades\nsignificantly when evaluated across datasets and deepfake manipulation\ntechniques. This draws our attention towards learning more subtle, local, and\ndiscriminative features for deepfake detection. In this paper, we formulate\ndeepfake detection as a fine-grained classification problem and propose a new\nfine-grained solution to it. Specifically, our method is based on learning\nsubtle and generalizable features by effectively suppressing background noise\nand learning discriminative features at various scales for deepfake detection.\nThrough extensive experimental validation, we demonstrate the superiority of\nour method over the published research in cross-dataset and cross-manipulation\ngeneralization of deepfake detectors for the majority of the experimental\nscenarios.\n","authors":["Aakash Varma Nadimpalli","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2310.07028v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.07027v1","updated":"2023-10-10T21:29:41Z","published":"2023-10-10T21:29:41Z","title":"Utilizing Synthetic Data for Medical Vision-Language Pre-training:\n  Bypassing the Need for Real Images","summary":"  Medical Vision-Language Pre-training (VLP) learns representations jointly\nfrom medical images and paired radiology reports. It typically requires\nlarge-scale paired image-text datasets to achieve effective pre-training for\nboth the image encoder and text encoder. The advent of text-guided generative\nmodels raises a compelling question: Can VLP be implemented solely with\nsynthetic images generated from genuine radiology reports, thereby mitigating\nthe need for extensively pairing and curating image-text datasets? In this\nwork, we scrutinize this very question by examining the feasibility and\neffectiveness of employing synthetic images for medical VLP. We replace real\nmedical images with their synthetic equivalents, generated from authentic\nmedical reports. Utilizing three state-of-the-art VLP algorithms, we\nexclusively train on these synthetic samples. Our empirical evaluation across\nthree subsequent tasks, namely image classification, semantic segmentation and\nobject detection, reveals that the performance achieved through synthetic data\nis on par with or even exceeds that obtained with real images. As a pioneering\ncontribution to this domain, we introduce a large-scale synthetic medical image\ndataset, paired with anonymized real radiology reports. This alleviates the\nneed of sharing medical images, which are not easy to curate and share in\npractice. The code and the dataset will be made publicly available upon paper\nacceptance.\n","authors":["Che Liu","Anand Shah","Wenjia Bai","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2310.07027v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2310.07021v1","updated":"2023-10-10T21:16:29Z","published":"2023-10-10T21:16:29Z","title":"Pre-Trained Masked Image Model for Mobile Robot Navigation","summary":"  2D top-down maps are commonly used for the navigation and exploration of\nmobile robots through unknown areas. Typically, the robot builds the navigation\nmaps incrementally from local observations using onboard sensors. Recent works\nhave shown that predicting the structural patterns in the environment through\nlearning-based approaches can greatly enhance task efficiency. While many such\nworks build task-specific networks using limited datasets, we show that the\nexisting foundational vision networks can accomplish the same without any\nfine-tuning. Specifically, we use Masked Autoencoders, pre-trained on street\nimages, to present novel applications for field-of-view expansion, single-agent\ntopological exploration, and multi-agent exploration for indoor mapping, across\ndifferent input modalities. Our work motivates the use of foundational vision\nmodels for generalized structure prediction-driven applications, especially in\nthe dearth of training data. For more qualitative results see\nhttps://raaslab.org/projects/MIM4Robots.\n","authors":["Vishnu Dutt Sharma","Anukriti Singh","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2310.07021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08123v2","updated":"2023-10-10T20:28:15Z","published":"2023-07-16T18:42:01Z","title":"Solving Inverse Problems with Latent Diffusion Models via Hard Data\n  Consistency","summary":"  Diffusion models have recently emerged as powerful generative priors for\nsolving inverse problems. However, training diffusion models in the pixel space\nare both data-intensive and computationally demanding, which restricts their\napplicability as priors for high-dimensional real-world data such as medical\nimages. Latent diffusion models, which operate in a much lower-dimensional\nspace, offer a solution to these challenges. However, incorporating latent\ndiffusion models to solve inverse problems remains a challenging problem due to\nthe nonlinearity of the encoder and decoder. To address these issues, we\npropose \\textit{ReSample}, an algorithm that can solve general inverse problems\nwith pre-trained latent diffusion models. Our algorithm incorporates data\nconsistency by solving an optimization problem during the reverse sampling\nprocess, a concept that we term as hard data consistency. Upon solving this\noptimization problem, we propose a novel resampling scheme to map the\nmeasurement-consistent sample back onto the noisy data manifold and\ntheoretically demonstrate its benefits. Lastly, we apply our algorithm to solve\na wide range of linear and nonlinear inverse problems in both natural and\nmedical images, demonstrating that our approach outperforms existing\nstate-of-the-art approaches, including those based on pixel-space diffusion\nmodels.\n","authors":["Bowen Song","Soo Min Kwon","Zecheng Zhang","Xinyu Hu","Qing Qu","Liyue Shen"],"pdf_url":"https://arxiv.org/pdf/2307.08123v2.pdf","comment":"27 pages, 20 figures"},{"id":"http://arxiv.org/abs/2306.12511v3","updated":"2023-10-10T20:27:25Z","published":"2023-06-21T18:49:22Z","title":"Semi-Implicit Denoising Diffusion Models (SIDDMs)","summary":"  Despite the proliferation of generative models, achieving fast sampling\nduring inference without compromising sample diversity and quality remains\nchallenging. Existing models such as Denoising Diffusion Probabilistic Models\n(DDPM) deliver high-quality, diverse samples but are slowed by an inherently\nhigh number of iterative steps. The Denoising Diffusion Generative Adversarial\nNetworks (DDGAN) attempted to circumvent this limitation by integrating a GAN\nmodel for larger jumps in the diffusion process. However, DDGAN encountered\nscalability limitations when applied to large datasets. To address these\nlimitations, we introduce a novel approach that tackles the problem by matching\nimplicit and explicit factors. More specifically, our approach involves\nutilizing an implicit model to match the marginal distributions of noisy data\nand the explicit conditional distribution of the forward diffusion. This\ncombination allows us to effectively match the joint denoising distributions.\nUnlike DDPM but similar to DDGAN, we do not enforce a parametric distribution\nfor the reverse step, enabling us to take large steps during inference. Similar\nto the DDPM but unlike DDGAN, we take advantage of the exact form of the\ndiffusion process. We demonstrate that our proposed method obtains comparable\ngenerative performance to diffusion-based models and vastly superior results to\nmodels with a small number of sampling steps.\n","authors":["Yanwu Xu","Mingming Gong","Shaoan Xie","Wei Wei","Matthias Grundmann","Kayhan Batmanghelich","Tingbo Hou"],"pdf_url":"https://arxiv.org/pdf/2306.12511v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06992v1","updated":"2023-10-10T20:25:30Z","published":"2023-10-10T20:25:30Z","title":"Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models","summary":"  Object tracking is central to robot perception and scene understanding.\nTracking-by-detection has long been a dominant paradigm for object tracking of\nspecific object categories. Recently, large-scale pre-trained models have shown\npromising advances in detecting and segmenting objects and parts in 2D static\nimages in the wild. This begs the question: can we re-purpose these large-scale\npre-trained static image models for open-vocabulary video tracking? In this\npaper, we re-purpose an open-vocabulary detector, segmenter, and dense optical\nflow estimator, into a model that tracks and segments objects of any category\nin 2D videos. Our method predicts object and part tracks with associated\nlanguage descriptions in monocular videos, rebuilding the pipeline of Tractor\nwith modern large pre-trained models for static image detection and\nsegmentation: we detect open-vocabulary object instances and propagate their\nboxes from frame to frame using a flow-based motion model, refine the\npropagated boxes with the box regression module of the visual detector, and\nprompt an open-world segmenter with the refined box to segment the objects. We\ndecide the termination of an object track based on the objectness score of the\npropagated boxes, as well as forward-backward optical flow consistency. We\nre-identify objects across occlusions using deep feature matching. We show that\nour model achieves strong performance on multiple established video object\nsegmentation and tracking benchmarks, and can produce reasonable tracks in\nmanipulation data. In particular, our model outperforms previous\nstate-of-the-art in UVO and BURST, benchmarks for open-world object tracking\nand segmentation, despite never being explicitly trained for tracking. We hope\nthat our approach can serve as a simple and extensible framework for future\nresearch.\n","authors":["Wen-Hsuan Chu","Adam W. Harley","Pavel Tokmakov","Achal Dave","Leonidas Guibas","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2310.06992v1.pdf","comment":"Project page available at https://wenhsuanchu.github.io/ovtracktor/"},{"id":"http://arxiv.org/abs/2310.06984v1","updated":"2023-10-10T20:11:13Z","published":"2023-10-10T20:11:13Z","title":"Leveraging Neural Radiance Fields for Uncertainty-Aware Visual\n  Localization","summary":"  As a promising fashion for visual localization, scene coordinate regression\n(SCR) has seen tremendous progress in the past decade. Most recent methods\nusually adopt neural networks to learn the mapping from image pixels to 3D\nscene coordinates, which requires a vast amount of annotated training data. We\npropose to leverage Neural Radiance Fields (NeRF) to generate training samples\nfor SCR. Despite NeRF's efficiency in rendering, many of the rendered data are\npolluted by artifacts or only contain minimal information gain, which can\nhinder the regression accuracy or bring unnecessary computational costs with\nredundant data. These challenges are addressed in three folds in this paper:\n(1) A NeRF is designed to separately predict uncertainties for the rendered\ncolor and depth images, which reveal data reliability at the pixel level. (2)\nSCR is formulated as deep evidential learning with epistemic uncertainty, which\nis used to evaluate information gain and scene coordinate quality. (3) Based on\nthe three arts of uncertainties, a novel view selection policy is formed that\nsignificantly improves data efficiency. Experiments on public datasets\ndemonstrate that our method could select the samples that bring the most\ninformation gain and promote the performance with the highest efficiency.\n","authors":["Le Chen","Weirong Chen","Rui Wang","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2310.06984v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.06982v1","updated":"2023-10-10T20:04:44Z","published":"2023-10-10T20:04:44Z","title":"Data Distillation Can Be Like Vodka: Distilling More Times For Better\n  Quality","summary":"  Dataset distillation aims to minimize the time and memory needed for training\ndeep networks on large datasets, by creating a small set of synthetic images\nthat has a similar generalization performance to that of the full dataset.\nHowever, current dataset distillation techniques fall short, showing a notable\nperformance gap when compared to training on the original data. In this work,\nwe are the first to argue that using just one synthetic subset for distillation\nwill not yield optimal generalization performance. This is because the training\ndynamics of deep networks drastically change during the training. Hence,\nmultiple synthetic subsets are required to capture the training dynamics at\ndifferent phases of training. To address this issue, we propose Progressive\nDataset Distillation (PDD). PDD synthesizes multiple small sets of synthetic\nimages, each conditioned on the previous sets, and trains the model on the\ncumulative union of these subsets without requiring additional training time.\nOur extensive experiments show that PDD can effectively improve the performance\nof existing dataset distillation methods by up to 4.3%. In addition, our method\nfor the first time enable generating considerably larger synthetic datasets.\n","authors":["Xuxi Chen","Yu Yang","Zhangyang Wang","Baharan Mirzasoleiman"],"pdf_url":"https://arxiv.org/pdf/2310.06982v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2310.06968v1","updated":"2023-10-10T19:46:58Z","published":"2023-10-10T19:46:58Z","title":"ObjectComposer: Consistent Generation of Multiple Objects Without\n  Fine-tuning","summary":"  Recent text-to-image generative models can generate high-fidelity images from\ntext prompts. However, these models struggle to consistently generate the same\nobjects in different contexts with the same appearance. Consistent object\ngeneration is important to many downstream tasks like generating comic book\nillustrations with consistent characters and setting. Numerous approaches\nattempt to solve this problem by extending the vocabulary of diffusion models\nthrough fine-tuning. However, even lightweight fine-tuning approaches can be\nprohibitively expensive to run at scale and in real-time. We introduce a method\ncalled ObjectComposer for generating compositions of multiple objects that\nresemble user-specified images. Our approach is training-free, leveraging the\nabilities of preexisting models. We build upon the recent BLIP-Diffusion model,\nwhich can generate images of single objects specified by reference images.\nObjectComposer enables the consistent generation of compositions containing\nmultiple specific objects simultaneously, all without modifying the weights of\nthe underlying models.\n","authors":["Alec Helbling","Evan Montoya","Duen Horng Chau"],"pdf_url":"https://arxiv.org/pdf/2310.06968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15321v5","updated":"2023-10-10T19:44:36Z","published":"2023-08-29T14:16:09Z","title":"Elucidating the Exposure Bias in Diffusion Models","summary":"  Diffusion models have demonstrated impressive generative capabilities, but\ntheir \\textit{exposure bias} problem, described as the input mismatch between\ntraining and sampling, lacks in-depth exploration. In this paper, we\nsystematically investigate the exposure bias problem in diffusion models by\nfirst analytically modelling the sampling distribution, based on which we then\nattribute the prediction error at each sampling step as the root cause of the\nexposure bias issue. Furthermore, we discuss potential solutions to this issue\nand propose an intuitive metric for it. Along with the elucidation of exposure\nbias, we propose a simple, yet effective, training-free method called Epsilon\nScaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly\nmoves the sampling trajectory closer to the vector field learned in the\ntraining phase by scaling down the network output (Epsilon), mitigating the\ninput mismatch between training and sampling. Experiments on various diffusion\nframeworks (ADM, DDPM/DDIM, EDM, LDM), unconditional and conditional settings,\nand deterministic vs. stochastic sampling verify the effectiveness of our\nmethod. Remarkably, our ADM-ES, as a SOTA stochastic sampler, obtains 2.17 FID\non CIFAR-10 under 100-step unconditional generation. The code is available at\n\\url{https://github.com/forever208/ADM-ES} and\n\\url{https://github.com/forever208/EDM-ES}.\n","authors":["Mang Ning","Mingxiao Li","Jianlin Su","Albert Ali Salah","Itir Onal Ertugrul"],"pdf_url":"https://arxiv.org/pdf/2308.15321v5.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2310.06966v1","updated":"2023-10-10T19:32:59Z","published":"2023-10-10T19:32:59Z","title":"On the Interpretability of Part-Prototype Based Classifiers: A Human\n  Centric Analysis","summary":"  Part-prototype networks have recently become methods of interest as an\ninterpretable alternative to many of the current black-box image classifiers.\nHowever, the interpretability of these methods from the perspective of human\nusers has not been sufficiently explored. In this work, we have devised a\nframework for evaluating the interpretability of part-prototype-based models\nfrom a human perspective. The proposed framework consists of three actionable\nmetrics and experiments. To demonstrate the usefulness of our framework, we\nperformed an extensive set of experiments using Amazon Mechanical Turk. They\nnot only show the capability of our framework in assessing the interpretability\nof various part-prototype-based models, but they also are, to the best of our\nknowledge, the most comprehensive work on evaluating such methods in a unified\nframework.\n","authors":["Omid Davoodi","Shayan Mohammadizadehsamakosh","Majid Komeili"],"pdf_url":"https://arxiv.org/pdf/2310.06966v1.pdf","comment":"Intended for submission to Nature Scientific Reports"},{"id":"http://arxiv.org/abs/2310.06958v1","updated":"2023-10-10T19:21:41Z","published":"2023-10-10T19:21:41Z","title":"Comparing the robustness of modern no-reference image- and video-quality\n  metrics to adversarial attacks","summary":"  Nowadays neural-network-based image- and video-quality metrics show better\nperformance compared to traditional methods. However, they also became more\nvulnerable to adversarial attacks that increase metrics' scores without\nimproving visual quality. The existing benchmarks of quality metrics compare\ntheir performance in terms of correlation with subjective quality and\ncalculation time. However, the adversarial robustness of image-quality metrics\nis also an area worth researching. In this paper, we analyse modern metrics'\nrobustness to different adversarial attacks. We adopted adversarial attacks\nfrom computer vision tasks and compared attacks' efficiency against 15\nno-reference image/video-quality metrics. Some metrics showed high resistance\nto adversarial attacks which makes their usage in benchmarks safer than\nvulnerable metrics. The benchmark accepts new metrics submissions for\nresearchers who want to make their metrics more robust to attacks or to find\nsuch metrics for their needs. Try our benchmark using pip install\nrobustness-benchmark.\n","authors":["Anastasia Antsiferova","Khaled Abud","Aleksandr Gushchin","Sergey Lavrushkin","Ekaterina Shumitskaya","Maksim Velikanov","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2310.06958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06945v1","updated":"2023-10-10T19:06:10Z","published":"2023-10-10T19:06:10Z","title":"End-to-end Evaluation of Practical Video Analytics Systems for Face\n  Detection and Recognition","summary":"  Practical video analytics systems that are deployed in bandwidth constrained\nenvironments like autonomous vehicles perform computer vision tasks such as\nface detection and recognition. In an end-to-end face analytics system, inputs\nare first compressed using popular video codecs like HEVC and then passed onto\nmodules that perform face detection, alignment, and recognition sequentially.\nTypically, the modules of these systems are evaluated independently using\ntask-specific imbalanced datasets that can misconstrue performance estimates.\nIn this paper, we perform a thorough end-to-end evaluation of a face analytics\nsystem using a driving-specific dataset, which enables meaningful\ninterpretations. We demonstrate how independent task evaluations, dataset\nimbalances, and inconsistent annotations can lead to incorrect system\nperformance estimates. We propose strategies to create balanced evaluation\nsubsets of our dataset and to make its annotations consistent across multiple\nanalytics tasks and scenarios. We then evaluate the end-to-end system\nperformance sequentially to account for task interdependencies. Our experiments\nshow that our approach provides consistent, accurate, and interpretable\nestimates of the system's performance which is critical for real-world\napplications.\n","authors":["Praneet Singh","Edward J. Delp","Amy R. Reibman"],"pdf_url":"https://arxiv.org/pdf/2310.06945v1.pdf","comment":"Accepted to Autonomous Vehicles and Machines 2023 Conference, IS&T\n  Electronic Imaging (EI) Symposium"},{"id":"http://arxiv.org/abs/2305.09900v2","updated":"2023-10-10T19:01:41Z","published":"2023-05-17T02:20:34Z","title":"Efficient Equivariant Transfer Learning from Pretrained Models","summary":"  Efficient transfer learning algorithms are key to the success of foundation\nmodels on diverse downstream tasks even with limited data. Recent works of Basu\net al. (2023) and Kaba et al. (2022) propose group averaging (equitune) and\noptimization-based methods, respectively, over features from group-transformed\ninputs to obtain equivariant outputs from non-equivariant neural networks.\nWhile Kaba et al. (2022) are only concerned with training from scratch, we find\nthat equitune performs poorly on equivariant zero-shot tasks despite good\nfinetuning results. We hypothesize that this is because pretrained models\nprovide better quality features for certain transformations than others and\nsimply averaging them is deleterious. Hence, we propose {\\lambda}-equitune that\naverages the features using importance weights, {\\lambda}s. These weights are\nlearned directly from the data using a small neural network, leading to\nexcellent zero-shot and finetuned results that outperform equitune. Further, we\nprove that {\\lambda}-equitune is equivariant and a universal approximator of\nequivariant functions. Additionally, we show that the method of Kaba et al.\n(2022) used with appropriate loss functions, which we call equizero, also gives\nexcellent zero-shot and finetuned performance. Both equitune and equizero are\nspecial cases of {\\lambda}-equitune. To show the simplicity and generality of\nour method, we validate on a wide range of diverse applications and models such\nas 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in\nnatural language generation (NLG), 4) compositional generalization in\nlanguages, and 5) image classification using pretrained CNNs such as Resnet and\nAlexnet.\n","authors":["Sourya Basu","Pulkit Katdare","Prasanna Sattigeri","Vijil Chenthamarakshan","Katherine Driggs-Campbell","Payel Das","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2305.09900v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06916v1","updated":"2023-10-10T18:12:46Z","published":"2023-10-10T18:12:46Z","title":"Distributed Transfer Learning with 4th Gen Intel Xeon Processors","summary":"  In this paper, we explore how transfer learning, coupled with Intel Xeon,\nspecifically 4th Gen Intel Xeon scalable processor, defies the conventional\nbelief that training is primarily GPU-dependent. We present a case study where\nwe achieved near state-of-the-art accuracy for image classification on a\npublicly available Image Classification TensorFlow dataset using Intel Advanced\nMatrix Extensions(AMX) and distributed training with Horovod.\n","authors":["Lakshmi Arunachalam","Fahim Mohammad","Vrushabh H. Sanghavi"],"pdf_url":"https://arxiv.org/pdf/2310.06916v1.pdf","comment":"4 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.06907v1","updated":"2023-10-10T18:03:41Z","published":"2023-10-10T18:03:41Z","title":"Self-supervised Object-Centric Learning for Videos","summary":"  Unsupervised multi-object segmentation has shown impressive results on images\nby utilizing powerful semantics learned from self-supervised pretraining. An\nadditional modality such as depth or motion is often used to facilitate the\nsegmentation in video sequences. However, the performance improvements observed\nin synthetic sequences, which rely on the robustness of an additional cue, do\nnot translate to more challenging real-world scenarios. In this paper, we\npropose the first fully unsupervised method for segmenting multiple objects in\nreal-world sequences. Our object-centric learning framework spatially binds\nobjects to slots on each frame and then relates these slots across frames. From\nthese temporally-aware slots, the training objective is to reconstruct the\nmiddle frame in a high-level semantic feature space. We propose a masking\nstrategy by dropping a significant portion of tokens in the feature space for\nefficiency and regularization. Additionally, we address over-clustering by\nmerging slots based on similarity. Our method can successfully segment multiple\ninstances of complex and high-variety classes in YouTube videos.\n","authors":["Görkay Aydemir","Weidi Xie","Fatma Güney"],"pdf_url":"https://arxiv.org/pdf/2310.06907v1.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.06906v1","updated":"2023-10-10T18:03:29Z","published":"2023-10-10T18:03:29Z","title":"Distillation Improves Visual Place Recognition for Low-Quality Queries","summary":"  The shift to online computing for real-time visual localization often\nrequires streaming query images/videos to a server for visual place recognition\n(VPR), where fast video transmission may result in reduced resolution or\nincreased quantization. This compromises the quality of global image\ndescriptors, leading to decreased VPR performance. To improve the low recall\nrate for low-quality query images, we present a simple yet effective method\nthat uses high-quality queries only during training to distill better feature\nrepresentations for deep-learning-based VPR, such as NetVLAD. Specifically, we\nuse mean squared error (MSE) loss between the global descriptors of queries\nwith different qualities, and inter-channel correlation knowledge distillation\n(ICKD) loss over their corresponding intermediate features. We validate our\napproach using the both Pittsburgh 250k dataset and our own indoor dataset with\nvarying quantization levels. By fine-tuning NetVLAD parameters with our\ndistillation-augmented losses, we achieve notable VPR recall-rate improvements\nover low-quality queries, as demonstrated in our extensive experimental\nresults. We believe this work not only pushes forward the VPR research but also\nprovides valuable insights for applications needing dependable place\nrecognition under resource-limited conditions.\n","authors":["Anbang Yang","Yao Wang","John-Ross Rizzo","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2310.06906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06904v1","updated":"2023-10-10T18:01:52Z","published":"2023-10-10T18:01:52Z","title":"Mitigating stereotypical biases in text to image generative systems","summary":"  State-of-the-art generative text-to-image models are known to exhibit social\nbiases and over-represent certain groups like people of perceived lighter skin\ntones and men in their outcomes. In this work, we propose a method to mitigate\nsuch biases and ensure that the outcomes are fair across different groups of\npeople. We do this by finetuning text-to-image models on synthetic data that\nvaries in perceived skin tones and genders constructed from diverse text\nprompts. These text prompts are constructed from multiplicative combinations of\nethnicities, genders, professions, age groups, and so on, resulting in diverse\nsynthetic data. Our diversity finetuned (DFT) model improves the group fairness\nmetric by 150% for perceived skin tone and 97.7% for perceived gender. Compared\nto baselines, DFT models generate more people with perceived darker skin tone\nand more women. To foster open research, we will release all text prompts and\ncode to generate training images.\n","authors":["Piero Esposito","Parmida Atighehchian","Anastasis Germanidis","Deepti Ghadiyaram"],"pdf_url":"https://arxiv.org/pdf/2310.06904v1.pdf","comment":"4 figures, 8 pages"}]},"2023-10-11T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2305.05658v2","updated":"2023-10-11T17:59:44Z","published":"2023-05-09T17:52:59Z","title":"TidyBot: Personalized Robot Assistance with Large Language Models","summary":"  For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people's preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.\n","authors":["Jimmy Wu","Rika Antonova","Adam Kan","Marion Lepert","Andy Zeng","Shuran Song","Jeannette Bohg","Szymon Rusinkiewicz","Thomas Funkhouser"],"pdf_url":"https://arxiv.org/pdf/2305.05658v2.pdf","comment":"Accepted to Autonomous Robots (AuRo) - Special Issue: Large Language\n  Models in Robotics, 2023 and IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 2023. Project page:\n  https://tidybot.cs.princeton.edu"},{"id":"http://arxiv.org/abs/2310.07706v1","updated":"2023-10-11T17:57:13Z","published":"2023-10-11T17:57:13Z","title":"Pixel State Value Network for Combined Prediction and Planning in\n  Interactive Environments","summary":"  Automated vehicles operating in urban environments have to reliably interact\nwith other traffic participants. Planning algorithms often utilize separate\nprediction modules forecasting probabilistic, multi-modal, and interactive\nbehaviors of objects. Designing prediction and planning as two separate modules\nintroduces significant challenges, particularly due to the interdependence of\nthese modules. This work proposes a deep learning methodology to combine\nprediction and planning. A conditional GAN with the U-Net architecture is\ntrained to predict two high-resolution image sequences. The sequences represent\nexplicit motion predictions, mainly used to train context understanding, and\npixel state values suitable for planning encoding kinematic reachability,\nobject dynamics, safety, and driving comfort. The model can be trained offline\non target images rendered by a sampling-based model-predictive planner,\nleveraging real-world driving data. Our results demonstrate intuitive behavior\nin complex situations, such as lane changes amidst conflicting objectives.\n","authors":["Sascha Rosbach","Stefan M. Leupold","Simon Großjohann","Stefan Roth"],"pdf_url":"https://arxiv.org/pdf/2310.07706v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2310.07649v1","updated":"2023-10-11T16:46:44Z","published":"2023-10-11T16:46:44Z","title":"Automated Layout Design and Control of Robust Cooperative Grasped-Load\n  Aerial Transportation Systems","summary":"  We present a novel approach to cooperative aerial transportation through a\nteam of drones, using optimal control theory and a hierarchical control\nstrategy. We assume the drones are connected to the payload through rigid\nattachments, essentially transforming the whole system into a larger flying\nobject with \"thrust modules\" at the attachment locations of the drones. We\ninvestigate the optimal arrangement of the thrust modules around the payload,\nso that the resulting system is robust to disturbances. We choose the\n$\\mathcal{H}_2$ norm as a measure of robustness, and propose an iterative\noptimization routine to compute the optimal layout of the vehicles around the\nobject. We experimentally validate our approach using four drones and comparing\nthe disturbance rejection performances achieved by two different layouts (the\noptimal one and a sub-optimal one), and observe that the results match our\npredictions.\n","authors":["Carlo Bosio","Jerry Tang","Ting-Hao Wang","Mark W. Mueller"],"pdf_url":"https://arxiv.org/pdf/2310.07649v1.pdf","comment":"7 pages, 7 figures, conference paper"},{"id":"http://arxiv.org/abs/2303.08268v3","updated":"2023-10-11T16:17:20Z","published":"2023-03-14T23:01:27Z","title":"Chat with the Environment: Interactive Multimodal Perception Using Large\n  Language Models","summary":"  Programming robot behavior in a complex world faces challenges on multiple\nlevels, from dextrous low-level skills to high-level planning and reasoning.\nRecent pre-trained Large Language Models (LLMs) have shown remarkable reasoning\nability in few-shot robotic planning. However, it remains challenging to ground\nLLMs in multimodal sensory input and continuous action output, while enabling a\nrobot to interact with its environment and acquire novel information as its\npolicies unfold. We develop a robot interaction scenario with a partially\nobservable state, which necessitates a robot to decide on a range of epistemic\nactions in order to sample sensory information among multiple modalities,\nbefore being able to execute the task correctly. Matcha (Multimodal environment\nchatting) agent, an interactive perception framework, is therefore proposed\nwith an LLM as its backbone, whose ability is exploited to instruct epistemic\nactions and to reason over the resulting multimodal sensations (vision, sound,\nhaptics, proprioception), as well as to plan an entire task execution based on\nthe interactively acquired information. Our study demonstrates that LLMs can\nprovide high-level planning and reasoning skills and control interactive robot\nbehavior in a multimodal environment, while multimodal modules with the context\nof the environmental state help ground the LLMs and extend their processing\nability. The project website can be found at https://matcha-agent.github.io.\n","authors":["Xufeng Zhao","Mengdi Li","Cornelius Weber","Muhammad Burhan Hafez","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2303.08268v3.pdf","comment":"IROS2023, Detroit. See the project website at\n  https://matcha-agent.github.io"},{"id":"http://arxiv.org/abs/2310.07621v1","updated":"2023-10-11T16:04:02Z","published":"2023-10-11T16:04:02Z","title":"AG-CVG: Coverage Planning with a Mobile Recharging UGV and an\n  Energy-Constrained UAV","summary":"  In this paper, we present an approach for coverage path planning for a team\nof an energy-constrained Unmanned Aerial Vehicle (UAV) and an Unmanned Ground\nVehicle (UGV). Both the UAV and the UGV have predefined areas that they have to\ncover. The goal is to perform complete coverage by both robots while minimizing\nthe coverage time. The UGV can also serve as a mobile recharging station. The\nUAV and UGV need to occasionally rendezvous for recharging. We propose a\nheuristic method to address this NP-Hard planning problem. Our approach\ninvolves initially determining coverage paths without factoring in energy\nconstraints. Subsequently, we cluster segments of these paths and employ graph\nmatching to assign UAV clusters to UGV clusters for efficient recharging\nmanagement. We perform numerical analysis on real-world coverage applications\nand show that compared with a greedy approach our method reduces rendezvous\noverhead on average by 11.33\\%. We demonstrate proof-of-concept with a team of\na VOXL m500 drone and a Clearpath Jackal ground vehicle, providing a complete\nsystem from the offline algorithm to the field execution.\n","authors":["Nare Karapetyan","Ahmad Bilal Asghar","Amisha Bhaskar","Guangyao Shi","Dinesh Manocha","Pratap Tokekar"],"pdf_url":"https://arxiv.org/pdf/2310.07621v1.pdf","comment":"Submitted to ICRA 2024 Conference"},{"id":"http://arxiv.org/abs/2310.07608v1","updated":"2023-10-11T15:50:41Z","published":"2023-10-11T15:50:41Z","title":"Leader-Follower Formation Control of Perturbed Nonholonomic Agents along\n  Parametric Curves with Directed Communication","summary":"  In this paper, we propose a novel formation controller for nonholonomic\nagents to form general parametric curves. First, we derive a unified parametric\nrepresentation for both open and closed curves. Then, a leader-follower\nformation controller is designed to form the parametric curves. We consider\ndirected communications and constant input disturbances rejection in the\ncontroller design. Rigorous Lyapunov-based stability analysis proves the\nasymptotic stability of the proposed controller. Detailed numerical simulations\nand experimental studies are conducted to verify the performance of the\nproposed method.\n","authors":["Bin Zhang","Hui Zhi","Jose Guadalupe Romero","David Navarro-Alarcon"],"pdf_url":"https://arxiv.org/pdf/2310.07608v1.pdf","comment":"6 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.07525v1","updated":"2023-10-11T14:24:20Z","published":"2023-10-11T14:24:20Z","title":"ViT-A*: Legged Robot Path Planning using Vision Transformer A*","summary":"  Legged robots, particularly quadrupeds, offer promising navigation\ncapabilities, especially in scenarios requiring traversal over diverse terrains\nand obstacle avoidance. This paper addresses the challenge of enabling legged\nrobots to navigate complex environments effectively through the integration of\ndata-driven path-planning methods. We propose an approach that utilizes\ndifferentiable planners, allowing the learning of end-to-end global plans via a\nneural network for commanding quadruped robots. The approach leverages 2D maps\nand obstacle specifications as inputs to generate a global path. To enhance the\nfunctionality of the developed neural network-based path planner, we use Vision\nTransformers (ViT) for map pre-processing, to enable the effective handling of\nlarger maps. Experimental evaluations on two real robotic quadrupeds (Boston\nDynamics Spot and Unitree Go1) demonstrate the effectiveness and versatility of\nthe proposed approach in generating reliable path plans.\n","authors":["Jianwei Liu","Shirui Lyu","Denis Hadjivelichkov","Valerio Modugno","Dimitrios Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2310.07525v1.pdf","comment":"6 pages, 6 figures, conference"},{"id":"http://arxiv.org/abs/2310.07473v1","updated":"2023-10-11T13:19:29Z","published":"2023-10-11T13:19:29Z","title":"FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation","summary":"  Learning to navigate to an image-specified goal is an important but\nchallenging task for autonomous systems. The agent is required to reason the\ngoal location from where a picture is shot. Existing methods try to solve this\nproblem by learning a navigation policy, which captures semantic features of\nthe goal image and observation image independently and lastly fuses them for\npredicting a sequence of navigation actions. However, these methods suffer from\ntwo major limitations. 1) They may miss detailed information in the goal image,\nand thus fail to reason the goal location. 2) More critically, it is hard to\nfocus on the goal-relevant regions in the observation image, because they\nattempt to understand observation without goal conditioning. In this paper, we\naim to overcome these limitations by designing a Fine-grained Goal Prompting\n(FGPrompt) method for image-goal navigation. In particular, we leverage\nfine-grained and high-resolution feature maps in the goal image as prompts to\nperform conditioned embedding, which preserves detailed information in the goal\nimage and guides the observation encoder to pay attention to goal-relevant\nregions. Compared with existing methods on the image-goal navigation benchmark,\nour method brings significant performance improvement on 3 benchmark datasets\n(i.e., Gibson, MP3D, and HM3D). Especially on Gibson, we surpass the\nstate-of-the-art success rate by 8% with only 1/50 model size. Project page:\nhttps://xinyusun.github.io/fgprompt-pages\n","authors":["Xinyu Sun","Peihao Chen","Jugang Fan","Thomas H. Li","Jian Chen","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07473v1.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.07438v1","updated":"2023-10-11T12:41:32Z","published":"2023-10-11T12:41:32Z","title":"DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for\n  Trajectory Prediction","summary":"  Predicting temporally consistent road users' trajectories in a multi-agent\nsetting is a challenging task due to unknown characteristics of agents and\ntheir varying intentions. Besides using semantic map information and modeling\ninteractions, it is important to build an effective mechanism capable of\nreasoning about behaviors at different levels of granularity. To this end, we\npropose Dynamic goal quErieS with temporal Transductive alIgNmEnt (DESTINE)\nmethod. Unlike past arts, our approach 1) dynamically predicts agents' goals\nirrespective of particular road structures, such as lanes, allowing the method\nto produce a more accurate estimation of destinations; 2) achieves map\ncompliant predictions by generating future trajectories in a coarse-to-fine\nfashion, where the coarser predictions at a lower frame rate serve as\nintermediate goals; and 3) uses an attention module designed to temporally\nalign predicted trajectories via masked attention. Using the common Argoverse\nbenchmark dataset, we show that our method achieves state-of-the-art\nperformance on various metrics, and further investigate the contributions of\nproposed modules via comprehensive ablation studies.\n","authors":["Rezaul Karim","Soheil Mohamad Alizadeh Shabestary","Amir Rasouli"],"pdf_url":"https://arxiv.org/pdf/2310.07438v1.pdf","comment":"6 tables 4 figures"},{"id":"http://arxiv.org/abs/2310.07434v1","updated":"2023-10-11T12:36:38Z","published":"2023-10-11T12:36:38Z","title":"HealthWalk: Promoting Health and Mobility through Sensor-Based Rollator\n  Walker Assistance","summary":"  Rollator walkers allow people with physical limitations to increase their\nmobility and give them the confidence and independence to participate in\nsociety for longer. However, rollator walker users often have poor posture,\nleading to further health problems and, in the worst case, falls. Integrating\nsensors into rollator walker designs can help to address this problem and\nresults in a platform that allows several other interesting use cases. This\npaper briefly overviews existing systems and the current research directions\nand challenges in this field. We also present our early HealthWalk rollator\nwalker prototype for data collection with older people, rheumatism, multiple\nsclerosis and Parkinson patients, and individuals with visual impairments.\n","authors":["Ivanna Kramer","Kevin Weirauch","Sabine Bauer","Mark Oliver Mints","Peer Neubert"],"pdf_url":"https://arxiv.org/pdf/2310.07434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07433v1","updated":"2023-10-11T12:34:39Z","published":"2023-10-11T12:34:39Z","title":"Imitation Learning from Observation with Automatic Discount Scheduling","summary":"  Humans often acquire new skills through observation and imitation. For\nrobotic agents, learning from the plethora of unlabeled video demonstration\ndata available on the Internet necessitates imitating the expert without access\nto its action, presenting a challenge known as Imitation Learning from\nObservations (ILfO). A common approach to tackle ILfO problems is to convert\nthem into inverse reinforcement learning problems, utilizing a proxy reward\ncomputed from the agent's and the expert's observations. Nonetheless, we\nidentify that tasks characterized by a progress dependency property pose\nsignificant challenges for such approaches; in these tasks, the agent needs to\ninitially learn the expert's preceding behaviors before mastering the\nsubsequent ones. Our investigation reveals that the main cause is that the\nreward signals assigned to later steps hinder the learning of initial\nbehaviors. To address this challenge, we present a novel ILfO framework that\nenables the agent to master earlier behaviors before advancing to later ones.\nWe introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively\nalters the discount factor in reinforcement learning during the training phase,\nprioritizing earlier rewards initially and gradually engaging later rewards\nonly when the earlier behaviors have been mastered. Our experiments, conducted\non nine Meta-World tasks, demonstrate that our method significantly outperforms\nstate-of-the-art methods across all tasks, including those that are unsolvable\nby them.\n","authors":["Yuyang Liu","Weijun Dong","Yingdong Hu","Chuan Wen","Zhao-Heng Yin","Chongjie Zhang","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2310.07433v1.pdf","comment":"Submitted to ICLR 2024"},{"id":"http://arxiv.org/abs/2310.07393v1","updated":"2023-10-11T11:21:45Z","published":"2023-10-11T11:21:45Z","title":"RANS: Highly-Parallelised Simulator for Reinforcement Learning based\n  Autonomous Navigating Spacecrafts","summary":"  Nowadays, realistic simulation environments are essential to validate and\nbuild reliable robotic solutions. This is particularly true when using\nReinforcement Learning (RL) based control policies. To this end, both robotics\nand RL developers need tools and workflows to create physically accurate\nsimulations and synthetic datasets. Gazebo, MuJoCo, Webots, Pybullets or Isaac\nSym are some of the many tools available to simulate robotic systems.\nDeveloping learning-based methods for space navigation is, due to the highly\ncomplex nature of the problem, an intensive data-driven process that requires\nhighly parallelized simulations. When it comes to the control of spacecrafts,\nthere is no easy to use simulation library designed for RL. We address this gap\nby harnessing the capabilities of NVIDIA Isaac Gym, where both physics\nsimulation and the policy training reside on GPU. Building on this tool, we\nprovide an open-source library enabling users to simulate thousands of parallel\nspacecrafts, that learn a set of maneuvering tasks, such as position, attitude,\nand velocity control. These tasks enable to validate complex space scenarios,\nsuch as trajectory optimization for landing, docking, rendezvous and more.\n","authors":["Matteo El-Hariry","Antoine Richard","Miguel Olivares-Mendez"],"pdf_url":"https://arxiv.org/pdf/2310.07393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07392v1","updated":"2023-10-11T11:20:35Z","published":"2023-10-11T11:20:35Z","title":"Deep Kernel and Image Quality Estimators for Optimizing Robotic\n  Ultrasound Controller using Bayesian Optimization","summary":"  Ultrasound is a commonly used medical imaging modality that requires expert\nsonographers to manually maneuver the ultrasound probe based on the acquired\nimage. Autonomous Robotic Ultrasound (A-RUS) is an appealing alternative to\nthis manual procedure in order to reduce sonographers' workload. The key\nchallenge to A-RUS is optimizing the ultrasound image quality for the region of\ninterest across different patients. This requires knowledge of anatomy,\nrecognition of error sources and precise probe position, orientation and\npressure. Sample efficiency is important while optimizing these parameters\nassociated with the robotized probe controller. Bayesian Optimization (BO), a\nsample-efficient optimization framework, has recently been applied to optimize\nthe 2D motion of the probe. Nevertheless, further improvements are needed to\nimprove the sample efficiency for high-dimensional control of the probe. We aim\nto overcome this problem by using a neural network to learn a low-dimensional\nkernel in BO, termed as Deep Kernel (DK). The neural network of DK is trained\nusing probe and image data acquired during the procedure. The two image quality\nestimators are proposed that use a deep convolution neural network and provide\nreal-time feedback to the BO. We validated our framework using these two\nfeedback functions on three urinary bladder phantoms. We obtained over 50%\nincrease in sample efficiency for 6D control of the robotized probe.\nFurthermore, our results indicate that this performance enhancement in BO is\nindependent of the specific training dataset, demonstrating inter-patient\nadaptability.\n","authors":["Deepak Raina","SH Chandrashekhara","Richard Voyles","Juan Wachs","Subir Kumar Saha"],"pdf_url":"https://arxiv.org/pdf/2310.07392v1.pdf","comment":"Accepted in IEEE International Symposium on Medical Robotics (ISMR)\n  2023"},{"id":"http://arxiv.org/abs/2310.07390v1","updated":"2023-10-11T11:10:43Z","published":"2023-10-11T11:10:43Z","title":"LESS-Map: Lightweight and Evolving Semantic Map in Parking Lots for\n  Long-term Self-Localization","summary":"  Precise and long-term stable localization is essential in parking lots for\ntasks like autonomous driving or autonomous valet parking, \\textit{etc}.\nExisting methods rely on a fixed and memory-inefficient map, which lacks robust\ndata association approaches. And it is not suitable for precise localization or\nlong-term map maintenance. In this paper, we propose a novel mapping,\nlocalization, and map update system based on ground semantic features,\nutilizing low-cost cameras. We present a precise and lightweight\nparameterization method to establish improved data association and achieve\naccurate localization at centimeter-level. Furthermore, we propose a novel map\nupdate approach by implementing high-quality data association for parameterized\nsemantic features, allowing continuous map update and refinement during\nre-localization, while maintaining centimeter-level accuracy. We validate the\nperformance of the proposed method in real-world experiments and compare it\nagainst state-of-the-art algorithms. The proposed method achieves an average\naccuracy improvement of 5cm during the registration process. The generated maps\nconsume only a compact size of 450 KB/km and remain adaptable to evolving\nenvironments through continuous update.\n","authors":["Mingrui Liu","Xinyang Tang","Yeqiang Qian","Jiming Chen","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07390v1.pdf","comment":"6 pages, 8 figures"},{"id":"http://arxiv.org/abs/2307.14721v3","updated":"2023-10-11T09:51:04Z","published":"2023-07-27T09:21:08Z","title":"Singularity Distance Computations for 3-RPR Manipulators Using Intrinsic\n  Metrics","summary":"  We present an efficient algorithm for computing the closest singular\nconfiguration to each non-singular pose of a 3-RPR planar manipulator\nperforming a 1-parametric motion. By considering a 3-RPR manipulator as a\nplanar framework, one can use methods from rigidity theory to compute the\nsingularity distance with respect to an intrinsic metric. Such a metric has the\nadvantage over any performance index used for indicating the closeness to\nsingularities, that the obtained value is a distance, which equals the radius\nof a guaranteed singularity-free sphere in the joint space of the manipulator.\nThe proposed method can take different design options into account as the\nplatform/base can be seen as a triangular plate or as a pin-jointed triangular\nbar structure. Moreover, we also allow the additional possibility of pinning\ndown the base/platform triangle to the fixed/moving system thus it cannot be\ndeformed. For the resulting nine interpretations, we compute the corresponding\nintrinsic metrics based on the total elastic strain energy density of the\nframework using the physical concept of Green-Lagrange strain. The global\noptimization problem of finding the closest singular configuration with respect\nto these metrics is solved by using tools from numerical algebraic geometry.\nThe proposed algorithm is demonstrated based on an example, which is also used\nto compare the obtained intrinsic singularity distances with the corresponding\nextrinsic ones.\n","authors":["Aditya Kapilavai","Georg Nawratil"],"pdf_url":"https://arxiv.org/pdf/2307.14721v3.pdf","comment":"30 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.07335v1","updated":"2023-10-11T09:25:24Z","published":"2023-10-11T09:25:24Z","title":"Exploring Social Motion Latent Space and Human Awareness for Effective\n  Robot Navigation in Crowded Environments","summary":"  This work proposes a novel approach to social robot navigation by learning to\ngenerate robot controls from a social motion latent space. By leveraging this\nsocial motion latent space, the proposed method achieves significant\nimprovements in social navigation metrics such as success rate, navigation\ntime, and trajectory length while producing smoother (less jerk and angular\ndeviations) and more anticipatory trajectories. The superiority of the proposed\nmethod is demonstrated through comparison with baseline models in various\nscenarios. Additionally, the concept of humans' awareness towards the robot is\nintroduced into the social robot navigation framework, showing that\nincorporating human awareness leads to shorter and smoother trajectories owing\nto humans' ability to positively interact with the robot.\n","authors":["Junaid Ahmed Ansari","Satyajit Tourani","Gourav Kumar","Brojeshwar Bhowmick"],"pdf_url":"https://arxiv.org/pdf/2310.07335v1.pdf","comment":"Accepted at IROS 2023"},{"id":"http://arxiv.org/abs/2310.02044v2","updated":"2023-10-11T09:21:23Z","published":"2023-10-03T13:35:49Z","title":"Video Transformers under Occlusion: How Physics and Background\n  Attributes Impact Large Models for Robotic Manipulation","summary":"  As transformer architectures and dataset sizes continue to scale, the need to\nunderstand the specific dataset factors affecting model performance becomes\nincreasingly urgent. This paper investigates how object physics attributes\n(color, friction coefficient, shape) and background characteristics (static,\ndynamic, background complexity) influence the performance of Video Transformers\nin trajectory prediction tasks under occlusion. Beyond mere occlusion\nchallenges, this study aims to investigate three questions: How do object\nphysics attributes and background characteristics influence the model\nperformance? What kinds of attributes are most influential to the model\ngeneralization? Is there a data saturation point for large transformer model\nperformance within a single task? To facilitate this research, we present\nOccluManip, a real-world video-based robot pushing dataset comprising 460,000\nconsistent recordings of objects with different physics and varying\nbackgrounds. 1.4 TB and in total 1278 hours of high-quality videos of flexible\ntemporal length along with target object trajectories are collected,\naccommodating tasks with different temporal requirements. Additionally, we\npropose Video Occlusion Transformer (VOT), a generic video-transformer-based\nnetwork achieving an average 96% accuracy across all 18 sub-datasets provided\nin OccluManip. OccluManip and VOT will be released at:\nhttps://github.com/ShutongJIN/OccluManip.git\n","authors":["Shutong Jin","Ruiyu Wang","Muhammad Zahid","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2310.02044v2.pdf","comment":"Under review at IEEE ICRA 2024"},{"id":"http://arxiv.org/abs/2104.05859v5","updated":"2023-10-11T09:07:01Z","published":"2021-04-12T23:14:41Z","title":"Rapid Exploration for Open-World Navigation with Latent Goal Models","summary":"  We describe a robotic learning system for autonomous exploration and\nnavigation in diverse, open-world environments. At the core of our method is a\nlearned latent variable model of distances and actions, along with a\nnon-parametric topological memory of images. We use an information bottleneck\nto regularize the learned policy, giving us (i) a compact visual representation\nof goals, (ii) improved generalization capabilities, and (iii) a mechanism for\nsampling feasible goals for exploration. Trained on a large offline dataset of\nprior experience, the model acquires a representation of visual goals that is\nrobust to task-irrelevant distractors. We demonstrate our method on a mobile\nground robot in open-world exploration scenarios. Given an image of a goal that\nis up to 80 meters away, our method leverages its representation to explore and\ndiscover the goal in under 20 minutes, even amidst previously-unseen obstacles\nand weather conditions. Please check out the project website for videos of our\nexperiments and information about the real-world dataset used at\nhttps://sites.google.com/view/recon-robot.\n","authors":["Dhruv Shah","Benjamin Eysenbach","Gregory Kahn","Nicholas Rhinehart","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2104.05859v5.pdf","comment":"Presented at 5th Annual Conference on Robot Learning (CoRL 2021),\n  London, UK as an Oral Talk. Project page and dataset release at\n  https://sites.google.com/view/recon-robot"},{"id":"http://arxiv.org/abs/2303.05674v2","updated":"2023-10-11T08:54:22Z","published":"2023-03-10T02:55:50Z","title":"Robotic Applications of Pre-Trained Vision-Language Models to Various\n  Recognition Behaviors","summary":"  In recent years, a number of models that learn the relations between vision\nand language from large datasets have been released. These models perform a\nvariety of tasks, such as answering questions about images, retrieving\nsentences that best correspond to images, and finding regions in images that\ncorrespond to phrases. Although there are some examples, the connection between\nthese pre-trained vision-language models and robotics is still weak. If they\nare directly connected to robot motions, they lose their versatility due to the\nembodiment of the robot and the difficulty of data collection, and become\ninapplicable to a wide range of bodies and situations. Therefore, in this\nstudy, we categorize and summarize the methods to utilize the pre-trained\nvision-language models flexibly and easily in a way that the robot can\nunderstand, without directly connecting them to robot motions. We discuss how\nto use these models for robot motion selection and motion planning without\nre-training the models. We consider five types of methods to extract\ninformation understandable for robots, and show the results of state\nrecognition, object recognition, affordance recognition, relation recognition,\nand anomaly detection based on the combination of these five methods. We expect\nthat this study will add flexibility and ease-of-use, as well as new\napplications, to the recognition behavior of existing robots.\n","authors":["Kento Kawaharazuka","Yoshiki Obinata","Naoaki Kanazawa","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2303.05674v2.pdf","comment":"Accepted at Humanoids2023"},{"id":"http://arxiv.org/abs/2310.07263v1","updated":"2023-10-11T07:39:42Z","published":"2023-10-11T07:39:42Z","title":"CoPAL: Corrective Planning of Robot Actions with Large Language Models","summary":"  In the pursuit of fully autonomous robotic systems capable of taking over\ntasks traditionally performed by humans, the complexity of open-world\nenvironments poses a considerable challenge. Addressing this imperative, this\nstudy contributes to the field of Large Language Models (LLMs) applied to task\nand motion planning for robots. We propose a system architecture that\norchestrates a seamless interplay between multiple cognitive levels,\nencompassing reasoning, planning, and motion generation. At its core lies a\nnovel replanning strategy that handles physically grounded, logical, and\nsemantic errors in the generated plans. We demonstrate the efficacy of the\nproposed feedback architecture, particularly its impact on executability,\ncorrectness, and time complexity via empirical evaluation in the context of a\nsimulation and two intricate real-world scenarios: blocks world, barman and\npizza preparation.\n","authors":["Frank Joublin","Antonello Ceravola","Pavel Smirnov","Felix Ocker","Joerg Deigmoeller","Anna Belardinelli","Chao Wang","Stephan Hasler","Daniel Tanneberg","Michael Gienger"],"pdf_url":"https://arxiv.org/pdf/2310.07263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07247v1","updated":"2023-10-11T07:24:27Z","published":"2023-10-11T07:24:27Z","title":"Optimizing the Placement of Roadside LiDARs for Autonomous Driving","summary":"  Multi-agent cooperative perception is an increasingly popular topic in the\nfield of autonomous driving, where roadside LiDARs play an essential role.\nHowever, how to optimize the placement of roadside LiDARs is a crucial but\noften overlooked problem. This paper proposes an approach to optimize the\nplacement of roadside LiDARs by selecting optimized positions within the scene\nfor better perception performance. To efficiently obtain the best combination\nof locations, a greedy algorithm based on perceptual gain is proposed, which\nselects the location that can maximize the perceptual gain sequentially. We\ndefine perceptual gain as the increased perceptual capability when a new LiDAR\nis placed. To obtain the perception capability, we propose a perception\npredictor that learns to evaluate LiDAR placement using only a single point\ncloud frame. A dataset named Roadside-Opt is created using the CARLA simulator\nto facilitate research on the roadside LiDAR placement problem.\n","authors":["Wentao Jiang","Hao Xiang","Xinyu Cai","Runsheng Xu","Jiaqi Ma","Yikang Li","Gim Hee Lee","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2310.07247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07237v1","updated":"2023-10-11T06:58:22Z","published":"2023-10-11T06:58:22Z","title":"SAGE-ICP: Semantic Information-Assisted ICP","summary":"  Robust and accurate pose estimation in unknown environments is an essential\npart of robotic applications. We focus on LiDAR-based point-to-point ICP\ncombined with effective semantic information. This paper proposes a novel\nsemantic information-assisted ICP method named SAGE-ICP, which leverages\nsemantics in odometry. The semantic information for the whole scan is timely\nand efficiently extracted by a 3D convolution network, and these point-wise\nlabels are deeply involved in every part of the registration, including\nsemantic voxel downsampling, data association, adaptive local map, and dynamic\nvehicle removal. Unlike previous semantic-aided approaches, the proposed method\ncan improve localization accuracy in large-scale scenes even if the semantic\ninformation has certain errors. Experimental evaluations on KITTI and KITTI-360\nshow that our method outperforms the baseline methods, and improves accuracy\nwhile maintaining real-time performance, i.e., runs faster than the sensor\nframe rate.\n","authors":["Jiaming Cui","Jiming Chen","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07237v1.pdf","comment":"6+1 pages, 4 figures"},{"id":"http://arxiv.org/abs/2307.09531v3","updated":"2023-10-11T06:41:24Z","published":"2023-07-18T18:20:56Z","title":"LOG-LIO: A LiDAR-Inertial Odometry with Efficient Local Geometric\n  Information Estimation","summary":"  Local geometric information, i.e. normal and distribution of points, is\ncrucial for LiDAR-based simultaneous localization and mapping (SLAM) because it\nprovides constraints for data association, which further determines the\ndirection of optimization and ultimately affects the accuracy of localization.\nHowever, estimating normal and distribution of points are time-consuming tasks\neven with the assistance of kdtree or volumetric maps. To achieve fast normal\nestimation, we look into the structure of LiDAR scan and propose a ring-based\nfast approximate least squares (Ring FALS) method. With the Ring structural\ninformation, estimating the normal requires only the range information of the\npoints when a new scan arrives. To efficiently estimate the distribution of\npoints, we extend the ikd-tree to manage the map in voxels and update the\ndistribution of points in each voxel incrementally while maintaining its\nconsistency with the normal estimation. We further fix the distribution after\nits convergence to balance the time consumption and the correctness of\nrepresentation. Based on the extracted and maintained local geometric\ninformation, we devise a robust and accurate hierarchical data association\nscheme where point-to-surfel association is prioritized over point-to-plane.\nExtensive experiments on diverse public datasets demonstrate the advantages of\nour system compared to other state-of-the-art methods. Our open source\nimplementation is available at https://github.com/tiev-tongji/LOG-LIO.\n","authors":["Kai Huang","Junqiao Zhao","Zhongyang Zhu","Chen Ye","Tiantian Feng"],"pdf_url":"https://arxiv.org/pdf/2307.09531v3.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2305.16612v2","updated":"2023-10-11T04:56:34Z","published":"2023-05-26T04:13:28Z","title":"Spatio-Temporal Transformer-Based Reinforcement Learning for Robot Crowd\n  Navigation","summary":"  The social robot navigation is an open and challenging problem. In existing\nwork, separate modules are used to capture spatial and temporal features,\nrespectively. However, such methods lead to extra difficulties in improving the\nutilization of spatio-temporal features and reducing the conservative nature of\nnavigation policy. In light of this, we present a spatio-temporal\ntransformer-based policy optimization algorithm to enhance the utilization of\nspatio-temporal features, thereby facilitating the capture of human-robot\ninteractions. Specifically, this paper introduces a gated embedding mechanism\nthat effectively aligns the spatial and temporal representations by integrating\nboth modalities at the feature level. Then Transformer is leveraged to encode\nthe spatio-temporal semantic information, with hope of finding the optimal\nnavigation policy. Finally, a combination of spatio-temporal Transformer and\nself-adjusting policy entropy significantly reduces the conservatism of\nnavigation policies. Experimental results demonstrate the effectiveness of the\nproposed framework, where our method shows superior performance.\n","authors":["Haodong He","Hao Fu","Qiang Wang","Shuai Zhou","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2305.16612v2.pdf","comment":"The duplication rate is too high and the manuscript needs to be\n  withdrawn"},{"id":"http://arxiv.org/abs/2310.04675v2","updated":"2023-10-11T03:31:48Z","published":"2023-10-07T03:20:26Z","title":"Terrain-Aware Quadrupedal Locomotion via Reinforcement Learning","summary":"  In nature, legged animals have developed the ability to adapt to challenging\nterrains through perception, allowing them to plan safe body and foot\ntrajectories in advance, which leads to safe and energy-efficient locomotion.\nInspired by this observation, we present a novel approach to train a Deep\nNeural Network (DNN) policy that integrates proprioceptive and exteroceptive\nstates with a parameterized trajectory generator for quadruped robots to\ntraverse rough terrains. Our key idea is to use a DNN policy that can modify\nthe parameters of the trajectory generator, such as foot height and frequency,\nto adapt to different terrains. To encourage the robot to step on safe regions\nand save energy consumption, we propose foot terrain reward and lifting foot\nheight reward, respectively. By incorporating these rewards, our method can\nlearn a safer and more efficient terrain-aware locomotion policy that can move\na quadruped robot flexibly in any direction. To evaluate the effectiveness of\nour approach, we conduct simulation experiments on challenging terrains,\nincluding stairs, stepping stones, and poles. The simulation results\ndemonstrate that our approach can successfully direct the robot to traverse\nsuch tough terrains in any direction. Furthermore, we validate our method on a\nreal legged robot, which learns to traverse stepping stones with gaps over\n25.5cm.\n","authors":["Haojie Shi","Qingxu Zhu","Lei Han","Wanchao Chi","Tingguang Li","Max Q. -H. Meng"],"pdf_url":"https://arxiv.org/pdf/2310.04675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12088v2","updated":"2023-10-11T01:31:51Z","published":"2023-08-23T12:20:06Z","title":"Trajectory Tracking Control of Dual-PAM Soft Actuator with Hysteresis\n  Compensator","summary":"  Soft robotics is an emergent and swiftly evolving field. Pneumatic actuators\nare suitable for driving soft robots because of their superior performance.\nHowever, their control is not easy due to their hysteresis characteristics. In\nresponse to these challenges, we propose an adaptive control method to\ncompensate hysteresis of a soft actuator. Employing a novel dual pneumatic\nartificial muscle (PAM) bending actuator, the innovative control strategy\nabates hysteresis effects by dynamically modulating gains within a traditional\nPID controller corresponding with the predicted motion of the reference\ntrajectory. Through comparative experimental evaluation, we found that the new\ncontrol method outperforms its conventional counterparts regarding tracking\naccuracy and response speed. Our work reveals a new direction for advancing\ncontrol in soft actuators.\n","authors":["Junyi Shen","Tetsuro Miyazaki","Shingo Ohno","Maina Sogabe","Kenji Kawashima"],"pdf_url":"https://arxiv.org/pdf/2308.12088v2.pdf","comment":"Submitted to the IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2310.07937v1","updated":"2023-10-11T23:17:43Z","published":"2023-10-11T23:17:43Z","title":"Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using\n  Large Language Models","summary":"  In advanced human-robot interaction tasks, visual target navigation is\ncrucial for autonomous robots navigating unknown environments. While numerous\napproaches have been developed in the past, most are designed for single-robot\noperations, which often suffer from reduced efficiency and robustness due to\nenvironmental complexities. Furthermore, learning policies for multi-robot\ncollaboration are resource-intensive. To address these challenges, we propose\nCo-NavGPT, an innovative framework that integrates Large Language Models (LLMs)\nas a global planner for multi-robot cooperative visual target navigation.\nCo-NavGPT encodes the explored environment data into prompts, enhancing LLMs'\nscene comprehension. It then assigns exploration frontiers to each robot for\nefficient target search. Experimental results on Habitat-Matterport 3D (HM3D)\ndemonstrate that Co-NavGPT surpasses existing models in success rates and\nefficiency without any learning process, demonstrating the vast potential of\nLLMs in multi-robot collaboration domains. The supplementary video, prompts,\nand code can be accessed via the following link:\n\\href{https://sites.google.com/view/co-navgpt}{https://sites.google.com/view/co-navgpt}.\n","authors":["Bangguo Yu","Hamidreza Kasaei","Ming Cao"],"pdf_url":"https://arxiv.org/pdf/2310.07937v1.pdf","comment":"7 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2310.07932v1","updated":"2023-10-11T23:04:07Z","published":"2023-10-11T23:04:07Z","title":"What Matters to You? Towards Visual Representation Alignment for Robot\n  Learning","summary":"  When operating in service of people, robots need to optimize rewards aligned\nwith end-user preferences. Since robots will rely on raw perceptual inputs like\nRGB images, their rewards will inevitably use visual representations. Recently\nthere has been excitement in using representations from pre-trained visual\nmodels, but key to making these work in robotics is fine-tuning, which is\ntypically done via proxy tasks like dynamics prediction or enforcing temporal\ncycle-consistency. However, all these proxy tasks bypass the human's input on\nwhat matters to them, exacerbating spurious correlations and ultimately leading\nto robot behaviors that are misaligned with user preferences. In this work, we\npropose that robots should leverage human feedback to align their visual\nrepresentations with the end-user and disentangle what matters for the task. We\npropose Representation-Aligned Preference-based Learning (RAPL), a method for\nsolving the visual representation alignment problem and visual reward learning\nproblem through the lens of preference-based learning and optimal transport.\nAcross experiments in X-MAGICAL and in robotic manipulation, we find that\nRAPL's reward consistently generates preferred robot behaviors with high sample\nefficiency, and shows strong zero-shot generalization when the visual\nrepresentation is learned from a different embodiment than the robot's.\n","authors":["Ran Tian","Chenfeng Xu","Masayoshi Tomizuka","Jitendra Malik","Andrea Bajcsy"],"pdf_url":"https://arxiv.org/pdf/2310.07932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07902v1","updated":"2023-10-11T21:16:01Z","published":"2023-10-11T21:16:01Z","title":"Unraveling the Single Tangent Space Fallacy: An Analysis and\n  Clarification for Applying Riemannian Geometry in Robot Learning","summary":"  In the realm of robotics, numerous downstream robotics tasks leverage machine\nlearning methods for processing, modeling, or synthesizing data. Often, this\ndata comprises variables that inherently carry geometric constraints, such as\nthe unit-norm condition of quaternions representing rigid-body orientations or\nthe positive definiteness of stiffness and manipulability ellipsoids. Handling\nsuch geometric constraints effectively requires the incorporation of tools from\ndifferential geometry into the formulation of machine learning methods. In this\ncontext, Riemannian manifolds emerge as a powerful mathematical framework to\nhandle such geometric constraints. Nevertheless, their recent adoption in robot\nlearning has been largely characterized by a mathematically-flawed\nsimplification, hereinafter referred to as the ``single tangent space fallacy\".\nThis approach involves merely projecting the data of interest onto a single\ntangent (Euclidean) space, over which an off-the-shelf learning algorithm is\napplied. This paper provides a theoretical elucidation of various\nmisconceptions surrounding this approach and offers experimental evidence of\nits shortcomings. Finally, it presents valuable insights to promote best\npractices when employing Riemannian geometry within robot learning\napplications.\n","authors":["Noémie Jaquier","Leonel Rozo","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2310.07902v1.pdf","comment":"8 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2212.14116v2","updated":"2023-10-11T21:14:38Z","published":"2022-12-28T22:46:50Z","title":"Coordination of Drones at Scale: Decentralized Energy-aware Swarm\n  Intelligence for Spatio-temporal Sensing","summary":"  Smart City applications, such as traffic monitoring and disaster response,\noften use swarms of intelligent and cooperative drones to efficiently collect\nsensor data over different areas of interest and time spans. However, when the\nrequired sensing becomes spatio-temporally large and varying, a collective\narrangement of sensing tasks to a large number of battery-restricted and\ndistributed drones is challenging. To address this problem, this paper\nintroduces a scalable and energy-aware model for planning and coordination of\nspatio-temporal sensing. The coordination model is built upon a decentralized\nmulti-agent collective learning algorithm (EPOS) to ensure scalability,\nresilience, and flexibility that existing approaches lack of. Experimental\nresults illustrate the outstanding performance of the proposed method compared\nto state-of-the-art methods. Analytical results contribute a deeper\nunderstanding of how coordinated mobility of drones influences sensing\nperformance. This novel coordination solution is applied to traffic monitoring\nusing real-world data to demonstrate a $46.45\\%$ more accurate and $2.88\\%$\nmore efficient detection of vehicles as the number of drones become a scarce\nresource.\n","authors":["Chuhao Qin","Evangelos Pournaras"],"pdf_url":"https://arxiv.org/pdf/2212.14116v2.pdf","comment":"14 pages, 8 figures, 6 tables. Accepted in Transportation Research\n  Part C: Emerging Technologies"},{"id":"http://arxiv.org/abs/2310.07899v1","updated":"2023-10-11T21:10:21Z","published":"2023-10-11T21:10:21Z","title":"RoboCLIP: One Demonstration is Enough to Learn Robot Policies","summary":"  Reward specification is a notoriously difficult problem in reinforcement\nlearning, requiring extensive expert supervision to design robust reward\nfunctions. Imitation learning (IL) methods attempt to circumvent these problems\nby utilizing expert demonstrations but typically require a large number of\nin-domain expert demonstrations. Inspired by advances in the field of\nVideo-and-Language Models (VLMs), we present RoboCLIP, an online imitation\nlearning method that uses a single demonstration (overcoming the large data\nrequirement) in the form of a video demonstration or a textual description of\nthe task to generate rewards without manual reward function design.\nAdditionally, RoboCLIP can also utilize out-of-domain demonstrations, like\nvideos of humans solving the task for reward generation, circumventing the need\nto have the same demonstration and deployment domains. RoboCLIP utilizes\npretrained VLMs without any finetuning for reward generation. Reinforcement\nlearning agents trained with RoboCLIP rewards demonstrate 2-3 times higher\nzero-shot performance than competing imitation learning methods on downstream\nrobot manipulation tasks, doing so using only one video/text demonstration.\n","authors":["Sumedh A Sontakke","Jesse Zhang","Sébastien M. R. Arnold","Karl Pertsch","Erdem Bıyık","Dorsa Sadigh","Chelsea Finn","Laurent Itti"],"pdf_url":"https://arxiv.org/pdf/2310.07899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06292v2","updated":"2023-10-11T21:08:57Z","published":"2023-05-10T16:27:55Z","title":"Joint Metrics Matter: A Better Standard for Trajectory Forecasting","summary":"  Multi-modal trajectory forecasting methods commonly evaluate using\nsingle-agent metrics (marginal metrics), such as minimum Average Displacement\nError (ADE) and Final Displacement Error (FDE), which fail to capture joint\nperformance of multiple interacting agents. Only focusing on marginal metrics\ncan lead to unnatural predictions, such as colliding trajectories or diverging\ntrajectories for people who are clearly walking together as a group.\nConsequently, methods optimized for marginal metrics lead to overly-optimistic\nestimations of performance, which is detrimental to progress in trajectory\nforecasting research. In response to the limitations of marginal metrics, we\npresent the first comprehensive evaluation of state-of-the-art (SOTA)\ntrajectory forecasting methods with respect to multi-agent metrics (joint\nmetrics): JADE, JFDE, and collision rate. We demonstrate the importance of\njoint metrics as opposed to marginal metrics with quantitative evidence and\nqualitative examples drawn from the ETH / UCY and Stanford Drone datasets. We\nintroduce a new loss function incorporating joint metrics that, when applied to\na SOTA trajectory forecasting method, achieves a 7\\% improvement in JADE / JFDE\non the ETH / UCY datasets with respect to the previous SOTA. Our results also\nindicate that optimizing for joint metrics naturally leads to an improvement in\ninteraction modeling, as evidenced by a 16\\% decrease in mean collision rate on\nthe ETH / UCY datasets with respect to the previous SOTA. Code is available at\n\\texttt{\\hyperlink{https://github.com/ericaweng/joint-metrics-matter}{github.com/ericaweng/joint-metrics-matter}}.\n","authors":["Erica Weng","Hana Hoshino","Deva Ramanan","Kris Kitani"],"pdf_url":"https://arxiv.org/pdf/2305.06292v2.pdf","comment":"Published as a conference paper at ICCV 2023"},{"id":"http://arxiv.org/abs/2310.07896v1","updated":"2023-10-11T21:07:14Z","published":"2023-10-11T21:07:14Z","title":"NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration","summary":"  Robotic learning for navigation in unfamiliar environments needs to provide\npolicies for both task-oriented navigation (i.e., reaching a goal that the\nrobot has located), and task-agnostic exploration (i.e., searching for a goal\nin a novel setting). Typically, these roles are handled by separate models, for\nexample by using subgoal proposals, planning, or separate navigation\nstrategies. In this paper, we describe how we can train a single unified\ndiffusion policy to handle both goal-directed navigation and goal-agnostic\nexploration, with the latter providing the ability to search novel\nenvironments, and the former providing the ability to reach a user-specified\ngoal once it has been located. We show that this unified policy results in\nbetter overall performance when navigating to visually indicated goals in novel\nenvironments, as compared to approaches that use subgoal proposals from\ngenerative models, or prior methods based on latent variable models. We\ninstantiate our method by using a large-scale Transformer-based policy trained\non data from multiple ground robots, with a diffusion model decoder to flexibly\nhandle both goal-conditioned and goal-agnostic navigation. Our experiments,\nconducted on a real-world mobile robot platform, show effective navigation in\nunseen environments in comparison with five alternative methods, and\ndemonstrate significant improvements in performance and lower collision rates,\ndespite utilizing smaller models than state-of-the-art approaches. For more\nvideos, code, and pre-trained model checkpoints, see\nhttps://general-navigation-models.github.io/nomad/\n","authors":["Ajay Sridhar","Dhruv Shah","Catherine Glossop","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2310.07896v1.pdf","comment":"Project page https://general-navigation-models.github.io/nomad/"},{"id":"http://arxiv.org/abs/2310.07892v1","updated":"2023-10-11T20:55:13Z","published":"2023-10-11T20:55:13Z","title":"ASV Station Keeping under Wind Disturbances using Neural Network\n  Simulation Error Minimization Model Predictive Control","summary":"  Station keeping is an essential maneuver for Autonomous Surface Vehicles\n(ASVs), mainly when used in confined spaces, to carry out surveys that require\nthe ASV to keep its position or in collaboration with other vehicles where the\nrelative position has an impact over the mission. However, this maneuver can\nbecome challenging for classic feedback controllers due to the need for an\naccurate model of the ASV dynamics and the environmental disturbances. This\nwork proposes a Model Predictive Controller using Neural Network Simulation\nError Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV\nunder wind disturbances. The performance of the proposed scheme under wind\ndisturbances is tested and compared against other controllers in simulation,\nusing the Robotics Operating System (ROS) and the multipurpose simulation\nenvironment Gazebo. A set of six tests were conducted by combining two wind\nspeeds (3 m/s and 6 m/s) and three wind directions (0$^\\circ$, 90$^\\circ$, and\n180$^\\circ$). The simulation results clearly show the advantage of the\nNNSEM-MPC over the following methods: backstepping controller, sliding mode\ncontroller, simplified dynamics MPC (SD-MPC), neural ordinary differential\nequation MPC (NODE-MPC), and knowledge-based NODE MPC (KNODE-MPC). The proposed\nNNSEM-MPC approach performs better than the rest in 4 out of the 6 test\nconditions, and it is the second best in the 2 remaining test cases, reducing\nthe mean position and heading error by at least 31\\% and 46\\% respectively\nacross all the test cases. In terms of execution speed, the proposed NNSEM-MPC\nis at least 36\\% faster than the rest of the MPC controllers. The field\nexperiments on two different ASV platforms showed that ASVs can effectively\nkeep the station utilizing the proposed method, with a position error as low as\n$1.68$ m and a heading error as low as $6.14^{\\circ}$ within time windows of at\nleast $150$s.\n","authors":["Jalil Chavez-Galaviz","Jianwen Li","Ajinkya Chaudhary","Nina Mahmoudian"],"pdf_url":"https://arxiv.org/pdf/2310.07892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07889v1","updated":"2023-10-11T20:52:30Z","published":"2023-10-11T20:52:30Z","title":"LangNav: Language as a Perceptual Representation for Navigation","summary":"  We explore the use of language as a perceptual representation for\nvision-and-language navigation. Our approach uses off-the-shelf vision systems\n(for image captioning and object detection) to convert an agent's egocentric\npanoramic view at each time step into natural language descriptions. We then\nfinetune a pretrained language model to select an action, based on the current\nview and the trajectory history, that would best fulfill the navigation\ninstructions. In contrast to the standard setup which adapts a pretrained\nlanguage model to work directly with continuous visual features from pretrained\nvision models, our approach instead uses (discrete) language as the perceptual\nrepresentation. We explore two use cases of our language-based navigation\n(LangNav) approach on the R2R vision-and-language navigation benchmark:\ngenerating synthetic trajectories from a prompted large language model (GPT-4)\nwith which to finetune a smaller language model; and sim-to-real transfer where\nwe transfer a policy learned on a simulated environment (ALFRED) to a\nreal-world environment (R2R). Our approach is found to improve upon strong\nbaselines that rely on visual features in settings where only a few gold\ntrajectories (10-100) are available, demonstrating the potential of using\nlanguage as a perceptual representation for navigation tasks.\n","authors":["Bowen Pan","Rameswar Panda","SouYoung Jin","Rogerio Feris","Aude Oliva","Phillip Isola","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2310.07889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.02809v2","updated":"2023-10-11T20:05:09Z","published":"2022-08-04T17:58:15Z","title":"The Role of Morphological Variation in Evolutionary Robotics: Maximizing\n  Performance and Robustness","summary":"  Exposing an Evolutionary Algorithm that is used to evolve robot controllers\nto variable conditions is necessary to obtain solutions which are robust and\ncan cross the reality gap. However, we do not yet have methods for analyzing\nand understanding the impact of the varying morphological conditions which\nimpact the evolutionary process, and therefore for choosing suitable variation\nranges. By morphological conditions, we refer to the starting state of the\nrobot, and to variations in its sensor readings during operation due to noise.\nIn this article, we introduce a method that permits us to measure the impact of\nthese morphological variations and we analyze the relation between the\namplitude of variations, the modality with which they are introduced, and the\nperformance and robustness of evolving agents. Our results demonstrate that (i)\nthe evolutionary algorithm can tolerate morphological variations which have a\nvery high impact, (ii) variations affecting the actions of the agent are\ntolerated much better than variations affecting the initial state of the agent\nor of the environment, and (iii) improving the accuracy of the fitness measure\nthrough multiple evaluations is not always useful. Moreover, our results show\nthat morphological variations permit generating solutions which perform better\nboth in varying and non-varying conditions.\n","authors":["Jonata Tyska Carvalho","Stefano Nolfi"],"pdf_url":"https://arxiv.org/pdf/2208.02809v2.pdf","comment":"submitted to MIT Evolutionary Computation Journal"},{"id":"http://arxiv.org/abs/2310.07854v1","updated":"2023-10-11T19:56:59Z","published":"2023-10-11T19:56:59Z","title":"VaPr: Variable-Precision Tensors to Accelerate Robot Motion Planning","summary":"  High-dimensional motion generation requires numerical precision for smooth,\ncollision-free solutions. Typically, double-precision or single-precision\nfloating-point (FP) formats are utilized. Using these for big tensors imposes a\nstrain on the memory bandwidth provided by the devices and alters the memory\nfootprint, hence limiting their applicability to low-power edge devices needed\nfor mobile robots. The uniform application of reduced precision can be\nadvantageous but severely degrades solutions. Using decreased precision data\ntypes for important tensors, we propose to accelerate motion generation by\nremoving memory bottlenecks. We propose variable-precision (VaPr) search\noptimization to determine the appropriate precision for large tensors from a\nvast search space of approximately 4 million unique combinations for FP data\ntypes across the tensors. To obtain the efficiency gains, we exploit existing\nplatform support for an out-of-the-box GPU speedup and evaluate prospective\nprecision converter units for GPU types that are not currently supported. Our\nexperimental results on 800 planning problems for the Franka Panda robot on the\nMotionBenchmaker dataset across 8 environments show that a 4-bit FP format is\nsufficient for the largest set of tensors in the motion generation stack. With\nthe software-only solution, VaPr achieves 6.3% and 6.3% speedups on average for\na significant portion of motion generation over the SOTA solution (CuRobo) on\nJetson Orin and RTX2080 Ti GPU, respectively, and 9.9%, 17.7% speedups with the\nFP converter.\n","authors":["Yu-Shun Hsiao","Siva Kumar Sastry Hari","Balakumar Sundaralingam","Jason Yik","Thierry Tambe","Charbel Sakr","Stephen W. Keckler","Vijay Janapa Reddi"],"pdf_url":"https://arxiv.org/pdf/2310.07854v1.pdf","comment":"7 pages, 5 figures, 8 tables, to be published in 2023 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2310.07844v1","updated":"2023-10-11T19:42:21Z","published":"2023-10-11T19:42:21Z","title":"Saturation-Aware Angular Velocity Estimation: Extending the Robustness\n  of SLAM to Aggressive Motions","summary":"  We propose a novel angular velocity estimation method to increase the\nrobustness of Simultaneous Localization And Mapping (SLAM) algorithms against\ngyroscope saturations induced by aggressive motions. Field robotics expose\nrobots to various hazards, including steep terrains, landslides, and\nstaircases, where substantial accelerations and angular velocities can occur if\nthe robot loses stability and tumbles. These extreme motions can saturate\nsensor measurements, especially gyroscopes, which are the first sensors to\nbecome inoperative. While the structural integrity of the robot is at risk, the\nresilience of the SLAM framework is oftentimes given little consideration.\nConsequently, even if the robot is physically capable of continuing the\nmission, its operation will be compromised due to a corrupted representation of\nthe world. Regarding this problem, we propose a way to estimate the angular\nvelocity using accelerometers during extreme rotations caused by tumbling. We\nshow that our method reduces the median localization error by 71.5 % in\ntranslation and 65.5 % in rotation and reduces the number of SLAM failures by\n73.3 % on the collected data. We also propose the Tumbling-Induced Gyroscope\nSaturation (TIGS) dataset, which consists of outdoor experiments recording the\nmotion of a lidar subject to angular velocities four times higher than other\navailable datasets. The dataset is available online at\nhttps://github.com/norlab-ulaval/Norlab_wiki/wiki/TIGS-Dataset.\n","authors":["Simon-Pierre Deschênes","Dominic Baril","Matěj Boxan","Johann Laconte","Philippe Giguère","François Pomerleau"],"pdf_url":"https://arxiv.org/pdf/2310.07844v1.pdf","comment":"7 pages, 7 figures, submitted to the 2024 IEEE International\n  Conference on Robotics and Automation (ICRA2024), Yokohama, Japan"},{"id":"http://arxiv.org/abs/2310.07842v1","updated":"2023-10-11T19:34:46Z","published":"2023-10-11T19:34:46Z","title":"DiPPeR: Diffusion-based 2D Path Planner applied on Legged Robots","summary":"  In this work, we present DiPPeR, a novel and fast 2D path planning framework\nfor quadrupedal locomotion, leveraging diffusion-driven techniques. Our\ncontributions include a scalable dataset of map images and corresponding\nend-to-end trajectories, an image-conditioned diffusion planner for mobile\nrobots, and a training/inference pipeline employing CNNs. We validate our\napproach in several mazes, as well as in real-world deployment scenarios on\nBoston Dynamic's Spot and Unitree's Go1 robots. DiPPeR performs on average 70\ntimes faster for trajectory generation against both search based and data\ndriven path planning algorithms with an average of 80% consistency in producing\nfeasible paths of various length in maps of variable size, and obstacle\nstructure.\n","authors":["Jianwei Liu","Maria Stamatopoulou","Dimitrios Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2310.07842v1.pdf","comment":"7 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.07840v1","updated":"2023-10-11T19:34:26Z","published":"2023-10-11T19:34:26Z","title":"Active Learning with Dual Model Predictive Path-Integral Control for\n  Interaction-Aware Autonomous Highway On-ramp Merging","summary":"  Merging into dense highway traffic for an autonomous vehicle is a complex\ndecision-making task, wherein the vehicle must identify a potential gap and\ncoordinate with surrounding human drivers, each of whom may exhibit diverse\ndriving behaviors. Many existing methods consider other drivers to be dynamic\nobstacles and, as a result, are incapable of capturing the full intent of the\nhuman drivers via this passive planning. In this paper, we propose a novel dual\ncontrol framework based on Model Predictive Path-Integral control to generate\ninteractive trajectories. This framework incorporates a Bayesian inference\napproach to actively learn the agents' parameters, i.e., other drivers' model\nparameters. The proposed framework employs a sampling-based approach that is\nsuitable for real-time implementation through the utilization of GPUs. We\nillustrate the effectiveness of our proposed methodology through comprehensive\nnumerical simulations conducted in both high and low-fidelity simulation\nscenarios focusing on autonomous on-ramp merging.\n","authors":["Jacob Knaup","Jovin D'sa","Behdad Chalaki","Tyler Naes","Hossein Nourkhiz Mahjoub","Ehsan Moradi-Pari","Panagiotis Tsiotras"],"pdf_url":"https://arxiv.org/pdf/2310.07840v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.07822v1","updated":"2023-10-11T19:03:55Z","published":"2023-10-11T19:03:55Z","title":"Body-mounted MR-conditional Robot for Minimally Invasive Liver\n  Intervention","summary":"  MR-guided microwave ablation (MWA) has proven effective in treating\nhepatocellular carcinoma (HCC) with small-sized tumors, but the\nstate-of-the-art technique suffers from sub-optimal workflow due to speed and\naccuracy of needle placement. This paper presents a compact body-mounted\nMR-conditional robot that can operate in closed-bore MR scanners for accurate\nneedle guidance. The robotic platform consists of two stacked Cartesian XY\nstages, each with two degrees of freedom, that facilitate needle guidance. The\nrobot is actuated using 3D-printed pneumatic turbines with MR-conditional bevel\ngear transmission systems. Pneumatic valves and control mechatronics are\nlocated inside the MRI control room and are connected to the robot with\npneumatic transmission lines and optical fibers. Free space experiments\nindicated robot-assisted needle insertion error of 2.6$\\pm$1.3 mm at an\ninsertion depth of 80 mm. The MR-guided phantom studies were conducted to\nverify the MR-conditionality and targeting performance of the robot. Future\nwork will focus on the system optimization and validations in animal trials.\n","authors":["Zhefeng Huang","Anthony L. Gunderman","Samuel E. Wilcox","Saikat Sengupta","Aiming Lu","David Woodrum","Jay Shah","Yue Chen"],"pdf_url":"https://arxiv.org/pdf/2310.07822v1.pdf","comment":"19 pages, 10figures"},{"id":"http://arxiv.org/abs/2310.07794v1","updated":"2023-10-11T18:28:15Z","published":"2023-10-11T18:28:15Z","title":"CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory\n  Prediction Models for Autonomous Driving","summary":"  Benchmarking is a common method for evaluating trajectory prediction models\nfor autonomous driving. Existing benchmarks rely on datasets, which are biased\ntowards more common scenarios, such as cruising, and distance-based metrics\nthat are computed by averaging over all scenarios. Following such a regiment\nprovides a little insight into the properties of the models both in terms of\nhow well they can handle different scenarios and how admissible and diverse\ntheir outputs are. There exist a number of complementary metrics designed to\nmeasure the admissibility and diversity of trajectories, however, they suffer\nfrom biases, such as length of trajectories.\n  In this paper, we propose a new benChmarking paRadIgm for evaluaTing\ntrajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a\nmethod for extracting driving scenarios at varying levels of specificity\naccording to the structure of the roads, models' performance, and data\nproperties for fine-grained ranking of prediction models; 2) A set of new\nbias-free metrics for measuring diversity, by incorporating the characteristics\nof a given scenario, and admissibility, by considering the structure of roads\nand kinematic compliancy, motivated by real-world driving constraints. 3) Using\nthe proposed benchmark, we conduct extensive experimentation on a\nrepresentative set of the prediction models using the large scale Argoverse\ndataset. We show that the proposed benchmark can produce a more accurate\nranking of the models and serve as a means of characterizing their behavior. We\nfurther present ablation studies to highlight contributions of different\nelements that are used to compute the proposed metrics.\n","authors":["Changhe Chen","Mozhgan Pourkeshavarz","Amir Rasouli"],"pdf_url":"https://arxiv.org/pdf/2310.07794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01813v2","updated":"2023-10-11T18:11:19Z","published":"2023-09-04T21:05:59Z","title":"Inverse Dynamics Trajectory Optimization for Contact-Implicit Model\n  Predictive Control","summary":"  Robots must make and break contact with the environment to perform useful\ntasks, but planning and control through contact remains a formidable challenge.\nIn this work, we achieve real-time contact-implicit model predictive control\nwith a surprisingly simple method: inverse dynamics trajectory optimization.\nWhile trajectory optimization with inverse dynamics is not new, we introduce a\nseries of incremental innovations that collectively enable fast model\npredictive control on a variety of challenging manipulation and locomotion\ntasks. We implement these innovations in an open-source solver and present\nsimulation examples to support the effectiveness of the proposed approach.\nAdditionally, we demonstrate contact-implicit model predictive control on\nhardware at over 100 Hz for a 20-degree-of-freedom bi-manual manipulation task.\nVideo and code are available at https://idto.github.io.\n","authors":["Vince Kurtz","Alejandro Castro","Aykut Özgün Önol","Hai Lin"],"pdf_url":"https://arxiv.org/pdf/2309.01813v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16605v4","updated":"2023-10-11T18:09:59Z","published":"2023-06-29T00:12:21Z","title":"KITE: Keypoint-Conditioned Policies for Semantic Manipulation","summary":"  While natural language offers a convenient shared interface for humans and\nrobots, enabling robots to interpret and follow language commands remains a\nlongstanding challenge in manipulation. A crucial step to realizing a\nperformant instruction-following robot is achieving semantic manipulation,\nwhere a robot interprets language at different specificities, from high-level\ninstructions like \"Pick up the stuffed animal\" to more detailed inputs like\n\"Grab the left ear of the elephant.\" To tackle this, we propose Keypoints +\nInstructions to Execution (KITE), a two-step framework for semantic\nmanipulation which attends to both scene semantics (distinguishing between\ndifferent objects in a visual scene) and object semantics (precisely localizing\ndifferent parts within an object instance). KITE first grounds an input\ninstruction in a visual scene through 2D image keypoints, providing a highly\naccurate object-centric bias for downstream action inference. Provided an RGB-D\nscene observation, KITE then executes a learned keypoint-conditioned skill to\ncarry out the instruction. The combined precision of keypoints and\nparameterized skills enables fine-grained manipulation with generalization to\nscene and object variations. Empirically, we demonstrate KITE in 3 real-world\nenvironments: long-horizon 6-DoF tabletop manipulation, semantic grasping, and\na high-precision coffee-making task. In these settings, KITE achieves a 75%,\n70%, and 71% overall success rate for instruction-following, respectively. KITE\noutperforms frameworks that opt for pre-trained visual language models over\nkeypoint-based grounding, or omit skills in favor of end-to-end visuomotor\ncontrol, all while being trained from fewer or comparable amounts of\ndemonstrations. Supplementary material, datasets, code, and videos can be found\non our website: http://tinyurl.com/kite-site.\n","authors":["Priya Sundaresan","Suneel Belkhale","Dorsa Sadigh","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2306.16605v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07748v1","updated":"2023-10-11T17:54:38Z","published":"2023-10-11T17:54:38Z","title":"Implementation of Fuzzy Control Algorithm in Two-Wheeled Differential\n  Drive Platform","summary":"  Designing and developing Artificial Intelligence controllers on separately\ndedicated chips have many advantages. This report reviews the development of a\nreal-time fuzzy logic controller for optimizing locomotion control of a\ntwo-wheeled differential drive platform using an Arduino Uno board. Based on\nthe Raspberry Pi board, fuzzy sets are used to optimize color recognition,\nenabling the color sensor to correctly recognize color at long distances,\nacross a wide range of light intensity, and with high fault tolerance.\n","authors":["Guoyi Chen"],"pdf_url":"https://arxiv.org/pdf/2310.07748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07747v1","updated":"2023-10-11T17:20:32Z","published":"2023-10-11T17:20:32Z","title":"Accountability in Offline Reinforcement Learning: Explaining Decisions\n  with a Corpus of Examples","summary":"  Learning transparent, interpretable controllers with offline data in\ndecision-making systems is an essential area of research due to its potential\nto reduce the risk of applications in real-world systems. However, in\nresponsibility-sensitive settings such as healthcare, decision accountability\nis of paramount importance, yet has not been adequately addressed by the\nliterature. This paper introduces the Accountable Offline Controller (AOC) that\nemploys the offline dataset as the Decision Corpus and performs accountable\ncontrol based on a tailored selection of examples, referred to as the Corpus\nSubset. ABC operates effectively in low-data scenarios, can be extended to the\nstrictly offline imitation setting, and displays qualities of both conservation\nand adaptability. We assess ABC's performance in both simulated and real-world\nhealthcare scenarios, emphasizing its capability to manage offline control\ntasks with high levels of performance while maintaining accountability.\n  Keywords: Interpretable Reinforcement Learning, Explainable Reinforcement\nLearning, Reinforcement Learning Transparency, Offline Reinforcement Learning,\nBatched Control.\n","authors":["Hao Sun","Alihan Hüyük","Daniel Jarrett","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2310.07747v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07744v1","updated":"2023-10-11T13:52:55Z","published":"2023-10-11T13:52:55Z","title":"Terrain-adaptive Central Pattern Generators with Reinforcement Learning\n  for Hexapod Locomotion","summary":"  Inspired by biological motion generation, central pattern generators (CPGs)\nis frequently employed in legged robot locomotion control to produce natural\ngait pattern with low-dimensional control signals. However, the limited\nadaptability and stability over complex terrains hinder its application. To\naddress this issue, this paper proposes a terrain-adaptive locomotion control\nmethod that incorporates deep reinforcement learning (DRL) framework into CPG,\nwhere the CPG model is responsible for the generation of synchronized signals,\nproviding basic locomotion gait, while DRL is integrated to enhance the\nadaptability of robot towards uneven terrains by adjusting the parameters of\nCPG mapping functions. The experiments conducted on the hexapod robot in Isaac\nGym simulation environment demonstrated the superiority of the proposed method\nin terrain-adaptability, convergence rate and reward design complexity.\n","authors":["Qiyue Yang","Yue Gao","Shaoyuan Li"],"pdf_url":"https://arxiv.org/pdf/2310.07744v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.07716v1","updated":"2023-10-11T17:59:56Z","published":"2023-10-11T17:59:56Z","title":"PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection","summary":"  Object anomaly detection is an important problem in the field of machine\nvision and has seen remarkable progress recently. However, two significant\nchallenges hinder its research and application. First, existing datasets lack\ncomprehensive visual information from various pose angles. They usually have an\nunrealistic assumption that the anomaly-free training dataset is pose-aligned,\nand the testing samples have the same pose as the training data. However, in\npractice, anomaly may exist in any regions on a object, the training and query\nsamples may have different poses, calling for the study on pose-agnostic\nanomaly detection. Second, the absence of a consensus on experimental protocols\nfor pose-agnostic anomaly detection leads to unfair comparisons of different\nmethods, hindering the research on pose-agnostic anomaly detection. To address\nthese issues, we develop Multi-pose Anomaly Detection (MAD) dataset and\nPose-agnostic Anomaly Detection (PAD) benchmark, which takes the first step to\naddress the pose-agnostic anomaly detection problem. Specifically, we build MAD\nusing 20 complex-shaped LEGO toys including 4K views with various poses, and\nhigh-quality and diverse 3D anomalies in both simulated and real environments.\nAdditionally, we propose a novel method OmniposeAD, trained using MAD,\nspecifically designed for pose-agnostic anomaly detection. Through\ncomprehensive evaluations, we demonstrate the relevance of our dataset and\nmethod. Furthermore, we provide an open-source benchmark library, including\ndataset and baseline methods that cover 8 anomaly detection paradigms, to\nfacilitate future research and application in this domain. Code, data, and\nmodels are publicly available at https://github.com/EricLee0224/PAD.\n","authors":["Qiang Zhou","Weize Li","Lihan Jiang","Guoliang Wang","Guyue Zhou","Shanghang Zhang","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.07716v1.pdf","comment":"Accepted by NeurIPS 2023. Codes are available at\n  https://github.com/EricLee0224/PAD"},{"id":"http://arxiv.org/abs/2305.05658v2","updated":"2023-10-11T17:59:44Z","published":"2023-05-09T17:52:59Z","title":"TidyBot: Personalized Robot Assistance with Large Language Models","summary":"  For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people's preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.\n","authors":["Jimmy Wu","Rika Antonova","Adam Kan","Marion Lepert","Andy Zeng","Shuran Song","Jeannette Bohg","Szymon Rusinkiewicz","Thomas Funkhouser"],"pdf_url":"https://arxiv.org/pdf/2305.05658v2.pdf","comment":"Accepted to Autonomous Robots (AuRo) - Special Issue: Large Language\n  Models in Robotics, 2023 and IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 2023. Project page:\n  https://tidybot.cs.princeton.edu"},{"id":"http://arxiv.org/abs/2310.07707v1","updated":"2023-10-11T17:57:14Z","published":"2023-10-11T17:57:14Z","title":"MatFormer: Nested Transformer for Elastic Inference","summary":"  Transformer models are deployed in a wide range of settings, from\nmulti-accelerator clusters to standalone mobile phones. The diverse inference\nconstraints in these scenarios necessitate practitioners to train foundation\nmodels such as PaLM 2, Llama, & ViTs as a series of models of varying sizes.\nDue to significant training costs, only a select few model sizes are trained\nand supported, limiting more fine-grained control over relevant tradeoffs,\nincluding latency, cost, and accuracy. This work introduces MatFormer, a nested\nTransformer architecture designed to offer elasticity in a variety of\ndeployment constraints. Each Feed Forward Network (FFN) block of a MatFormer\nmodel is jointly optimized with a few nested smaller FFN blocks. This training\nprocedure allows for the Mix'n'Match of model granularities across layers --\ni.e., a trained universal MatFormer model enables extraction of hundreds of\naccurate smaller models, which were never explicitly optimized. We empirically\ndemonstrate MatFormer's effectiveness across different model classes (decoders\n& encoders), modalities (language & vision), and scales (up to 2.6B\nparameters). We find that a 2.6B decoder-only MatFormer language model (MatLM)\nallows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting\ncomparable validation loss and one-shot downstream evaluations to their\nindependently trained counterparts. Furthermore, we observe that smaller\nencoders extracted from a universal MatFormer-based ViT (MatViT) encoder\npreserve the metric-space structure for adaptive large-scale retrieval.\nFinally, we showcase that speculative decoding with the accurate and consistent\nsubmodels extracted from MatFormer can further reduce inference latency.\n","authors":[" Devvrit","Sneha Kudugunta","Aditya Kusupati","Tim Dettmers","Kaifeng Chen","Inderjit Dhillon","Yulia Tsvetkov","Hannaneh Hajishirzi","Sham Kakade","Ali Farhadi","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2310.07707v1.pdf","comment":"31 pages, 12 figures, first three authors contributed equally"},{"id":"http://arxiv.org/abs/2310.07704v1","updated":"2023-10-11T17:55:15Z","published":"2023-10-11T17:55:15Z","title":"Ferret: Refer and Ground Anything Anywhere at Any Granularity","summary":"  We introduce Ferret, a new Multimodal Large Language Model (MLLM) capable of\nunderstanding spatial referring of any shape or granularity within an image and\naccurately grounding open-vocabulary descriptions. To unify referring and\ngrounding in the LLM paradigm, Ferret employs a novel and powerful hybrid\nregion representation that integrates discrete coordinates and continuous\nfeatures jointly to represent a region in the image. To extract the continuous\nfeatures of versatile regions, we propose a spatial-aware visual sampler, adept\nat handling varying sparsity across different shapes. Consequently, Ferret can\naccept diverse region inputs, such as points, bounding boxes, and free-form\nshapes. To bolster the desired capability of Ferret, we curate GRIT, a\ncomprehensive refer-and-ground instruction tuning dataset including 1.1M\nsamples that contain rich hierarchical spatial knowledge, with 95K hard\nnegative data to promote model robustness. The resulting model not only\nachieves superior performance in classical referring and grounding tasks, but\nalso greatly outperforms existing MLLMs in region-based and\nlocalization-demanded multimodal chatting. Our evaluations also reveal a\nsignificantly improved capability of describing image details and a remarkable\nalleviation in object hallucination. Code and data will be available at\nhttps://github.com/apple/ml-ferret\n","authors":["Haoxuan You","Haotian Zhang","Zhe Gan","Xianzhi Du","Bowen Zhang","Zirui Wang","Liangliang Cao","Shih-Fu Chang","Yinfei Yang"],"pdf_url":"https://arxiv.org/pdf/2310.07704v1.pdf","comment":"30 pages, 10 figures. Code/Project Website:\n  https://github.com/apple/ml-ferret"},{"id":"http://arxiv.org/abs/2310.07702v1","updated":"2023-10-11T17:52:39Z","published":"2023-10-11T17:52:39Z","title":"ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with\n  Diffusion Models","summary":"  In this work, we investigate the capability of generating images from\npre-trained diffusion models at much higher resolutions than the training image\nsizes. In addition, the generated images should have arbitrary image aspect\nratios. When generating images directly at a higher resolution, 1024 x 1024,\nwith the pre-trained Stable Diffusion using training images of resolution 512 x\n512, we observe persistent problems of object repetition and unreasonable\nobject structures. Existing works for higher-resolution generation, such as\nattention-based and joint-diffusion approaches, cannot well address these\nissues. As a new perspective, we examine the structural components of the U-Net\nin diffusion models and identify the crucial cause as the limited perception\nfield of convolutional kernels. Based on this key observation, we propose a\nsimple yet effective re-dilation that can dynamically adjust the convolutional\nperception field during inference. We further propose the dispersed convolution\nand noise-damped classifier-free guidance, which can enable\nultra-high-resolution image generation (e.g., 4096 x 4096). Notably, our\napproach does not require any training or optimization. Extensive experiments\ndemonstrate that our approach can address the repetition issue well and achieve\nstate-of-the-art performance on higher-resolution image synthesis, especially\nin texture details. Our work also suggests that a pre-trained diffusion model\ntrained on low-resolution images can be directly used for high-resolution\nvisual generation without further tuning, which may provide insights for future\nresearch on ultra-high-resolution image and video synthesis.\n","authors":["Yingqing He","Shaoshu Yang","Haoxin Chen","Xiaodong Cun","Menghan Xia","Yong Zhang","Xintao Wang","Ran He","Qifeng Chen","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2310.07702v1.pdf","comment":"Project page: https://yingqinghe.github.io/scalecrafter/ Github:\n  https://github.com/YingqingHe/ScaleCrafter"},{"id":"http://arxiv.org/abs/2310.07699v1","updated":"2023-10-11T17:49:13Z","published":"2023-10-11T17:49:13Z","title":"From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched\n  Captions","summary":"  Web-crawled datasets are pivotal to the success of pre-training\nvision-language models, exemplified by CLIP. However, web-crawled AltTexts can\nbe noisy and potentially irrelevant to images, thereby undermining the crucial\nimage-text alignment. Existing methods for rewriting captions using large\nlanguage models (LLMs) have shown promise on small, curated datasets like CC3M\nand CC12M. Nevertheless, their efficacy on massive web-captured captions is\nconstrained by the inherent noise and randomness in such data. In this study,\nwe address this limitation by focusing on two key aspects: data quality and\ndata variety. Unlike recent LLM rewriting techniques, we emphasize exploiting\nvisual concepts and their integration into the captions to improve data\nquality. For data variety, we propose a novel mixed training scheme that\noptimally leverages AltTexts alongside newly generated Visual-enriched Captions\n(VeC). We use CLIP as one example and adapt the method for CLIP training on\nlarge-scale web-crawled datasets, named VeCLIP. We conduct a comprehensive\nevaluation of VeCLIP across small, medium, and large scales of raw data. Our\nresults show significant advantages in image-text alignment and overall model\nperformance, underscoring the effectiveness of VeCLIP in improving CLIP\ntraining. For example, VeCLIP achieves a remarkable over 20% improvement in\nCOCO and Flickr30k retrieval tasks under the 12M setting. For data efficiency,\nwe also achieve a notable over 3% improvement while using only 14% of the data\nemployed in the vanilla CLIP and 11% in ALIGN.\n","authors":["Zhengfeng Lai","Haotian Zhang","Wentao Wu","Haoping Bai","Aleksei Timofeev","Xianzhi Du","Zhe Gan","Jiulong Shan","Chen-Nee Chuah","Yinfei Yang","Meng Cao"],"pdf_url":"https://arxiv.org/pdf/2310.07699v1.pdf","comment":"CV/ML"},{"id":"http://arxiv.org/abs/2310.07697v1","updated":"2023-10-11T17:46:28Z","published":"2023-10-11T17:46:28Z","title":"ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation","summary":"  Recent works have successfully extended large-scale text-to-image models to\nthe video domain, producing promising results but at a high computational cost\nand requiring a large amount of video data. In this work, we introduce\nConditionVideo, a training-free approach to text-to-video generation based on\nthe provided condition, video, and input text, by leveraging the power of\noff-the-shelf text-to-image generation methods (e.g., Stable Diffusion).\nConditionVideo generates realistic dynamic videos from random noise or given\nscene videos. Our method explicitly disentangles the motion representation into\ncondition-guided and scenery motion components. To this end, the ConditionVideo\nmodel is designed with a UNet branch and a control branch. To improve temporal\ncoherence, we introduce sparse bi-directional spatial-temporal attention\n(sBiST-Attn). The 3D control network extends the conventional 2D controlnet\nmodel, aiming to strengthen conditional generation accuracy by additionally\nleveraging the bi-directional frames in the temporal domain. Our method\nexhibits superior performance in terms of frame consistency, clip score, and\nconditional accuracy, outperforming other compared methods.\n","authors":["Bo Peng","Xinyuan Chen","Yaohui Wang","Chaochao Lu","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2310.07697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09535v3","updated":"2023-10-11T17:46:21Z","published":"2023-03-16T17:51:13Z","title":"FateZero: Fusing Attentions for Zero-shot Text-based Video Editing","summary":"  The diffusion-based generative models have achieved remarkable success in\ntext-based image generation. However, since it contains enormous randomness in\ngeneration progress, it is still challenging to apply such models for\nreal-world visual content editing, especially in videos. In this paper, we\npropose FateZero, a zero-shot text-based editing method on real-world videos\nwithout per-prompt training or use-specific mask. To edit videos consistently,\nwe propose several techniques based on the pre-trained models. Firstly, in\ncontrast to the straightforward DDIM inversion technique, our approach captures\nintermediate attention maps during inversion, which effectively retain both\nstructural and motion information. These maps are directly fused in the editing\nprocess rather than generated during denoising. To further minimize semantic\nleakage of the source video, we then fuse self-attentions with a blending mask\nobtained by cross-attention features from the source prompt. Furthermore, we\nhave implemented a reform of the self-attention mechanism in denoising UNet by\nintroducing spatial-temporal attention to ensure frame consistency. Yet\nsuccinct, our method is the first one to show the ability of zero-shot\ntext-driven video style and local attribute editing from the trained\ntext-to-image model. We also have a better zero-shot shape-aware editing\nability based on the text-to-video model. Extensive experiments demonstrate our\nsuperior temporal consistency and editing capability than previous works.\n","authors":["Chenyang Qi","Xiaodong Cun","Yong Zhang","Chenyang Lei","Xintao Wang","Ying Shan","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2303.09535v3.pdf","comment":"Accepted to ICCV 2023 as an Oral Presentation. Project page:\n  https://fate-zero-edit.github.io ; GitHub repository:\n  https://github.com/ChenyangQiQi/FateZero"},{"id":"http://arxiv.org/abs/2303.05078v2","updated":"2023-10-11T17:46:03Z","published":"2023-03-09T07:26:49Z","title":"Efficient Transformer-based 3D Object Detection with Dynamic Token\n  Halting","summary":"  Balancing efficiency and accuracy is a long-standing problem for deploying\ndeep learning models. The trade-off is even more important for real-time\nsafety-critical systems like autonomous vehicles. In this paper, we propose an\neffective approach for accelerating transformer-based 3D object detectors by\ndynamically halting tokens at different layers depending on their contribution\nto the detection task. Although halting a token is a non-differentiable\noperation, our method allows for differentiable end-to-end learning by\nleveraging an equivalent differentiable forward-pass. Furthermore, our\nframework allows halted tokens to be reused to inform the model's predictions\nthrough a straightforward token recycling mechanism. Our method significantly\nimproves the Pareto frontier of efficiency versus accuracy when compared with\nthe existing approaches. By halting tokens and increasing model capacity, we\nare able to improve the baseline model's performance without increasing the\nmodel's latency on the Waymo Open Dataset.\n","authors":["Mao Ye","Gregory P. Meyer","Yuning Chai","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2303.05078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07687v1","updated":"2023-10-11T17:36:17Z","published":"2023-10-11T17:36:17Z","title":"Orbital Polarimetric Tomography of a Flare Near the Sagittarius A*\n  Supermassive Black Hole","summary":"  The interaction between the supermassive black hole at the center of the\nMilky Way, Sagittarius A$^*$, and its accretion disk, occasionally produces\nhigh energy flares seen in X-ray, infrared and radio. One mechanism for\nobserved flares is the formation of compact bright regions that appear within\nthe accretion disk and close to the event horizon. Understanding these flares\ncan provide a window into black hole accretion processes. Although\nsophisticated simulations predict the formation of these flares, their\nstructure has yet to be recovered by observations. Here we show the first\nthree-dimensional (3D) reconstruction of an emission flare in orbit recovered\nfrom ALMA light curves observed on April 11, 2017. Our recovery results show\ncompact bright regions at a distance of roughly 6 times the event horizon.\nMoreover, our recovery suggests a clockwise rotation in a low-inclination\norbital plane, a result consistent with prior studies by EHT and GRAVITY\ncollaborations. To recover this emission structure we solve a highly ill-posed\ntomography problem by integrating a neural 3D representation (an emergent\nartificial intelligence approach for 3D reconstruction) with a gravitational\nmodel for black holes. Although the recovered 3D structure is subject, and\nsometimes sensitive, to the model assumptions, under physically motivated\nchoices we find that our results are stable and our approach is successful on\nsimulated data. We anticipate that in the future, this approach could be used\nto analyze a richer collection of time-series data that could shed light on the\nmechanisms governing black hole and plasma dynamics.\n","authors":["Aviad Levis","Andrew A. Chael","Katherine L. Bouman","Maciek Wielgus","Pratul P. Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2310.07687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12424v2","updated":"2023-10-11T17:34:19Z","published":"2023-06-21T17:59:51Z","title":"VisoGender: A dataset for benchmarking gender bias in image-text pronoun\n  resolution","summary":"  We introduce VisoGender, a novel dataset for benchmarking gender bias in\nvision-language models. We focus on occupation-related biases within a\nhegemonic system of binary gender, inspired by Winograd and Winogender schemas,\nwhere each image is associated with a caption containing a pronoun relationship\nof subjects and objects in the scene. VisoGender is balanced by gender\nrepresentation in professional roles, supporting bias evaluation in two ways:\ni) resolution bias, where we evaluate the difference between pronoun resolution\naccuracies for image subjects with gender presentations perceived as masculine\nversus feminine by human annotators and ii) retrieval bias, where we compare\nratios of professionals perceived to have masculine and feminine gender\npresentations retrieved for a gender-neutral search query. We benchmark several\nstate-of-the-art vision-language models and find that they demonstrate bias in\nresolving binary gender in complex scenes. While the direction and magnitude of\ngender bias depends on the task and the model being evaluated, captioning\nmodels are generally less biased than Vision-Language Encoders. Dataset and\ncode are available at https://github.com/oxai/visogender\n","authors":["Siobhan Mackenzie Hall","Fernanda Gonçalves Abrantes","Hanwen Zhu","Grace Sodunke","Aleksandar Shtedritski","Hannah Rose Kirk"],"pdf_url":"https://arxiv.org/pdf/2306.12424v2.pdf","comment":"Data and code available at https://github.com/oxai/visogender"},{"id":"http://arxiv.org/abs/2310.07682v1","updated":"2023-10-11T17:32:24Z","published":"2023-10-11T17:32:24Z","title":"Prediction of MET Overexpression in Non-Small Cell Lung Adenocarcinomas\n  from Hematoxylin and Eosin Images","summary":"  MET protein overexpression is a targetable event in non-small cell lung\ncancer (NSCLC) and is the subject of active drug development. Challenges in\nidentifying patients for these therapies include lack of access to validated\ntesting, such as standardized immunohistochemistry (IHC) assessment, and\nconsumption of valuable tissue for a single gene/protein assay. Development of\npre-screening algorithms using routinely available digitized hematoxylin and\neosin (H&E)-stained slides to predict MET overexpression could promote testing\nfor those who will benefit most. While assessment of MET expression using IHC\nis currently not routinely performed in NSCLC, next-generation sequencing is\ncommon and in some cases includes RNA expression panel testing. In this work,\nwe leveraged a large database of matched H&E slides and RNA expression data to\ntrain a weakly supervised model to predict MET RNA overexpression directly from\nH&E images. This model was evaluated on an independent holdout test set of 300\nover-expressed and 289 normal patients, demonstrating an ROC-AUC of 0.70 (95th\npercentile interval: 0.66 - 0.74) with stable performance characteristics\nacross different patient clinical variables and robust to synthetic noise on\nthe test set. These results suggest that H&E-based predictive models could be\nuseful to prioritize patients for confirmatory testing of MET protein or MET\ngene expression status.\n","authors":["Kshitij Ingale","Sun Hae Hong","Josh S. K. Bell","Abbas Rizvi","Amy Welch","Lingdao Sha","Irvin Ho","Kunal Nagpal","Aicha BenTaieb","Rohan P Joshi","Martin C Stumpe"],"pdf_url":"https://arxiv.org/pdf/2310.07682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07678v1","updated":"2023-10-11T17:21:48Z","published":"2023-10-11T17:21:48Z","title":"Explainable Image Similarity: Integrating Siamese Networks and Grad-CAM","summary":"  With the proliferation of image-based applications in various domains, the\nneed for accurate and interpretable image similarity measures has become\nincreasingly critical. Existing image similarity models often lack\ntransparency, making it challenging to understand the reasons why two images\nare considered similar. In this paper, we propose the concept of explainable\nimage similarity, where the goal is the development of an approach, which is\ncapable of providing similarity scores along with visual factual and\ncounterfactual explanations. Along this line, we present a new framework, which\nintegrates Siamese Networks and Grad-CAM for providing explainable image\nsimilarity and discuss the potential benefits and challenges of adopting this\napproach. In addition, we provide a comprehensive discussion about factual and\ncounterfactual explanations provided by the proposed framework for assisting\ndecision making. The proposed approach has the potential to enhance the\ninterpretability, trustworthiness and user acceptance of image-based systems in\nreal-world image similarity applications. The implementation code can be found\nin https://github.com/ioannislivieris/Grad_CAM_Siamese.git.\n","authors":["Ioannis E. Livieris","Emmanuel Pintelas","Niki Kiriakidou","Panagiotis Pintelas"],"pdf_url":"https://arxiv.org/pdf/2310.07678v1.pdf","comment":"The manuscript has been submitted for publication in \"Journal of\n  Imaging\""},{"id":"http://arxiv.org/abs/2310.07669v1","updated":"2023-10-11T17:18:15Z","published":"2023-10-11T17:18:15Z","title":"HaarNet: Large-scale Linear-Morphological Hybrid Network for RGB-D\n  Semantic Segmentation","summary":"  Signals from different modalities each have their own combination algebra\nwhich affects their sampling processing. RGB is mostly linear; depth is a\ngeometric signal following the operations of mathematical morphology. If a\nnetwork obtaining RGB-D input has both kinds of operators available in its\nlayers, it should be able to give effective output with fewer parameters. In\nthis paper, morphological elements in conjunction with more familiar linear\nmodules are used to construct a mixed linear-morphological network called\nHaarNet. This is the first large-scale linear-morphological hybrid, evaluated\non a set of sizeable real-world datasets. In the network, morphological Haar\nsampling is applied to both feature channels in several layers, which splits\nextreme values and high-frequency information such that both can be processed\nto improve both modalities. Moreover, morphologically parameterised ReLU is\nused, and morphologically-sound up-sampling is applied to obtain a\nfull-resolution output. Experiments show that HaarNet is competitive with a\nstate-of-the-art CNN, implying that morphological networks are a promising\nresearch direction for geometry-based learning tasks.\n","authors":["Rick Groenendijk","Leo Dorst","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2310.07669v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07664v1","updated":"2023-10-11T17:09:19Z","published":"2023-10-11T17:09:19Z","title":"Accelerating Vision Transformers Based on Heterogeneous Attention\n  Patterns","summary":"  Recently, Vision Transformers (ViTs) have attracted a lot of attention in the\nfield of computer vision. Generally, the powerful representative capacity of\nViTs mainly benefits from the self-attention mechanism, which has a high\ncomputation complexity. To accelerate ViTs, we propose an integrated\ncompression pipeline based on observed heterogeneous attention patterns across\nlayers. On one hand, different images share more similar attention patterns in\nearly layers than later layers, indicating that the dynamic query-by-key\nself-attention matrix may be replaced with a static self-attention matrix in\nearly layers. Then, we propose a dynamic-guided static self-attention (DGSSA)\nmethod where the matrix inherits self-attention information from the replaced\ndynamic self-attention to effectively improve the feature representation\nability of ViTs. On the other hand, the attention maps have more low-rank\npatterns, which reflect token redundancy, in later layers than early layers. In\na view of linear dimension reduction, we further propose a method of global\naggregation pyramid (GLAD) to reduce the number of tokens in later layers of\nViTs, such as Deit. Experimentally, the integrated compression pipeline of\nDGSSA and GLAD can accelerate up to 121% run-time throughput compared with\nDeiT, which surpasses all SOTA approaches.\n","authors":["Deli Yu","Teng Xi","Jianwei Li","Baopu Li","Gang Zhang","Haocheng Feng","Junyu Han","Jingtuo Liu","Errui Ding","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07663v1","updated":"2023-10-11T17:03:21Z","published":"2023-10-11T17:03:21Z","title":"Deep Video Inpainting Guided by Audio-Visual Self-Supervision","summary":"  Humans can easily imagine a scene from auditory information based on their\nprior knowledge of audio-visual events. In this paper, we mimic this innate\nhuman ability in deep learning models to improve the quality of video\ninpainting. To implement the prior knowledge, we first train the audio-visual\nnetwork, which learns the correspondence between auditory and visual\ninformation. Then, the audio-visual network is employed as a guider that\nconveys the prior knowledge of audio-visual correspondence to the video\ninpainting network. This prior knowledge is transferred through our proposed\ntwo novel losses: audio-visual attention loss and audio-visual pseudo-class\nconsistency loss. These two losses further improve the performance of the video\ninpainting by encouraging the inpainting result to have a high correspondence\nto its synchronized audio. Experimental results demonstrate that our proposed\nmethod can restore a wider domain of video scenes and is particularly effective\nwhen the sounding object in the scene is partially blinded.\n","authors":["Kyuyeon Kim","Junsik Jung","Woo Jae Kim","Sung-Eui Yoon"],"pdf_url":"https://arxiv.org/pdf/2310.07663v1.pdf","comment":"Accepted at ICASSP 2022"},{"id":"http://arxiv.org/abs/2305.13172v2","updated":"2023-10-11T16:51:50Z","published":"2023-05-22T16:00:00Z","title":"Editing Large Language Models: Problems, Methods, and Opportunities","summary":"  Despite the ability to train capable LLMs, the methodology for maintaining\ntheir relevancy and rectifying errors remains elusive. To this end, the past\nfew years have witnessed a surge in techniques for editing LLMs, the objective\nof which is to efficiently alter the behavior of LLMs within a specific domain\nwithout negatively impacting performance across other inputs. This paper\nembarks on a deep exploration of the problems, methods, and opportunities\nrelated to model editing for LLMs. In particular, we provide an exhaustive\noverview of the task definition and challenges associated with model editing,\nalong with an in-depth empirical analysis of the most progressive methods\ncurrently at our disposal. We also build a new benchmark dataset to facilitate\na more robust evaluation and pinpoint enduring issues intrinsic to existing\ntechniques. Our objective is to provide valuable insights into the\neffectiveness and feasibility of each editing technique, thereby assisting the\ncommunity in making informed decisions on the selection of the most appropriate\nmethod for a specific task or context. Code and datasets are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Yunzhi Yao","Peng Wang","Bozhong Tian","Siyuan Cheng","Zhoubo Li","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.13172v2.pdf","comment":"EMNLP 2023. Updated with new experiments"},{"id":"http://arxiv.org/abs/2310.07638v1","updated":"2023-10-11T16:33:30Z","published":"2023-10-11T16:33:30Z","title":"Context-Enhanced Detector For Building Detection From Remote Sensing\n  Images","summary":"  The field of building detection from remote sensing images has made\nsignificant progress, but faces challenges in achieving high-accuracy detection\ndue to the diversity in building appearances and the complexity of vast scenes.\nTo address these challenges, we propose a novel approach called\nContext-Enhanced Detector (CEDet). Our approach utilizes a three-stage cascade\nstructure to enhance the extraction of contextual information and improve\nbuilding detection accuracy. Specifically, we introduce two modules: the\nSemantic Guided Contextual Mining (SGCM) module, which aggregates multi-scale\ncontexts and incorporates an attention mechanism to capture long-range\ninteractions, and the Instance Context Mining Module (ICMM), which captures\ninstance-level relationship context by constructing a spatial relationship\ngraph and aggregating instance features. Additionally, we introduce a semantic\nsegmentation loss based on pseudo-masks to guide contextual information\nextraction. Our method achieves state-of-the-art performance on three building\ndetection benchmarks, including CNBuilding-9P, CNBuilding-23P, and SpaceNet.\n","authors":["Ziyue Huang","Mingming Zhang","Qingjie Liu","Wei Wang","Zhe Dong","Yunhong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07638v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.07633v1","updated":"2023-10-11T16:28:24Z","published":"2023-10-11T16:28:24Z","title":"Attention-Map Augmentation for Hypercomplex Breast Cancer Classification","summary":"  Breast cancer is the most widespread neoplasm among women and early detection\nof this disease is critical. Deep learning techniques have become of great\ninterest to improve diagnostic performance. Nonetheless, discriminating between\nmalignant and benign masses from whole mammograms remains challenging due to\nthem being almost identical to an untrained eye and the region of interest\n(ROI) occupying a minuscule portion of the entire image. In this paper, we\npropose a framework, parameterized hypercomplex attention maps (PHAM), to\novercome these problems. Specifically, we deploy an augmentation step based on\ncomputing attention maps. Then, the attention maps are used to condition the\nclassification step by constructing a multi-dimensional input comprised of the\noriginal breast cancer image and the corresponding attention map. In this step,\na parameterized hypercomplex neural network (PHNN) is employed to perform\nbreast cancer classification. The framework offers two main advantages. First,\nattention maps provide critical information regarding the ROI and allow the\nneural model to concentrate on it. Second, the hypercomplex architecture has\nthe ability to model local relations between input dimensions thanks to\nhypercomplex algebra rules, thus properly exploiting the information provided\nby the attention map. We demonstrate the efficacy of the proposed framework on\nboth mammography images as well as histopathological ones, surpassing\nattention-based state-of-the-art networks and the real-valued counterpart of\nour method. The code of our work is available at\nhttps://github.com/elelo22/AttentionBCS.\n","authors":["Eleonora Lopez","Filippo Betello","Federico Carmignani","Eleonora Grassucci","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2310.07633v1.pdf","comment":"Submitted to Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2310.07632v1","updated":"2023-10-11T16:25:45Z","published":"2023-10-11T16:25:45Z","title":"Prompt Backdoors in Visual Prompt Learning","summary":"  Fine-tuning large pre-trained computer vision models is infeasible for\nresource-limited users. Visual prompt learning (VPL) has thus emerged to\nprovide an efficient and flexible alternative to model fine-tuning through\nVisual Prompt as a Service (VPPTaaS). Specifically, the VPPTaaS provider\noptimizes a visual prompt given downstream data, and downstream users can use\nthis prompt together with the large pre-trained model for prediction. However,\nthis new learning paradigm may also pose security risks when the VPPTaaS\nprovider instead provides a malicious visual prompt. In this paper, we take the\nfirst step to explore such risks through the lens of backdoor attacks.\nSpecifically, we propose BadVisualPrompt, a simple yet effective backdoor\nattack against VPL. For example, poisoning $5\\%$ CIFAR10 training data leads to\nabove $99\\%$ attack success rates with only negligible model accuracy drop by\n$1.5\\%$. In particular, we identify and then address a new technical challenge\nrelated to interactions between the backdoor trigger and visual prompt, which\ndoes not exist in conventional, model-level backdoors. Moreover, we provide\nin-depth analyses of seven backdoor defenses from model, prompt, and input\nlevels. Overall, all these defenses are either ineffective or impractical to\nmitigate our BadVisualPrompt, implying the critical vulnerability of VPL.\n","authors":["Hai Huang","Zhengyu Zhao","Michael Backes","Yun Shen","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.07632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07623v1","updated":"2023-10-11T16:06:14Z","published":"2023-10-11T16:06:14Z","title":"Dual Quaternion Rotational and Translational Equivariance in 3D Rigid\n  Motion Modelling","summary":"  Objects' rigid motions in 3D space are described by rotations and\ntranslations of a highly-correlated set of points, each with associated $x,y,z$\ncoordinates that real-valued networks consider as separate entities, losing\ninformation. Previous works exploit quaternion algebra and their ability to\nmodel rotations in 3D space. However, these algebras do not properly encode\ntranslations, leading to sub-optimal performance in 3D learning tasks. To\novercome these limitations, we employ a dual quaternion representation of rigid\nmotions in the 3D space that jointly describes rotations and translations of\npoint sets, processing each of the points as a single entity. Our approach is\ntranslation and rotation equivariant, so it does not suffer from shifts in the\ndata and better learns object trajectories, as we validate in the experimental\nevaluations. Models endowed with this formulation outperform previous\napproaches in a human pose forecasting application, attesting to the\neffectiveness of the proposed dual quaternion formulation for rigid motions in\n3D space.\n","authors":["Guilherme Vieira","Eleonora Grassucci","Marcos Eduardo Valle","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2310.07623v1.pdf","comment":"Accepted at IEEE MLSP 2023 (Honorable Mention Top 10% Outstanding\n  Paper)"},{"id":"http://arxiv.org/abs/2310.07602v1","updated":"2023-10-11T15:41:52Z","published":"2023-10-11T15:41:52Z","title":"Dual Radar: A Multi-modal Dataset with Dual 4D Radar for Autononous\n  Driving","summary":"  Radar has stronger adaptability in adverse scenarios for autonomous driving\nenvironmental perception compared to widely adopted cameras and LiDARs.\nCompared with commonly used 3D radars, latest 4D radars have precise vertical\nresolution and higher point cloud density, making it a highly promising sensor\nfor autonomous driving in complex environmental perception. However, due to the\nmuch higher noise than LiDAR, manufacturers choose different filtering\nstrategies, resulting in an inverse ratio between noise level and point cloud\ndensity. There is still a lack of comparative analysis on which method is\nbeneficial for deep learning-based perception algorithms in autonomous driving.\nOne of the main reasons is that current datasets only adopt one type of 4D\nradar, making it difficult to compare different 4D radars in the same scene.\nTherefore, in this paper, we introduce a novel large-scale multi-modal dataset\nfeaturing, for the first time, two types of 4D radars captured simultaneously.\nThis dataset enables further research into effective 4D radar perception\nalgorithms.Our dataset consists of 151 consecutive series, most of which last\n20 seconds and contain 10,007 meticulously synchronized and annotated frames.\nMoreover, our dataset captures a variety of challenging driving scenarios,\nincluding many road conditions, weather conditions, nighttime and daytime with\ndifferent lighting intensities and periods. Our dataset annotates consecutive\nframes, which can be applied to 3D object detection and tracking, and also\nsupports the study of multi-modal tasks. We experimentally validate our\ndataset, providing valuable results for studying different types of 4D radars.\nThis dataset is released on https://github.com/adept-thu/Dual-Radar.\n","authors":["Xinyu Zhang","Li Wang","Jian Chen","Cheng Fang","Lei Yang","Ziying Song","Guangqi Yang","Yichen Wang","Xiaofei Zhang","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2310.07602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07591v1","updated":"2023-10-11T15:33:10Z","published":"2023-10-11T15:33:10Z","title":"PeP: a Point enhanced Painting method for unified point cloud tasks","summary":"  Point encoder is of vital importance for point cloud recognition. As the very\nbeginning step of whole model pipeline, adding features from diverse sources\nand providing stronger feature encoding mechanism would provide better input\nfor downstream modules. In our work, we proposed a novel PeP module to tackle\nabove issue. PeP contains two main parts, a refined point painting method and a\nLM-based point encoder. Experiments results on the nuScenes and KITTI datasets\nvalidate the superior performance of our PeP. The advantages leads to strong\nperformance on both semantic segmentation and object detection, in both lidar\nand multi-modal settings. Notably, our PeP module is model agnostic and\nplug-and-play. Our code will be publicly available soon.\n","authors":["Zichao Dong","Hang Ji","Xufeng Huang","Weikun Zhang","Xin Zhan","Junbo Chen"],"pdf_url":"https://arxiv.org/pdf/2310.07591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07585v1","updated":"2023-10-11T15:21:40Z","published":"2023-10-11T15:21:40Z","title":"A Discrepancy Aware Framework for Robust Anomaly Detection","summary":"  Defect detection is a critical research area in artificial intelligence.\nRecently, synthetic data-based self-supervised learning has shown great\npotential on this task. Although many sophisticated synthesizing strategies\nexist, little research has been done to investigate the robustness of models\nwhen faced with different strategies. In this paper, we focus on this issue and\nfind that existing methods are highly sensitive to them. To alleviate this\nissue, we present a Discrepancy Aware Framework (DAF), which demonstrates\nrobust performance consistently with simple and cheap strategies across\ndifferent anomaly detection benchmarks. We hypothesize that the high\nsensitivity to synthetic data of existing self-supervised methods arises from\ntheir heavy reliance on the visual appearance of synthetic data during\ndecoding. In contrast, our method leverages an appearance-agnostic cue to guide\nthe decoder in identifying defects, thereby alleviating its reliance on\nsynthetic appearance. To this end, inspired by existing knowledge distillation\nmethods, we employ a teacher-student network, which is trained based on\nsynthesized outliers, to compute the discrepancy map as the cue. Extensive\nexperiments on two challenging datasets prove the robustness of our method.\nUnder the simple synthesis strategies, it outperforms existing methods by a\nlarge margin. Furthermore, it also achieves the state-of-the-art localization\nperformance. Code is available at: https://github.com/caiyuxuan1120/DAF.\n","authors":["Yuxuan Cai","Dingkang Liang","Dongliang Luo","Xinwei He","Xin Yang","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2310.07585v1.pdf","comment":"Accepted by IEEE Transactions on Industrial Informatics. Code is\n  available at: https://github.com/caiyuxuan1120/DAF"},{"id":"http://arxiv.org/abs/2310.07584v1","updated":"2023-10-11T15:20:44Z","published":"2023-10-11T15:20:44Z","title":"Centrality of the Fingerprint Core Location","summary":"  Fingerprints have long been recognized as a unique and reliable means of\npersonal identification. Central to the analysis and enhancement of\nfingerprints is the concept of the fingerprint core. Although the location of\nthe core is used in many applications, to the best of our knowledge, this study\nis the first to investigate the empirical distribution of the core over a\nlarge, combined dataset of rolled, as well as plain fingerprint recordings. We\nidentify and investigate the extent of incomplete rolling during the rolled\nfingerprint acquisition and investigate the centrality of the core. After\ncorrecting for the incomplete rolling, we find that the core deviates from the\nfingerprint center by 5.7% $\\pm$ 5.2% to 7.6% $\\pm$ 6.9%, depending on the\nfinger. Additionally, we find that the assumption of normal distribution of the\ncore position of plain fingerprint recordings cannot be rejected, but for\nrolled ones it can. Therefore, we use a multi-step process to find the\ndistribution of the rolled fingerprint recordings. The process consists of an\nAnderson-Darling normality test, the Bayesian Information Criterion to reduce\nthe number of possible candidate distributions and finally a Generalized Monte\nCarlo goodness-of-fit procedure to find the best fitting distribution. We find\nthe non-central Fischer distribution best describes the cores' horizontal\npositions. Finally, we investigate the correlation between mean core position\noffset and the NFIQ 2 score and find that the NFIQ 2 prefers rolled fingerprint\nrecordings where the core sits slightly below the fingerprint center.\n","authors":["Laurenz Ruzicka","Bernhard Strobl","Bernhard Kohn","Clemens Heitzinger"],"pdf_url":"https://arxiv.org/pdf/2310.07584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07573v1","updated":"2023-10-11T15:15:05Z","published":"2023-10-11T15:15:05Z","title":"Relational Prior Knowledge Graphs for Detection and Instance\n  Segmentation","summary":"  Humans have a remarkable ability to perceive and reason about the world\naround them by understanding the relationships between objects. In this paper,\nwe investigate the effectiveness of using such relationships for object\ndetection and instance segmentation. To this end, we propose a Relational\nPrior-based Feature Enhancement Model (RP-FEM), a graph transformer that\nenhances object proposal features using relational priors. The proposed\narchitecture operates on top of scene graphs obtained from initial proposals\nand aims to concurrently learn relational context modeling for object detection\nand instance segmentation. Experimental evaluations on COCO show that the\nutilization of scene graphs, augmented with relational priors, offer benefits\nfor object detection and instance segmentation. RP-FEM demonstrates its\ncapacity to suppress improbable class predictions within the image while also\npreventing the model from generating duplicate predictions, leading to\nimprovements over the baseline model on which it is built.\n","authors":["Osman Ülger","Yu Wang","Ysbrand Galama","Sezer Karaoglu","Theo Gevers","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2310.07573v1.pdf","comment":"Published in ICCV2023 SG2RL Workshop"},{"id":"http://arxiv.org/abs/2310.07572v1","updated":"2023-10-11T15:14:54Z","published":"2023-10-11T15:14:54Z","title":"Impact of Label Types on Training SWIN Models with Overhead Imagery","summary":"  Understanding the impact of data set design on model training and performance\ncan help alleviate the costs associated with generating remote sensing and\noverhead labeled data. This work examined the impact of training shifted window\ntransformers using bounding boxes and segmentation labels, where the latter are\nmore expensive to produce. We examined classification tasks by comparing models\ntrained with both target and backgrounds against models trained with only\ntarget pixels, extracted by segmentation labels. For object detection models,\nwe compared performance using either label type when training. We found that\nthe models trained on only target pixels do not show performance improvement\nfor classification tasks, appearing to conflate background pixels in the\nevaluation set with target pixels. For object detection, we found that models\ntrained with either label type showed equivalent performance across testing. We\nfound that bounding boxes appeared to be sufficient for tasks that did not\nrequire more complex labels, such as object segmentation. Continuing work to\ndetermine consistency of this result across data types and model architectures\ncould potentially result in substantial savings in generating remote sensing\ndata sets for deep learning.\n","authors":["Ryan Ford","Kenneth Hutchison","Nicholas Felts","Benjamin Cheng","Jesse Lew","Kyle Jackson"],"pdf_url":"https://arxiv.org/pdf/2310.07572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.15751v3","updated":"2023-10-11T15:13:02Z","published":"2022-11-28T20:11:37Z","title":"Edge Video Analytics: A Survey on Applications, Systems and Enabling\n  Techniques","summary":"  Video, as a key driver in the global explosion of digital information, can\ncreate tremendous benefits for human society. Governments and enterprises are\ndeploying innumerable cameras for a variety of applications, e.g., law\nenforcement, emergency management, traffic control, and security surveillance,\nall facilitated by video analytics (VA). This trend is spurred by the rapid\nadvancement of deep learning (DL), which enables more precise models for object\nclassification, detection, and tracking. Meanwhile, with the proliferation of\nInternet-connected devices, massive amounts of data are generated daily,\noverwhelming the cloud. Edge computing, an emerging paradigm that moves\nworkloads and services from the network core to the network edge, has been\nwidely recognized as a promising solution. The resulting new intersection, edge\nvideo analytics (EVA), begins to attract widespread attention. Nevertheless,\nonly a few loosely-related surveys exist on this topic. The basic concepts of\nEVA (e.g., definition, architectures) were not fully elucidated due to the\nrapid development of this domain. To fill these gaps, we provide a\ncomprehensive survey of the recent efforts on EVA. In this paper, we first\nreview the fundamentals of edge computing, followed by an overview of VA. EVA\nsystems and their enabling techniques are discussed next. In addition, we\nintroduce prevalent frameworks and datasets to aid future researchers in the\ndevelopment of EVA systems. Finally, we discuss existing challenges and foresee\nfuture research directions. We believe this survey will help readers comprehend\nthe relationship between VA and edge computing, and spark new ideas on EVA.\n","authors":["Renjie Xu","Saiedeh Razavi","Rong Zheng"],"pdf_url":"https://arxiv.org/pdf/2211.15751v3.pdf","comment":"Accepted in IEEE Communications Surveys and Tutorials, 2023"},{"id":"http://arxiv.org/abs/2304.14933v2","updated":"2023-10-11T15:08:51Z","published":"2023-04-28T15:43:21Z","title":"An Empirical Study of Multimodal Model Merging","summary":"  Model merging (e.g., via interpolation or task arithmetic) fuses multiple\nmodels trained on different tasks to generate a multi-task solution. The\ntechnique has been proven successful in previous studies, where the models are\ntrained on similar tasks and with the same initialization. In this paper, we\nexpand on this concept to a multimodal setup by merging transformers trained on\ndifferent modalities. Furthermore, we conduct our study for a novel goal where\nwe can merge vision, language, and cross-modal transformers of a\nmodality-specific architecture to create a parameter-efficient\nmodality-agnostic architecture. Through comprehensive experiments, we\nsystematically investigate the key factors impacting model performance after\nmerging, including initialization, merging mechanisms, and model architectures.\nWe also propose two metrics that assess the distance between weights to be\nmerged and can serve as an indicator of the merging outcomes. Our analysis\nleads to an effective training recipe for matching the performance of the\nmodality-agnostic baseline (i.e., pre-trained from scratch) via model merging.\nOur method also outperforms naive merging significantly on various tasks, with\nimprovements of 3% on VQA, 7% on COCO retrieval, 25% on NLVR2, 14% on Flickr30k\nand 3% on ADE20k. Our code is available at https://github.com/ylsung/vl-merging\n","authors":["Yi-Lin Sung","Linjie Li","Kevin Lin","Zhe Gan","Mohit Bansal","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2304.14933v2.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2310.07555v1","updated":"2023-10-11T15:00:11Z","published":"2023-10-11T15:00:11Z","title":"Does resistance to Style-Transfer equal Shape Bias? Evaluating Shape\n  Bias by Distorted Shape","summary":"  Deep learning models are known to exhibit a strong texture bias, while human\ntends to rely heavily on global shape for object recognition. The current\nbenchmark for evaluating a model's shape bias is a set of style-transferred\nimages with the assumption that resistance to the attack of style transfer is\nrelated to the development of shape sensitivity in the model. In this work, we\nshow that networks trained with style-transfer images indeed learn to ignore\nstyle, but its shape bias arises primarily from local shapes. We provide a\nDistorted Shape Testbench (DiST) as an alternative measurement of global shape\nsensitivity. Our test includes 2400 original images from ImageNet-1K, each of\nwhich is accompanied by two images with the global shapes of the original image\ndistorted while preserving its texture via the texture synthesis program. We\nfound that (1) models that performed well on the previous shape bias evaluation\ndo not fare well in the proposed DiST; (2) the widely adopted ViT models do not\nshow significant advantages over Convolutional Neural Networks (CNNs) on this\nbenchmark despite that ViTs rank higher on the previous shape bias tests. (3)\ntraining with DiST images bridges the significant gap between human and\nexisting SOTA models' performance while preserving the models' accuracy on\nstandard image classification tasks; training with DiST images and\nstyle-transferred images are complementary, and can be combined to train\nnetwork together to enhance both the global and local shape sensitivity of the\nnetwork. Our code will be host at: https://github.com/leelabcnbc/DiST\n","authors":["Ziqi Wen","Tianqin Li","Tai Sing Lee"],"pdf_url":"https://arxiv.org/pdf/2310.07555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07552v1","updated":"2023-10-11T14:54:40Z","published":"2023-10-11T14:54:40Z","title":"ProtoHPE: Prototype-guided High-frequency Patch Enhancement for\n  Visible-Infrared Person Re-identification","summary":"  Visible-infrared person re-identification is challenging due to the large\nmodality gap. To bridge the gap, most studies heavily rely on the correlation\nof visible-infrared holistic person images, which may perform poorly under\nsevere distribution shifts. In contrast, we find that some cross-modal\ncorrelated high-frequency components contain discriminative visual patterns and\nare less affected by variations such as wavelength, pose, and background\nclutter than holistic images. Therefore, we are motivated to bridge the\nmodality gap based on such high-frequency components, and propose\n\\textbf{Proto}type-guided \\textbf{H}igh-frequency \\textbf{P}atch\n\\textbf{E}nhancement (ProtoHPE) with two core designs. \\textbf{First}, to\nenhance the representation ability of cross-modal correlated high-frequency\ncomponents, we split patches with such components by Wavelet Transform and\nexponential moving average Vision Transformer (ViT), then empower ViT to take\nthe split patches as auxiliary input. \\textbf{Second}, to obtain semantically\ncompact and discriminative high-frequency representations of the same identity,\nwe propose Multimodal Prototypical Contrast. To be specific, it hierarchically\ncaptures the comprehensive semantics of different modal instances, facilitating\nthe aggregation of high-frequency representations belonging to the same\nidentity. With it, ViT can capture key high-frequency components during\ninference without relying on ProtoHPE, thus bringing no extra complexity.\nExtensive experiments validate the effectiveness of ProtoHPE.\n","authors":["Guiwei Zhang","Yongfei Zhang","Zichang Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07548v1","updated":"2023-10-11T14:50:52Z","published":"2023-10-11T14:50:52Z","title":"Attribute Localization and Revision Network for Zero-Shot Learning","summary":"  Zero-shot learning enables the model to recognize unseen categories with the\naid of auxiliary semantic information such as attributes. Current works\nproposed to detect attributes from local image regions and align extracted\nfeatures with class-level semantics. In this paper, we find that the choice\nbetween local and global features is not a zero-sum game, global features can\nalso contribute to the understanding of attributes. In addition, aligning\nattribute features with class-level semantics ignores potential intra-class\nattribute variation. To mitigate these disadvantages, we present Attribute\nLocalization and Revision Network in this paper. First, we design Attribute\nLocalization Module (ALM) to capture both local and global features from image\nregions, a novel module called Scale Control Unit is incorporated to fuse\nglobal and local representations. Second, we propose Attribute Revision Module\n(ARM), which generates image-level semantics by revising the ground-truth value\nof each attribute, compensating for performance degradation caused by ignoring\nintra-class variation. Finally, the output of ALM will be aligned with revised\nsemantics produced by ARM to achieve the training process. Comprehensive\nexperimental results on three widely used benchmarks demonstrate the\neffectiveness of our model in the zero-shot prediction task.\n","authors":["Junzhe Xu","Suling Duan","Chenwei Tang","Zhenan He","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2310.07548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12067v2","updated":"2023-10-11T14:49:26Z","published":"2023-08-23T11:27:30Z","title":"InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4","summary":"  Multimodal large language models are typically trained in two stages: first\npre-training on image-text pairs, and then fine-tuning using supervised\nvision-language instruction data. Recent studies have shown that large language\nmodels can achieve satisfactory results even with a limited amount of\nhigh-quality instruction-following data. In this paper, we introduce\nInstructionGPT-4, which is fine-tuned on a small dataset comprising only 200\nexamples, amounting to approximately 6\\% of the instruction-following data used\nin the alignment dataset for MiniGPT-4. To achieve this, we first propose\nseveral metrics to access the quality of multimodal instruction data. Based on\nthese metrics, we present an effective and trainable data selector to\nautomatically identify and filter low-quality vision-language data. By\nemploying this method, InstructionGPT-4 outperforms the original MiniGPT-4 on\nvarious evaluations. Overall, our findings demonstrate that less but\nhigh-quality instruction tuning data is efficient in enabling multimodal large\nlanguage models to generate better output. Our code is available at\nhttps://github.com/waltonfuture/InstructionGPT-4.\n","authors":["Lai Wei","Zihao Jiang","Weiran Huang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2308.12067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07534v1","updated":"2023-10-11T14:39:12Z","published":"2023-10-11T14:39:12Z","title":"Human-Centered Evaluation of XAI Methods","summary":"  In the ever-evolving field of Artificial Intelligence, a critical challenge\nhas been to decipher the decision-making processes within the so-called \"black\nboxes\" in deep learning. Over recent years, a plethora of methods have emerged,\ndedicated to explaining decisions across diverse tasks. Particularly in tasks\nlike image classification, these methods typically identify and emphasize the\npivotal pixels that most influence a classifier's prediction. Interestingly,\nthis approach mirrors human behavior: when asked to explain our rationale for\nclassifying an image, we often point to the most salient features or aspects.\nCapitalizing on this parallel, our research embarked on a user-centric study.\nWe sought to objectively measure the interpretability of three leading\nexplanation methods: (1) Prototypical Part Network, (2) Occlusion, and (3)\nLayer-wise Relevance Propagation. Intriguingly, our results highlight that\nwhile the regions spotlighted by these methods can vary widely, they all offer\nhumans a nearly equivalent depth of understanding. This enables users to\ndiscern and categorize images efficiently, reinforcing the value of these\nmethods in enhancing AI transparency.\n","authors":["Karam Dawoud","Wojciech Samek","Sebastian Lapuschkin","Sebastian Bosse"],"pdf_url":"https://arxiv.org/pdf/2310.07534v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12813v2","updated":"2023-10-11T14:35:26Z","published":"2023-07-24T14:06:54Z","title":"Described Object Detection: Liberating Object Detection with Flexible\n  Expressions","summary":"  Detecting objects based on language information is a popular task that\nincludes Open-Vocabulary object Detection (OVD) and Referring Expression\nComprehension (REC). In this paper, we advance them to a more practical setting\ncalled Described Object Detection (DOD) by expanding category names to flexible\nlanguage expressions for OVD and overcoming the limitation of REC only\ngrounding the pre-existing object. We establish the research foundation for DOD\nby constructing a Description Detection Dataset ($D^3$). This dataset features\nflexible language expressions, whether short category names or long\ndescriptions, and annotating all described objects on all images without\nomission. By evaluating previous SOTA methods on $D^3$, we find some\ntroublemakers that fail current REC, OVD, and bi-functional methods. REC\nmethods struggle with confidence scores, rejecting negative instances, and\nmulti-target scenarios, while OVD methods face constraints with long and\ncomplex descriptions. Recent bi-functional methods also do not work well on DOD\ndue to their separated training procedures and inference strategies for REC and\nOVD tasks. Building upon the aforementioned findings, we propose a baseline\nthat largely improves REC methods by reconstructing the training data and\nintroducing a binary classification sub-task, outperforming existing methods.\nData and code are available at https://github.com/shikras/d-cube and related\nworks are tracked in\nhttps://github.com/Charles-Xie/awesome-described-object-detection.\n","authors":["Chi Xie","Zhao Zhang","Yixuan Wu","Feng Zhu","Rui Zhao","Shuang Liang"],"pdf_url":"https://arxiv.org/pdf/2307.12813v2.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2304.01950v2","updated":"2023-10-11T14:21:29Z","published":"2023-04-01T09:16:40Z","title":"MP-FedCL: Multiprototype Federated Contrastive Learning for Edge\n  Intelligence","summary":"  Federated learning-assisted edge intelligence enables privacy protection in\nmodern intelligent services. However, not independent and identically\ndistributed (non-IID) distribution among edge clients can impair the local\nmodel performance. The existing single prototype-based strategy represents a\nclass by using the mean of the feature space. However, feature spaces are\nusually not clustered, and a single prototype may not represent a class well.\nMotivated by this, this paper proposes a multi-prototype federated contrastive\nlearning approach (MP-FedCL) which demonstrates the effectiveness of using a\nmulti-prototype strategy over a single-prototype under non-IID settings,\nincluding both label and feature skewness. Specifically, a multi-prototype\ncomputation strategy based on \\textit{k-means} is first proposed to capture\ndifferent embedding representations for each class space, using multiple\nprototypes ($k$ centroids) to represent a class in the embedding space. In each\nglobal round, the computed multiple prototypes and their respective model\nparameters are sent to the edge server for aggregation into a global prototype\npool, which is then sent back to all clients to guide their local training.\nFinally, local training for each client minimizes their own supervised learning\ntasks and learns from shared prototypes in the global prototype pool through\nsupervised contrastive learning, which encourages them to learn knowledge\nrelated to their own class from others and reduces the absorption of unrelated\nknowledge in each global iteration. Experimental results on MNIST, Digit-5,\nOffice-10, and DomainNet show that our method outperforms multiple baselines,\nwith an average test accuracy improvement of about 4.6\\% and 10.4\\% under\nfeature and label non-IID distributions, respectively.\n","authors":["Yu Qiao","Md. Shirajum Munir","Apurba Adhikary","Huy Q. Le","Avi Deb Raha","Chaoning Zhang","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2304.01950v2.pdf","comment":"Accepted by IEEE Internet of Things"},{"id":"http://arxiv.org/abs/2310.07522v1","updated":"2023-10-11T14:19:05Z","published":"2023-10-11T14:19:05Z","title":"S4C: Self-Supervised Semantic Scene Completion with Neural Fields","summary":"  3D semantic scene understanding is a fundamental challenge in computer\nvision. It enables mobile agents to autonomously plan and navigate arbitrary\nenvironments. SSC formalizes this challenge as jointly estimating dense\ngeometry and semantic information from sparse observations of a scene. Current\nmethods for SSC are generally trained on 3D ground truth based on aggregated\nLiDAR scans. This process relies on special sensors and annotation by hand\nwhich are costly and do not scale well. To overcome this issue, our work\npresents the first self-supervised approach to SSC called S4C that does not\nrely on 3D ground truth data. Our proposed method can reconstruct a scene from\na single image and only relies on videos and pseudo segmentation ground truth\ngenerated from off-the-shelf image segmentation network during training. Unlike\nexisting methods, which use discrete voxel grids, we represent scenes as\nimplicit semantic fields. This formulation allows querying any point within the\ncamera frustum for occupancy and semantic class. Our architecture is trained\nthrough rendering-based self-supervised losses. Nonetheless, our method\nachieves performance close to fully supervised state-of-the-art methods.\nAdditionally, our method demonstrates strong generalization capabilities and\ncan synthesize accurate segmentation maps for far away viewpoints.\n","authors":["Adrian Hayler","Felix Wimbauer","Dominik Muhle","Christian Rupprecht","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2310.07522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07517v1","updated":"2023-10-11T14:15:25Z","published":"2023-10-11T14:15:25Z","title":"CM-PIE: Cross-modal perception for interactive-enhanced audio-visual\n  video parsing","summary":"  Audio-visual video parsing is the task of categorizing a video at the segment\nlevel with weak labels, and predicting them as audible or visible events.\nRecent methods for this task leverage the attention mechanism to capture the\nsemantic correlations among the whole video across the audio-visual modalities.\nHowever, these approaches have overlooked the importance of individual segments\nwithin a video and the relationship among them, and tend to rely on a single\nmodality when learning features. In this paper, we propose a novel\ninteractive-enhanced cross-modal perception method~(CM-PIE), which can learn\nfine-grained features by applying a segment-based attention module.\nFurthermore, a cross-modal aggregation block is introduced to jointly optimize\nthe semantic representation of audio and visual signals by enhancing\ninter-modal interactions. The experimental results show that our model offers\nimproved parsing performance on the Look, Listen, and Parse dataset compared to\nother methods.\n","authors":["Yaru Chen","Ruohao Guo","Xubo Liu","Peipei Wu","Guangyao Li","Zhenbo Li","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07517v1.pdf","comment":"5 pages, 3 figures, 15 references"},{"id":"http://arxiv.org/abs/2310.07511v1","updated":"2023-10-11T14:07:05Z","published":"2023-10-11T14:07:05Z","title":"A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes\n  via Deviation Relationship Learning","summary":"  Remote sensing anomaly detector can find the objects deviating from the\nbackground as potential targets. Given the diversity in earth anomaly types, a\nunified anomaly detector across modalities and scenes should be cost-effective\nand flexible to new earth observation sources and anomaly types. However, the\ncurrent anomaly detectors are limited to a single modality and single scene,\nsince they aim to learn the varying background distribution. Motivated by the\nuniversal anomaly deviation pattern, in that anomalies exhibit deviations from\ntheir local context, we exploit this characteristic to build a unified anomaly\ndetector. Firstly, we reformulate the anomaly detection task as an undirected\nbilayer graph based on the deviation relationship, where the anomaly score is\nmodeled as the conditional probability, given the pattern of the background and\nnormal objects. The learning objective is then expressed as a conditional\nprobability ranking problem. Furthermore, we design an instantiation of the\nreformulation in the data, architecture, and optimization aspects. Simulated\nspectral and spatial anomalies drive the instantiated architecture. The model\nis optimized directly for the conditional probability ranking. The proposed\nmodel was validated in five modalities including the hyperspectral, visible\nlight, synthetic aperture radar (SAR), infrared and low light to show its\nunified detection ability.\n","authors":["Jingtao Li","Xinyu Wang","Hengwei Zhao","Liangpei Zhang","Yanfei Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.07511v1.pdf","comment":"Journal paper"},{"id":"http://arxiv.org/abs/2310.07510v1","updated":"2023-10-11T14:06:04Z","published":"2023-10-11T14:06:04Z","title":"Heuristic Vision Pre-Training with Self-Supervised and Supervised\n  Multi-Task Learning","summary":"  To mimic human vision with the way of recognizing the diverse and open world,\nfoundation vision models are much critical. While recent techniques of\nself-supervised learning show the promising potentiality of this mission, we\nargue that signals from labelled data are also important for common-sense\nrecognition, and properly chosen pre-text tasks can facilitate the efficiency\nof vision representation learning. To this end, we propose a novel pre-training\nframework by adopting both self-supervised and supervised visual pre-text tasks\nin a multi-task manner. Specifically, given an image, we take a heuristic way\nby considering its intrinsic style properties, inside objects with their\nlocations and correlations, and how it looks like in 3D space for basic visual\nunderstanding. However, large-scale object bounding boxes and correlations are\nusually hard to achieve. Alternatively, we develop a hybrid method by\nleveraging both multi-label classification and self-supervised learning. On the\none hand, under the multi-label supervision, the pre-trained model can explore\nthe detailed information of an image, e.g., image types, objects, and part of\nsemantic relations. On the other hand, self-supervised learning tasks, with\nrespect to Masked Image Modeling (MIM) and contrastive learning, can help the\nmodel learn pixel details and patch correlations. Results show that our\npre-trained models can deliver results on par with or better than\nstate-of-the-art (SOTA) results on multiple visual tasks. For example, with a\nvanilla Swin-B backbone, we achieve 85.3\\% top-1 accuracy on ImageNet-1K\nclassification, 47.9 box AP on COCO object detection for Mask R-CNN, and 50.6\nmIoU on ADE-20K semantic segmentation when using Upernet. The performance shows\nthe ability of our vision foundation model to serve general purpose vision\ntasks.\n","authors":["Zhiming Qian"],"pdf_url":"https://arxiv.org/pdf/2310.07510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07506v1","updated":"2023-10-11T14:02:11Z","published":"2023-10-11T14:02:11Z","title":"Leveraging Hierarchical Feature Sharing for Efficient Dataset\n  Condensation","summary":"  Given a real-world dataset, data condensation (DC) aims to synthesize a\nsignificantly smaller dataset that captures the knowledge of this dataset for\nmodel training with high performance. Recent works propose to enhance DC with\ndata parameterization, which condenses data into parameterized data containers\nrather than pixel space. The intuition behind data parameterization is to\nencode shared features of images to avoid additional storage costs. In this\npaper, we recognize that images share common features in a hierarchical way due\nto the inherent hierarchical structure of the classification system, which is\noverlooked by current data parameterization methods. To better align DC with\nthis hierarchical nature and encourage more efficient information sharing\ninside data containers, we propose a novel data parameterization architecture,\nHierarchical Memory Network (HMN). HMN stores condensed data in a three-tier\nstructure, representing the dataset-level, class-level, and instance-level\nfeatures. Another helpful property of the hierarchical architecture is that HMN\nnaturally ensures good independence among images despite achieving information\nsharing. This enables instance-level pruning for HMN to reduce redundant\ninformation, thereby further minimizing redundancy and enhancing performance.\nWe evaluate HMN on four public datasets (SVHN, CIFAR10, CIFAR100, and\nTiny-ImageNet) and compare HMN with eight DC baselines. The evaluation results\nshow that our proposed method outperforms all baselines, even when trained with\na batch-based loss consuming less GPU memory.\n","authors":["Haizhong Zheng","Jiachen Sun","Shutong Wu","Bhavya Kailkhura","Zhuoqing Mao","Chaowei Xiao","Atul Prakash"],"pdf_url":"https://arxiv.org/pdf/2310.07506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07504v1","updated":"2023-10-11T14:01:36Z","published":"2023-10-11T14:01:36Z","title":"PtychoDV: Vision Transformer-Based Deep Unrolling Network for\n  Ptychographic Image Reconstruction","summary":"  Ptychography is an imaging technique that captures multiple overlapping\nsnapshots of a sample, illuminated coherently by a moving localized probe. The\nimage recovery from ptychographic data is generally achieved via an iterative\nalgorithm that solves a nonlinear phase-field problem derived from measured\ndiffraction patterns. However, these approaches have high computational cost.\nIn this paper, we introduce PtychoDV, a novel deep model-based network designed\nfor efficient, high-quality ptychographic image reconstruction. PtychoDV\ncomprises a vision transformer that generates an initial image from the set of\nraw measurements, taking into consideration their mutual correlations. This is\nfollowed by a deep unrolling network that refines the initial image using\nlearnable convolutional priors and the ptychography measurement model.\nExperimental results on simulated data demonstrate that PtychoDV is capable of\noutperforming existing deep learning methods for this problem, and\nsignificantly reduces computational cost compared to iterative methodologies,\nwhile maintaining competitive performance.\n","authors":["Weijie Gan","Qiuchen Zhai","Michael Thompson McCann","Cristina Garcia Cardona","Ulugbek S. Kamilov","Brendt Wohlberg"],"pdf_url":"https://arxiv.org/pdf/2310.07504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.08313v3","updated":"2023-10-11T13:55:29Z","published":"2023-08-16T12:18:27Z","title":"ECPC-IDS:A benchmark endometrail cancer PET/CT image dataset for\n  evaluation of semantic segmentation and detection of hypermetabolic regions","summary":"  Endometrial cancer is one of the most common tumors in the female\nreproductive system and is the third most common gynecological malignancy that\ncauses death after ovarian and cervical cancer. Early diagnosis can\nsignificantly improve the 5-year survival rate of patients. With the\ndevelopment of artificial intelligence, computer-assisted diagnosis plays an\nincreasingly important role in improving the accuracy and objectivity of\ndiagnosis, as well as reducing the workload of doctors. However, the absence of\npublicly available endometrial cancer image datasets restricts the application\nof computer-assisted diagnostic techniques.In this paper, a publicly available\nEndometrial Cancer PET/CT Image Dataset for Evaluation of Semantic Segmentation\nand Detection of Hypermetabolic Regions (ECPC-IDS) are published. Specifically,\nthe segmentation section includes PET and CT images, with a total of 7159\nimages in multiple formats. In order to prove the effectiveness of segmentation\nmethods on ECPC-IDS, five classical deep learning semantic segmentation methods\nare selected to test the image segmentation task. The object detection section\nalso includes PET and CT images, with a total of 3579 images and XML files with\nannotation information. Six deep learning methods are selected for experiments\non the detection task.This study conduct extensive experiments using deep\nlearning-based semantic segmentation and object detection methods to\ndemonstrate the differences between various methods on ECPC-IDS. As far as we\nknow, this is the first publicly available dataset of endometrial cancer with a\nlarge number of multiple images, including a large amount of information\nrequired for image and target detection. ECPC-IDS can aid researchers in\nexploring new algorithms to enhance computer-assisted technology, benefiting\nboth clinical doctors and patients greatly.\n","authors":["Dechao Tang","Tianming Du","Deguo Ma","Zhiyu Ma","Hongzan Sun","Marcin Grzegorzek","Huiyan Jiang","Chen Li"],"pdf_url":"https://arxiv.org/pdf/2308.08313v3.pdf","comment":"14 pages,6 figures"},{"id":"http://arxiv.org/abs/2310.07492v1","updated":"2023-10-11T13:39:11Z","published":"2023-10-11T13:39:11Z","title":"Boosting Black-box Attack to Deep Neural Networks with Conditional\n  Diffusion Models","summary":"  Existing black-box attacks have demonstrated promising potential in creating\nadversarial examples (AE) to deceive deep learning models. Most of these\nattacks need to handle a vast optimization space and require a large number of\nqueries, hence exhibiting limited practical impacts in real-world scenarios. In\nthis paper, we propose a novel black-box attack strategy, Conditional Diffusion\nModel Attack (CDMA), to improve the query efficiency of generating AEs under\nquery-limited situations. The key insight of CDMA is to formulate the task of\nAE synthesis as a distribution transformation problem, i.e., benign examples\nand their corresponding AEs can be regarded as coming from two distinctive\ndistributions and can transform from each other with a particular converter.\nUnlike the conventional \\textit{query-and-optimization} approach, we generate\neligible AEs with direct conditional transform using the aforementioned data\nconverter, which can significantly reduce the number of queries needed. CDMA\nadopts the conditional Denoising Diffusion Probabilistic Model as the\nconverter, which can learn the transformation from clean samples to AEs, and\nensure the smooth development of perturbed noise resistant to various defense\nstrategies. We demonstrate the effectiveness and efficiency of CDMA by\ncomparing it with nine state-of-the-art black-box attacks across three\nbenchmark datasets. On average, CDMA can reduce the query count to a handful of\ntimes; in most cases, the query count is only ONE. We also show that CDMA can\nobtain $>99\\%$ attack success rate for untarget attacks over all datasets and\ntargeted attack over CIFAR-10 with the noise budget of $\\epsilon=16$.\n","authors":["Renyang Liu","Wei Zhou","Tianwei Zhang","Kangjie Chen","Jun Zhao","Kwok-Yan Lam"],"pdf_url":"https://arxiv.org/pdf/2310.07492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11161v2","updated":"2023-10-11T13:29:23Z","published":"2023-04-02T16:03:44Z","title":"altiro3D: Scene representation from single image and novel view\n  synthesis","summary":"  We introduce altiro3D, a free extended library developed to represent reality\nstarting from a given original RGB image or flat video. It allows to generate a\nlight-field (or Native) image or video and get a realistic 3D experience. To\nsynthesize N-number of virtual images and add them sequentially into a Quilt\ncollage, we apply MiDaS models for the monocular depth estimation, simple\nOpenCV and Telea inpainting techniques to map all pixels, and implement a\n'Fast' algorithm to handle 3D projection camera and scene transformations along\nN-viewpoints. We use the degree of depth to move proportionally the pixels,\nassuming the original image to be at the center of all the viewpoints. altiro3D\ncan also be used with DIBR algorithm to compute intermediate snapshots from a\nequivalent 'Real (slower)' camera with N-geometric viewpoints, which requires\nto calibrate a priori several intrinsic and extrinsic camera parameters. We\nadopt a pixel- and device-based Lookup Table to optimize computing time. The\nmultiple viewpoints and video generated from a single image or frame can be\ndisplayed in a free-view LCD display.\n","authors":["E. Canessa","L. Tenze"],"pdf_url":"https://arxiv.org/pdf/2304.11161v2.pdf","comment":"In press (2023) Springer International Journal of Information\n  Technology (IJIT) 10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.07473v1","updated":"2023-10-11T13:19:29Z","published":"2023-10-11T13:19:29Z","title":"FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation","summary":"  Learning to navigate to an image-specified goal is an important but\nchallenging task for autonomous systems. The agent is required to reason the\ngoal location from where a picture is shot. Existing methods try to solve this\nproblem by learning a navigation policy, which captures semantic features of\nthe goal image and observation image independently and lastly fuses them for\npredicting a sequence of navigation actions. However, these methods suffer from\ntwo major limitations. 1) They may miss detailed information in the goal image,\nand thus fail to reason the goal location. 2) More critically, it is hard to\nfocus on the goal-relevant regions in the observation image, because they\nattempt to understand observation without goal conditioning. In this paper, we\naim to overcome these limitations by designing a Fine-grained Goal Prompting\n(FGPrompt) method for image-goal navigation. In particular, we leverage\nfine-grained and high-resolution feature maps in the goal image as prompts to\nperform conditioned embedding, which preserves detailed information in the goal\nimage and guides the observation encoder to pay attention to goal-relevant\nregions. Compared with existing methods on the image-goal navigation benchmark,\nour method brings significant performance improvement on 3 benchmark datasets\n(i.e., Gibson, MP3D, and HM3D). Especially on Gibson, we surpass the\nstate-of-the-art success rate by 8% with only 1/50 model size. Project page:\nhttps://xinyusun.github.io/fgprompt-pages\n","authors":["Xinyu Sun","Peihao Chen","Jugang Fan","Thomas H. Li","Jian Chen","Mingkui Tan"],"pdf_url":"https://arxiv.org/pdf/2310.07473v1.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2310.07449v1","updated":"2023-10-11T12:51:16Z","published":"2023-10-11T12:51:16Z","title":"PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction","summary":"  Neural surface reconstruction is sensitive to the camera pose noise, even if\nstate-of-the-art pose estimators like COLMAP or ARKit are used. More\nimportantly, existing Pose-NeRF joint optimisation methods have struggled to\nimprove pose accuracy in challenging real-world scenarios. To overcome the\nchallenges, we introduce the pose residual field (\\textbf{PoRF}), a novel\nimplicit representation that uses an MLP for regressing pose updates. This is\nmore robust than the conventional pose parameter optimisation due to parameter\nsharing that leverages global information over the entire sequence.\nFurthermore, we propose an epipolar geometry loss to enhance the supervision\nthat leverages the correspondences exported from COLMAP results without the\nextra computational overhead. Our method yields promising results. On the DTU\ndataset, we reduce the rotation error by 78\\% for COLMAP poses, leading to the\ndecreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the\nMobileBrick dataset that contains casually captured unbounded 360-degree\nvideos, our method refines ARKit poses and improves the reconstruction F1 score\nfrom 69.18 to 75.67, outperforming that with the dataset provided ground-truth\npose (75.14). These achievements demonstrate the efficacy of our approach in\nrefining camera poses and improving the accuracy of neural surface\nreconstruction in real-world scenarios.\n","authors":["Jia-Wang Bian","Wenjing Bian","Victor Adrian Prisacariu","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2310.07449v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.07440v1","updated":"2023-10-11T12:46:11Z","published":"2023-10-11T12:46:11Z","title":"Distance-based Weighted Transformer Network for Image Completion","summary":"  The challenge of image generation has been effectively modeled as a problem\nof structure priors or transformation. However, existing models have\nunsatisfactory performance in understanding the global input image structures\nbecause of particular inherent features (for example, local inductive prior).\nRecent studies have shown that self-attention is an efficient modeling\ntechnique for image completion problems. In this paper, we propose a new\narchitecture that relies on Distance-based Weighted Transformer (DWT) to better\nunderstand the relationships between an image's components. In our model, we\nleverage the strengths of both Convolutional Neural Networks (CNNs) and DWT\nblocks to enhance the image completion process. Specifically, CNNs are used to\naugment the local texture information of coarse priors and DWT blocks are used\nto recover certain coarse textures and coherent visual structures. Unlike\ncurrent approaches that generally use CNNs to create feature maps, we use the\nDWT to encode global dependencies and compute distance-based weighted feature\nmaps, which substantially minimizes the problem of visual ambiguities.\nMeanwhile, to better produce repeated textures, we introduce Residual Fast\nFourier Convolution (Res-FFC) blocks to combine the encoder's skip features\nwith the coarse features provided by our generator. Furthermore, a simple yet\neffective technique is proposed to normalize the non-zero values of\nconvolutions, and fine-tune the network layers for regularization of the\ngradient norms to provide an efficient training stabiliser. Extensive\nquantitative and qualitative experiments on three challenging datasets\ndemonstrate the superiority of our proposed model compared to existing\napproaches.\n","authors":["Pourya Shamsolmoali","Masoumeh Zareapoor","Huiyu Zhou","Xuelong Li","Yue Lu"],"pdf_url":"https://arxiv.org/pdf/2310.07440v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07438v1","updated":"2023-10-11T12:41:32Z","published":"2023-10-11T12:41:32Z","title":"DESTINE: Dynamic Goal Queries with Temporal Transductive Alignment for\n  Trajectory Prediction","summary":"  Predicting temporally consistent road users' trajectories in a multi-agent\nsetting is a challenging task due to unknown characteristics of agents and\ntheir varying intentions. Besides using semantic map information and modeling\ninteractions, it is important to build an effective mechanism capable of\nreasoning about behaviors at different levels of granularity. To this end, we\npropose Dynamic goal quErieS with temporal Transductive alIgNmEnt (DESTINE)\nmethod. Unlike past arts, our approach 1) dynamically predicts agents' goals\nirrespective of particular road structures, such as lanes, allowing the method\nto produce a more accurate estimation of destinations; 2) achieves map\ncompliant predictions by generating future trajectories in a coarse-to-fine\nfashion, where the coarser predictions at a lower frame rate serve as\nintermediate goals; and 3) uses an attention module designed to temporally\nalign predicted trajectories via masked attention. Using the common Argoverse\nbenchmark dataset, we show that our method achieves state-of-the-art\nperformance on various metrics, and further investigate the contributions of\nproposed modules via comprehensive ablation studies.\n","authors":["Rezaul Karim","Soheil Mohamad Alizadeh Shabestary","Amir Rasouli"],"pdf_url":"https://arxiv.org/pdf/2310.07438v1.pdf","comment":"6 tables 4 figures"},{"id":"http://arxiv.org/abs/2205.09615v4","updated":"2023-10-11T12:09:35Z","published":"2022-05-19T15:13:00Z","title":"EXACT: How to Train Your Accuracy","summary":"  Classification tasks are usually evaluated in terms of accuracy. However,\naccuracy is discontinuous and cannot be directly optimized using gradient\nascent. Popular methods minimize cross-entropy, hinge loss, or other surrogate\nlosses, which can lead to suboptimal results. In this paper, we propose a new\noptimization framework by introducing stochasticity to a model's output and\noptimizing expected accuracy, i.e. accuracy of the stochastic model. Extensive\nexperiments on linear models and deep image classification show that the\nproposed optimization method is a powerful alternative to widely used\nclassification losses.\n","authors":["Ivan Karpukhin","Stanislav Dereka","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2205.09615v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07419v1","updated":"2023-10-11T12:05:44Z","published":"2023-10-11T12:05:44Z","title":"Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing\n  Else","summary":"  Recent advances in text-to-image diffusion models have enabled the\nphotorealistic generation of images from text prompts. Despite the great\nprogress, existing models still struggle to generate compositional\nmulti-concept images naturally, limiting their ability to visualize human\nimagination. While several recent works have attempted to address this issue,\nthey either introduce additional training or adopt guidance at inference time.\nIn this work, we consider a more ambitious goal: natural multi-concept\ngeneration using a pre-trained diffusion model, and with almost no extra cost.\nTo achieve this goal, we identify the limitations in the text embeddings used\nfor the pre-trained text-to-image diffusion models. Specifically, we observe\nconcept dominance and non-localized contribution that severely degrade\nmulti-concept generation performance. We further design a minimal low-cost\nsolution that overcomes the above issues by tweaking (not re-training) the text\nembeddings for more realistic multi-concept text-to-image generation. Our\nCorrection by Similarities method tweaks the embedding of concepts by\ncollecting semantic features from most similar tokens to localize the\ncontribution. To avoid mixing features of concepts, we also apply Cross-Token\nNon-Maximum Suppression, which excludes the overlap of contributions from\ndifferent concepts. Experiments show that our approach outperforms previous\nmethods in text-to-image, image manipulation, and personalization tasks,\ndespite not introducing additional training or inference costs to the diffusion\nsteps.\n","authors":["Hazarapet Tunanyan","Dejia Xu","Shant Navasardyan","Zhangyang Wang","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2310.07419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07416v1","updated":"2023-10-11T12:01:52Z","published":"2023-10-11T12:01:52Z","title":"A Novel Voronoi-based Convolutional Neural Network Framework for Pushing\n  Person Detection in Crowd Videos","summary":"  Analyzing the microscopic dynamics of pushing behavior within crowds can\noffer valuable insights into crowd patterns and interactions. By identifying\ninstances of pushing in crowd videos, a deeper understanding of when, where,\nand why such behavior occurs can be achieved. This knowledge is crucial to\ncreating more effective crowd management strategies, optimizing crowd flow, and\nenhancing overall crowd experiences. However, manually identifying pushing\nbehavior at the microscopic level is challenging, and the existing automatic\napproaches cannot detect such microscopic behavior. Thus, this article\nintroduces a novel automatic framework for identifying pushing in videos of\ncrowds on a microscopic level. The framework comprises two main components: i)\nFeature extraction and ii) Video labeling. In the feature extraction component,\na new Voronoi-based method is developed for determining the local regions\nassociated with each person in the input video. Subsequently, these regions are\nfed into EfficientNetV1B0 Convolutional Neural Network to extract the deep\nfeatures of each person over time. In the second component, a combination of a\nfully connected layer with a Sigmoid activation function is employed to analyze\nthese deep features and annotate the individuals involved in pushing within the\nvideo. The framework is trained and evaluated on a new dataset created using\nsix real-world experiments, including their corresponding ground truths. The\nexperimental findings indicate that the suggested framework outperforms seven\nbaseline methods that are employed for comparative analysis purposes.\n","authors":["Ahmed Alia","Mohammed Maree","Mohcine Chraibi","Armin Seyfried"],"pdf_url":"https://arxiv.org/pdf/2310.07416v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2309.14065v4","updated":"2023-10-11T11:43:41Z","published":"2023-09-25T11:57:16Z","title":"AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile\n  Platform Real-Time RGB-D Semantic Segmentation","summary":"  In the realm of robotic intelligence, achieving efficient and precise RGB-D\nsemantic segmentation is a key cornerstone. State-of-the-art multimodal\nsemantic segmentation methods, primarily rooted in symmetrical skeleton\nnetworks, find it challenging to harmonize computational efficiency and\nprecision. In this work, we propose AsymFormer, a novel network for real-time\nRGB-D semantic segmentation, which targets the minimization of superfluous\nparameters by optimizing the distribution of computational resources and\nintroduces an asymmetrical backbone to allow for the effective fusion of\nmultimodal features. Furthermore, we explore techniques to bolster network\naccuracy by redefining feature selection and extracting multi-modal\nself-similarity features without a substantial increase in the parameter count,\nthereby ensuring real-time execution on robotic platforms. Additionally, a\nLocal Attention-Guided Feature Selection (LAFS) module is used to selectively\nfuse features from different modalities by leveraging their dependencies.\nSubsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding\n(CMA) module is introduced to further extract cross-modal representations. This\nmethod is evaluated on NYUv2 and SUNRGBD datasets, with AsymFormer\ndemonstrating competitive results with 52.0% mIoU on NYUv2 and 49.1% mIoU on\nSUNRGBD. Notably, AsymFormer achieves an inference speed of 65 FPS and after\nimplementing mixed precision quantization, it attains an impressive inference\nspeed of 79 FPS on RTX3090. This significantly outperforms existing multi-modal\nmethods, thereby demonstrating that AsymFormer can strike a balance between\nhigh accuracy and efficiency for RGB-D semantic segmentation.\n","authors":["Siqi Du","Weixi Wang","Renzhong Guo","Shengjun Tang"],"pdf_url":"https://arxiv.org/pdf/2309.14065v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04707v2","updated":"2023-10-11T11:30:32Z","published":"2023-03-08T16:48:24Z","title":"DiM: Distilling Dataset into Generative Model","summary":"  Dataset distillation reduces the network training cost by synthesizing small\nand informative datasets from large-scale ones. Despite the success of the\nrecent dataset distillation algorithms, three drawbacks still limit their wider\napplication: i). the synthetic images perform poorly on large architectures;\nii). they need to be re-optimized when the distillation ratio changes; iii).\nthe limited diversity restricts the performance when the distillation ratio is\nlarge. In this paper, we propose a novel distillation scheme to\n\\textbf{D}istill information of large train sets \\textbf{i}nto generative\n\\textbf{M}odels, named DiM. Specifically, DiM learns to use a generative model\nto store the information of the target dataset. During the distillation phase,\nwe minimize the differences in logits predicted by a models pool between real\nand generated images. At the deployment stage, the generative model synthesizes\nvarious training samples from random noises on the fly. Due to the simple yet\neffective designs, the trained DiM can be directly applied to different\ndistillation ratios and large architectures without extra cost. We validate the\nproposed DiM across 4 datasets and achieve state-of-the-art results on all of\nthem. To the best of our knowledge, we are the first to achieve higher accuracy\non complex architectures than simple ones, such as 75.1\\% with ResNet-18 and\n72.6\\% with ConvNet-3 on ten images per class of CIFAR-10. Besides, DiM\noutperforms previous methods with 10\\% $\\sim$ 22\\% when images per class are 1\nand 10 on the SVHN dataset.\n","authors":["Kai Wang","Jianyang Gu","Daquan Zhou","Zheng Zhu","Wei Jiang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2303.04707v2.pdf","comment":"Distilling datasets into generative models"},{"id":"http://arxiv.org/abs/2310.05682v2","updated":"2023-10-11T11:28:40Z","published":"2023-10-09T12:51:46Z","title":"Analysis of Rainfall Variability and Water Extent of Selected Hydropower\n  Reservoir Using Google Earth Engine (GEE): A Case Study from Two Tropical\n  Countries, Sri Lanka and Vietnam","summary":"  This study presents a comprehensive remote sensing analysis of rainfall\npatterns and selected hydropower reservoir water extent in two tropical monsoon\ncountries, Vietnam and Sri Lanka. The aim is to understand the relationship\nbetween remotely sensed rainfall data and the dynamic changes (monthly) in\nreservoir water extent. The analysis utilizes high-resolution optical imagery\nand Sentinel-1 Synthetic Aperture Radar (SAR) data to observe and monitor water\nbodies during different weather conditions, especially during the monsoon\nseason. The average annual rainfall for both countries is determined, and\nspatiotemporal variations in monthly average rainfall are examined at regional\nand reservoir basin levels using the Climate Hazards Group InfraRed\nPrecipitation with Station (CHIRPS) dataset from 1981 to 2022. Water extents\nare derived for selected reservoirs using Sentinel-1 SAR Ground Range Detected\n(GRD) images in Vietnam and Sri Lanka from 2017 to 2022. The images are\npre-processed and corrected using terrain correction and refined Lee filter. An\nautomated thresholding algorithm, OTSU, distinguishes water and land, taking\nadvantage of both VV and VH polarization data. The connected pixel count\nthreshold is applied to enhance result accuracy. The results indicate a clear\nrelationship between rainfall patterns and reservoir water extent, with\nincreased precipitation during the monsoon season leading to higher water\nextents in the later months. This study contributes to understanding how\nrainfall variability impacts reservoir water resources in tropical monsoon\nregions. The preliminary findings can inform water resource management\nstrategies and support these countries' decision-making processes related to\nhydropower generation, flood management, and irrigation.\n","authors":["Punsisi Rajakaruna","Surajit Ghosh","Bunyod Holmatov"],"pdf_url":"https://arxiv.org/pdf/2310.05682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07394v1","updated":"2023-10-11T11:26:35Z","published":"2023-10-11T11:26:35Z","title":"CLIP for Lightweight Semantic Segmentation","summary":"  The large-scale pretrained model CLIP, trained on 400 million image-text\npairs, offers a promising paradigm for tackling vision tasks, albeit at the\nimage level. Later works, such as DenseCLIP and LSeg, extend this paradigm to\ndense prediction, including semantic segmentation, and have achieved excellent\nresults. However, the above methods either rely on CLIP-pretrained visual\nbackbones or use none-pretrained but heavy backbones such as Swin, while\nfalling ineffective when applied to lightweight backbones. The reason for this\nis that the lightweitht networks, feature extraction ability of which are\nrelatively limited, meet difficulty embedding the image feature aligned with\ntext embeddings perfectly. In this work, we present a new feature fusion module\nwhich tackles this problem and enables language-guided paradigm to be applied\nto lightweight networks. Specifically, the module is a parallel design of CNN\nand transformer with a two-way bridge in between, where CNN extracts spatial\ninformation and visual context of the feature map from the image encoder, and\nthe transformer propagates text embeddings from the text encoder forward. The\ncore of the module is the bidirectional fusion of visual and text feature\nacross the bridge which prompts their proximity and alignment in embedding\nspace. The module is model-agnostic, which can not only make language-guided\nlightweight semantic segmentation practical, but also fully exploit the\npretrained knowledge of language priors and achieve better performance than\nprevious SOTA work, such as DenseCLIP, whatever the vision backbone is.\nExtensive experiments have been conducted to demonstrate the superiority of our\nmethod.\n","authors":["Ke Jin","Wankou Yang"],"pdf_url":"https://arxiv.org/pdf/2310.07394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07379v1","updated":"2023-10-11T10:54:44Z","published":"2023-10-11T10:54:44Z","title":"Causal Unsupervised Semantic Segmentation","summary":"  Unsupervised semantic segmentation aims to achieve high-quality semantic\ngrouping without human-labeled annotations. With the advent of self-supervised\npre-training, various frameworks utilize the pre-trained features to train\nprediction heads for unsupervised dense prediction. However, a significant\nchallenge in this unsupervised setup is determining the appropriate level of\nclustering required for segmenting concepts. To address it, we propose a novel\nframework, CAusal Unsupervised Semantic sEgmentation (CAUSE), which leverages\ninsights from causal inference. Specifically, we bridge intervention-oriented\napproach (i.e., frontdoor adjustment) to define suitable two-step tasks for\nunsupervised prediction. The first step involves constructing a concept\nclusterbook as a mediator, which represents possible concept prototypes at\ndifferent levels of granularity in a discretized form. Then, the mediator\nestablishes an explicit link to the subsequent concept-wise self-supervised\nlearning for pixel-level grouping. Through extensive experiments and analyses\non various datasets, we corroborate the effectiveness of CAUSE and achieve\nstate-of-the-art performance in unsupervised semantic segmentation.\n","authors":["Junho Kim","Byung-Kwan Lee","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2310.07379v1.pdf","comment":"code available:\n  https://github.com/ByungKwanLee/Causal-Unsupervised-Segmentation"},{"id":"http://arxiv.org/abs/2310.07376v1","updated":"2023-10-11T10:50:15Z","published":"2023-10-11T10:50:15Z","title":"Point Cloud Denoising and Outlier Detection with Local Geometric\n  Structure by Dynamic Graph CNN","summary":"  The digitalization of society is rapidly developing toward the realization of\nthe digital twin and metaverse. In particular, point clouds are attracting\nattention as a media format for 3D space. Point cloud data is contaminated with\nnoise and outliers due to measurement errors. Therefore, denoising and outlier\ndetection are necessary for point cloud processing. Among them, PointCleanNet\nis an effective method for point cloud denoising and outlier detection.\nHowever, it does not consider the local geometric structure of the patch. We\nsolve this problem by applying two types of graph convolutional layer designed\nbased on the Dynamic Graph CNN. Experimental results show that the proposed\nmethods outperform the conventional method in AUPR, which indicates outlier\ndetection accuracy, and Chamfer Distance, which indicates denoising accuracy.\n","authors":["Kosuke Nakayama","Hiroto Fukuta","Hiroshi Watanabe"],"pdf_url":"https://arxiv.org/pdf/2310.07376v1.pdf","comment":"2023 IEEE 12th Global Conference on Consumer Electronics (GCCE 2023)"},{"id":"http://arxiv.org/abs/2303.16570v2","updated":"2023-10-11T10:41:11Z","published":"2023-03-29T10:08:29Z","title":"Point2Vec for Self-Supervised Representation Learning on Point Clouds","summary":"  Recently, the self-supervised learning framework data2vec has shown inspiring\nperformance for various modalities using a masked student-teacher approach.\nHowever, it remains open whether such a framework generalizes to the unique\nchallenges of 3D point clouds. To answer this question, we extend data2vec to\nthe point cloud domain and report encouraging results on several downstream\ntasks. In an in-depth analysis, we discover that the leakage of positional\ninformation reveals the overall object shape to the student even under heavy\nmasking and thus hampers data2vec to learn strong representations for point\nclouds. We address this 3D-specific shortcoming by proposing point2vec, which\nunleashes the full potential of data2vec-like pre-training on point clouds. Our\nexperiments show that point2vec outperforms other self-supervised methods on\nshape classification and few-shot learning on ModelNet40 and ScanObjectNN,\nwhile achieving competitive results on part segmentation on ShapeNetParts.\nThese results suggest that the learned representations are strong and\ntransferable, highlighting point2vec as a promising direction for\nself-supervised learning of point cloud representations.\n","authors":["Karim Abou Zeid","Jonas Schult","Alexander Hermans","Bastian Leibe"],"pdf_url":"https://arxiv.org/pdf/2303.16570v2.pdf","comment":"Accepted at GCPR 2023. Project page at\n  https://vision.rwth-aachen.de/point2vec"},{"id":"http://arxiv.org/abs/2307.00773v3","updated":"2023-10-11T10:29:59Z","published":"2023-07-03T06:33:49Z","title":"DifFSS: Diffusion Model for Few-Shot Semantic Segmentation","summary":"  Diffusion models have demonstrated excellent performance in image generation.\nAlthough various few-shot semantic segmentation (FSS) models with different\nnetwork structures have been proposed, performance improvement has reached a\nbottleneck. This paper presents the first work to leverage the diffusion model\nfor FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve\nthe performance of the state-of-the-art FSS models by a large margin without\nmodifying their network structure. Specifically, we utilize the powerful\ngeneration ability of diffusion models to generate diverse auxiliary support\nimages by using the semantic mask, scribble or soft HED boundary of the support\nimage as control conditions. This generation process simulates the variety\nwithin the class of the query image, such as color, texture variation,\nlighting, $etc$. As a result, FSS models can refer to more diverse support\nimages, yielding more robust representations, thereby achieving a consistent\nimprovement in segmentation performance. Extensive experiments on three\npublicly available datasets based on existing advanced FSS models demonstrate\nthe effectiveness of the diffusion model for FSS task. Furthermore, we explore\nin detail the impact of different input settings of the diffusion model on\nsegmentation performance. Hopefully, this completely new paradigm will bring\ninspiration to the study of FSS task integrated with AI-generated content. Code\nis available at https://github.com/TrinitialChan/DifFSS\n","authors":["Weimin Tan","Siyuan Chen","Bo Yan"],"pdf_url":"https://arxiv.org/pdf/2307.00773v3.pdf","comment":"code is available at https://github.com/TrinitialChan/DifFSS"},{"id":"http://arxiv.org/abs/2305.03989v2","updated":"2023-10-11T10:26:27Z","published":"2023-05-06T09:29:12Z","title":"LEO: Generative Latent Image Animator for Human Video Synthesis","summary":"  Spatio-temporal coherency is a major challenge in synthesizing high quality\nvideos, particularly in synthesizing human videos that contain rich global and\nlocal deformations. To resolve this challenge, previous approaches have\nresorted to different features in the generation process aimed at representing\nappearance and motion. However, in the absence of strict mechanisms to\nguarantee such disentanglement, a separation of motion from appearance has\nremained challenging, resulting in spatial distortions and temporal jittering\nthat break the spatio-temporal coherency. Motivated by this, we here propose\nLEO, a novel framework for human video synthesis, placing emphasis on\nspatio-temporal coherency. Our key idea is to represent motion as a sequence of\nflow maps in the generation process, which inherently isolate motion from\nappearance. We implement this idea via a flow-based image animator and a Latent\nMotion Diffusion Model (LMDM). The former bridges a space of motion codes with\nthe space of flow maps, and synthesizes video frames in a warp-and-inpaint\nmanner. LMDM learns to capture motion prior in the training data by\nsynthesizing sequences of motion codes. Extensive quantitative and qualitative\nanalysis suggests that LEO significantly improves coherent synthesis of human\nvideos over previous methods on the datasets TaichiHD, FaceForensics and\nCelebV-HQ. In addition, the effective disentanglement of appearance and motion\nin LEO allows for two additional tasks, namely infinite-length human video\nsynthesis, as well as content-preserving video editing.\n","authors":["Yaohui Wang","Xin Ma","Xinyuan Chen","Antitza Dantcheva","Bo Dai","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2305.03989v2.pdf","comment":"Project webpage: https://wyhsirius.github.io/LEO-project/"},{"id":"http://arxiv.org/abs/2310.07361v1","updated":"2023-10-11T10:21:34Z","published":"2023-10-11T10:21:34Z","title":"Domain Generalization Guided by Gradient Signal to Noise Ratio of\n  Parameters","summary":"  Overfitting to the source domain is a common issue in gradient-based training\nof deep neural networks. To compensate for the over-parameterized models,\nnumerous regularization techniques have been introduced such as those based on\ndropout. While these methods achieve significant improvements on classical\nbenchmarks such as ImageNet, their performance diminishes with the introduction\nof domain shift in the test set i.e. when the unseen data comes from a\nsignificantly different distribution. In this paper, we move away from the\nclassical approach of Bernoulli sampled dropout mask construction and propose\nto base the selection on gradient-signal-to-noise ratio (GSNR) of network's\nparameters. Specifically, at each training step, parameters with high GSNR will\nbe discarded. Furthermore, we alleviate the burden of manually searching for\nthe optimal dropout ratio by leveraging a meta-learning approach. We evaluate\nour method on standard domain generalization benchmarks and achieve competitive\nresults on classification and face anti-spoofing problems.\n","authors":["Mateusz Michalkiewicz","Masoud Faraki","Xiang Yu","Manmohan Chandraker","Mahsa Baktashmotlagh"],"pdf_url":"https://arxiv.org/pdf/2310.07361v1.pdf","comment":"Paper was accepted to ICCV 2023"},{"id":"http://arxiv.org/abs/2310.07359v1","updated":"2023-10-11T10:17:41Z","published":"2023-10-11T10:17:41Z","title":"Diagnosing Bipolar Disorder from 3-D Structural Magnetic Resonance\n  Images Using a Hybrid GAN-CNN Method","summary":"  Bipolar Disorder (BD) is a psychiatric condition diagnosed by repetitive\ncycles of hypomania and depression. Since diagnosing BD relies on subjective\nbehavioral assessments over a long period, a solid diagnosis based on objective\ncriteria is not straightforward. The current study responded to the described\nobstacle by proposing a hybrid GAN-CNN model to diagnose BD from 3-D structural\nMRI Images (sMRI). The novelty of this study stems from diagnosing BD from sMRI\nsamples rather than conventional datasets such as functional MRI (fMRI),\nelectroencephalography (EEG), and behavioral symptoms while removing the data\ninsufficiency usually encountered when dealing with sMRI samples. The impact of\nvarious augmentation ratios is also tested using 5-fold cross-validation. Based\non the results, this study obtains an accuracy rate of 75.8%, a sensitivity of\n60.3%, and a specificity of 82.5%, which are 3-5% higher than prior work while\nutilizing less than 6% sample counts. Next, it is demonstrated that a 2- D\nlayer-based GAN generator can effectively reproduce complex 3D brain samples, a\nmore straightforward technique than manual image processing. Lastly, the\noptimum augmentation threshold for the current study using 172 sMRI samples is\n50%, showing the applicability of the described method for larger sMRI\ndatasets. In conclusion, it is established that data augmentation using GAN\nimproves the accuracy of the CNN classifier using sMRI samples, thus developing\nmore reliable decision support systems to assist practitioners in identifying\nBD patients more reliably and in a shorter period\n","authors":["Masood Hamed Saghayan","Mohammad Hossein Zolfagharnasab","Ali Khadem","Farzam Matinfar","Hassan Rashidi"],"pdf_url":"https://arxiv.org/pdf/2310.07359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07355v1","updated":"2023-10-11T10:12:43Z","published":"2023-10-11T10:12:43Z","title":"IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training","summary":"  In the field of medical Vision-Language Pre-training (VLP), significant\nefforts have been devoted to deriving text and image features from both\nclinical reports and associated medical images. However, most existing methods\nmay have overlooked the opportunity in leveraging the inherent hierarchical\nstructure of clinical reports, which are generally split into `findings' for\ndescriptive content and `impressions' for conclusive observation. Instead of\nutilizing this rich, structured format, current medical VLP approaches often\nsimplify the report into either a unified entity or fragmented tokens. In this\nwork, we propose a novel clinical prior guided VLP framework named IMITATE to\nlearn the structure information from medical reports with hierarchical\nvision-language alignment. The framework derives multi-level visual features\nfrom the chest X-ray (CXR) images and separately aligns these features with the\ndescriptive and the conclusive text encoded in the hierarchical medical report.\nFurthermore, a new clinical-informed contrastive loss is introduced for\ncross-modal learning, which accounts for clinical prior knowledge in\nformulating sample correlations in contrastive learning. The proposed model,\nIMITATE, outperforms baseline VLP methods across six different datasets,\nspanning five medical imaging downstream tasks. Comprehensive experimental\nresults highlight the advantages of integrating the hierarchical structure of\nmedical reports for vision-language alignment.\n","authors":["Che Liu","Sibo Cheng","Miaojing Shi","Anand Shah","Wenjia Bai","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2310.07355v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2307.00040v2","updated":"2023-10-11T10:11:36Z","published":"2023-06-30T17:37:48Z","title":"DisCo: Disentangled Control for Realistic Human Dance Generation","summary":"  Generative AI has made significant strides in computer vision, particularly\nin text-driven image/video synthesis (T2I/T2V). Despite the notable\nadvancements, it remains challenging in human-centric content synthesis such as\nrealistic dance generation. Current methodologies, primarily tailored for human\nmotion transfer, encounter difficulties when confronted with real-world dance\nscenarios (e.g., social media dance) which require to generalize across a wide\nspectrum of poses and intricate human details. In this paper, we depart from\nthe traditional paradigm of human motion transfer and emphasize two additional\ncritical attributes for the synthesis of human dance content in social media\ncontexts: (i) Generalizability: the model should be able to generalize beyond\ngeneric human viewpoints as well as unseen human subjects, backgrounds, and\nposes; (ii) Compositionality: it should allow for composition of seen/unseen\nsubjects, backgrounds, and poses from different sources seamlessly. To address\nthese challenges, we introduce DisCo, which includes a novel model architecture\nwith disentangled control to improve the compositionality of dance synthesis,\nand an effective human attribute pre-training for better generalizability to\nunseen humans. Extensive qualitative and quantitative results demonstrate that\nDisCo can generate high-quality human dance images and videos with diverse\nappearances and flexible motions. Code, demo, video and visualization are\navailable at: https://disco-dance.github.io/.\n","authors":["Tan Wang","Linjie Li","Kevin Lin","Yuanhao Zhai","Chung-Ching Lin","Zhengyuan Yang","Hanwang Zhang","Zicheng Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2307.00040v2.pdf","comment":"Project Page: https://disco-dance.github.io/ ; Add temporal module ;\n  Synchronize FVD computation with MCVD ; More baselines and visualizations"},{"id":"http://arxiv.org/abs/2307.02869v2","updated":"2023-10-11T10:03:08Z","published":"2023-07-06T09:12:13Z","title":"MomentDiff: Generative Video Moment Retrieval from Random to Real","summary":"  Video moment retrieval pursues an efficient and generalized solution to\nidentify the specific temporal segments within an untrimmed video that\ncorrespond to a given language description. To achieve this goal, we provide a\ngenerative diffusion-based framework called MomentDiff, which simulates a\ntypical human retrieval process from random browsing to gradual localization.\nSpecifically, we first diffuse the real span to random noise, and learn to\ndenoise the random noise to the original span with the guidance of similarity\nbetween text and video. This allows the model to learn a mapping from arbitrary\nrandom locations to real moments, enabling the ability to locate segments from\nrandom initialization. Once trained, MomentDiff could sample random temporal\nsegments as initial guesses and iteratively refine them to generate an accurate\ntemporal boundary. Different from discriminative works (e.g., based on\nlearnable proposals or queries), MomentDiff with random initialized spans could\nresist the temporal location biases from datasets. To evaluate the influence of\nthe temporal location biases, we propose two anti-bias datasets with location\ndistribution shifts, named Charades-STA-Len and Charades-STA-Mom. The\nexperimental results demonstrate that our efficient framework consistently\noutperforms state-of-the-art methods on three public benchmarks, and exhibits\nbetter generalization and robustness on the proposed anti-bias datasets. The\ncode, model, and anti-bias evaluation datasets are available at\nhttps://github.com/IMCCretrieval/MomentDiff.\n","authors":["Pandeng Li","Chen-Wei Xie","Hongtao Xie","Liming Zhao","Lei Zhang","Yun Zheng","Deli Zhao","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.02869v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2304.06841v2","updated":"2023-10-11T09:53:51Z","published":"2023-04-13T22:20:54Z","title":"Video alignment using unsupervised learning of local and global features","summary":"  In this paper, we tackle the problem of video alignment, the process of\nmatching the frames of a pair of videos containing similar actions. The main\nchallenge in video alignment is that accurate correspondence should be\nestablished despite the differences in the execution processes and appearances\nbetween the two videos. We introduce an unsupervised method for alignment that\nuses global and local features of the frames. In particular, we introduce\neffective features for each video frame using three machine vision tools:\nperson detection, pose estimation, and VGG network. Then, the features are\nprocessed and combined to construct a multidimensional time series that\nrepresents the video. The resulting time series are used to align videos of the\nsame actions using a novel version of dynamic time warping named Diagonalized\nDynamic Time Warping(DDTW). The main advantage of our approach is that no\ntraining is required, which makes it applicable for any new type of action\nwithout any need to collect training samples for it. For evaluation, we\nconsidered video synchronization and phase classification tasks on the Penn\naction dataset. Also, for an effective evaluation of the video synchronization\ntask, we present a new metric called Enclosed Area Error(EAE). The results show\nthat our method outperforms previous state-of-the-art methods, such as TCC, and\nother self-supervised and weakly supervised methods.\n","authors":["Niloufar Fakhfour","Mohammad ShahverdiKondori","Hoda Mohammadzade"],"pdf_url":"https://arxiv.org/pdf/2304.06841v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2310.02044v2","updated":"2023-10-11T09:21:23Z","published":"2023-10-03T13:35:49Z","title":"Video Transformers under Occlusion: How Physics and Background\n  Attributes Impact Large Models for Robotic Manipulation","summary":"  As transformer architectures and dataset sizes continue to scale, the need to\nunderstand the specific dataset factors affecting model performance becomes\nincreasingly urgent. This paper investigates how object physics attributes\n(color, friction coefficient, shape) and background characteristics (static,\ndynamic, background complexity) influence the performance of Video Transformers\nin trajectory prediction tasks under occlusion. Beyond mere occlusion\nchallenges, this study aims to investigate three questions: How do object\nphysics attributes and background characteristics influence the model\nperformance? What kinds of attributes are most influential to the model\ngeneralization? Is there a data saturation point for large transformer model\nperformance within a single task? To facilitate this research, we present\nOccluManip, a real-world video-based robot pushing dataset comprising 460,000\nconsistent recordings of objects with different physics and varying\nbackgrounds. 1.4 TB and in total 1278 hours of high-quality videos of flexible\ntemporal length along with target object trajectories are collected,\naccommodating tasks with different temporal requirements. Additionally, we\npropose Video Occlusion Transformer (VOT), a generic video-transformer-based\nnetwork achieving an average 96% accuracy across all 18 sub-datasets provided\nin OccluManip. OccluManip and VOT will be released at:\nhttps://github.com/ShutongJIN/OccluManip.git\n","authors":["Shutong Jin","Ruiyu Wang","Muhammad Zahid","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2310.02044v2.pdf","comment":"Under review at IEEE ICRA 2024"},{"id":"http://arxiv.org/abs/2310.07324v1","updated":"2023-10-11T09:14:30Z","published":"2023-10-11T09:14:30Z","title":"Guided Attention for Interpretable Motion Captioning","summary":"  While much effort has been invested in generating human motion from text,\nrelatively few studies have been dedicated to the reverse direction, that is,\ngenerating text from motion. Much of the research focuses on maximizing\ngeneration quality without any regard for the interpretability of the\narchitectures, particularly regarding the influence of particular body parts in\nthe generation and the temporal synchronization of words with specific\nmovements and actions. This study explores the combination of movement encoders\nwith spatio-temporal attention models and proposes strategies to guide the\nattention during training to highlight perceptually pertinent areas of the\nskeleton in time. We show that adding guided attention with adaptive gate leads\nto interpretable captioning while improving performance compared to higher\nparameter-count non-interpretable SOTA systems. On the KIT MLD dataset, we\nobtain a BLEU@4 of 24.4% (SOTA+6%), a ROUGE-L of 58.30% (SOTA +14.1%), a CIDEr\nof 112.10 (SOTA +32.6) and a Bertscore of 41.20% (SOTA +18.20%). On HumanML3D,\nwe obtain a BLEU@4 of 25.00 (SOTA +2.7%), a ROUGE-L score of 55.4% (SOTA\n+6.1%), a CIDEr of 61.6 (SOTA -10.9%), a Bertscore of 40.3% (SOTA +2.5%). Our\ncode implementation and reproduction details will be soon available at\nhttps://github.com/rd20karim/M2T-Interpretable/tree/main.\n","authors":["Karim Radouane","Andon Tchechmedjiev","Sylvie Ranwez","Julien Lagarde"],"pdf_url":"https://arxiv.org/pdf/2310.07324v1.pdf","comment":"arXiv preprint"},{"id":"http://arxiv.org/abs/2310.07322v1","updated":"2023-10-11T09:12:42Z","published":"2023-10-11T09:12:42Z","title":"A webcam-based machine learning approach for three-dimensional range of\n  motion evaluation","summary":"  Background. Joint range of motion (ROM) is an important quantitative measure\nfor physical therapy. Commonly relying on a goniometer, accurate and reliable\nROM measurement requires extensive training and practice. This, in turn,\nimposes a significant barrier for those who have limited in-person access to\nhealthcare.\n  Objective. The current study presents and evaluates an alternative machine\nlearning-based ROM evaluation method that could be remotely accessed via a\nwebcam.\n  Methods. To evaluate its reliability, the ROM measurements for a diverse set\nof joints (neck, spine, and upper and lower extremities) derived using this\nmethod were compared to those obtained from a marker-based optical motion\ncapture system.\n  Results. Data collected from 25 healthy adults demonstrated that the webcam\nsolution exhibited high test-retest reliability, with substantial to almost\nperfect intraclass correlation coefficients for most joints. Compared with the\nmarker-based system, the webcam-based system demonstrated substantial to almost\nperfect inter-rater reliability for some joints, and lower inter-rater\nreliability for other joints (e.g., shoulder flexion and elbow flexion), which\ncould be attributed to the reduced sensitivity to joint locations at the apex\nof the movement.\n  Conclusions. The proposed webcam-based method exhibited high test-retest and\ninter-rater reliability, making it a versatile alternative for existing ROM\nevaluation methods in clinical practice and the tele-implementation of physical\ntherapy and rehabilitation.\n","authors":["Xiaoye Michael Wang","Derek T. Smith","Qin Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.07322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07310v1","updated":"2023-10-11T08:47:29Z","published":"2023-10-11T08:47:29Z","title":"Deep Aramaic: Towards a Synthetic Data Paradigm Enabling Machine\n  Learning in Epigraphy","summary":"  Epigraphy increasingly turns to modern artificial intelligence (AI)\ntechnologies such as machine learning (ML) for extracting insights from ancient\ninscriptions. However, scarce labeled data for training ML algorithms severely\nlimits current techniques, especially for ancient scripts like Old Aramaic. Our\nresearch pioneers an innovative methodology for generating synthetic training\ndata tailored to Old Aramaic letters. Our pipeline synthesizes photo-realistic\nAramaic letter datasets, incorporating textural features, lighting, damage, and\naugmentations to mimic real-world inscription diversity. Despite minimal real\nexamples, we engineer a dataset of 250,000 training and 25,000 validation\nimages covering the 22 letter classes in the Aramaic alphabet. This\ncomprehensive corpus provides a robust volume of data for training a residual\nneural network (ResNet) to classify highly degraded Aramaic letters. The ResNet\nmodel demonstrates high accuracy in classifying real images from the 8th\ncentury BCE Hadad statue inscription. Additional experiments validate\nperformance on varying materials and styles, proving effective generalization.\nOur results validate the model's capabilities in handling diverse real-world\nscenarios, proving the viability of our synthetic data approach and avoiding\nthe dependence on scarce training data that has constrained epigraphic\nanalysis. Our innovative framework elevates interpretation accuracy on damaged\ninscriptions, thus enhancing knowledge extraction from these historical\nresources.\n","authors":["Andrei C. Aioanei","Regine Hunziker-Rodewald","Konstantin Klein","Dominik L. Michels"],"pdf_url":"https://arxiv.org/pdf/2310.07310v1.pdf","comment":"41 pages, 19 images"},{"id":"http://arxiv.org/abs/2206.07255v2","updated":"2023-10-11T08:41:34Z","published":"2022-06-15T02:35:51Z","title":"GRAM-HD: 3D-Consistent Image Generation at High Resolution with\n  Generative Radiance Manifolds","summary":"  Recent works have shown that 3D-aware GANs trained on unstructured single\nimage collections can generate multiview images of novel instances. The key\nunderpinnings to achieve this are a 3D radiance field generator and a volume\nrendering process. However, existing methods either cannot generate\nhigh-resolution images (e.g., up to 256X256) due to the high computation cost\nof neural volume rendering, or rely on 2D CNNs for image-space upsampling which\njeopardizes the 3D consistency across different views. This paper proposes a\nnovel 3D-aware GAN that can generate high resolution images (up to 1024X1024)\nwhile keeping strict 3D consistency as in volume rendering. Our motivation is\nto achieve super-resolution directly in the 3D space to preserve 3D\nconsistency. We avoid the otherwise prohibitively-expensive computation cost by\napplying 2D convolutions on a set of 2D radiance manifolds defined in the\nrecent generative radiance manifold (GRAM) approach, and apply dedicated loss\nfunctions for effective GAN training at high resolution. Experiments on FFHQ\nand AFHQv2 datasets show that our method can produce high-quality 3D-consistent\nresults that significantly outperform existing methods. It makes a significant\nstep towards closing the gap between traditional 2D image generation and\n3D-consistent free-view generation.\n","authors":["Jianfeng Xiang","Jiaolong Yang","Yu Deng","Xin Tong"],"pdf_url":"https://arxiv.org/pdf/2206.07255v2.pdf","comment":"ICCV2023 camera ready version (more results and method comparisons).\n  Project page: https://jeffreyxiang.github.io/GRAM-HD/"},{"id":"http://arxiv.org/abs/2301.11986v2","updated":"2023-10-11T08:25:26Z","published":"2023-01-27T20:54:58Z","title":"Enhancing Face Recognition with Latent Space Data Augmentation and\n  Facial Posture Reconstruction","summary":"  The small amount of training data for many state-of-the-art deep\nlearning-based Face Recognition (FR) systems causes a marked deterioration in\ntheir performance. Although a considerable amount of research has addressed\nthis issue by inventing new data augmentation techniques, using either input\nspace transformations or Generative Adversarial Networks (GAN) for feature\nspace augmentations, these techniques have yet to satisfy expectations. In this\npaper, we propose an approach named the Face Representation Augmentation (FRA)\nfor augmenting face datasets. To the best of our knowledge, FRA is the first\nmethod that shifts its focus towards manipulating the face embeddings generated\nby any face representation learning algorithm to create new embeddings\nrepresenting the same identity and facial emotion but with an altered posture.\nExtensive experiments conducted in this study convince of the efficacy of our\nmethodology and its power to provide noiseless, completely new facial\nrepresentations to improve the training procedure of any FR algorithm.\nTherefore, FRA can help the recent state-of-the-art FR methods by providing\nmore data for training FR systems. The proposed method, using experiments\nconducted on the Karolinska Directed Emotional Faces (KDEF) dataset, improves\nthe identity classification accuracies by 9.52 %, 10.04 %, and 16.60 %, in\ncomparison with the base models of MagFace, ArcFace, and CosFace, respectively.\n","authors":["Soroush Hashemifar","Abdolreza Marefat","Javad Hassannataj Joloudari","Hamid Hassanpour"],"pdf_url":"https://arxiv.org/pdf/2301.11986v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04189v2","updated":"2023-10-11T08:01:11Z","published":"2023-10-06T12:08:15Z","title":"Bridging the Gap between Human Motion and Action Semantics via Kinematic\n  Phrases","summary":"  The goal of motion understanding is to establish a reliable mapping between\nmotion and action semantics, while it is a challenging many-to-many problem. An\nabstract action semantic (i.e., walk forwards) could be conveyed by\nperceptually diverse motions (walk with arms up or swinging), while a motion\ncould carry different semantics w.r.t. its context and intention. This makes an\nelegant mapping between them difficult. Previous attempts adopted\ndirect-mapping paradigms with limited reliability. Also, current automatic\nmetrics fail to provide reliable assessments of the consistency between motions\nand action semantics. We identify the source of these problems as the\nsignificant gap between the two modalities. To alleviate this gap, we propose\nKinematic Phrases (KP) that take the objective kinematic facts of human motion\nwith proper abstraction, interpretability, and generality characteristics.\nBased on KP as a mediator, we can unify a motion knowledge base and build a\nmotion understanding system. Meanwhile, KP can be automatically converted from\nmotions and to text descriptions with no subjective bias, inspiring Kinematic\nPrompt Generation (KPG) as a novel automatic motion generation benchmark. In\nextensive experiments, our approach shows superiority over other methods. Our\ncode and data would be made publicly available at https://foruck.github.io/KP.\n","authors":["Xinpeng Liu","Yong-Lu Li","Ailing Zeng","Zizheng Zhou","Yang You","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2310.04189v2.pdf","comment":"Yong-Lu Li and Cewu Lu are the corresponding authors. Project page is\n  available at https://foruck.github.io/KP/"},{"id":"http://arxiv.org/abs/2310.07265v1","updated":"2023-10-11T07:45:37Z","published":"2023-10-11T07:45:37Z","title":"Distilling Efficient Vision Transformers from CNNs for Semantic\n  Segmentation","summary":"  In this paper, we tackle a new problem: how to transfer knowledge from the\npre-trained cumbersome yet well-performed CNN-based model to learn a compact\nVision Transformer (ViT)-based model while maintaining its learning capacity?\nDue to the completely different characteristics of ViT and CNN and the\nlong-existing capacity gap between teacher and student models in Knowledge\nDistillation (KD), directly transferring the cross-model knowledge is\nnon-trivial. To this end, we subtly leverage the visual and\nlinguistic-compatible feature character of ViT (i.e., student), and its\ncapacity gap with the CNN (i.e., teacher) and propose a novel CNN-to-ViT KD\nframework, dubbed C2VKD. Importantly, as the teacher's features are\nheterogeneous to those of the student, we first propose a novel\nvisual-linguistic feature distillation (VLFD) module that explores efficient KD\namong the aligned visual and linguistic-compatible representations. Moreover,\ndue to the large capacity gap between the teacher and student and the\ninevitable prediction errors of the teacher, we then propose a pixel-wise\ndecoupled distillation (PDD) module to supervise the student under the\ncombination of labels and teacher's predictions from the decoupled target and\nnon-target classes. Experiments on three semantic segmentation benchmark\ndatasets consistently show that the increment of mIoU of our method is over\n200% of the SoTA KD methods\n","authors":["Xu Zheng","Yunhao Luo","Pengyuan Zhou","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.13720v5","updated":"2023-10-11T07:39:48Z","published":"2023-06-23T18:08:00Z","title":"Decoupled Diffusion Models: Image to Zero and Zero to Noise","summary":"  Recent diffusion probabilistic models (DPMs) have shown remarkable abilities\nof generated content, however, they often suffer from complex forward\nprocesses, resulting in inefficient solutions for the reversed process and\nprolonged sampling times. In this paper, we aim to address the aforementioned\nchallenges by focusing on the diffusion process itself that we propose to\ndecouple the intricate diffusion process into two comparatively simpler process\nto improve the generative efficacy and speed. In particular, we present a novel\ndiffusion paradigm named DDM (Decoupled Diffusion Models) based on the Ito\ndiffusion process, in which the image distribution is approximated by an\nexplicit transition probability while the noise path is controlled by the\nstandard Wiener process. We find that decoupling the diffusion process reduces\nthe learning difficulty and the explicit transition probability improves the\ngenerative speed significantly. We prove a new training objective for DPM,\nwhich enables the model to learn to predict the noise and image components\nseparately. Moreover, given the novel forward diffusion equation, we derive the\nreverse denoising formula of DDM that naturally supports fewer steps of\ngeneration without ordinary differential equation (ODE) based accelerators. Our\nexperiments demonstrate that DDM outperforms previous DPMs by a large margin in\nfewer function evaluations setting and gets comparable performances in long\nfunction evaluations setting. We also show that our framework can be applied to\nimage-conditioned generation and high-resolution image synthesis, and that it\ncan generate high-quality images with only 10 function evaluations.\n","authors":["Yuhang Huang","Liang Zheng","Zheng Qin","Xinwang Liu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2306.13720v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07259v1","updated":"2023-10-11T07:37:13Z","published":"2023-10-11T07:37:13Z","title":"Uncovering Hidden Connections: Iterative Tracking and Reasoning for\n  Video-grounded Dialog","summary":"  In contrast to conventional visual question answering, video-grounded dialog\nnecessitates a profound understanding of both dialog history and video content\nfor accurate response generation. Despite commendable strides made by existing\nmethodologies, they often grapple with the challenges of incrementally\nunderstanding intricate dialog histories and assimilating video information. In\nresponse to this gap, we present an iterative tracking and reasoning strategy\nthat amalgamates a textual encoder, a visual encoder, and a generator. At its\ncore, our textual encoder is fortified with a path tracking and aggregation\nmechanism, adept at gleaning nuances from dialog history that are pivotal to\ndeciphering the posed questions. Concurrently, our visual encoder harnesses an\niterative reasoning network, meticulously crafted to distill and emphasize\ncritical visual markers from videos, enhancing the depth of visual\ncomprehension. Culminating this enriched information, we employ the pre-trained\nGPT-2 model as our response generator, stitching together coherent and\ncontextually apt answers. Our empirical assessments, conducted on two renowned\ndatasets, testify to the prowess and adaptability of our proposed design.\n","authors":["Haoyu Zhang","Meng Liu","Yaowei Wang","Da Cao","Weili Guan","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2310.07259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07255v1","updated":"2023-10-11T07:30:37Z","published":"2023-10-11T07:30:37Z","title":"ADASR: An Adversarial Auto-Augmentation Framework for Hyperspectral and\n  Multispectral Data Fusion","summary":"  Deep learning-based hyperspectral image (HSI) super-resolution, which aims to\ngenerate high spatial resolution HSI (HR-HSI) by fusing hyperspectral image\n(HSI) and multispectral image (MSI) with deep neural networks (DNNs), has\nattracted lots of attention. However, neural networks require large amounts of\ntraining data, hindering their application in real-world scenarios. In this\nletter, we propose a novel adversarial automatic data augmentation framework\nADASR that automatically optimizes and augments HSI-MSI sample pairs to enrich\ndata diversity for HSI-MSI fusion. Our framework is sample-aware and optimizes\nan augmentor network and two downsampling networks jointly by adversarial\nlearning so that we can learn more robust downsampling networks for training\nthe upsampling network. Extensive experiments on two public classical\nhyperspectral datasets demonstrate the effectiveness of our ADASR compared to\nthe state-of-the-art methods.\n","authors":["Jinghui Qin","Lihuang Fang","Ruitao Lu","Liang Lin","Yukai Shi"],"pdf_url":"https://arxiv.org/pdf/2310.07255v1.pdf","comment":"This paper has been accepted by IEEE Geoscience and Remote Sensing\n  Letters. Code is released at https://github.com/fangfang11-plog/ADASR"},{"id":"http://arxiv.org/abs/2310.07252v1","updated":"2023-10-11T07:30:01Z","published":"2023-10-11T07:30:01Z","title":"A Comparative Study of Pre-trained CNNs and GRU-Based Attention for\n  Image Caption Generation","summary":"  Image captioning is a challenging task involving generating a textual\ndescription for an image using computer vision and natural language processing\ntechniques. This paper proposes a deep neural framework for image caption\ngeneration using a GRU-based attention mechanism. Our approach employs multiple\npre-trained convolutional neural networks as the encoder to extract features\nfrom the image and a GRU-based language model as the decoder to generate\ndescriptive sentences. To improve performance, we integrate the Bahdanau\nattention model with the GRU decoder to enable learning to focus on specific\nimage parts. We evaluate our approach using the MSCOCO and Flickr30k datasets\nand show that it achieves competitive scores compared to state-of-the-art\nmethods. Our proposed framework can bridge the gap between computer vision and\nnatural language and can be extended to specific domains.\n","authors":["Rashid Khan","Bingding Huang","Haseeb Hassan","Asim Zaman","Zhongfu Ye"],"pdf_url":"https://arxiv.org/pdf/2310.07252v1.pdf","comment":"15pages, 10 figures, 5 tables. 2023 the 5th International Conference\n  on Robotics and Computer Vision (ICRCV 2023). arXiv admin note: substantial\n  text overlap with arXiv:2203.01594"},{"id":"http://arxiv.org/abs/2310.07250v1","updated":"2023-10-11T07:27:28Z","published":"2023-10-11T07:27:28Z","title":"Synthesizing Missing MRI Sequences from Available Modalities using\n  Generative Adversarial Networks in BraTS Dataset","summary":"  Glioblastoma is a highly aggressive and lethal form of brain cancer. Magnetic\nresonance imaging (MRI) plays a significant role in the diagnosis, treatment\nplanning, and follow-up of glioblastoma patients due to its non-invasive and\nradiation-free nature. The International Brain Tumor Segmentation (BraTS)\nchallenge has contributed to generating numerous AI algorithms to accurately\nand efficiently segment glioblastoma sub-compartments using four structural\n(T1, T1Gd, T2, T2-FLAIR) MRI scans. However, these four MRI sequences may not\nalways be available. To address this issue, Generative Adversarial Networks\n(GANs) can be used to synthesize the missing MRI sequences. In this paper, we\nimplement and utilize an open-source GAN approach that takes any three MRI\nsequences as input to generate the missing fourth structural sequence. Our\nproposed approach is contributed to the community-driven generally nuanced deep\nlearning framework (GaNDLF) and demonstrates promising results in synthesizing\nhigh-quality and realistic MRI sequences, enabling clinicians to improve their\ndiagnostic capabilities and support the application of AI methods to brain\ntumor MRI quantification.\n","authors":["Ibrahim Ethem Hamamci"],"pdf_url":"https://arxiv.org/pdf/2310.07250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07248v1","updated":"2023-10-11T07:25:50Z","published":"2023-10-11T07:25:50Z","title":"IBoxCLA: Towards Robust Box-supervised Segmentation of Polyp via\n  Improved Box-dice and Contrastive Latent-anchors","summary":"  Box-supervised polyp segmentation attracts increasing attention for its\ncost-effective potential. Existing solutions often rely on learning-free\nmethods or pretrained models to laboriously generate pseudo masks, triggering\nDice constraint subsequently. In this paper, we found that a model guided by\nthe simplest box-filled masks can accurately predict polyp locations/sizes, but\nsuffers from shape collapsing. In response, we propose two innovative learning\nfashions, Improved Box-dice (IBox) and Contrastive Latent-Anchors (CLA), and\ncombine them to train a robust box-supervised model IBoxCLA. The core idea\nbehind IBoxCLA is to decouple the learning of location/size and shape, allowing\nfor focused constraints on each of them. Specifically, IBox transforms the\nsegmentation map into a proxy map using shape decoupling and confusion-region\nswapping sequentially. Within the proxy map, shapes are disentangled, while\nlocations/sizes are encoded as box-like responses. By constraining the proxy\nmap instead of the raw prediction, the box-filled mask can well supervise\nIBoxCLA without misleading its shape learning. Furthermore, CLA contributes to\nshape learning by generating two types of latent anchors, which are learned and\nupdated using momentum and segmented polyps to steadily represent polyp and\nbackground features. The latent anchors facilitate IBoxCLA to capture\ndiscriminative features within and outside boxes in a contrastive manner,\nyielding clearer boundaries. We benchmark IBoxCLA on five public polyp\ndatasets. The experimental results demonstrate the competitive performance of\nIBoxCLA compared to recent fully-supervised polyp segmentation methods, and its\nsuperiority over other box-supervised state-of-the-arts with a relative\nincrease of overall mDice and mIoU by at least 6.5% and 7.5%, respectively.\n","authors":["Zhiwei Wang","Qiang Hu","Hongkuan Shi","Li He","Man He","Wenxuan Dai","Ting Li","Yitong Zhang","Dun Li","Mei Liu","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07247v1","updated":"2023-10-11T07:24:27Z","published":"2023-10-11T07:24:27Z","title":"Optimizing the Placement of Roadside LiDARs for Autonomous Driving","summary":"  Multi-agent cooperative perception is an increasingly popular topic in the\nfield of autonomous driving, where roadside LiDARs play an essential role.\nHowever, how to optimize the placement of roadside LiDARs is a crucial but\noften overlooked problem. This paper proposes an approach to optimize the\nplacement of roadside LiDARs by selecting optimized positions within the scene\nfor better perception performance. To efficiently obtain the best combination\nof locations, a greedy algorithm based on perceptual gain is proposed, which\nselects the location that can maximize the perceptual gain sequentially. We\ndefine perceptual gain as the increased perceptual capability when a new LiDAR\nis placed. To obtain the perception capability, we propose a perception\npredictor that learns to evaluate LiDAR placement using only a single point\ncloud frame. A dataset named Roadside-Opt is created using the CARLA simulator\nto facilitate research on the roadside LiDAR placement problem.\n","authors":["Wentao Jiang","Hao Xiang","Xinyu Cai","Runsheng Xu","Jiaqi Ma","Yikang Li","Gim Hee Lee","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2310.07247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07245v1","updated":"2023-10-11T07:22:37Z","published":"2023-10-11T07:22:37Z","title":"Crowd Counting in Harsh Weather using Image Denoising with Pix2Pix GANs","summary":"  Visual crowd counting estimates the density of the crowd using deep learning\nmodels such as convolution neural networks (CNNs). The performance of the model\nheavily relies on the quality of the training data that constitutes crowd\nimages. In harsh weather such as fog, dust, and low light conditions, the\ninference performance may severely degrade on the noisy and blur images. In\nthis paper, we propose the use of Pix2Pix generative adversarial network (GAN)\nto first denoise the crowd images prior to passing them to the counting model.\nA Pix2Pix network is trained using synthetic noisy images generated from\noriginal crowd images and then the pretrained generator is then used in the\ninference engine to estimate the crowd density in unseen, noisy crowd images.\nThe performance is tested on JHU-Crowd dataset to validate the significance of\nthe proposed method particularly when high reliability and accuracy are\nrequired.\n","authors":["Muhammad Asif Khan","Hamid Menouar","Ridha Hamila"],"pdf_url":"https://arxiv.org/pdf/2310.07245v1.pdf","comment":"The paper has been accepted for presentation in IEEE 38th\n  International Conference on Image and Vision Computing New Zealand (IVCNZ\n  2023). The final manuscript can be accessed at ieeexplore"},{"id":"http://arxiv.org/abs/2310.04991v3","updated":"2023-10-11T07:20:32Z","published":"2023-10-08T03:35:27Z","title":"Video-Teller: Enhancing Cross-Modal Generation with Fusion and\n  Decoupling","summary":"  This paper proposes Video-Teller, a video-language foundation model that\nleverages multi-modal fusion and fine-grained modality alignment to\nsignificantly enhance the video-to-text generation task. Video-Teller boosts\nthe training efficiency by utilizing frozen pretrained vision and language\nmodules. It capitalizes on the robust linguistic capabilities of large language\nmodels, enabling the generation of both concise and elaborate video\ndescriptions. To effectively integrate visual and auditory information,\nVideo-Teller builds upon the image-based BLIP-2 model and introduces a cascaded\nQ-Former which fuses information across frames and ASR texts. To better guide\nvideo summarization, we introduce a fine-grained modality alignment objective,\nwhere the cascaded Q-Former's output embedding is trained to align with the\ncaption/summary embedding created by a pretrained text auto-encoder.\nExperimental results demonstrate the efficacy of our proposed video-language\nfoundation model in accurately comprehending videos and generating coherent and\nprecise language descriptions. It is worth noting that the fine-grained\nalignment enhances the model's capabilities (4% improvement of CIDEr score on\nMSR-VTT) with only 13% extra parameters in training and zero additional cost in\ninference.\n","authors":["Haogeng Liu","Qihang Fan","Tingkai Liu","Linjie Yang","Yunzhe Tao","Huaibo Huang","Ran He","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04991v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05447v2","updated":"2023-10-11T07:10:49Z","published":"2023-10-09T06:43:48Z","title":"Towards Fair and Comprehensive Comparisons for Image-Based 3D Object\n  Detection","summary":"  In this work, we build a modular-designed codebase, formulate strong training\nrecipes, design an error diagnosis toolbox, and discuss current methods for\nimage-based 3D object detection. In particular, different from other highly\nmature tasks, e.g., 2D object detection, the community of image-based 3D object\ndetection is still evolving, where methods often adopt different training\nrecipes and tricks resulting in unfair evaluations and comparisons. What is\nworse, these tricks may overwhelm their proposed designs in performance, even\nleading to wrong conclusions. To address this issue, we build a module-designed\ncodebase and formulate unified training standards for the community.\nFurthermore, we also design an error diagnosis toolbox to measure the detailed\ncharacterization of detection models. Using these tools, we analyze current\nmethods in-depth under varying settings and provide discussions for some open\nquestions, e.g., discrepancies in conclusions on KITTI-3D and nuScenes\ndatasets, which have led to different dominant methods for these datasets. We\nhope that this work will facilitate future research in image-based 3D object\ndetection. Our codes will be released at\n\\url{https://github.com/OpenGVLab/3dodi}\n","authors":["Xinzhu Ma","Yongtao Wang","Yinmin Zhang","Zhiyi Xia","Yuan Meng","Zhihui Wang","Haojie Li","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.05447v2.pdf","comment":"ICCV23, code will be released soon"},{"id":"http://arxiv.org/abs/2301.07807v3","updated":"2023-10-11T07:07:18Z","published":"2023-01-18T22:38:03Z","title":"Measuring uncertainty in human visual segmentation","summary":"  Segmenting visual stimuli into distinct groups of features and visual objects\nis central to visual function. Classical psychophysical methods have helped\nuncover many rules of human perceptual segmentation, and recent progress in\nmachine learning has produced successful algorithms. Yet, the computational\nlogic of human segmentation remains unclear, partially because we lack\nwell-controlled paradigms to measure perceptual segmentation maps and compare\nmodels quantitatively. Here we propose a new, integrated approach: given an\nimage, we measure multiple pixel-based same--different judgments and perform\nmodel--based reconstruction of the underlying segmentation map. The\nreconstruction is robust to several experimental manipulations and captures the\nvariability of individual participants. We demonstrate the validity of the\napproach on human segmentation of natural images and composite textures. We\nshow that image uncertainty affects measured human variability, and it\ninfluences how participants weigh different visual features. Because any\nputative segmentation algorithm can be inserted to perform the reconstruction,\nour paradigm affords quantitative tests of theories of perception as well as\nnew benchmarks for segmentation algorithms.\n","authors":["Jonathan Vacher","Claire Launay","Pascal Mamassian","Ruben Coen-Cagli"],"pdf_url":"https://arxiv.org/pdf/2301.07807v3.pdf","comment":"32 pages, 9 figures, 5 appendix, 5 figures in appendix"},{"id":"http://arxiv.org/abs/2305.17382v3","updated":"2023-10-11T07:02:45Z","published":"2023-05-27T06:24:43Z","title":"APRIL-GAN: A Zero-/Few-Shot Anomaly Classification and Segmentation\n  Method for CVPR 2023 VAND Workshop Challenge Tracks 1&2: 1st Place on\n  Zero-shot AD and 4th Place on Few-shot AD","summary":"  In this technical report, we briefly introduce our solution for the\nZero/Few-shot Track of the Visual Anomaly and Novelty Detection (VAND) 2023\nChallenge. For industrial visual inspection, building a single model that can\nbe rapidly adapted to numerous categories without or with only a few normal\nreference images is a promising research direction. This is primarily because\nof the vast variety of the product types. For the zero-shot track, we propose a\nsolution based on the CLIP model by adding extra linear layers. These layers\nare used to map the image features to the joint embedding space, so that they\ncan compare with the text features to generate the anomaly maps. Besides, when\nthe reference images are available, we utilize multiple memory banks to store\ntheir features and compare them with the features of the test images during the\ntesting phase. In this challenge, our method achieved first place in the\nzero-shot track, especially excelling in segmentation with an impressive F1\nscore improvement of 0.0489 over the second-ranked participant. Furthermore, in\nthe few-shot track, we secured the fourth position overall, with our\nclassification F1 score of 0.8687 ranking first among all participating teams.\n","authors":["Xuhai Chen","Yue Han","Jiangning Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.17382v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16254v3","updated":"2023-10-11T06:59:47Z","published":"2023-03-28T18:59:17Z","title":"CryoFormer: Continuous Heterogeneous Cryo-EM Reconstruction using\n  Transformer-based Neural Representations","summary":"  Cryo-electron microscopy (cryo-EM) allows for the high-resolution\nreconstruction of 3D structures of proteins and other biomolecules. Successful\nreconstruction of both shape and movement greatly helps understand the\nfundamental processes of life. However, it is still challenging to reconstruct\nthe continuous motions of 3D structures from hundreds of thousands of noisy and\nrandomly oriented 2D cryo-EM images. Recent advancements use Fourier domain\ncoordinate-based neural networks to continuously model 3D conformations, yet\nthey often struggle to capture local flexible regions accurately. We propose\nCryoFormer, a new approach for continuous heterogeneous cryo-EM reconstruction.\nOur approach leverages an implicit feature volume directly in the real domain\nas the 3D representation. We further introduce a novel query-based deformation\ntransformer decoder to improve the reconstruction quality. Our approach is\ncapable of refining pre-computed pose estimations and locating flexible\nregions. In experiments, our method outperforms current approaches on three\npublic datasets (1 synthetic and 2 experimental) and a new synthetic dataset of\nPEDV spike protein. The code and new synthetic dataset will be released for\nbetter reproducibility of our results. Project page:\nhttps://cryoformer.github.io.\n","authors":["Xinhang Liu","Yan Zeng","Yifan Qin","Hao Li","Jiakai Zhang","Lan Xu","Jingyi Yu"],"pdf_url":"https://arxiv.org/pdf/2303.16254v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07237v1","updated":"2023-10-11T06:58:22Z","published":"2023-10-11T06:58:22Z","title":"SAGE-ICP: Semantic Information-Assisted ICP","summary":"  Robust and accurate pose estimation in unknown environments is an essential\npart of robotic applications. We focus on LiDAR-based point-to-point ICP\ncombined with effective semantic information. This paper proposes a novel\nsemantic information-assisted ICP method named SAGE-ICP, which leverages\nsemantics in odometry. The semantic information for the whole scan is timely\nand efficiently extracted by a 3D convolution network, and these point-wise\nlabels are deeply involved in every part of the registration, including\nsemantic voxel downsampling, data association, adaptive local map, and dynamic\nvehicle removal. Unlike previous semantic-aided approaches, the proposed method\ncan improve localization accuracy in large-scale scenes even if the semantic\ninformation has certain errors. Experimental evaluations on KITTI and KITTI-360\nshow that our method outperforms the baseline methods, and improves accuracy\nwhile maintaining real-time performance, i.e., runs faster than the sensor\nframe rate.\n","authors":["Jiaming Cui","Jiming Chen","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07237v1.pdf","comment":"6+1 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.07236v1","updated":"2023-10-11T06:56:08Z","published":"2023-10-11T06:56:08Z","title":"AdaMesh: Personalized Facial Expressions and Head Poses for\n  Speech-Driven 3D Facial Animation","summary":"  Speech-driven 3D facial animation aims at generating facial movements that\nare synchronized with the driving speech, which has been widely explored\nrecently. Existing works mostly neglect the person-specific talking style in\ngeneration, including facial expression and head pose styles. Several works\nintend to capture the personalities by fine-tuning modules. However, limited\ntraining data leads to the lack of vividness. In this work, we propose AdaMesh,\na novel adaptive speech-driven facial animation approach, which learns the\npersonalized talking style from a reference video of about 10 seconds and\ngenerates vivid facial expressions and head poses. Specifically, we propose\nmixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter,\nwhich efficiently captures the facial expression style. For the personalized\npose style, we propose a pose adapter by building a discrete pose prior and\nretrieving the appropriate style embedding with a semantic-aware pose style\nmatrix without fine-tuning. Extensive experimental results show that our\napproach outperforms state-of-the-art methods, preserves the talking style in\nthe reference video, and generates vivid facial animation. The supplementary\nvideo and code will be available at https://adamesh.github.io.\n","authors":["Liyang Chen","Weihong Bao","Shun Lei","Boshi Tang","Zhiyong Wu","Shiyin Kang","Haozhi Huang"],"pdf_url":"https://arxiv.org/pdf/2310.07236v1.pdf","comment":"Project Page: https://adamesh.github.io"},{"id":"http://arxiv.org/abs/2309.13438v3","updated":"2023-10-11T06:43:08Z","published":"2023-09-23T17:29:38Z","title":"Rethinking Superpixel Segmentation from Biologically Inspired Mechanisms","summary":"  Recently, advancements in deep learning-based superpixel segmentation methods\nhave brought about improvements in both the efficiency and the performance of\nsegmentation. However, a significant challenge remains in generating\nsuperpixels that strictly adhere to object boundaries while conveying rich\nvisual significance, especially when cross-surface color correlations may\ninterfere with objects. Drawing inspiration from neural structure and visual\nmechanisms, we propose a biological network architecture comprising an Enhanced\nScreening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel\nsegmentation. The ESM enhances semantic information by simulating the\ninteractive projection mechanisms of the visual cortex. Additionally, the BAL\nemulates the spatial frequency characteristics of visual cortical cells to\nfacilitate the generation of superpixels with strong boundary adherence. We\ndemonstrate the effectiveness of our approach through evaluations on both the\nBSDS500 dataset and the NYUv2 dataset.\n","authors":["Tingyu Zhao","Bo Peng","Yuan Sun","Daipeng Yang","Zhenguang Zhang","Xi Wu"],"pdf_url":"https://arxiv.org/pdf/2309.13438v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13311v2","updated":"2023-10-11T06:28:41Z","published":"2023-05-22T17:59:45Z","title":"VDT: General-purpose Video Diffusion Transformers via Mask Modeling","summary":"  This work introduces Video Diffusion Transformer (VDT), which pioneers the\nuse of transformers in diffusion-based video generation. It features\ntransformer blocks with modularized temporal and spatial attention modules to\nleverage the rich spatial-temporal representation inherited in transformers. We\nalso propose a unified spatial-temporal mask modeling mechanism, seamlessly\nintegrated with the model, to cater to diverse video generation scenarios. VDT\noffers several appealing benefits. 1) It excels at capturing temporal\ndependencies to produce temporally consistent video frames and even simulate\nthe physics and dynamics of 3D objects over time. 2) It facilitates flexible\nconditioning information, \\eg, simple concatenation in the token space,\neffectively unifying different token lengths and modalities. 3) Pairing with\nour proposed spatial-temporal mask modeling mechanism, it becomes a\ngeneral-purpose video diffuser for harnessing a range of tasks, including\nunconditional generation, video prediction, interpolation, animation, and\ncompletion, etc. Extensive experiments on these tasks spanning various\nscenarios, including autonomous driving, natural weather, human action, and\nphysics-based simulation, demonstrate the effectiveness of VDT. Additionally,\nwe present comprehensive studies on how \\model handles conditioning information\nwith the mask modeling mechanism, which we believe will benefit future research\nand advance the field. Project page: https:VDT-2023.github.io\n","authors":["Haoyu Lu","Guoxing Yang","Nanyi Fei","Yuqi Huo","Zhiwu Lu","Ping Luo","Mingyu Ding"],"pdf_url":"https://arxiv.org/pdf/2305.13311v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07223v1","updated":"2023-10-11T06:13:50Z","published":"2023-10-11T06:13:50Z","title":"Deep Learning for blind spectral unmixing of LULC classes with MODIS\n  multispectral time series and ancillary data","summary":"  Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC)\ntypes. Spectral unmixing is a technique to extract information from mixed\npixels into their constituent LULC types and corresponding abundance fractions.\nTraditionally, solving this task has relied on either classical methods that\nrequire prior knowledge of endmembers or machine learning methods that avoid\nexplicit endmembers calculation, also known as blind spectral unmixing (BSU).\nMost BSU studies based on Deep Learning (DL) focus on one time-step\nhyperspectral data, yet its acquisition remains quite costly compared with\nmultispectral data. To our knowledge, here we provide the first study on BSU of\nLULC classes using multispectral time series data with DL models. We further\nboost the performance of a Long-Short Term Memory (LSTM)-based model by\nincorporating geographic plus topographic (geo-topographic) and climatic\nancillary information. Our experiments show that combining spectral-temporal\ninput data together with geo-topographic and climatic information substantially\nimproves the abundance estimation of LULC classes in mixed pixels. To carry out\nthis study, we built a new labeled dataset of the region of Andalusia (Spain)\nwith monthly multispectral time series of pixels for the year 2013 from MODIS\nat 460m resolution, for two hierarchical levels of LULC classes, named\nAndalusia MultiSpectral MultiTemporal Unmixing (Andalusia-MSMTU). This dataset\nprovides, at the pixel level, a multispectral time series plus ancillary\ninformation annotated with the abundance of each LULC class inside each pixel.\nThe dataset and code are available to the public.\n","authors":["José Rodríguez-Ortega","Rohaifa Khaldi","Domingo Alcaraz-Segura","Siham Tabik"],"pdf_url":"https://arxiv.org/pdf/2310.07223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07222v1","updated":"2023-10-11T06:11:42Z","published":"2023-10-11T06:11:42Z","title":"Uni-paint: A Unified Framework for Multimodal Image Inpainting with\n  Pretrained Diffusion Model","summary":"  Recently, text-to-image denoising diffusion probabilistic models (DDPMs) have\ndemonstrated impressive image generation capabilities and have also been\nsuccessfully applied to image inpainting. However, in practice, users often\nrequire more control over the inpainting process beyond textual guidance,\nespecially when they want to composite objects with customized appearance,\ncolor, shape, and layout. Unfortunately, existing diffusion-based inpainting\nmethods are limited to single-modal guidance and require task-specific\ntraining, hindering their cross-modal scalability. To address these\nlimitations, we propose Uni-paint, a unified framework for multimodal\ninpainting that offers various modes of guidance, including unconditional,\ntext-driven, stroke-driven, exemplar-driven inpainting, as well as a\ncombination of these modes. Furthermore, our Uni-paint is based on pretrained\nStable Diffusion and does not require task-specific training on specific\ndatasets, enabling few-shot generalizability to customized images. We have\nconducted extensive qualitative and quantitative evaluations that show our\napproach achieves comparable results to existing single-modal methods while\noffering multimodal inpainting capabilities not available in other methods.\nCode will be available at https://github.com/ysy31415/unipaint.\n","authors":["Shiyuan Yang","Xiaodong Chen","Jing Liao"],"pdf_url":"https://arxiv.org/pdf/2310.07222v1.pdf","comment":"Accepted by ACMMM'23"},{"id":"http://arxiv.org/abs/2310.04895v2","updated":"2023-10-11T05:59:53Z","published":"2023-10-07T18:47:17Z","title":"Cell Tracking-by-detection using Elliptical Bounding Boxes","summary":"  Cell detection and tracking are paramount for bio-analysis. Recent approaches\nrely on the tracking-by-model evolution paradigm, which usually consists of\ntraining end-to-end deep learning models to detect and track the cells on the\nframes with promising results. However, such methods require extensive amounts\nof annotated data, which is time-consuming to obtain and often requires\nspecialized annotators. This work proposes a new approach based on the\nclassical tracking-by-detection paradigm that alleviates the requirement of\nannotated data. More precisely, it approximates the cell shapes as oriented\nellipses and then uses generic-purpose oriented object detectors to identify\nthe cells in each frame. We then rely on a global data association algorithm\nthat explores temporal cell similarity using probability distance metrics,\nconsidering that the ellipses relate to two-dimensional Gaussian distributions.\nOur results show that our method can achieve detection and tracking results\ncompetitively with state-of-the-art techniques that require considerably more\nextensive data annotation. Our code is available at:\nhttps://github.com/LucasKirsten/Deep-Cell-Tracking-EBB.\n","authors":["Lucas N. Kirsten","Cláudio R. Jung"],"pdf_url":"https://arxiv.org/pdf/2310.04895v2.pdf","comment":"Paper under review on IEEE/ACM Transactions on Computational Biology\n  and Bioinformatics"},{"id":"http://arxiv.org/abs/2310.07212v1","updated":"2023-10-11T05:58:14Z","published":"2023-10-11T05:58:14Z","title":"Multi-Task Learning-Enabled Automatic Vessel Draft Reading for\n  Intelligent Maritime Surveillance","summary":"  The accurate and efficient vessel draft reading (VDR) is an important\ncomponent of intelligent maritime surveillance, which could be exploited to\nassist in judging whether the vessel is normally loaded or overloaded. The\ncomputer vision technique with an excellent price-to-performance ratio has\nbecome a popular medium to estimate vessel draft depth. However, the\ntraditional estimation methods easily suffer from several limitations, such as\nsensitivity to low-quality images, high computational cost, etc. In this work,\nwe propose a multi-task learning-enabled computational method (termed MTL-VDR)\nfor generating highly reliable VDR. In particular, our MTL-VDR mainly consists\nof four components, i.e., draft mark detection, draft scale recognition,\nvessel/water segmentation, and final draft depth estimation. We first construct\na benchmark dataset related to draft mark detection and employ a powerful and\nefficient convolutional neural network to accurately perform the detection\ntask. The multi-task learning method is then proposed for simultaneous draft\nscale recognition and vessel/water segmentation. To obtain more robust VDR\nunder complex conditions (e.g., damaged and stained scales, etc.), the accurate\ndraft scales are generated by an automatic correction method, which is\npresented based on the spatial distribution rules of draft scales. Finally, an\nadaptive computational method is exploited to yield an accurate and robust\ndraft depth. Extensive experiments have been implemented on the realistic\ndataset to compare our MTL-VDR with state-of-the-art methods. The results have\ndemonstrated its superior performance in terms of accuracy, robustness, and\nefficiency. The computational speed exceeds 40 FPS, which satisfies the\nrequirements of real-time maritime surveillance to guarantee vessel traffic\nsafety.\n","authors":["Jingxiang Qu","Ryan Wen Liu","Chenjie Zhao","Yu Guo","Sendren Sheng-Dong Xu","Fenghua Zhu","Yisheng Lv"],"pdf_url":"https://arxiv.org/pdf/2310.07212v1.pdf","comment":"12 pages,11 figures, submitted to IEEE T-ITS"},{"id":"http://arxiv.org/abs/2310.07209v1","updated":"2023-10-11T05:49:47Z","published":"2023-10-11T05:49:47Z","title":"Multi-task Explainable Skin Lesion Classification","summary":"  Skin cancer is one of the deadliest diseases and has a high mortality rate if\nleft untreated. The diagnosis generally starts with visual screening and is\nfollowed by a biopsy or histopathological examination. Early detection can aid\nin lowering mortality rates. Visual screening can be limited by the experience\nof the doctor. Due to the long tail distribution of dermatological datasets and\nsignificant intra-variability between classes, automatic classification\nutilizing computer-aided methods becomes challenging. In this work, we propose\na multitask few-shot-based approach for skin lesions that generalizes well with\nfew labelled data to address the small sample space challenge. The proposed\napproach comprises a fusion of a segmentation network that acts as an attention\nmodule and classification network. The output of the segmentation network helps\nto focus on the most discriminatory features while making a decision by the\nclassification network. To further enhance the classification performance, we\nhave combined segmentation and classification loss in a weighted manner. We\nhave also included the visualization results that explain the decisions made by\nthe algorithm. Three dermatological datasets are used to evaluate the proposed\nmethod thoroughly. We also conducted cross-database experiments to ensure that\nthe proposed approach is generalizable across similar datasets. Experimental\nresults demonstrate the efficacy of the proposed work.\n","authors":["Mahapara Khurshid","Mayank Vatsa","Richa Singh"],"pdf_url":"https://arxiv.org/pdf/2310.07209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05341v3","updated":"2023-10-11T05:46:28Z","published":"2023-10-09T01:59:49Z","title":"A Critical Look at Classic Test-Time Adaptation Methods in Semantic\n  Segmentation","summary":"  Test-time adaptation (TTA) aims to adapt a model, initially trained on\ntraining data, to potential distribution shifts in the test data. Most existing\nTTA studies, however, focus on classification tasks, leaving a notable gap in\nthe exploration of TTA for semantic segmentation. This pronounced emphasis on\nclassification might lead numerous newcomers and engineers to mistakenly assume\nthat classic TTA methods designed for classification can be directly applied to\nsegmentation. Nonetheless, this assumption remains unverified, posing an open\nquestion. To address this, we conduct a systematic, empirical study to disclose\nthe unique challenges of segmentation TTA, and to determine whether classic TTA\nstrategies can effectively address this task. Our comprehensive results have\nled to three key observations. First, the classic batch norm updating strategy,\ncommonly used in classification TTA, only brings slight performance\nimprovement, and in some cases it might even adversely affect the results. Even\nwith the application of advanced distribution estimation techniques like batch\nrenormalization, the problem remains unresolved. Second, the teacher-student\nscheme does enhance training stability for segmentation TTA in the presence of\nnoisy pseudo-labels. However, it cannot directly result in performance\nimprovement compared to the original model without TTA. Third, segmentation TTA\nsuffers a severe long-tailed imbalance problem, which is substantially more\ncomplex than that in TTA for classification. This long-tailed challenge\nsignificantly affects segmentation TTA performance, even when the accuracy of\npseudo-labels is high. In light of these observations, we conclude that TTA for\nsegmentation presents significant challenges, and simply using classic TTA\nmethods cannot address this problem well.\n","authors":["Chang'an Yi","Haotian Chen","Yifan Zhang","Yonghui Xu","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2310.05341v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05917v2","updated":"2023-10-11T05:41:16Z","published":"2023-10-09T17:59:12Z","title":"Drivable Avatar Clothing: Faithful Full-Body Telepresence with Dynamic\n  Clothing Driven by Sparse RGB-D Input","summary":"  Clothing is an important part of human appearance but challenging to model in\nphotorealistic avatars. In this work we present avatars with dynamically moving\nloose clothing that can be faithfully driven by sparse RGB-D inputs as well as\nbody and face motion. We propose a Neural Iterative Closest Point (N-ICP)\nalgorithm that can efficiently track the coarse garment shape given sparse\ndepth input. Given the coarse tracking results, the input RGB-D images are then\nremapped to texel-aligned features, which are fed into the drivable avatar\nmodels to faithfully reconstruct appearance details. We evaluate our method\nagainst recent image-driven synthesis baselines, and conduct a comprehensive\nanalysis of the N-ICP algorithm. We demonstrate that our method can generalize\nto a novel testing environment, while preserving the ability to produce\nhigh-fidelity and faithful clothing dynamics and appearance.\n","authors":["Donglai Xiang","Fabian Prada","Zhe Cao","Kaiwen Guo","Chenglei Wu","Jessica Hodgins","Timur Bagautdinov"],"pdf_url":"https://arxiv.org/pdf/2310.05917v2.pdf","comment":"SIGGRAPH Asia 2023 Conference Paper. Project website:\n  https://xiangdonglai.github.io/www-sa23-drivable-clothing/"},{"id":"http://arxiv.org/abs/2310.07206v1","updated":"2023-10-11T05:34:36Z","published":"2023-10-11T05:34:36Z","title":"DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via\n  Physics Simulation","summary":"  This paper addresses the task of 3D pose estimation for a hand interacting\nwith an object from a single image observation. When modeling hand-object\ninteraction, previous works mainly exploit proximity cues, while overlooking\nthe dynamical nature that the hand must stably grasp the object to counteract\ngravity and thus preventing the object from slipping or falling. These works\nfail to leverage dynamical constraints in the estimation and consequently often\nproduce unstable results. Meanwhile, refining unstable configurations with\nphysics-based reasoning remains challenging, both by the complexity of contact\ndynamics and by the lack of effective and efficient physics inference in the\ndata-driven learning framework. To address both issues, we present DeepSimHO: a\nnovel deep-learning pipeline that combines forward physics simulation and\nbackward gradient approximation with a neural network. Specifically, for an\ninitial hand-object pose estimated by a base network, we forward it to a\nphysics simulator to evaluate its stability. However, due to non-smooth contact\ngeometry and penetration, existing differentiable simulators can not provide\nreliable state gradient. To remedy this, we further introduce a deep network to\nlearn the stability evaluation process from the simulator, while smoothly\napproximating its gradient and thus enabling effective back-propagation.\nExtensive experiments show that our method noticeably improves the stability of\nthe estimation and achieves superior efficiency over test-time optimization.\nThe code is available at https://github.com/rongakowang/DeepSimHO.\n","authors":["Rong Wang","Wei Mao","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2310.07206v1.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2309.17421v2","updated":"2023-10-11T05:07:37Z","published":"2023-09-29T17:34:51Z","title":"The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)","summary":"  Large multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic\nintelligence. In this paper, we analyze the latest model, GPT-4V(ision), to\ndeepen the understanding of LMMs. The analysis focuses on the intriguing tasks\nthat GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V's capabilities, its supported inputs and working modes,\nand the effective ways to prompt the model. In our approach to exploring\nGPT-4V, we curate and organize a collection of carefully designed qualitative\nsamples spanning a variety of domains and tasks. Observations from these\nsamples demonstrate that GPT-4V's unprecedented ability in processing\narbitrarily interleaved multimodal inputs and the genericity of its\ncapabilities together make GPT-4V a powerful multimodal generalist system.\nFurthermore, GPT-4V's unique capability of understanding visual markers drawn\non input images can give rise to new human-computer interaction methods such as\nvisual referring prompting. We conclude the report with in-depth discussions on\nthe emerging application scenarios and the future research directions for\nGPT-4V-based systems. We hope that this preliminary exploration will inspire\nfuture research on the next-generation multimodal task formulation, new ways to\nexploit and enhance LMMs to solve real-world problems, and gaining better\nunderstanding of multimodal foundation models. Finally, we acknowledge that the\nmodel under our study is solely the product of OpenAI's innovative work, and\nthey should be fully credited for its development. Please see the GPT-4V\ncontributions paper for the authorship and credit attribution:\nhttps://cdn.openai.com/contributions/gpt-4v.pdf\n","authors":["Zhengyuan Yang","Linjie Li","Kevin Lin","Jianfeng Wang","Chung-Ching Lin","Zicheng Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2309.17421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07189v1","updated":"2023-10-11T04:38:21Z","published":"2023-10-11T04:38:21Z","title":"SpikePoint: An Efficient Point-based Spiking Neural Network for Event\n  Cameras Action Recognition","summary":"  Event cameras are bio-inspired sensors that respond to local changes in light\nintensity and feature low latency, high energy efficiency, and high dynamic\nrange. Meanwhile, Spiking Neural Networks (SNNs) have gained significant\nattention due to their remarkable efficiency and fault tolerance. By\nsynergistically harnessing the energy efficiency inherent in event cameras and\nthe spike-based processing capabilities of SNNs, their integration could enable\nultra-low-power application scenarios, such as action recognition tasks.\nHowever, existing approaches often entail converting asynchronous events into\nconventional frames, leading to additional data mapping efforts and a loss of\nsparsity, contradicting the design concept of SNNs and event cameras. To\naddress this challenge, we propose SpikePoint, a novel end-to-end point-based\nSNN architecture. SpikePoint excels at processing sparse event cloud data,\neffectively extracting both global and local features through a singular-stage\nstructure. Leveraging the surrogate training method, SpikePoint achieves high\naccuracy with few parameters and maintains low power consumption, specifically\nemploying the identity mapping feature extractor on diverse datasets.\nSpikePoint achieves state-of-the-art (SOTA) performance on four event-based\naction recognition datasets using only 16 timesteps, surpassing other SNN\nmethods. Moreover, it also achieves SOTA performance across all methods on\nthree datasets, utilizing approximately 0.3\\% of the parameters and 0.5\\% of\npower consumption employed by artificial neural networks (ANNs). These results\nemphasize the significance of Point Cloud and pave the way for many\nultra-low-power event-based data processing applications.\n","authors":["Hongwei Ren","Yue Zhou","Yulong Huang","Haotian Fu","Xiaopeng Lin","Jie Song","Bojun Cheng"],"pdf_url":"https://arxiv.org/pdf/2310.07189v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.07186v1","updated":"2023-10-11T04:25:24Z","published":"2023-10-11T04:25:24Z","title":"Multiview Transformer: Rethinking Spatial Information in Hyperspectral\n  Image Classification","summary":"  Identifying the land cover category for each pixel in a hyperspectral image\n(HSI) relies on spectral and spatial information. An HSI cuboid with a specific\npatch size is utilized to extract spatial-spectral feature representation for\nthe central pixel. In this article, we investigate that scene-specific but not\nessential correlations may be recorded in an HSI cuboid. This additional\ninformation improves the model performance on existing HSI datasets and makes\nit hard to properly evaluate the ability of a model. We refer to this problem\nas the spatial overfitting issue and utilize strict experimental settings to\navoid it. We further propose a multiview transformer for HSI classification,\nwhich consists of multiview principal component analysis (MPCA), spectral\nencoder-decoder (SED), and spatial-pooling tokenization transformer (SPTT).\nMPCA performs dimension reduction on an HSI via constructing spectral multiview\nobservations and applying PCA on each view data to extract low-dimensional view\nrepresentation. The combination of view representations, named multiview\nrepresentation, is the dimension reduction output of the MPCA. To aggregate the\nmultiview information, a fully-convolutional SED with a U-shape in spectral\ndimension is introduced to extract a multiview feature map. SPTT transforms the\nmultiview features into tokens using the spatial-pooling tokenization strategy\nand learns robust and discriminative spatial-spectral features for land cover\nidentification. Classification is conducted with a linear classifier.\nExperiments on three HSI datasets with rigid settings demonstrate the\nsuperiority of the proposed multiview transformer over the state-of-the-art\nmethods.\n","authors":["Jie Zhang","Yongshan Zhang","Yicong Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.07186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07184v1","updated":"2023-10-11T04:20:32Z","published":"2023-10-11T04:20:32Z","title":"NeuroInspect: Interpretable Neuron-based Debugging Framework through\n  Class-conditional Visualizations","summary":"  Despite deep learning (DL) has achieved remarkable progress in various\ndomains, the DL models are still prone to making mistakes. This issue\nnecessitates effective debugging tools for DL practitioners to interpret the\ndecision-making process within the networks. However, existing debugging\nmethods often demand extra data or adjustments to the decision process,\nlimiting their applicability. To tackle this problem, we present NeuroInspect,\nan interpretable neuron-based debugging framework with three key stages:\ncounterfactual explanations, feature visualizations, and false correlation\nmitigation. Our debugging framework first pinpoints neurons responsible for\nmistakes in the network and then visualizes features embedded in the neurons to\nbe human-interpretable. To provide these explanations, we introduce\nCLIP-Illusion, a novel feature visualization method that generates images\nrepresenting features conditioned on classes to examine the connection between\nneurons and the decision layer. We alleviate convoluted explanations of the\nconventional visualization approach by employing class information, thereby\nisolating mixed properties. This process offers more human-interpretable\nexplanations for model errors without altering the trained network or requiring\nadditional data. Furthermore, our framework mitigates false correlations\nlearned from a dataset under a stochastic perspective, modifying decisions for\nthe neurons considered as the main causes. We validate the effectiveness of our\nframework by addressing false correlations and improving inferences for classes\nwith the worst performance in real-world settings. Moreover, we demonstrate\nthat NeuroInspect helps debug the mistakes of DL models through evaluation for\nhuman understanding. The code is openly available at\nhttps://github.com/yeongjoonJu/NeuroInspect.\n","authors":["Yeong-Joon Ju","Ji-Hoon Park","Seong-Whan Lee"],"pdf_url":"https://arxiv.org/pdf/2310.07184v1.pdf","comment":"Summitted to IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS)"},{"id":"http://arxiv.org/abs/2310.07179v1","updated":"2023-10-11T04:05:11Z","published":"2023-10-11T04:05:11Z","title":"rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera","summary":"  Novel view synthesis of satellite images holds a wide range of practical\napplications. While recent advances in the Neural Radiance Field have\npredominantly targeted pin-hole cameras, and models for satellite cameras often\ndemand sufficient input views. This paper presents rpcPRF, a Multiplane Images\n(MPI) based Planar neural Radiance Field for Rational Polynomial Camera (RPC).\nUnlike coordinate-based neural radiance fields in need of sufficient views of\none scene, our model is applicable to single or few inputs and performs well on\nimages from unseen scenes. To enable generalization across scenes, we propose\nto use reprojection supervision to induce the predicted MPI to learn the\ncorrect geometry between the 3D coordinates and the images. Moreover, we remove\nthe stringent requirement of dense depth supervision from deep\nmultiview-stereo-based methods by introducing rendering techniques of radiance\nfields. rpcPRF combines the superiority of implicit representations and the\nadvantages of the RPC model, to capture the continuous altitude space while\nlearning the 3D structure. Given an RGB image and its corresponding RPC, the\nend-to-end model learns to synthesize the novel view with a new RPC and\nreconstruct the altitude of the scene. When multiple views are provided as\ninputs, rpcPRF exerts extra supervision provided by the extra views. On the TLC\ndataset from ZY-3, and the SatMVS3D dataset with urban scenes from WV-3, rpcPRF\noutperforms state-of-the-art nerf-based methods by a significant margin in\nterms of image fidelity, reconstruction accuracy, and efficiency, for both\nsingle-view and multiview task.\n","authors":["Tongtong Zhang","Yuanxiang Li"],"pdf_url":"https://arxiv.org/pdf/2310.07179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05136v3","updated":"2023-10-11T04:04:07Z","published":"2023-10-08T12:10:44Z","title":"InstructDET: Diversifying Referring Object Detection with Generalized\n  Instructions","summary":"  We propose InstructDET, a data-centric method for referring object detection\n(ROD) that localizes target objects based on user instructions. While deriving\nfrom referring expressions (REC), the instructions we leverage are greatly\ndiversified to encompass common user intentions related to object detection.\nFor one image, we produce tremendous instructions that refer to every single\nobject and different combinations of multiple objects. Each instruction and its\ncorresponding object bounding boxes (bbxs) constitute one training data pair.\nIn order to encompass common detection expressions, we involve emerging\nvision-language model (VLM) and large language model (LLM) to generate\ninstructions guided by text prompts and object bbxs, as the generalizations of\nfoundation models are effective to produce human-like expressions (e.g.,\ndescribing object property, category, and relationship). We name our\nconstructed dataset as InDET. It contains images, bbxs and generalized\ninstructions that are from foundation models. Our InDET is developed from\nexisting REC datasets and object detection datasets, with the expanding\npotential that any image with object bbxs can be incorporated through using our\nInstructDET method. By using our InDET dataset, we show that a conventional ROD\nmodel surpasses existing methods on standard REC datasets and our InDET test\nset. Our data-centric method InstructDET, with automatic data expansion by\nleveraging foundation models, directs a promising field that ROD can be greatly\ndiversified to execute common object detection instructions.\n","authors":["Ronghao Dang","Jiangyan Feng","Haodong Zhang","Chongjian Ge","Lin Song","Lijun Gong","Chengju Liu","Qijun Chen","Feng Zhu","Rui Zhao","Yibing Song"],"pdf_url":"https://arxiv.org/pdf/2310.05136v3.pdf","comment":"Adjust the subject"},{"id":"http://arxiv.org/abs/2310.07176v1","updated":"2023-10-11T04:00:17Z","published":"2023-10-11T04:00:17Z","title":"Improving mitosis detection on histopathology images using large\n  vision-language models","summary":"  In certain types of cancerous tissue, mitotic count has been shown to be\nassociated with tumor proliferation, poor prognosis, and therapeutic\nresistance. Due to the high inter-rater variability of mitotic counting by\npathologists, convolutional neural networks (CNNs) have been employed to reduce\nthe subjectivity of mitosis detection in hematoxylin and eosin (H&E)-stained\nwhole slide images. However, most existing models have performance that lags\nbehind expert panel review and only incorporate visual information. In this\nwork, we demonstrate that pre-trained large-scale vision-language models that\nleverage both visual features and natural language improve mitosis detection\naccuracy. We formulate the mitosis detection task as an image captioning task\nand a visual question answering (VQA) task by including metadata such as tumor\nand scanner types as context. The effectiveness of our pipeline is demonstrated\nvia comparison with various baseline models using 9,501 mitotic figures and\n11,051 hard negatives (non-mitotic figures that are difficult to characterize)\nfrom the publicly available Mitosis Domain Generalization Challenge (MIDOG22)\ndataset.\n","authors":["Ruiwen Ding","James Hall","Neil Tenenholtz","Kristen Severson"],"pdf_url":"https://arxiv.org/pdf/2310.07176v1.pdf","comment":"Submitted to IEEE ISBI 2024. Under review"},{"id":"http://arxiv.org/abs/2310.07166v1","updated":"2023-10-11T03:29:13Z","published":"2023-10-11T03:29:13Z","title":"Anchor-based Multi-view Subspace Clustering with Hierarchical Feature\n  Descent","summary":"  Multi-view clustering has attracted growing attention owing to its\ncapabilities of aggregating information from various sources and its promising\nhorizons in public affairs. Up till now, many advanced approaches have been\nproposed in recent literature. However, there are several ongoing difficulties\nto be tackled. One common dilemma occurs while attempting to align the features\nof different views. We dig out as well as deploy the dependency amongst views\nthrough hierarchical feature descent, which leads to a common latent space(\nSTAGE 1). This latent space, for the first time of its kind, is regarded as a\n'resemblance space', as it reveals certain correlations and dependencies of\ndifferent views. To be exact, the one-hot encoding of a category can also be\nreferred to as a resemblance space in its terminal phase. Moreover, due to the\nintrinsic fact that most of the existing multi-view clustering algorithms stem\nfrom k-means clustering and spectral clustering, this results in cubic time\ncomplexity w.r.t. the number of the objects. However, we propose Anchor-based\nMulti-view Subspace Clustering with Hierarchical Feature Descent(MVSC-HFD) to\nfurther reduce the computing complexity to linear time cost through a unified\nsampling strategy in resemblance space( STAGE 2), followed by subspace\nclustering to learn the representation collectively( STAGE 3). Extensive\nexperimental results on public benchmark datasets demonstrate that our proposed\nmodel consistently outperforms the state-of-the-art techniques.\n","authors":["Qiyuan Ou","Siwei Wang","Pei Zhang","Sihang Zhou","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.07166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17546v2","updated":"2023-10-11T03:19:18Z","published":"2023-03-30T17:13:56Z","title":"PAIR-Diffusion: A Comprehensive Multimodal Object-Level Image Editor","summary":"  Generative image editing has recently witnessed extremely fast-paced growth.\nSome works use high-level conditioning such as text, while others use low-level\nconditioning. Nevertheless, most of them lack fine-grained control over the\nproperties of the different objects present in the image, i.e.\\,object-level\nimage editing. In this work, we tackle the task by perceiving the images as an\namalgamation of various objects and aim to control the properties of each\nobject in a fine-grained manner. Out of these properties, we identify structure\nand appearance as the most intuitive to understand and useful for editing\npurposes. We propose \\textbf{PAIR} Diffusion, a generic framework that can\nenable a diffusion model to control the structure and appearance properties of\neach object in the image. We show that having control over the properties of\neach object in an image leads to comprehensive editing capabilities. Our\nframework allows for various object-level editing operations on real images\nsuch as reference image-based appearance editing, free-form shape editing,\nadding objects, and variations. Thanks to our design, we do not require any\ninversion step. Additionally, we propose multimodal classifier-free guidance\nwhich enables editing images using both reference images and text when using\nour approach with foundational diffusion models. We validate the above claims\nby extensively evaluating our framework on both unconditional and foundational\ndiffusion models. Please refer to\nhttps://vidit98.github.io/publication/conference-paper/pair_diff.html for code\nand model release.\n","authors":["Vidit Goel","Elia Peruzzo","Yifan Jiang","Dejia Xu","Xingqian Xu","Nicu Sebe","Trevor Darrell","Zhangyang Wang","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2303.17546v2.pdf","comment":"26 pages and 17 figures"},{"id":"http://arxiv.org/abs/2202.07870v2","updated":"2023-10-11T03:11:14Z","published":"2022-02-16T05:47:31Z","title":"IPD:An Incremental Prototype based DBSCAN for large-scale data with\n  cluster representatives","summary":"  DBSCAN is a fundamental density-based clustering technique that identifies\nany arbitrary shape of the clusters. However, it becomes infeasible while\nhandling big data. On the other hand, centroid-based clustering is important\nfor detecting patterns in a dataset since unprocessed data points can be\nlabeled to their nearest centroid. However, it can not detect non-spherical\nclusters. For a large data, it is not feasible to store and compute labels of\nevery samples. These can be done as and when the information is required. The\npurpose can be accomplished when clustering act as a tool to identify cluster\nrepresentatives and query is served by assigning cluster labels of nearest\nrepresentative. In this paper, we propose an Incremental Prototype-based DBSCAN\n(IPD) algorithm which is designed to identify arbitrary-shaped clusters for\nlarge-scale data. Additionally, it chooses a set of representatives for each\ncluster.\n","authors":["Jayasree Saha","Jayanta Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2202.07870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06925v2","updated":"2023-10-11T02:52:46Z","published":"2023-04-14T05:21:47Z","title":"YOLO-Drone:Airborne real-time detection of dense small objects from\n  high-altitude perspective","summary":"  Unmanned Aerial Vehicles (UAVs), specifically drones equipped with remote\nsensing object detection technology, have rapidly gained a broad spectrum of\napplications and emerged as one of the primary research focuses in the field of\ncomputer vision. Although UAV remote sensing systems have the ability to detect\nvarious objects, small-scale objects can be challenging to detect reliably due\nto factors such as object size, image degradation, and real-time limitations.\nTo tackle these issues, a real-time object detection algorithm (YOLO-Drone) is\nproposed and applied to two new UAV platforms as well as a specific light\nsource (silicon-based golden LED). YOLO-Drone presents several novelties: 1)\nincluding a new backbone Darknet59; 2) a new complex feature aggregation module\nMSPP-FPN that incorporated one spatial pyramid pooling and three atrous spatial\npyramid pooling modules; 3) and the use of Generalized Intersection over Union\n(GIoU) as the loss function. To evaluate performance, two benchmark datasets,\nUAVDT and VisDrone, along with one homemade dataset acquired at night under\nsilicon-based golden LEDs, are utilized. The experimental results show that, in\nboth UAVDT and VisDrone, the proposed YOLO-Drone outperforms state-of-the-art\n(SOTA) object detection methods by improving the mAP of 10.13% and 8.59%,\nrespectively. With regards to UAVDT, the YOLO-Drone exhibits both high\nreal-time inference speed of 53 FPS and a maximum mAP of 34.04%. Notably,\nYOLO-Drone achieves high performance under the silicon-based golden LEDs, with\na mAP of up to 87.71%, surpassing the performance of YOLO series under ordinary\nlight sources. To conclude, the proposed YOLO-Drone is a highly effective\nsolution for object detection in UAV applications, particularly for night\ndetection tasks where silicon-based golden light LED technology exhibits\nsignificant superiority.\n","authors":["Li Zhu","Jiahui Xiong","Feng Xiong","Hanzheng Hu","Zhengnan Jiang"],"pdf_url":"https://arxiv.org/pdf/2304.06925v2.pdf","comment":"Some contributing authors are not signed"},{"id":"http://arxiv.org/abs/2310.07149v1","updated":"2023-10-11T02:50:16Z","published":"2023-10-11T02:50:16Z","title":"Robust Unsupervised Domain Adaptation by Retaining Confident Entropy via\n  Edge Concatenation","summary":"  The generalization capability of unsupervised domain adaptation can mitigate\nthe need for extensive pixel-level annotations to train semantic segmentation\nnetworks by training models on synthetic data as a source with\ncomputer-generated annotations. Entropy-based adversarial networks are proposed\nto improve source domain prediction; however, they disregard significant\nexternal information, such as edges, which have the potential to identify and\ndistinguish various objects within an image accurately. To address this issue,\nwe introduce a novel approach to domain adaptation, leveraging the synergy of\ninternal and external information within entropy-based adversarial networks. In\nthis approach, we enrich the discriminator network with edge-predicted\nprobability values within this innovative framework to enhance the clarity of\nclass boundaries. Furthermore, we devised a probability-sharing network that\nintegrates diverse information for more effective segmentation. Incorporating\nobject edges addresses a pivotal aspect of unsupervised domain adaptation that\nhas frequently been neglected in the past -- the precise delineation of object\nboundaries. Conventional unsupervised domain adaptation methods usually center\naround aligning feature distributions and may not explicitly model object\nboundaries. Our approach effectively bridges this gap by offering clear\nguidance on object boundaries, thereby elevating the quality of domain\nadaptation. Our approach undergoes rigorous evaluation on the established\nunsupervised domain adaptation benchmarks, specifically in adapting SYNTHIA\n$\\rightarrow$ Cityscapes and SYNTHIA $\\rightarrow$ Mapillary. Experimental\nresults show that the proposed model attains better performance than\nstate-of-the-art methods. The superior performance across different\nunsupervised domain adaptation scenarios highlights the versatility and\nrobustness of the proposed method.\n","authors":["Hye-Seong Hong","Abhishek Kumar","Dong-Gyu Lee"],"pdf_url":"https://arxiv.org/pdf/2310.07149v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06282v2","updated":"2023-10-11T02:46:12Z","published":"2023-10-10T03:32:33Z","title":"MuseChat: A Conversational Music Recommendation System for Videos","summary":"  We introduce MuseChat, an innovative dialog-based music recommendation\nsystem. This unique platform not only offers interactive user engagement but\nalso suggests music tailored for input videos, so that users can refine and\npersonalize their music selections. In contrast, previous systems predominantly\nemphasized content compatibility, often overlooking the nuances of users'\nindividual preferences. For example, all the datasets only provide basic\nmusic-video pairings or such pairings with textual music descriptions. To\naddress this gap, our research offers three contributions. First, we devise a\nconversation-synthesis method that simulates a two-turn interaction between a\nuser and a recommendation system, which leverages pre-trained music tags and\nartist information. In this interaction, users submit a video to the system,\nwhich then suggests a suitable music piece with a rationale. Afterwards, users\ncommunicate their musical preferences, and the system presents a refined music\nrecommendation with reasoning. Second, we introduce a multi-modal\nrecommendation engine that matches music either by aligning it with visual cues\nfrom the video or by harmonizing visual information, feedback from previously\nrecommended music, and the user's textual input. Third, we bridge music\nrepresentations and textual data with a Large Language Model(Vicuna-7B). This\nalignment equips MuseChat to deliver music recommendations and their underlying\nreasoning in a manner resembling human communication. Our evaluations show that\nMuseChat surpasses existing state-of-the-art models in music retrieval tasks\nand pioneers the integration of the recommendation process within a natural\nlanguage framework.\n","authors":["Zhikang Dong","Bin Chen","Xiulong Liu","Pawel Polak","Peng Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.06282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15854v2","updated":"2023-10-11T02:34:23Z","published":"2023-08-30T08:40:15Z","title":"Zero-shot Inversion Process for Image Attribute Editing with Diffusion\n  Models","summary":"  Denoising diffusion models have shown outstanding performance in image\nediting. Existing works tend to use either image-guided methods, which provide\na visual reference but lack control over semantic coherence, or text-guided\nmethods, which ensure faithfulness to text guidance but lack visual quality. To\naddress the problem, we propose the Zero-shot Inversion Process (ZIP), a\nframework that injects a fusion of generated visual reference and text guidance\ninto the semantic latent space of a \\textit{frozen} pre-trained diffusion\nmodel. Only using a tiny neural network, the proposed ZIP produces diverse\ncontent and attributes under the intuitive control of the text prompt.\nMoreover, ZIP shows remarkable robustness for both in-domain and out-of-domain\nattribute manipulation on real images. We perform detailed experiments on\nvarious benchmark datasets. Compared to state-of-the-art methods, ZIP produces\nimages of equivalent quality while providing a realistic editing effect.\n","authors":["Zhanbo Feng","Zenan Ling","Ci Gong","Feng Zhou","Jie Li","Robert C. Qiu"],"pdf_url":"https://arxiv.org/pdf/2308.15854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07138v1","updated":"2023-10-11T02:23:18Z","published":"2023-10-11T02:23:18Z","title":"Denoising Task Routing for Diffusion Models","summary":"  Diffusion models generate highly realistic images through learning a\nmulti-step denoising process, naturally embodying the principles of multi-task\nlearning (MTL). Despite the inherent connection between diffusion models and\nMTL, there remains an unexplored area in designing neural architectures that\nexplicitly incorporate MTL into the framework of diffusion models. In this\npaper, we present Denoising Task Routing (DTR), a simple add-on strategy for\nexisting diffusion model architectures to establish distinct information\npathways for individual tasks within a single architecture by selectively\nactivating subsets of channels in the model. What makes DTR particularly\ncompelling is its seamless integration of prior knowledge of denoising tasks\ninto the framework: (1) Task Affinity: DTR activates similar channels for tasks\nat adjacent timesteps and shifts activated channels as sliding windows through\ntimesteps, capitalizing on the inherent strong affinity between tasks at\nadjacent timesteps. (2) Task Weights: During the early stages (higher\ntimesteps) of the denoising process, DTR assigns a greater number of\ntask-specific channels, leveraging the insight that diffusion models prioritize\nreconstructing global structure and perceptually rich contents in earlier\nstages, and focus on simple noise removal in later stages. Our experiments\ndemonstrate that DTR consistently enhances the performance of diffusion models\nacross various evaluation protocols, all without introducing additional\nparameters. Furthermore, DTR contributes to accelerating convergence during\ntraining. Finally, we show the complementarity between our architectural\napproach and existing MTL optimization techniques, providing a more complete\nview of MTL within the context of diffusion training.\n","authors":["Byeongjun Park","Sangmin Woo","Hyojun Go","Jin-Young Kim","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2310.07138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07131v1","updated":"2023-10-11T02:08:05Z","published":"2023-10-11T02:08:05Z","title":"Echocardiography video synthesis from end diastolic semantic map via\n  diffusion model","summary":"  Denoising Diffusion Probabilistic Models (DDPMs) have demonstrated\nsignificant achievements in various image and video generation tasks, including\nthe domain of medical imaging. However, generating echocardiography videos\nbased on semantic anatomical information remains an unexplored area of\nresearch. This is mostly due to the constraints imposed by the currently\navailable datasets, which lack sufficient scale and comprehensive frame-wise\nannotations for every cardiac cycle. This paper aims to tackle the\naforementioned challenges by expanding upon existing video diffusion models for\nthe purpose of cardiac video synthesis. More specifically, our focus lies in\ngenerating video using semantic maps of the initial frame during the cardiac\ncycle, commonly referred to as end diastole. To further improve the synthesis\nprocess, we integrate spatial adaptive normalization into multiscale feature\nmaps. This enables the inclusion of semantic guidance during synthesis,\nresulting in enhanced realism and coherence of the resultant video sequences.\nExperiments are conducted on the CAMUS dataset, which is a highly used dataset\nin the field of echocardiography. Our model exhibits better performance\ncompared to the standard diffusion technique in terms of multiple metrics,\nincluding FID, FVD, and SSMI.\n","authors":["Phi Nguyen Van","Duc Tran Minh","Hieu Pham Huy","Long Tran Quoc"],"pdf_url":"https://arxiv.org/pdf/2310.07131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01636v2","updated":"2023-10-11T02:02:48Z","published":"2023-10-02T21:02:23Z","title":"Adaptive Visual Scene Understanding: Incremental Scene Graph Generation","summary":"  Scene graph generation (SGG) involves analyzing images to extract meaningful\ninformation about objects and their relationships. Given the dynamic nature of\nthe visual world, it becomes crucial for AI systems to detect new objects and\nestablish their new relationships with existing objects. To address the lack of\ncontinual learning methodologies in SGG, we introduce the comprehensive\nContinual ScenE Graph Generation (CSEGG) dataset along with 3 learning\nscenarios and 8 evaluation metrics. Our research investigates the continual\nlearning performances of existing SGG methods on the retention of previous\nobject entities and relationships as they learn new ones. Moreover, we also\nexplore how continual object detection enhances generalization in classifying\nknown relationships on unknown objects. We conduct extensive experiments\nbenchmarking and analyzing the classical two-stage SGG methods and the most\nrecent transformer-based SGG methods in continual learning settings, and gain\nvaluable insights into the CSEGG problem. We invite the research community to\nexplore this emerging field of study.\n","authors":["Naitik Khandelwal","Xiao Liu","Mengmi Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.01636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15142v4","updated":"2023-10-11T01:33:21Z","published":"2023-06-27T02:03:46Z","title":"LRANet: Towards Accurate and Efficient Scene Text Detection with\n  Low-Rank Approximation Network","summary":"  Recently, regression-based methods, which predict parameterized text shapes\nfor text localization, have gained popularity in scene text detection. However,\nthe existing parameterized text shape methods still have limitations in\nmodeling arbitrary-shaped texts due to ignoring the utilization of\ntext-specific shape information. Moreover, the time consumption of the entire\npipeline has been largely overlooked, leading to a suboptimal overall inference\nspeed. To address these issues, we first propose a novel parameterized text\nshape method based on low-rank approximation. Unlike other shape representation\nmethods that employ data-irrelevant parameterization, our approach utilizes\nsingular value decomposition and reconstructs the text shape using a few\neigenvectors learned from labeled text contours. By exploring the shape\ncorrelation among different text contours, our method achieves consistency,\ncompactness, simplicity, and robustness in shape representation. Next, we\npropose a dual assignment scheme for speed acceleration. It adopts a sparse\nassignment branch to accelerate the inference speed, and meanwhile, provides\nample supervised signals for training through a dense assignment branch.\nBuilding upon these designs, we implement an accurate and efficient\narbitrary-shaped text detector named LRANet. Extensive experiments are\nconducted on several challenging benchmarks, demonstrating the superior\naccuracy and efficiency of LRANet compared to state-of-the-art methods. Code\nwill be released soon.\n","authors":["Yuchen Su","Zhineng Chen","Zhiwen Shao","Yuning Du","Zhilong Ji","Jinfeng Bai","Yong Zhou","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2306.15142v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.14349v3","updated":"2023-10-11T01:30:37Z","published":"2021-06-28T00:34:15Z","title":"PNet -- A Deep Learning Based Photometry and Astrometry Bayesian\n  Framework","summary":"  Time domain astronomy has emerged as a vibrant research field in recent\nyears, focusing on celestial objects that exhibit variable magnitudes or\npositions. Given the urgency of conducting follow-up observations for such\nobjects, the development of an algorithm capable of detecting them and\ndetermining their magnitudes and positions has become imperative. Leveraging\nthe advancements in deep neural networks, we present the PNet, an end-to-end\nframework designed not only to detect celestial objects and extract their\nmagnitudes and positions but also to estimate photometry uncertainty. The PNet\ncomprises two essential steps. Firstly, it detects stars and retrieves their\npositions, magnitudes, and calibrated magnitudes. Subsequently, in the second\nphase, the PNet estimates the uncertainty associated with the photometry\nresults, serving as a valuable reference for the light curve classification\nalgorithm. Our algorithm has been tested using both simulated and real\nobservation data, demonstrating the PNet's ability to deliver consistent and\nreliable outcomes. Integration of the PNet into data processing pipelines for\ntime-domain astronomy holds significant potential for enhancing response speed\nand improving the detection capabilities for celestial objects with variable\npositions and magnitudes.\n","authors":["Rui Sun","Peng Jia","Yongyang Sun","Zhimin Yang","Qiang Liu","Hongyan Wei"],"pdf_url":"https://arxiv.org/pdf/2106.14349v3.pdf","comment":"To be published in the AJ and welcome to any comments"},{"id":"http://arxiv.org/abs/2310.04780v2","updated":"2023-10-11T00:38:50Z","published":"2023-10-07T11:45:33Z","title":"IPMix: Label-Preserving Data Augmentation Method for Training Robust\n  Classifiers","summary":"  Data augmentation has been proven effective for training high-accuracy\nconvolutional neural network classifiers by preventing overfitting. However,\nbuilding deep neural networks in real-world scenarios requires not only high\naccuracy on clean data but also robustness when data distributions shift. While\nprior methods have proposed that there is a trade-off between accuracy and\nrobustness, we propose IPMix, a simple data augmentation approach to improve\nrobustness without hurting clean accuracy. IPMix integrates three levels of\ndata augmentation (image-level, patch-level, and pixel-level) into a coherent\nand label-preserving technique to increase the diversity of training data with\nlimited computational overhead. To further improve the robustness, IPMix\nintroduces structural complexity at different levels to generate more diverse\nimages and adopts the random mixing method for multi-scale information fusion.\nExperiments demonstrate that IPMix outperforms state-of-the-art corruption\nrobustness on CIFAR-C and ImageNet-C. In addition, we show that IPMix also\nsignificantly improves the other safety measures, including robustness to\nadversarial perturbations, calibration, prediction consistency, and anomaly\ndetection, achieving state-of-the-art or comparable results on several\nbenchmarks, including ImageNet-R, ImageNet-A, and ImageNet-O.\n","authors":["Zhenglin Huang","Xianan Bao","Na Zhang","Qingqi Zhang","Xiaomei Tu","Biao Wu","Xi Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/1910.11103v2","updated":"2023-10-11T00:11:45Z","published":"2019-10-16T23:30:22Z","title":"SPEC2: SPECtral SParsE CNN Accelerator on FPGAs","summary":"  To accelerate inference of Convolutional Neural Networks (CNNs), various\ntechniques have been proposed to reduce computation redundancy. Converting\nconvolutional layers into frequency domain significantly reduces the\ncomputation complexity of the sliding window operations in space domain. On the\nother hand, weight pruning techniques address the redundancy in model\nparameters by converting dense convolutional kernels into sparse ones. To\nobtain high-throughput FPGA implementation, we propose SPEC2 -- the first work\nto prune and accelerate spectral CNNs. First, we propose a systematic pruning\nalgorithm based on Alternative Direction Method of Multipliers (ADMM). The\noffline pruning iteratively sets the majority of spectral weights to zero,\nwithout using any handcrafted heuristics. Then, we design an optimized pipeline\narchitecture on FPGA that has efficient random access into the sparse kernels\nand exploits various dimensions of parallelism in convolutional layers.\nOverall, SPEC2 achieves high inference throughput with extremely low\ncomputation complexity and negligible accuracy degradation. We demonstrate\nSPEC2 by pruning and implementing LeNet and VGG16 on the Xilinx Virtex\nplatform. After pruning 75% of the spectral weights, SPEC2 achieves 0% accuracy\nloss for LeNet, and <1% accuracy loss for VGG16. The resulting accelerators\nachieve up to 24x higher throughput, compared with the state-of-the-art FPGA\nimplementations for VGG16.\n","authors":["Yue Niu","Hanqing Zeng","Ajitesh Srivastava","Kartik Lakhotia","Rajgopal Kannan","Yanzhi Wang","Viktor Prasanna"],"pdf_url":"https://arxiv.org/pdf/1910.11103v2.pdf","comment":"This is a 10-page conference paper in 26TH IEEE International\n  Conference On High Performance Computing, Data, and Analytics (HiPC)"},{"id":"http://arxiv.org/abs/2306.06323v2","updated":"2023-10-11T23:40:03Z","published":"2023-06-10T00:27:37Z","title":"Learning Joint Latent Space EBM Prior Model for Multi-layer Generator","summary":"  This paper studies the fundamental problem of learning multi-layer generator\nmodels. The multi-layer generator model builds multiple layers of latent\nvariables as a prior model on top of the generator, which benefits learning\ncomplex data distribution and hierarchical representations. However, such a\nprior model usually focuses on modeling inter-layer relations between latent\nvariables by assuming non-informative (conditional) Gaussian distributions,\nwhich can be limited in model expressivity. To tackle this issue and learn more\nexpressive prior models, we propose an energy-based model (EBM) on the joint\nlatent space over all layers of latent variables with the multi-layer generator\nas its backbone. Such joint latent space EBM prior model captures the\nintra-layer contextual relations at each layer through layer-wise energy terms,\nand latent variables across different layers are jointly corrected. We develop\na joint training scheme via maximum likelihood estimation (MLE), which involves\nMarkov Chain Monte Carlo (MCMC) sampling for both prior and posterior\ndistributions of the latent variables from different layers. To ensure\nefficient inference and learning, we further propose a variational training\nscheme where an inference model is used to amortize the costly posterior MCMC\nsampling. Our experiments demonstrate that the learned model can be expressive\nin generating high-quality images and capturing hierarchical features for\nbetter outlier detection.\n","authors":["Jiali Cui","Ying Nian Wu","Tian Han"],"pdf_url":"https://arxiv.org/pdf/2306.06323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03135v3","updated":"2023-10-11T23:38:36Z","published":"2023-07-06T17:05:26Z","title":"Distilling Large Vision-Language Model with Out-of-Distribution\n  Generalizability","summary":"  Large vision-language models have achieved outstanding performance, but their\nsize and computational requirements make their deployment on\nresource-constrained devices and time-sensitive tasks impractical. Model\ndistillation, the process of creating smaller, faster models that maintain the\nperformance of larger models, is a promising direction towards the solution.\nThis paper investigates the distillation of visual representations in large\nteacher vision-language models into lightweight student models using a small-\nor mid-scale dataset. Notably, this study focuses on open-vocabulary\nout-of-distribution (OOD) generalization, a challenging problem that has been\noverlooked in previous model distillation literature. We propose two principles\nfrom vision and language modality perspectives to enhance student's OOD\ngeneralization: (1) by better imitating teacher's visual representation space,\nand carefully promoting better coherence in vision-language alignment with the\nteacher; (2) by enriching the teacher's language representations with\ninformative and finegrained semantic attributes to effectively distinguish\nbetween different labels. We propose several metrics and conduct extensive\nexperiments to investigate their techniques. The results demonstrate\nsignificant improvements in zero-shot and few-shot student performance on\nopen-vocabulary out-of-distribution classification, highlighting the\neffectiveness of our proposed approaches. Poster:\nhttps://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf\nCode: https://github.com/xuanlinli17/large_vlm_distillation_ood\n","authors":["Xuanlin Li","Yunhao Fang","Minghua Liu","Zhan Ling","Zhuowen Tu","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2307.03135v3.pdf","comment":"Published at International Conference on Computer Vision (ICCV) 2023.\n  Poster at\n  https://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf"},{"id":"http://arxiv.org/abs/2310.07932v1","updated":"2023-10-11T23:04:07Z","published":"2023-10-11T23:04:07Z","title":"What Matters to You? Towards Visual Representation Alignment for Robot\n  Learning","summary":"  When operating in service of people, robots need to optimize rewards aligned\nwith end-user preferences. Since robots will rely on raw perceptual inputs like\nRGB images, their rewards will inevitably use visual representations. Recently\nthere has been excitement in using representations from pre-trained visual\nmodels, but key to making these work in robotics is fine-tuning, which is\ntypically done via proxy tasks like dynamics prediction or enforcing temporal\ncycle-consistency. However, all these proxy tasks bypass the human's input on\nwhat matters to them, exacerbating spurious correlations and ultimately leading\nto robot behaviors that are misaligned with user preferences. In this work, we\npropose that robots should leverage human feedback to align their visual\nrepresentations with the end-user and disentangle what matters for the task. We\npropose Representation-Aligned Preference-based Learning (RAPL), a method for\nsolving the visual representation alignment problem and visual reward learning\nproblem through the lens of preference-based learning and optimal transport.\nAcross experiments in X-MAGICAL and in robotic manipulation, we find that\nRAPL's reward consistently generates preferred robot behaviors with high sample\nefficiency, and shows strong zero-shot generalization when the visual\nrepresentation is learned from a different embodiment than the robot's.\n","authors":["Ran Tian","Chenfeng Xu","Masayoshi Tomizuka","Jitendra Malik","Andrea Bajcsy"],"pdf_url":"https://arxiv.org/pdf/2310.07932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07931v1","updated":"2023-10-11T23:01:29Z","published":"2023-10-11T23:01:29Z","title":"D2 Pruning: Message Passing for Balancing Diversity and Difficulty in\n  Data Pruning","summary":"  Analytical theories suggest that higher-quality data can lead to lower test\nerrors in models trained on a fixed data budget. Moreover, a model can be\ntrained on a lower compute budget without compromising performance if a dataset\ncan be stripped of its redundancies. Coreset selection (or data pruning) seeks\nto select a subset of the training data so as to maximize the performance of\nmodels trained on this subset, also referred to as coreset. There are two\ndominant approaches: (1) geometry-based data selection for maximizing data\ndiversity in the coreset, and (2) functions that assign difficulty scores to\nsamples based on training dynamics. Optimizing for data diversity leads to a\ncoreset that is biased towards easier samples, whereas, selection by difficulty\nranking omits easy samples that are necessary for the training of deep learning\nmodels. This demonstrates that data diversity and importance scores are two\ncomplementary factors that need to be jointly considered during coreset\nselection. We represent a dataset as an undirected graph and propose a novel\npruning algorithm, D2 Pruning, that uses forward and reverse message passing\nover this dataset graph for coreset selection. D2 Pruning updates the\ndifficulty scores of each example by incorporating the difficulty of its\nneighboring examples in the dataset graph. Then, these updated difficulty\nscores direct a graph-based sampling method to select a coreset that\nencapsulates both diverse and difficult regions of the dataset space. We\nevaluate supervised and self-supervised versions of our method on various\nvision and language datasets. Results show that D2 Pruning improves coreset\nselection over previous state-of-the-art methods for up to 70% pruning rates.\nAdditionally, we find that using D2 Pruning for filtering large multimodal\ndatasets leads to increased diversity in the dataset and improved\ngeneralization of pretrained models.\n","authors":["Adyasha Maharana","Prateek Yadav","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2310.07931v1.pdf","comment":"17 pages (Our code is available at\n  https://github.com/adymaharana/d2pruning)"},{"id":"http://arxiv.org/abs/2308.12364v2","updated":"2023-10-11T22:38:38Z","published":"2023-08-23T18:08:32Z","title":"Saliency-based Video Summarization for Face Anti-spoofing","summary":"  With the growing availability of databases for face presentation attack\ndetection, researchers are increasingly focusing on video-based face\nanti-spoofing methods that involve hundreds to thousands of images for training\nthe models. However, there is currently no clear consensus on the optimal\nnumber of frames in a video to improve face spoofing detection. Inspired by the\nvisual saliency theory, we present a video summarization method for face\nanti-spoofing detection that aims to enhance the performance and efficiency of\ndeep learning models by leveraging visual saliency. In particular, saliency\ninformation is extracted from the differences between the Laplacian and Wiener\nfilter outputs of the source images, enabling identification of the most\nvisually salient regions within each frame. Subsequently, the source images are\ndecomposed into base and detail images, enhancing the representation of the\nmost important information. Weighting maps are then computed based on the\nsaliency information, indicating the importance of each pixel in the image. By\nlinearly combining the base and detail images using the weighting maps, the\nmethod fuses the source images to create a single representative image that\nsummarizes the entire video. The key contribution of the proposed method lies\nin demonstrating how visual saliency can be used as a data-centric approach to\nimprove the performance and efficiency for face presentation attack detection.\nBy focusing on the most salient images or regions within the images, a more\nrepresentative and diverse training set can be created, potentially leading to\nmore effective models. To validate the method's effectiveness, a simple CNN-RNN\ndeep learning architecture was used, and the experimental results showcased\nstate-of-the-art performance on five challenging face anti-spoofing datasets\n","authors":["Usman Muhammad","Mourad Oussalah","Jorma Laaksonen"],"pdf_url":"https://arxiv.org/pdf/2308.12364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14616v3","updated":"2023-10-11T22:15:54Z","published":"2023-09-26T02:09:52Z","title":"NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized\n  Device Coordinates Space","summary":"  Monocular 3D Semantic Scene Completion (SSC) has garnered significant\nattention in recent years due to its potential to predict complex semantics and\ngeometry shapes from a single image, requiring no 3D inputs. In this paper, we\nidentify several critical issues in current state-of-the-art methods, including\nthe Feature Ambiguity of projected 2D features in the ray to the 3D space, the\nPose Ambiguity of the 3D convolution, and the Computation Imbalance in the 3D\nconvolution across different depth levels. To address these problems, we devise\na novel Normalized Device Coordinates scene completion network (NDC-Scene) that\ndirectly extends the 2D feature map to a Normalized Device Coordinates (NDC)\nspace, rather than to the world space directly, through progressive restoration\nof the dimension of depth with deconvolution operations. Experiment results\ndemonstrate that transferring the majority of computation from the target 3D\nspace to the proposed normalized device coordinates space benefits monocular\nSSC tasks. Additionally, we design a Depth-Adaptive Dual Decoder to\nsimultaneously upsample and fuse the 2D and 3D feature maps, further improving\noverall performance. Our extensive experiments confirm that the proposed method\nconsistently outperforms state-of-the-art methods on both outdoor SemanticKITTI\nand indoor NYUv2 datasets. Our code are available at\nhttps://github.com/Jiawei-Yao0812/NDCScene.\n","authors":["Jiawei Yao","Chuming Li","Keqiang Sun","Yingjie Cai","Hao Li","Wanli Ouyang","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2309.14616v3.pdf","comment":"Accepted at ICCV 2023. Project page:\n  https://jiawei-yao0812.github.io/NDC-Scene/"},{"id":"http://arxiv.org/abs/2310.07916v1","updated":"2023-10-11T22:04:33Z","published":"2023-10-11T22:04:33Z","title":"Dynamic Appearance Particle Neural Radiance Field","summary":"  Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D\nscenes. Dynamic NeRFs extend this model by capturing time-varying elements,\ntypically using deformation fields. The existing dynamic NeRFs employ a similar\nEulerian representation for both light radiance and deformation fields. This\nleads to a close coupling of appearance and motion and lacks a physical\ninterpretation. In this work, we propose Dynamic Appearance Particle Neural\nRadiance Field (DAP-NeRF), which introduces particle-based representation to\nmodel the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists\nof superposition of a static field and a dynamic field. The dynamic field is\nquantised as a collection of {\\em appearance particles}, which carries the\nvisual information of a small dynamic element in the scene and is equipped with\na motion model. All components, including the static field, the visual features\nand motion models of the particles, are learned from monocular videos without\nany prior geometric knowledge of the scene. We develop an efficient\ncomputational framework for the particle-based model. We also construct a new\ndataset to evaluate motion modelling. Experimental results show that DAP-NeRF\nis an effective technique to capture not only the appearance but also the\nphysically meaningful motions in a 3D dynamic scene.\n","authors":["Ancheng Lin","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2310.07916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07896v1","updated":"2023-10-11T21:07:14Z","published":"2023-10-11T21:07:14Z","title":"NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration","summary":"  Robotic learning for navigation in unfamiliar environments needs to provide\npolicies for both task-oriented navigation (i.e., reaching a goal that the\nrobot has located), and task-agnostic exploration (i.e., searching for a goal\nin a novel setting). Typically, these roles are handled by separate models, for\nexample by using subgoal proposals, planning, or separate navigation\nstrategies. In this paper, we describe how we can train a single unified\ndiffusion policy to handle both goal-directed navigation and goal-agnostic\nexploration, with the latter providing the ability to search novel\nenvironments, and the former providing the ability to reach a user-specified\ngoal once it has been located. We show that this unified policy results in\nbetter overall performance when navigating to visually indicated goals in novel\nenvironments, as compared to approaches that use subgoal proposals from\ngenerative models, or prior methods based on latent variable models. We\ninstantiate our method by using a large-scale Transformer-based policy trained\non data from multiple ground robots, with a diffusion model decoder to flexibly\nhandle both goal-conditioned and goal-agnostic navigation. Our experiments,\nconducted on a real-world mobile robot platform, show effective navigation in\nunseen environments in comparison with five alternative methods, and\ndemonstrate significant improvements in performance and lower collision rates,\ndespite utilizing smaller models than state-of-the-art approaches. For more\nvideos, code, and pre-trained model checkpoints, see\nhttps://general-navigation-models.github.io/nomad/\n","authors":["Ajay Sridhar","Dhruv Shah","Catherine Glossop","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2310.07896v1.pdf","comment":"Project page https://general-navigation-models.github.io/nomad/"},{"id":"http://arxiv.org/abs/2310.07894v1","updated":"2023-10-11T21:04:42Z","published":"2023-10-11T21:04:42Z","title":"Efficient Integrators for Diffusion Generative Models","summary":"  Diffusion models suffer from slow sample generation at inference time.\nTherefore, developing a principled framework for fast deterministic/stochastic\nsampling for a broader class of diffusion models is a promising direction. We\npropose two complementary frameworks for accelerating sample generation in\npre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate\nintegrators generalize DDIM, mapping the reverse diffusion dynamics to a more\namenable space for sampling. In contrast, splitting-based integrators, commonly\nused in molecular dynamics, reduce the numerical simulation error by cleverly\nalternating between numerical updates involving the data and auxiliary\nvariables. After extensively studying these methods empirically and\ntheoretically, we present a hybrid method that leads to the best-reported\nperformance for diffusion models in augmented spaces. Applied to Phase Space\nLangevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and\nstochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network\nfunction evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing\nbaselines, respectively. Our code and model checkpoints will be made publicly\navailable at \\url{https://github.com/mandt-lab/PSLD}.\n","authors":["Kushagra Pandey","Maja Rudolph","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2310.07894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07889v1","updated":"2023-10-11T20:52:30Z","published":"2023-10-11T20:52:30Z","title":"LangNav: Language as a Perceptual Representation for Navigation","summary":"  We explore the use of language as a perceptual representation for\nvision-and-language navigation. Our approach uses off-the-shelf vision systems\n(for image captioning and object detection) to convert an agent's egocentric\npanoramic view at each time step into natural language descriptions. We then\nfinetune a pretrained language model to select an action, based on the current\nview and the trajectory history, that would best fulfill the navigation\ninstructions. In contrast to the standard setup which adapts a pretrained\nlanguage model to work directly with continuous visual features from pretrained\nvision models, our approach instead uses (discrete) language as the perceptual\nrepresentation. We explore two use cases of our language-based navigation\n(LangNav) approach on the R2R vision-and-language navigation benchmark:\ngenerating synthetic trajectories from a prompted large language model (GPT-4)\nwith which to finetune a smaller language model; and sim-to-real transfer where\nwe transfer a policy learned on a simulated environment (ALFRED) to a\nreal-world environment (R2R). Our approach is found to improve upon strong\nbaselines that rely on visual features in settings where only a few gold\ntrajectories (10-100) are available, demonstrating the potential of using\nlanguage as a perceptual representation for navigation tasks.\n","authors":["Bowen Pan","Rameswar Panda","SouYoung Jin","Rogerio Feris","Aude Oliva","Phillip Isola","Yoon Kim"],"pdf_url":"https://arxiv.org/pdf/2310.07889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07887v1","updated":"2023-10-11T20:48:20Z","published":"2023-10-11T20:48:20Z","title":"Unsupervised Structured Noise Removal with Variational Lossy Autoencoder","summary":"  Most unsupervised denoising methods are based on the assumption that imaging\nnoise is either pixel-independent, i.e., spatially uncorrelated, or\nsignal-independent, i.e., purely additive. However, in practice many imaging\nsetups, especially in microscopy, suffer from a combination of signal-dependent\nnoise (e.g. Poisson shot noise) and axis-aligned correlated noise (e.g. stripe\nshaped scanning or readout artifacts). In this paper, we present the first\nunsupervised deep learning-based denoiser that can remove this type of noise\nwithout access to any clean images or a noise model. Unlike self-supervised\ntechniques, our method does not rely on removing pixels by masking or\nsubsampling so can utilize all available information. We implement a\nVariational Autoencoder (VAE) with a specially designed autoregressive decoder\ncapable of modelling the noise component of an image but incapable of\nindependently modelling the underlying clean signal component. As a\nconsequence, our VAE's encoder learns to encode only underlying clean signal\ncontent and to discard imaging noise. We also propose an additional decoder for\nmapping the encoder's latent variables back into image space, thereby sampling\ndenoised images. Experimental results demonstrate that our approach surpasses\nexisting methods for self- and unsupervised image denoising while being robust\nwith respect to the size of the autoregressive receptive field. Code for this\nproject can be found at https://github.com/krulllab/DVLAE.\n","authors":["Benjamin Salmon","Alexander Krull"],"pdf_url":"https://arxiv.org/pdf/2310.07887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07886v1","updated":"2023-10-11T20:48:19Z","published":"2023-10-11T20:48:19Z","title":"A Survey of Feature Types and Their Contributions for Camera Tampering\n  Detection","summary":"  Camera tamper detection is the ability to detect unauthorized and\nunintentional alterations in surveillance cameras by analyzing the video.\nCamera tampering can occur due to natural events or it can be caused\nintentionally to disrupt surveillance. We cast tampering detection as a change\ndetection problem, and perform a review of the existing literature with\nemphasis on feature types. We formulate tampering detection as a time series\nanalysis problem, and design experiments to study the robustness and capability\nof various feature types. We compute ten features on real-world surveillance\nvideo and apply time series analysis to ascertain their predictability, and\ntheir capability to detect tampering. Finally, we quantify the performance of\nvarious time series models using each feature type to detect tampering.\n","authors":["Pranav Mantini","Shishir K. Shah"],"pdf_url":"https://arxiv.org/pdf/2310.07886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16319v2","updated":"2023-10-11T20:33:37Z","published":"2023-05-25T17:59:50Z","title":"Image as First-Order Norm+Linear Autoregression: Unveiling Mathematical\n  Invariance","summary":"  This paper introduces a novel mathematical property applicable to diverse\nimages, referred to as FINOLA (First-Order Norm+Linear Autoregressive). FINOLA\nrepresents each image in the latent space as a first-order autoregressive\nprocess, in which each regression step simply applies a shared linear model on\nthe normalized value of its immediate neighbor. This intriguing property\nreveals a mathematical invariance that transcends individual images. Expanding\nfrom image grids to continuous coordinates, we unveil the presence of two\nunderlying partial differential equations. We validate the FINOLA property from\ntwo distinct angles: image reconstruction and self-supervised learning.\nFirstly, we demonstrate the ability of FINOLA to auto-regress up to a 256x256\nfeature map (the same resolution to the image) from a single vector placed at\nthe center, successfully reconstructing the original image by only using three\n3x3 convolution layers as decoder. Secondly, we leverage FINOLA for\nself-supervised learning by employing a simple masked prediction approach.\nEncoding a single unmasked quadrant block, we autoregressively predict the\nsurrounding masked region. Remarkably, this pre-trained representation proves\nhighly effective in image classification and object detection tasks, even when\nintegrated into lightweight networks, all without the need for extensive\nfine-tuning. The code will be made publicly available.\n","authors":["Yinpeng Chen","Xiyang Dai","Dongdong Chen","Mengchen Liu","Lu Yuan","Zicheng Liu","Youzuo Lin"],"pdf_url":"https://arxiv.org/pdf/2305.16319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01748v2","updated":"2023-10-11T20:28:58Z","published":"2023-03-03T07:20:58Z","title":"A Complete Recipe for Diffusion Generative Models","summary":"  Score-based Generative Models (SGMs) have demonstrated exceptional synthesis\noutcomes across various tasks. However, the current design landscape of the\nforward diffusion process remains largely untapped and often relies on physical\nheuristics or simplifying assumptions. Utilizing insights from the development\nof scalable Bayesian posterior samplers, we present a complete recipe for\nformulating forward processes in SGMs, ensuring convergence to the desired\ntarget distribution. Our approach reveals that several existing SGMs can be\nseen as specific manifestations of our framework. Building upon this method, we\nintroduce Phase Space Langevin Diffusion (PSLD), which relies on score-based\nmodeling within an augmented space enriched by auxiliary variables akin to\nphysical phase space. Empirical results exhibit the superior sample quality and\nimproved speed-quality trade-off of PSLD compared to various competing\napproaches on established image synthesis benchmarks. Remarkably, PSLD achieves\nsample quality akin to state-of-the-art SGMs (FID: 2.10 for unconditional\nCIFAR-10 generation). Lastly, we demonstrate the applicability of PSLD in\nconditional synthesis using pre-trained score networks, offering an appealing\nalternative as an SGM backbone for future advancements. Code and model\ncheckpoints can be accessed at \\url{https://github.com/mandt-lab/PSLD}.\n","authors":["Kushagra Pandey","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2303.01748v2.pdf","comment":"Accepted in ICCV'23 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2210.09222v2","updated":"2023-10-11T19:59:02Z","published":"2022-10-14T08:05:16Z","title":"MMTSA: Multimodal Temporal Segment Attention Network for Efficient Human\n  Activity Recognition","summary":"  Multimodal sensors provide complementary information to develop accurate\nmachine-learning methods for human activity recognition (HAR), but introduce\nsignificantly higher computational load, which reduces efficiency. This paper\nproposes an efficient multimodal neural architecture for HAR using an RGB\ncamera and inertial measurement units (IMUs) called Multimodal Temporal Segment\nAttention Network (MMTSA). MMTSA first transforms IMU sensor data into a\ntemporal and structure-preserving gray-scale image using the Gramian Angular\nField (GAF), representing the inherent properties of human activities. MMTSA\nthen applies a multimodal sparse sampling method to reduce data redundancy.\nLastly, MMTSA adopts an inter-segment attention module for efficient multimodal\nfusion. Using three well-established public datasets, we evaluated MMTSA's\neffectiveness and efficiency in HAR. Results show that our method achieves\nsuperior performance improvements 11.13% of cross-subject F1-score on the MMAct\ndataset than the previous state-of-the-art (SOTA) methods. The ablation study\nand analysis suggest that MMTSA's effectiveness in fusing multimodal data for\naccurate HAR. The efficiency evaluation on an edge device showed that MMTSA\nachieved significantly better accuracy, lower computational load, and lower\ninference latency than SOTA methods.\n","authors":["Ziqi Gao","Yuntao Wang","Jianguo Chen","Junliang Xing","Shwetak Patel","Xin Liu","Yuanchun Shi"],"pdf_url":"https://arxiv.org/pdf/2210.09222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07855v1","updated":"2023-10-11T19:57:51Z","published":"2023-10-11T19:57:51Z","title":"CrIBo: Self-Supervised Learning via Cross-Image Object-Level\n  Bootstrapping","summary":"  Leveraging nearest neighbor retrieval for self-supervised representation\nlearning has proven beneficial with object-centric images. However, this\napproach faces limitations when applied to scene-centric datasets, where\nmultiple objects within an image are only implicitly captured in the global\nrepresentation. Such global bootstrapping can lead to undesirable entanglement\nof object representations. Furthermore, even object-centric datasets stand to\nbenefit from a finer-grained bootstrapping approach. In response to these\nchallenges, we introduce a novel Cross-Image Object-Level Bootstrapping method\ntailored to enhance dense visual representation learning. By employing\nobject-level nearest neighbor bootstrapping throughout the training, CrIBo\nemerges as a notably strong and adequate candidate for in-context learning,\nleveraging nearest neighbor retrieval at test time. CrIBo shows\nstate-of-the-art performance on the latter task while being highly competitive\nin more standard downstream segmentation tasks. Our code and pretrained models\nwill be publicly available upon acceptance.\n","authors":["Tim Lebailly","Thomas Stegmüller","Behzad Bozorgtabar","Jean-Philippe Thiran","Tinne Tuytelaars"],"pdf_url":"https://arxiv.org/pdf/2310.07855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01662v3","updated":"2023-10-11T19:56:13Z","published":"2023-10-02T21:52:47Z","title":"SYRAC: Synthesize, Rank, and Count","summary":"  Crowd counting is a critical task in computer vision, with several important\napplications. However, existing counting methods rely on labor-intensive\ndensity map annotations, necessitating the manual localization of each\nindividual pedestrian. While recent efforts have attempted to alleviate the\nannotation burden through weakly or semi-supervised learning, these approaches\nfall short of significantly reducing the workload. We propose a novel approach\nto eliminate the annotation burden by leveraging latent diffusion models to\ngenerate synthetic data. However, these models struggle to reliably understand\nobject quantities, leading to noisy annotations when prompted to produce images\nwith a specific quantity of objects. To address this, we use latent diffusion\nmodels to create two types of synthetic data: one by removing pedestrians from\nreal images, which generates ranked image pairs with a weak but reliable object\nquantity signal, and the other by generating synthetic images with a\npredetermined number of objects, offering a strong but noisy counting signal.\nOur method utilizes the ranking image pairs for pre-training and then fits a\nlinear layer to the noisy synthetic images using these crowd quantity features.\nWe report state-of-the-art results for unsupervised crowd counting.\n","authors":["Adriano D'Alessandro","Ali Mahdavi-Amiri","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2310.01662v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07814v1","updated":"2023-10-11T18:53:57Z","published":"2023-10-11T18:53:57Z","title":"Explorable Mesh Deformation Subspaces from Unstructured Generative\n  Models","summary":"  Exploring variations of 3D shapes is a time-consuming process in traditional\n3D modeling tools. Deep generative models of 3D shapes often feature continuous\nlatent spaces that can, in principle, be used to explore potential variations\nstarting from a set of input shapes. In practice, doing so can be problematic:\nlatent spaces are high dimensional and hard to visualize, contain shapes that\nare not relevant to the input shapes, and linear paths through them often lead\nto sub-optimal shape transitions. Furthermore, one would ideally be able to\nexplore variations in the original high-quality meshes used to train the\ngenerative model, not its lower-quality output geometry. In this paper, we\npresent a method to explore variations among a given set of landmark shapes by\nconstructing a mapping from an easily-navigable 2D exploration space to a\nsubspace of a pre-trained generative model. We first describe how to find a\nmapping that spans the set of input landmark shapes and exhibits smooth\nvariations between them. We then show how to turn the variations in this\nsubspace into deformation fields, to transfer those variations to high-quality\nmeshes for the landmark shapes. Our results show that our method can produce\nvisually-pleasing and easily-navigable 2D exploration spaces for several\ndifferent shape categories, especially as compared to prior work on learning\ndeformation spaces for 3D shapes.\n","authors":["Arman Maesumi","Paul Guerrero","Vladimir G. Kim","Matthew Fisher","Siddhartha Chaudhuri","Noam Aigerman","Daniel Ritchie"],"pdf_url":"https://arxiv.org/pdf/2310.07814v1.pdf","comment":"SIGGRAPH Asia 2023, 15 pages"},{"id":"http://arxiv.org/abs/2304.13819v2","updated":"2023-10-11T18:39:59Z","published":"2023-04-26T20:42:40Z","title":"MAPConNet: Self-supervised 3D Pose Transfer with Mesh and Point\n  Contrastive Learning","summary":"  3D pose transfer is a challenging generation task that aims to transfer the\npose of a source geometry onto a target geometry with the target identity\npreserved. Many prior methods require keypoint annotations to find\ncorrespondence between the source and target. Current pose transfer methods\nallow end-to-end correspondence learning but require the desired final output\nas ground truth for supervision. Unsupervised methods have been proposed for\ngraph convolutional models but they require ground truth correspondence between\nthe source and target inputs. We present a novel self-supervised framework for\n3D pose transfer which can be trained in unsupervised, semi-supervised, or\nfully supervised settings without any correspondence labels. We introduce two\ncontrastive learning constraints in the latent space: a mesh-level loss for\ndisentangling global patterns including pose and identity, and a point-level\nloss for discriminating local semantics. We demonstrate quantitatively and\nqualitatively that our method achieves state-of-the-art results in supervised\n3D pose transfer, with comparable results in unsupervised and semi-supervised\nsettings. Our method is also generalisable to unseen human and animal data with\ncomplex topologies.\n","authors":["Jiaze Sun","Zhixiang Chen","Tae-Kyun Kim"],"pdf_url":"https://arxiv.org/pdf/2304.13819v2.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2210.14558v2","updated":"2023-10-11T18:28:27Z","published":"2022-10-26T08:25:03Z","title":"Compressing And Debiasing Vision-Language Pre-Trained Models for Visual\n  Question Answering","summary":"  Despite the excellent performance of vision-language pre-trained models\n(VLPs) on conventional VQA task, they still suffer from two problems: First,\nVLPs tend to rely on language biases in datasets and fail to generalize to\nout-of-distribution (OOD) data. Second, they are inefficient in terms of memory\nfootprint and computation. Although promising progress has been made in both\nproblems, most existing works tackle them independently. To facilitate the\napplication of VLP to VQA tasks, it is imperative to jointly study VLP\ncompression and OOD robustness, which, however, has not yet been explored. This\npaper investigates whether a VLP can be compressed and debiased simultaneously\nby searching sparse and robust subnetworks. To this end, we systematically\nstudy the design of a training and compression pipeline to search the\nsubnetworks, as well as the assignment of sparsity to different\nmodality-specific modules. Our experiments involve 3 VLPs, 2 compression\nmethods, 4 training methods, 2 datasets and a range of sparsity levels and\nrandom seeds. Our results show that there indeed exist sparse and robust\nsubnetworks, which are competitive with the debiased full VLP and clearly\noutperform the debiasing SoTAs with fewer parameters on OOD datasets VQA-CP v2\nand VQA-VS. The codes can be found at\nhttps://github.com/PhoebusSi/Compress-Robust-VQA.\n","authors":["Qingyi Si","Yuanxin Liu","Zheng Lin","Peng Fu","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2210.14558v2.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.07794v1","updated":"2023-10-11T18:28:15Z","published":"2023-10-11T18:28:15Z","title":"CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory\n  Prediction Models for Autonomous Driving","summary":"  Benchmarking is a common method for evaluating trajectory prediction models\nfor autonomous driving. Existing benchmarks rely on datasets, which are biased\ntowards more common scenarios, such as cruising, and distance-based metrics\nthat are computed by averaging over all scenarios. Following such a regiment\nprovides a little insight into the properties of the models both in terms of\nhow well they can handle different scenarios and how admissible and diverse\ntheir outputs are. There exist a number of complementary metrics designed to\nmeasure the admissibility and diversity of trajectories, however, they suffer\nfrom biases, such as length of trajectories.\n  In this paper, we propose a new benChmarking paRadIgm for evaluaTing\ntrajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a\nmethod for extracting driving scenarios at varying levels of specificity\naccording to the structure of the roads, models' performance, and data\nproperties for fine-grained ranking of prediction models; 2) A set of new\nbias-free metrics for measuring diversity, by incorporating the characteristics\nof a given scenario, and admissibility, by considering the structure of roads\nand kinematic compliancy, motivated by real-world driving constraints. 3) Using\nthe proposed benchmark, we conduct extensive experimentation on a\nrepresentative set of the prediction models using the large scale Argoverse\ndataset. We show that the proposed benchmark can produce a more accurate\nranking of the models and serve as a means of characterizing their behavior. We\nfurther present ablation studies to highlight contributions of different\nelements that are used to compute the proposed metrics.\n","authors":["Changhe Chen","Mozhgan Pourkeshavarz","Amir Rasouli"],"pdf_url":"https://arxiv.org/pdf/2310.07794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16605v4","updated":"2023-10-11T18:09:59Z","published":"2023-06-29T00:12:21Z","title":"KITE: Keypoint-Conditioned Policies for Semantic Manipulation","summary":"  While natural language offers a convenient shared interface for humans and\nrobots, enabling robots to interpret and follow language commands remains a\nlongstanding challenge in manipulation. A crucial step to realizing a\nperformant instruction-following robot is achieving semantic manipulation,\nwhere a robot interprets language at different specificities, from high-level\ninstructions like \"Pick up the stuffed animal\" to more detailed inputs like\n\"Grab the left ear of the elephant.\" To tackle this, we propose Keypoints +\nInstructions to Execution (KITE), a two-step framework for semantic\nmanipulation which attends to both scene semantics (distinguishing between\ndifferent objects in a visual scene) and object semantics (precisely localizing\ndifferent parts within an object instance). KITE first grounds an input\ninstruction in a visual scene through 2D image keypoints, providing a highly\naccurate object-centric bias for downstream action inference. Provided an RGB-D\nscene observation, KITE then executes a learned keypoint-conditioned skill to\ncarry out the instruction. The combined precision of keypoints and\nparameterized skills enables fine-grained manipulation with generalization to\nscene and object variations. Empirically, we demonstrate KITE in 3 real-world\nenvironments: long-horizon 6-DoF tabletop manipulation, semantic grasping, and\na high-precision coffee-making task. In these settings, KITE achieves a 75%,\n70%, and 71% overall success rate for instruction-following, respectively. KITE\noutperforms frameworks that opt for pre-trained visual language models over\nkeypoint-based grounding, or omit skills in favor of end-to-end visuomotor\ncontrol, all while being trained from fewer or comparable amounts of\ndemonstrations. Supplementary material, datasets, code, and videos can be found\non our website: http://tinyurl.com/kite-site.\n","authors":["Priya Sundaresan","Suneel Belkhale","Dorsa Sadigh","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2306.16605v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07782v1","updated":"2023-10-11T18:07:37Z","published":"2023-10-11T18:07:37Z","title":"An automated approach for improving the inference latency and energy\n  efficiency of pretrained CNNs by removing irrelevant pixels with focused\n  convolutions","summary":"  Computer vision often uses highly accurate Convolutional Neural Networks\n(CNNs), but these deep learning models are associated with ever-increasing\nenergy and computation requirements. Producing more energy-efficient CNNs often\nrequires model training which can be cost-prohibitive. We propose a novel,\nautomated method to make a pretrained CNN more energy-efficient without\nre-training. Given a pretrained CNN, we insert a threshold layer that filters\nactivations from the preceding layers to identify regions of the image that are\nirrelevant, i.e. can be ignored by the following layers while maintaining\naccuracy. Our modified focused convolution operation saves inference latency\n(by up to 25%) and energy costs (by up to 22%) on various popular pretrained\nCNNs, with little to no loss in accuracy.\n","authors":["Caleb Tung","Nicholas Eliopoulos","Purvish Jajal","Gowri Ramshankar","Chen-Yun Yang","Nicholas Synovic","Xuecen Zhang","Vipin Chaudhary","George K. Thiruvathukal","Yung-Hsiang Lu"],"pdf_url":"https://arxiv.org/pdf/2310.07782v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07781v1","updated":"2023-10-11T18:07:19Z","published":"2023-10-11T18:07:19Z","title":"3D TransUNet: Advancing Medical Image Segmentation through Vision\n  Transformers","summary":"  Medical image segmentation plays a crucial role in advancing healthcare\nsystems for disease diagnosis and treatment planning. The u-shaped\narchitecture, popularly known as U-Net, has proven highly successful for\nvarious medical image segmentation tasks. However, U-Net's convolution-based\noperations inherently limit its ability to model long-range dependencies\neffectively. To address these limitations, researchers have turned to\nTransformers, renowned for their global self-attention mechanisms, as\nalternative architectures. One popular network is our previous TransUNet, which\nleverages Transformers' self-attention to complement U-Net's localized\ninformation with the global context. In this paper, we extend the 2D TransUNet\narchitecture to a 3D network by building upon the state-of-the-art nnU-Net\narchitecture, and fully exploring Transformers' potential in both the encoder\nand decoder design. We introduce two key components: 1) A Transformer encoder\nthat tokenizes image patches from a convolution neural network (CNN) feature\nmap, enabling the extraction of global contexts, and 2) A Transformer decoder\nthat adaptively refines candidate regions by utilizing cross-attention between\ncandidate proposals and U-Net features. Our investigations reveal that\ndifferent medical tasks benefit from distinct architectural designs. The\nTransformer encoder excels in multi-organ segmentation, where the relationship\namong organs is crucial. On the other hand, the Transformer decoder proves more\nbeneficial for dealing with small and challenging segmented targets such as\ntumor segmentation. Extensive experiments showcase the significant potential of\nintegrating a Transformer-based encoder and decoder into the u-shaped medical\nimage segmentation architecture. TransUNet outperforms competitors in various\nmedical applications.\n","authors":["Jieneng Chen","Jieru Mei","Xianhang Li","Yongyi Lu","Qihang Yu","Qingyue Wei","Xiangde Luo","Yutong Xie","Ehsan Adeli","Yan Wang","Matthew Lungren","Lei Xing","Le Lu","Alan Yuille","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.07781v1.pdf","comment":"Code and models are available at\n  https://github.com/Beckschen/3D-TransUNet"},{"id":"http://arxiv.org/abs/2112.15202v2","updated":"2023-10-11T18:00:22Z","published":"2021-12-30T20:46:53Z","title":"Visual and Object Geo-localization: A Comprehensive Survey","summary":"  The concept of geo-localization refers to the process of determining where on\nearth some `entity' is located, typically using Global Positioning System (GPS)\ncoordinates. The entity of interest may be an image, sequence of images, a\nvideo, satellite image, or even objects visible within the image. As massive\ndatasets of GPS tagged media have rapidly become available due to smartphones\nand the internet, and deep learning has risen to enhance the performance\ncapabilities of machine learning models, the fields of visual and object\ngeo-localization have emerged due to its significant impact on a wide range of\napplications such as augmented reality, robotics, self-driving vehicles, road\nmaintenance, and 3D reconstruction. This paper provides a comprehensive survey\nof geo-localization involving images, which involves either determining from\nwhere an image has been captured (Image geo-localization) or geo-locating\nobjects within an image (Object geo-localization). We will provide an in-depth\nstudy, including a summary of popular algorithms, a description of proposed\ndatasets, and an analysis of performance results to illustrate the current\nstate of each field.\n","authors":["Daniel Wilson","Xiaohan Zhang","Waqas Sultani","Safwan Wshah"],"pdf_url":"https://arxiv.org/pdf/2112.15202v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07771v1","updated":"2023-10-11T18:00:08Z","published":"2023-10-11T18:00:08Z","title":"DrivingDiffusion: Layout-Guided multi-view driving scene video\n  generation with latent diffusion model","summary":"  With the increasing popularity of autonomous driving based on the powerful\nand unified bird's-eye-view (BEV) representation, a demand for high-quality and\nlarge-scale multi-view video data with accurate annotation is urgently\nrequired. However, such large-scale multi-view data is hard to obtain due to\nexpensive collection and annotation costs. To alleviate the problem, we propose\na spatial-temporal consistent diffusion framework DrivingDiffusion, to generate\nrealistic multi-view videos controlled by 3D layout. There are three challenges\nwhen synthesizing multi-view videos given a 3D layout: How to keep 1)\ncross-view consistency and 2) cross-frame consistency? 3) How to guarantee the\nquality of the generated instances? Our DrivingDiffusion solves the problem by\ncascading the multi-view single-frame image generation step, the single-view\nvideo generation step shared by multiple cameras, and post-processing that can\nhandle long video generation. In the multi-view model, the consistency of\nmulti-view images is ensured by information exchange between adjacent cameras.\nIn the temporal model, we mainly query the information that needs attention in\nsubsequent frame generation from the multi-view images of the first frame. We\nalso introduce the local prompt to effectively improve the quality of generated\ninstances. In post-processing, we further enhance the cross-view consistency of\nsubsequent frames and extend the video length by employing temporal sliding\nwindow algorithm. Without any extra cost, our model can generate large-scale\nrealistic multi-camera driving videos in complex urban scenes, fueling the\ndownstream driving tasks. The code will be made publicly available.\n","authors":["Xiaofan Li","Yifu Zhang","Xiaoqing Ye"],"pdf_url":"https://arxiv.org/pdf/2310.07771v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2310.07749v1","updated":"2023-10-11T17:58:33Z","published":"2023-10-11T17:58:33Z","title":"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation","summary":"  This work investigates a challenging task named open-domain interleaved\nimage-text generation, which generates interleaved texts and images following\nan input query. We propose a new interleaved generation framework based on\nprompting large-language models (LLMs) and pre-trained text-to-image (T2I)\nmodels, namely OpenLEAF. In OpenLEAF, the LLM generates textual descriptions,\ncoordinates T2I models, creates visual prompts for generating images, and\nincorporates global contexts into the T2I models. This global context improves\nthe entity and style consistencies of images in the interleaved generation. For\nmodel assessment, we first propose to use large multi-modal models (LMMs) to\nevaluate the entity and style consistencies of open-domain interleaved\nimage-text sequences. According to the LMM evaluation on our constructed\nevaluation set, the proposed interleaved generation framework can generate\nhigh-quality image-text content for various domains and applications, such as\nhow-to question answering, storytelling, graphical story rewriting, and\nwebpage/poster generation tasks. Moreover, we validate the effectiveness of the\nproposed LMM evaluation technique with human assessment. We hope our proposed\nframework, benchmark, and LMM evaluation could help establish the intriguing\ninterleaved image-text generation task.\n","authors":["Jie An","Zhengyuan Yang","Linjie Li","Jianfeng Wang","Kevin Lin","Zicheng Liu","Lijuan Wang","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2310.07749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11523v2","updated":"2023-10-11T14:51:59Z","published":"2023-09-20T00:57:48Z","title":"RMT: Retentive Networks Meet Vision Transformers","summary":"  Transformer first appears in the field of natural language processing and is\nlater migrated to the computer vision domain, where it demonstrates excellent\nperformance in vision tasks. However, recently, Retentive Network (RetNet) has\nemerged as an architecture with the potential to replace Transformer,\nattracting widespread attention in the NLP community. Therefore, we raise the\nquestion of whether transferring RetNet's idea to vision can also bring\noutstanding performance to vision tasks. To address this, we combine RetNet and\nTransformer to propose RMT. Inspired by RetNet, RMT introduces explicit decay\ninto the vision backbone, bringing prior knowledge related to spatial distances\nto the vision model. This distance-related spatial prior allows for explicit\ncontrol of the range of tokens that each token can attend to. Additionally, to\nreduce the computational cost of global modeling, we decompose this modeling\nprocess along the two coordinate axes of the image. Abundant experiments have\ndemonstrated that our RMT exhibits exceptional performance across various\ncomputer vision tasks. For example, RMT achieves 84.1% Top1-acc on ImageNet-1k\nusing merely 4.5G FLOPs. To the best of our knowledge, among all models, RMT\nachieves the highest Top1-acc when models are of similar size and trained with\nthe same strategy. Moreover, RMT significantly outperforms existing vision\nbackbones in downstream tasks such as object detection, instance segmentation,\nand semantic segmentation. Our work is still in progress.\n","authors":["Qihang Fan","Huaibo Huang","Mingrui Chen","Hongmin Liu","Ran He"],"pdf_url":"https://arxiv.org/pdf/2309.11523v2.pdf","comment":"The work is still in progress"}]},"2023-10-12T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2310.08588v1","updated":"2023-10-12T17:59:58Z","published":"2023-10-12T17:59:58Z","title":"Octopus: Embodied Vision-Language Programmer from Environmental Feedback","summary":"  Large vision-language models (VLMs) have achieved substantial progress in\nmultimodal perception and reasoning. Furthermore, when seamlessly integrated\ninto an embodied agent, it signifies a crucial stride towards the creation of\nautonomous and context-aware systems capable of formulating plans and executing\ncommands with precision. In this paper, we introduce Octopus, a novel VLM\ndesigned to proficiently decipher an agent's vision and textual task objectives\nand to formulate intricate action sequences and generate executable code. Our\ndesign allows the agent to adeptly handle a wide spectrum of tasks, ranging\nfrom mundane daily chores in simulators to sophisticated interactions in\ncomplex video games. Octopus is trained by leveraging GPT-4 to control an\nexplorative agent to generate training data, i.e., action blueprints and the\ncorresponding executable code, within our experimental environment called\nOctoVerse. We also collect the feedback that allows the enhanced training\nscheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a\nseries of experiments, we illuminate Octopus's functionality and present\ncompelling results, and the proposed RLEF turns out to refine the agent's\ndecision-making. By open-sourcing our model architecture, simulator, and\ndataset, we aspire to ignite further innovation and foster collaborative\napplications within the broader embodied AI community.\n","authors":["Jingkang Yang","Yuhao Dong","Shuai Liu","Bo Li","Ziyue Wang","Chencheng Jiang","Haoran Tan","Jiamu Kang","Yuanhan Zhang","Kaiyang Zhou","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2310.08588v1.pdf","comment":"Project Page: https://choiszt.github.io/Octopus/, Codebase:\n  https://github.com/dongyh20/Octopus"},{"id":"http://arxiv.org/abs/2310.08583v1","updated":"2023-10-12T17:59:51Z","published":"2023-10-12T17:59:51Z","title":"Discovering Fatigued Movements for Virtual Character Animation","summary":"  Virtual character animation and movement synthesis have advanced rapidly\nduring recent years, especially through a combination of extensive motion\ncapture datasets and machine learning. A remaining challenge is interactively\nsimulating characters that fatigue when performing extended motions, which is\nindispensable for the realism of generated animations. However, capturing such\nmovements is problematic, as performing movements like backflips with fatigued\nvariations up to exhaustion raises capture cost and risk of injury.\nSurprisingly, little research has been done on faithful fatigue modeling. To\naddress this, we propose a deep reinforcement learning-based approach, which --\nfor the first time in literature -- generates control policies for full-body\nphysically simulated agents aware of cumulative fatigue. For this, we first\nleverage Generative Adversarial Imitation Learning (GAIL) to learn an expert\npolicy for the skill; Second, we learn a fatigue policy by limiting the\ngenerated constant torque bounds based on endurance time to non-linear, state-\nand time-dependent limits in the joint-actuation space using a\nThree-Compartment Controller (3CC) model. Our results demonstrate that agents\ncan adapt to different fatigue and rest rates interactively, and discover\nrealistic recovery strategies without the need for any captured data of\nfatigued movement.\n","authors":["Noshaba Cheema","Rui Xu","Nam Hee Kim","Perttu Hämäläinen","Vladislav Golyanik","Marc Habermann","Christian Theobalt","Philipp Slusallek"],"pdf_url":"https://arxiv.org/pdf/2310.08583v1.pdf","comment":"16 pages, 22 figures. To be published in ACM SIGGRAPH Asia Conference\n  Papers 2023. ACM ISBN 979-8-4007-0315-7/23/12"},{"id":"http://arxiv.org/abs/2310.08582v1","updated":"2023-10-12T17:59:50Z","published":"2023-10-12T17:59:50Z","title":"Tree-Planner: Efficient Close-loop Task Planning with Large Language\n  Models","summary":"  This paper studies close-loop task planning, which refers to the process of\ngenerating a sequence of skills (a plan) to accomplish a specific goal while\nadapting the plan based on real-time observations. Recently, prompting Large\nLanguage Models (LLMs) to generate actions iteratively has become a prevalent\nparadigm due to its superior performance and user-friendliness. However, this\nparadigm is plagued by two inefficiencies: high token consumption and redundant\nerror correction, both of which hinder its scalability for large-scale testing\nand applications. To address these issues, we propose Tree-Planner, which\nreframes task planning with LLMs into three distinct phases: plan sampling,\naction tree construction, and grounded deciding. Tree-Planner starts by using\nan LLM to sample a set of potential plans before execution, followed by the\naggregation of them to form an action tree. Finally, the LLM performs a\ntop-down decision-making process on the tree, taking into account real-time\nenvironmental information. Experiments show that Tree-Planner achieves\nstate-of-the-art performance while maintaining high efficiency. By decomposing\nLLM queries into a single plan-sampling call and multiple grounded-deciding\ncalls, a considerable part of the prompt are less likely to be repeatedly\nconsumed. As a result, token consumption is reduced by 92.2% compared to the\npreviously best-performing model. Additionally, by enabling backtracking on the\naction tree as needed, the correction process becomes more flexible, leading to\na 40.5% decrease in error corrections. Project page:\nhttps://tree-planner.github.io/\n","authors":["Mengkang Hu","Yao Mu","Xinmiao Yu","Mingyu Ding","Shiguang Wu","Wenqi Shao","Qiguang Chen","Bin Wang","Yu Qiao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2310.08582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08581v1","updated":"2023-10-12T17:59:41Z","published":"2023-10-12T17:59:41Z","title":"Universal Visual Decomposer: Long-Horizon Manipulation Made Easy","summary":"  Real-world robotic tasks stretch over extended horizons and encompass\nmultiple stages. Learning long-horizon manipulation tasks, however, is a\nlong-standing challenge, and demands decomposing the overarching task into\nseveral manageable subtasks to facilitate policy learning and generalization to\nunseen tasks. Prior task decomposition methods require task-specific knowledge,\nare computationally intensive, and cannot readily be applied to new tasks. To\naddress these shortcomings, we propose Universal Visual Decomposer (UVD), an\noff-the-shelf task decomposition method for visual long horizon manipulation\nusing pre-trained visual representations designed for robotic control. At a\nhigh level, UVD discovers subgoals by detecting phase shifts in the embedding\nspace of the pre-trained representation. Operating purely on visual\ndemonstrations without auxiliary information, UVD can effectively extract\nvisual subgoals embedded in the videos, while incurring zero additional\ntraining cost on top of standard visuomotor policy training. Goal-conditioned\npolicies learned with UVD-discovered subgoals exhibit significantly improved\ncompositional generalization at test time to unseen tasks. Furthermore,\nUVD-discovered subgoals can be used to construct goal-based reward shaping that\njump-starts temporally extended exploration for reinforcement learning. We\nextensively evaluate UVD on both simulation and real-world tasks, and in all\ncases, UVD substantially outperforms baselines across imitation and\nreinforcement learning settings on in-domain and out-of-domain task sequences\nalike, validating the clear advantage of automated visual task decomposition\nwithin the simple, compact UVD framework.\n","authors":["Zichen Zhang","Yunshuang Li","Osbert Bastani","Abhishek Gupta","Dinesh Jayaraman","Yecheng Jason Ma","Luca Weihs"],"pdf_url":"https://arxiv.org/pdf/2310.08581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08576v1","updated":"2023-10-12T17:59:23Z","published":"2023-10-12T17:59:23Z","title":"Learning to Act from Actionless Videos through Dense Correspondences","summary":"  In this work, we present an approach to construct a video-based robot policy\ncapable of reliably executing diverse tasks across different robots and\nenvironments from few video demonstrations without using any action\nannotations. Our method leverages images as a task-agnostic representation,\nencoding both the state and action information, and text as a general\nrepresentation for specifying robot goals. By synthesizing videos that\n``hallucinate'' robot executing actions and in combination with dense\ncorrespondences between frames, our approach can infer the closed-formed action\nto execute to an environment without the need of any explicit action labels.\nThis unique capability allows us to train the policy solely based on RGB videos\nand deploy learned policies to various robotic tasks. We demonstrate the\nefficacy of our approach in learning policies on table-top manipulation and\nnavigation tasks. Additionally, we contribute an open-source framework for\nefficient video modeling, enabling the training of high-fidelity policy models\nwith four GPUs within a single day.\n","authors":["Po-Chen Ko","Jiayuan Mao","Yilun Du","Shao-Hua Sun","Joshua B. Tenenbaum"],"pdf_url":"https://arxiv.org/pdf/2310.08576v1.pdf","comment":"Project page: https://flow-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2310.08573v1","updated":"2023-10-12T17:57:32Z","published":"2023-10-12T17:57:32Z","title":"PolyTask: Learning Unified Policies through Behavior Distillation","summary":"  Unified models capable of solving a wide variety of tasks have gained\ntraction in vision and NLP due to their ability to share regularities and\nstructures across tasks, which improves individual task performance and reduces\ncomputational footprint. However, the impact of such models remains limited in\nembodied learning problems, which present unique challenges due to\ninteractivity, sample inefficiency, and sequential task presentation. In this\nwork, we present PolyTask, a novel method for learning a single unified model\nthat can solve various embodied tasks through a 'learn then distill' mechanism.\nIn the 'learn' step, PolyTask leverages a few demonstrations for each task to\ntrain task-specific policies. Then, in the 'distill' step, task-specific\npolicies are distilled into a single policy using a new distillation method\ncalled Behavior Distillation. Given a unified policy, individual task behavior\ncan be extracted through conditioning variables. PolyTask is designed to be\nconceptually simple while being able to leverage well-established algorithms in\nRL to enable interactivity, a handful of expert demonstrations to allow for\nsample efficiency, and preventing interactive access to tasks during\ndistillation to enable lifelong learning. Experiments across three simulated\nenvironment suites and a real-robot suite show that PolyTask outperforms prior\nstate-of-the-art approaches in multi-task and lifelong learning settings by\nsignificant margins.\n","authors":["Siddhant Haldar","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2310.08573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08565v1","updated":"2023-10-12T17:54:20Z","published":"2023-10-12T17:54:20Z","title":"Security Considerations in AI-Robotics: A Survey of Current Methods,\n  Challenges, and Opportunities","summary":"  Robotics and Artificial Intelligence (AI) have been inextricably intertwined\nsince their inception. Today, AI-Robotics systems have become an integral part\nof our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These\nsystems are built upon three fundamental architectural elements: perception,\nnavigation and planning, and control. However, while the integration of\nAI-Robotics systems has enhanced the quality our lives, it has also presented a\nserious problem - these systems are vulnerable to security attacks. The\nphysical components, algorithms, and data that make up AI-Robotics systems can\nbe exploited by malicious actors, potentially leading to dire consequences.\nMotivated by the need to address the security concerns in AI-Robotics systems,\nthis paper presents a comprehensive survey and taxonomy across three\ndimensions: attack surfaces, ethical and legal concerns, and Human-Robot\nInteraction (HRI) security. Our goal is to provide users, developers and other\nstakeholders with a holistic understanding of these areas to enhance the\noverall AI-Robotics system security. We begin by surveying potential attack\nsurfaces and provide mitigating defensive strategies. We then delve into\nethical issues, such as dependency and psychological impact, as well as the\nlegal concerns regarding accountability for these systems. Besides, emerging\ntrends such as HRI are discussed, considering privacy, integrity, safety,\ntrustworthiness, and explainability concerns. Finally, we present our vision\nfor future research directions in this dynamic and promising field.\n","authors":["Subash Neupane","Shaswata Mitra","Ivan A. Fernandez","Swayamjit Saha","Sudip Mittal","Jingdao Chen","Nisha Pillai","Shahram Rahimi"],"pdf_url":"https://arxiv.org/pdf/2310.08565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08558v1","updated":"2023-10-12T17:50:09Z","published":"2023-10-12T17:50:09Z","title":"Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate\n  Exploration Bias","summary":"  It is desirable for policies to optimistically explore new states and\nbehaviors during online reinforcement learning (RL) or fine-tuning, especially\nwhen prior offline data does not provide enough state coverage. However,\nexploration bonuses can bias the learned policy, and our experiments find that\nnaive, yet standard use of such bonuses can fail to recover a performant\npolicy. Concurrently, pessimistic training in offline RL has enabled recovery\nof performant policies from static datasets. Can we leverage offline RL to\nrecover better policies from online interaction? We make a simple observation\nthat a policy can be trained from scratch on all interaction data with\npessimistic objectives, thereby decoupling the policies used for data\ncollection and for evaluation. Specifically, we propose offline retraining, a\npolicy extraction step at the end of online fine-tuning in our\nOffline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL).\nAn optimistic (exploration) policy is used to interact with the environment,\nand a separate pessimistic (exploitation) policy is trained on all the observed\ndata for evaluation. Such decoupling can reduce any bias from online\ninteraction (intrinsic rewards, primacy bias) in the evaluation policy, and can\nallow more exploratory behaviors during online interaction which in turn can\ngenerate better data for exploitation. OOO is complementary to several\noffline-to-online RL and online RL methods, and improves their average\nperformance by 14% to 26% in our fine-tuning experiments, achieves\nstate-of-the-art performance on several environments in the D4RL benchmarks,\nand improves online RL performance by 165% on two OpenAI gym environments.\nFurther, OOO can enable fine-tuning from incomplete offline datasets where\nprior methods can fail to recover a performant policy. Implementation:\nhttps://github.com/MaxSobolMark/OOO\n","authors":["Max Sobol Mark","Archit Sharma","Fahim Tajwar","Rafael Rafailov","Sergey Levine","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2310.08558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04370v2","updated":"2023-10-12T17:48:22Z","published":"2023-09-08T15:02:46Z","title":"Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control","summary":"  Seeing-eye robots are very useful tools for guiding visually impaired people,\npotentially producing a huge societal impact given the low availability and\nhigh cost of real guide dogs. Although a few seeing-eye robot systems have\nalready been demonstrated, none considered external tugs from humans, which\nfrequently occur in a real guide dog setting. In this paper, we simultaneously\ntrain a locomotion controller that is robust to external tugging forces via\nReinforcement Learning (RL), and an external force estimator via supervised\nlearning. The controller ensures stable walking, and the force estimator\nenables the robot to respond to the external forces from the human. These\nforces are used to guide the robot to the global goal, which is unknown to the\nrobot, while the robot guides the human around nearby obstacles via a local\nplanner. Experimental results in simulation and on hardware show that our\ncontroller is robust to external forces, and our seeing-eye system can\naccurately detect force direction. We demonstrate our full seeing-eye robot\nsystem on a real quadruped robot with a blindfolded human. The video can be\nseen at our project page: https://bu-air-lab.github.io/guide_dog/\n","authors":["David DeFazio","Eisuke Hirota","Shiqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.04370v2.pdf","comment":"Accepted to CoRL 2023"},{"id":"http://arxiv.org/abs/2310.08549v1","updated":"2023-10-12T17:45:05Z","published":"2023-10-12T17:45:05Z","title":"Cross-Episodic Curriculum for Transformer Agents","summary":"  We present a new algorithm, Cross-Episodic Curriculum (CEC), to boost the\nlearning efficiency and generalization of Transformer agents. Central to CEC is\nthe placement of cross-episodic experiences into a Transformer's context, which\nforms the basis of a curriculum. By sequentially structuring online learning\ntrials and mixed-quality demonstrations, CEC constructs curricula that\nencapsulate learning progression and proficiency increase across episodes. Such\nsynergy combined with the potent pattern recognition capabilities of\nTransformer models delivers a powerful cross-episodic attention mechanism. The\neffectiveness of CEC is demonstrated under two representative scenarios: one\ninvolving multi-task reinforcement learning with discrete control, such as in\nDeepMind Lab, where the curriculum captures the learning progression in both\nindividual and progressively complex settings; and the other involving\nimitation learning with mixed-quality data for continuous control, as seen in\nRoboMimic, where the curriculum captures the improvement in demonstrators'\nexpertise. In all instances, policies resulting from CEC exhibit superior\nperformance and strong generalization. Code is open-sourced at\nhttps://cec-agent.github.io/ to facilitate research on Transformer agent\nlearning.\n","authors":["Lucy Xiaoyang Shi","Yunfan Jiang","Jake Grigsby","Linxi \"Jim\" Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2310.08549v1.pdf","comment":"To appear in NeurIPS 2023; The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2310.08494v1","updated":"2023-10-12T16:52:00Z","published":"2023-10-12T16:52:00Z","title":"An Experience-based TAMP Framework for Foliated Manifolds","summary":"  Due to their complexity, foliated structure problems often pose intricate\nchallenges to task and motion planning in robotics manipulation. To counter\nthis, our study presents the ``Foliated Repetition Roadmap.'' This roadmap\nassists task and motion planners by transforming the complex foliated structure\nproblem into a more accessible graph format. By leveraging query experiences\nfrom different foliated manifolds, our framework can dynamically and\nefficiently update this graph. The refined graph can generate distribution\nsets, optimizing motion planning performance in foliated structure problems. In\nour paper, we lay down the theoretical groundwork and illustrate its practical\napplications through real-world examples.\n","authors":["Jiaming Hu","Shrutheesh R. Iyer","Henrik I. Christensen"],"pdf_url":"https://arxiv.org/pdf/2310.08494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08435v1","updated":"2023-10-12T15:57:37Z","published":"2023-10-12T15:57:37Z","title":"MUN-FRL: A Visual Inertial LiDAR Dataset for Aerial Autonomous\n  Navigation and Mapping","summary":"  This paper presents a unique outdoor aerial visual-inertial-LiDAR dataset\ncaptured using a multi-sensor payload to promote the global navigation\nsatellite system (GNSS)-denied navigation research. The dataset features flight\ndistances ranging from 300m to 5km, collected using a DJI M600 hexacopter drone\nand the National Research Council (NRC) Bell 412 Advanced Systems Research\nAircraft (ASRA). The dataset consists of hardware synchronized monocular\nimages, IMU measurements, 3D LiDAR point-clouds, and high-precision real-time\nkinematic (RTK)-GNSS based ground truth. Ten datasets were collected as ROS\nbags over 100 mins of outdoor environment footage ranging from urban areas,\nhighways, hillsides, prairies, and waterfronts. The datasets were collected to\nfacilitate the development of visual-inertial-LiDAR odometry and mapping\nalgorithms, visual-inertial navigation algorithms, object detection,\nsegmentation, and landing zone detection algorithms based upon real-world drone\nand full-scale helicopter data. All the datasets contain raw sensor\nmeasurements, hardware timestamps, and spatio-temporally aligned ground truth.\nThe intrinsic and extrinsic calibrations of the sensors are also provided along\nwith raw calibration datasets. A performance summary of state-of-the-art\nmethods applied on the datasets is also provided.\n","authors":["Ravindu G. Thalagala","Sahan M. Gunawardena","Oscar De Silva","Awantha Jayasiri","Arthur Gubbels","George K. I Mann","Raymond G. Gosine"],"pdf_url":"https://arxiv.org/pdf/2310.08435v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06933v2","updated":"2023-10-12T15:13:30Z","published":"2023-10-10T18:42:55Z","title":"Eclares: Energy-Aware Clarity-Driven Ergodic Search","summary":"  Planning informative trajectories while considering the spatial distribution\nof the information over the environment, as well as constraints such as the\nrobot's limited battery capacity, makes the long-time horizon persistent\ncoverage problem complex. Ergodic search methods consider the spatial\ndistribution of environmental information while optimizing robot trajectories;\nhowever, current methods lack the ability to construct the target information\nspatial distribution for environments that vary stochastically across space and\ntime. Moreover, current coverage methods dealing with battery capacity\nconstraints either assume simple robot and battery models, or are\ncomputationally expensive. To address these problems, we propose a framework\ncalled Eclares, in which our contribution is two-fold. 1) First, we propose a\nmethod to construct the target information spatial distribution for ergodic\ntrajectory optimization using clarity, an information measure bounded between\n[0,1]. The clarity dynamics allows us to capture information decay due to lack\nof measurements and to quantify the maximum attainable information in\nstochastic spatiotemporal environments. 2) Second, instead of directly tracking\nthe ergodic trajectory, we introduce the energy-aware (eware) filter, which\niteratively validates the ergodic trajectory to ensure that the robot has\nenough energy to return to the charging station when needed. The proposed eware\nfilter is applicable to nonlinear robot models and is computationally\nlightweight. We demonstrate the working of the framework through a simulation\ncase study.\n","authors":["Kaleb Ben Naveed","Devansh Agrawal","Christopher Vermillion","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2310.06933v2.pdf","comment":"Submitted to International Conference of Robotics and Automation\n  (ICRA) 2024"},{"id":"http://arxiv.org/abs/2310.08398v1","updated":"2023-10-12T15:09:12Z","published":"2023-10-12T15:09:12Z","title":"Towards Design and Development of an ArUco Markers-Based Quantitative\n  Surface Tactile Sensor","summary":"  In this paper, with the goal of quantifying the qualitative image outputs of\na Vision-based Tactile Sensor (VTS), we present the design, fabrication, and\ncharacterization of a novel Quantitative Surface Tactile Sensor (called QS-TS).\nQS-TS directly estimates the sensor's gel layer deformation in real-time\nenabling safe and autonomous tactile manipulation and servoing of delicate\nobjects using robotic manipulators. The core of the proposed sensor is the\nutilization of miniature 1.5 mm x 1.5 mm synthetic square markers with inner\nbinary patterns and a broad black border, called ArUco Markers. Each ArUco\nmarker can provide real-time camera pose estimation that, in our design, is\nused as a quantitative measure for obtaining deformation of the QS-TS gel\nlayer. Moreover, thanks to the use of ArUco markers, we propose a unique\nfabrication procedure that mitigates various challenges associated with the\nfabrication of the existing marker-based VTSs and offers an intuitive and\nless-arduous method for the construction of the VTS. Remarkably, the proposed\nfabrication facilitates the integration and adherence of markers with the gel\nlayer to robustly and reliably obtain a quantitative measure of deformation in\nreal-time regardless of the orientation of ArUco Markers. The performance and\nefficacy of the proposed QS-TS in estimating the deformation of the sensor's\ngel layer were experimentally evaluated and verified. Results demonstrate the\nphenomenal performance of the QS-TS in estimating the deformation of the gel\nlayer with a relative error of <5%.\n","authors":["Ozdemir Can Kara","Charles Everson","Farshid Alambeigi"],"pdf_url":"https://arxiv.org/pdf/2310.08398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08396v1","updated":"2023-10-12T15:08:36Z","published":"2023-10-12T15:08:36Z","title":"Uncertainty-Aware Planning for Heterogeneous Robot Teams using Dynamic\n  Topological Graphs and Mixed-Integer Programming","summary":"  Planning under uncertainty is a fundamental challenge in robotics. For\nmulti-robot teams, the challenge is further exacerbated, since the planning\nproblem can quickly become computationally intractable as the number of robots\nincrease. In this paper, we propose a novel approach for planning under\nuncertainty using heterogeneous multi-robot teams. In particular, we leverage\nthe notion of a dynamic topological graph and mixed-integer programming to\ngenerate multi-robot plans that deploy fast scout team members to reduce\nuncertainty about the environment. We test our approach in a number of\nrepresentative scenarios where the robot team must move through an environment\nwhile minimizing detection in the presence of uncertain observer positions. We\ndemonstrate that our approach is sufficiently computationally tractable for\nreal-time re-planning in changing environments, can improve performance in the\npresence of imperfect information, and can be adjusted to accommodate different\nrisk profiles.\n","authors":["Cora A. Dimmig","Kevin C. Wolfe","Marin Kobilarov","Joseph Moore"],"pdf_url":"https://arxiv.org/pdf/2310.08396v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2310.08350v1","updated":"2023-10-12T14:18:47Z","published":"2023-10-12T14:18:47Z","title":"ALPHA: Attention-based Long-horizon Pathfinding in Highly-structured\n  Areas","summary":"  The multi-agent pathfinding (MAPF) problem seeks collision-free paths for a\nteam of agents from their current positions to their pre-set goals in a known\nenvironment, and is an essential problem found at the core of many logistics,\ntransportation, and general robotics applications. Existing learning-based MAPF\napproaches typically only let each agent make decisions based on a limited\nfield-of-view (FOV) around its position, as a natural means to fix the input\ndimensions of its policy network. However, this often makes policies\nshort-sighted, since agents lack the ability to perceive and plan for\nobstacles/agents beyond their FOV. To address this challenge, we propose ALPHA,\na new framework combining the use of ground truth proximal (local) information\nand fuzzy distal (global) information to let agents sequence local decisions\nbased on the full current state of the system, and avoid such myopicity. We\nfurther allow agents to make short-term predictions about each others' paths,\nas a means to reason about each others' path intentions, thereby enhancing the\nlevel of cooperation among agents at the whole system level. Our neural\nstructure relies on a Graph Transformer architecture to allow agents to\nselectively combine these different sources of information and reason about\ntheir inter-dependencies at different spatial scales. Our simulation\nexperiments demonstrate that ALPHA outperforms both globally-guided MAPF\nsolvers and communication-learning based ones, showcasing its potential towards\nscalability in realistic deployments.\n","authors":["Chengyang He","Tianze Yang","Tanishq Duhan","Yutong Wang","Guillaume Sartoretti"],"pdf_url":"https://arxiv.org/pdf/2310.08350v1.pdf","comment":"Submitted to the IEEE International Conference on Robotics and\n  Automation (ICRA 2024)"},{"id":"http://arxiv.org/abs/2310.08308v1","updated":"2023-10-12T13:13:59Z","published":"2023-10-12T13:13:59Z","title":"Multicriteria Optimization of Lower Limb Exoskeleton Mechanism","summary":"  Typical leg exoskeletons employ open-loop kinematic chains with motors placed\ndirectly on movable joints; while this design offers flexibility, it leads to\nincreased costs and heightened control complexity due to the high number of\ndegrees of freedom. The use of heavy servo-motors to handle torque in active\njoints results in complex and bulky designs, as highlighted in existing\nliterature. In this study, we introduced a novel synthesis method with\nanalytical solutions provided for synthesizing lower-limb exoskeleton.\nAdditionally, we have incorporated multicriteria optimization by six designing\ncriteria. As a result, we offer several mechanisms, comprising only six links,\nwell-suited to the human anatomical structure, exhibit superior trajectory\naccuracy, efficient force transmission, satisfactory step height, and having\ninternal transfer segment of the foot.\n","authors":["Sayat Ibrayev","Arman Ibrayeva","Ayaulym Rakhmatullina","Aizhan Ibrayeva","Bekzat Amanov","Nurbibi Imanbayeva"],"pdf_url":"https://arxiv.org/pdf/2310.08308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04874v3","updated":"2023-10-12T12:56:33Z","published":"2023-10-07T17:08:22Z","title":"AirIMU: Learning Uncertainty Propagation for Inertial Odometry","summary":"  Accurate uncertainty estimation for inertial odometry is the foundation to\nachieve optimal fusion in multi-sensor systems, such as visual or LiDAR\ninertial odometry. Prior studies often simplify the assumptions regarding the\nuncertainty of inertial measurements, presuming fixed covariance parameters and\nempirical IMU sensor models. However, the inherent physical limitations and\nnon-linear characteristics of sensors are difficult to capture. Moreover,\nuncertainty may fluctuate based on sensor rates and motion modalities, leading\nto variations across different IMUs. To address these challenges, we formulate\na learning-based method that not only encapsulate the non-linearities inherent\nto IMUs but also ensure the accurate propagation of covariance in a data-driven\nmanner. We extend the PyPose library to enable differentiable batched IMU\nintegration with covariance propagation on manifolds, leading to significant\nruntime speedup. To demonstrate our method's adaptability, we evaluate it on\nseveral benchmarks as well as a large-scale helicopter dataset spanning over\n262 kilometers. The drift rate of the inertial odometry on these datasets is\nreduced by a factor of between 2.2 and 4 times. Our method lays the groundwork\nfor advanced developments in inertial odometry.\n","authors":["Yuheng Qiu","Chen Wang","Xunfei Zhou","Youjie Xia","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2310.04874v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.00480v2","updated":"2023-10-12T12:27:07Z","published":"2023-09-01T14:17:02Z","title":"Learning-based NLOS Detection and Uncertainty Prediction of GNSS\n  Observations with Transformer-Enhanced LSTM Network","summary":"  The global navigation satellite systems (GNSS) play a vital role in transport\nsystems for accurate and consistent vehicle localization. However, GNSS\nobservations can be distorted due to multipath effects and non-line-of-sight\n(NLOS) receptions in challenging environments such as urban canyons. In such\ncases, traditional methods to classify and exclude faulty GNSS observations may\nfail, leading to unreliable state estimation and unsafe system operations. This\nwork proposes a deep-learning-based method to detect NLOS receptions and\npredict GNSS pseudorange errors by analyzing GNSS observations as a\nspatio-temporal modeling problem. Compared to previous works, we construct a\ntransformer-like attention mechanism to enhance the long short-term memory\n(LSTM) networks, improving model performance and generalization. For the\ntraining and evaluation of the proposed network, we used labeled datasets from\nthe cities of Hong Kong and Aachen. We also introduce a dataset generation\nprocess to label the GNSS observations using lidar maps. In experimental\nstudies, we compare the proposed network with a deep-learning-based model and\nclassical machine-learning models. Furthermore, we conduct ablation studies of\nour network components and integrate the NLOS detection with data\nout-of-distribution in a state estimator. As a result, our network presents\nimproved precision and recall ratios compared to other models. Additionally, we\nshow that the proposed method avoids trajectory divergence in real-world\nvehicle localization by classifying and excluding NLOS observations.\n","authors":["Haoming Zhang","Zhanxin Wang","Heike Vallery"],"pdf_url":"https://arxiv.org/pdf/2309.00480v2.pdf","comment":"Accepted for the IEEE ITSC2023"},{"id":"http://arxiv.org/abs/2310.08270v1","updated":"2023-10-12T12:18:19Z","published":"2023-10-12T12:18:19Z","title":"Hilbert Space Embedding-based Trajectory Optimization for Multi-Modal\n  Uncertain Obstacle Trajectory Prediction","summary":"  Safe autonomous driving critically depends on how well the ego-vehicle can\npredict the trajectories of neighboring vehicles. To this end, several\ntrajectory prediction algorithms have been presented in the existing\nliterature. Many of these approaches output a multi-modal distribution of\nobstacle trajectories instead of a single deterministic prediction to account\nfor the underlying uncertainty. However, existing planners cannot handle the\nmulti-modality based on just sample-level information of the predictions. With\nthis motivation, this paper proposes a trajectory optimizer that can leverage\nthe distributional aspects of the prediction in a computationally tractable and\nsample-efficient manner. Our optimizer can work with arbitrarily complex\ndistributions and thus can be used with output distribution represented as a\ndeep neural network. The core of our approach is built on embedding\ndistribution in Reproducing Kernel Hilbert Space (RKHS), which we leverage in\ntwo ways. First, we propose an RKHS embedding approach to select probable\nsamples from the obstacle trajectory distribution. Second, we rephrase\nchance-constrained optimization as distribution matching in RKHS and propose a\nnovel sampling-based optimizer for its solution. We validate our approach with\nhand-crafted and neural network-based predictors trained on real-world datasets\nand show improvement over the existing stochastic optimization approaches in\nsafety metrics.\n","authors":["Basant Sharma","Aditya Sharma","K. Madhava Krishna","Arun Kumar Singh"],"pdf_url":"https://arxiv.org/pdf/2310.08270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08233v1","updated":"2023-10-12T11:27:01Z","published":"2023-10-12T11:27:01Z","title":"The Impact of Time Step Frequency on the Realism of Robotic Manipulation\n  Simulation for Objects of Different Scales","summary":"  This work evaluates the impact of time step frequency and component scale on\nrobotic manipulation simulation accuracy. Increasing the time step frequency\nfor small-scale objects is shown to improve simulation accuracy. This\nsimulation, demonstrating pre-assembly part picking for two object geometries,\nserves as a starting point for discussing how to improve Sim2Real transfer in\nrobotic assembly processes.\n","authors":["Minh Q. Ta","Holly Dinkel","Hameed Abdul-Rashid","Yangfei Dai","Jessica Myers","Tan Chen","Junyi Geng","Timothy Bretl"],"pdf_url":"https://arxiv.org/pdf/2310.08233v1.pdf","comment":"3 pages, 3 figures, Best Poster Finalist at the 2023 Robotics and AI\n  in Future Factory Workshop at the IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS). Video presentation\n  [https://www.youtube.com/watch?v=JOXrBpMmI0A]. Robotics and AI in Future\n  Factory workshop [https://sites.google.com/view/robot-ai-future-factory/]"},{"id":"http://arxiv.org/abs/2309.16292v2","updated":"2023-10-12T11:11:47Z","published":"2023-09-28T09:41:35Z","title":"DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large\n  Language Models","summary":"  Recent advancements in autonomous driving have relied on data-driven\napproaches, which are widely adopted but face challenges including dataset\nbias, overfitting, and uninterpretability. Drawing inspiration from the\nknowledge-driven nature of human driving, we explore the question of how to\ninstill similar capabilities into autonomous driving systems and summarize a\nparadigm that integrates an interactive environment, a driver agent, as well as\na memory component to address this question. Leveraging large language models\nwith emergent abilities, we propose the DiLu framework, which combines a\nReasoning and a Reflection module to enable the system to perform\ndecision-making based on common-sense knowledge and evolve continuously.\nExtensive experiments prove DiLu's capability to accumulate experience and\ndemonstrate a significant advantage in generalization ability over\nreinforcement learning-based methods. Moreover, DiLu is able to directly\nacquire experiences from real-world datasets which highlights its potential to\nbe deployed on practical autonomous driving systems. To the best of our\nknowledge, we are the first to instill knowledge-driven capability into\nautonomous driving systems from the perspective of how humans drive.\n","authors":["Licheng Wen","Daocheng Fu","Xin Li","Xinyu Cai","Tao Ma","Pinlong Cai","Min Dou","Botian Shi","Liang He","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2309.16292v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07312v2","updated":"2023-10-12T10:56:20Z","published":"2023-03-13T17:33:47Z","title":"Enhancing LiDAR performance: Robust De-skewing Exclusively Relying on\n  Range Measurements","summary":"  Most commercially available Light Detection and Ranging (LiDAR)s measure the\ndistances along a 2D section of the environment by sequentially sampling the\nfree range along directions centered at the sensor's origin. When the sensor\nmoves during the acquisition, the measured ranges are affected by a phenomenon\nknown as \"skewing\", which appears as a distortion in the acquired scan. Skewing\npotentially affects all systems that rely on LiDAR data, however, it could be\ncompensated if the position of the sensor were known each time a single range\nis measured. Most methods to de-skew a LiDAR are based on external sensors such\nas IMU or wheel odometry, to estimate these intermediate LiDAR positions. In\nthis paper, we present a method that relies exclusively on range measurements\nto effectively estimate the robot velocities which are then used for\nde-skewing. Our approach is suitable for low-frequency LiDAR where the skewing\nis more evident. It can be seamlessly integrated into existing pipelines,\nenhancing their performance at a negligible computational cost.\n","authors":["Omar Salem","Emanuele Giacomini","Leonardo Brizi","Luca Di Giammarino","Giorgio Grisetti"],"pdf_url":"https://arxiv.org/pdf/2303.07312v2.pdf","comment":"6 pages , 5 figures"},{"id":"http://arxiv.org/abs/2310.08192v1","updated":"2023-10-12T10:36:45Z","published":"2023-10-12T10:36:45Z","title":"Slip Detection and Surface Prediction Through Bio-Inspired Tactile\n  Feedback","summary":"  High resolution tactile sensing has great potential in autonomous mobile\nrobotics, particularly for legged robots. One particular area where it has\nsignificant promise is the traversal of challenging, varied terrain. Depending\non whether an environment is slippery, soft, hard or dry, a robot must adapt\nits method of locomotion accordingly. Currently many multi-legged robots, such\nas Boston Dynamic's Spot robot, have preset gaits for different surface types,\nbut struggle over terrains where the surface type changes frequently. Being\nable to automatically detect changes within an environment would allow a robot\nto autonomously adjust its method of locomotion to better suit conditions,\nwithout requiring a human user to manually set the change in surface type. In\nthis paper we report on the first detailed investigation of the properties of a\nparticular bio-inspired tactile sensor, the TacTip, to test its suitability for\nthis kind of automatic detection of surface conditions. We explored different\nprocessing techniques and a regression model, using a custom made rig for data\ncollection to determine how a robot could sense directional and general force\non the sensor in a variety of conditions. This allowed us to successfully\ndemonstrate how the sensor can be used to distinguish between soft, hard, dry\nand (wet) slippery surfaces. We further explored a neural model to classify\nspecific surface textures. Pin movement (the movement of optical markers within\nthe sensor) was key to sensing this information, and all models relied on some\nform of temporal information. Our final trained models could successfully\ndetermine the direction the sensor is heading in, the amount of force acting on\nit, and determine differences in the surface texture such as Lego vs smooth\nhard surface, or concrete vs smooth hard surface.\n","authors":["Dexter R. Shepherd","Phil Husbands","Andy Philippides","Chris Johnson"],"pdf_url":"https://arxiv.org/pdf/2310.08192v1.pdf","comment":"6 pages + references, under review for ICRA 2024"},{"id":"http://arxiv.org/abs/2307.12664v2","updated":"2023-10-12T08:28:20Z","published":"2023-07-24T10:10:24Z","title":"SafeSteps: Learning Safer Footstep Planning Policies for Legged Robots\n  via Model-Based Priors","summary":"  We present a footstep planning policy for quadrupedal locomotion that is able\nto directly take into consideration a-priori safety information in its\ndecisions. At its core, a learning process analyzes terrain patches,\nclassifying each landing location by its kinematic feasibility, shin collision,\nand terrain roughness. This information is then encoded into a small vector\nrepresentation and passed as an additional state to the footstep planning\npolicy, which furthermore proposes only safe footstep location by applying a\nmasked variant of the Proximal Policy Optimization algorithm. The performance\nof the proposed approach is shown by comparative simulations and experiments on\nan electric quadruped robot walking in different rough terrain scenarios. We\nshow that violations of the above safety conditions are greatly reduced both\nduring training and the successive deployment of the policy, resulting in an\ninherently safer footstep planner. Furthermore, we show how, as a byproduct,\nfewer reward terms are needed to shape the behavior of the policy, which in\nreturn is able to achieve both better final performances and sample efficiency.\n","authors":["Shafeef Omar","Lorenzo Amatucci","Victor Barasuol","Giulio Turrisi","Claudio Semini"],"pdf_url":"https://arxiv.org/pdf/2307.12664v2.pdf","comment":"Accepted for publication at the 2023 IEEE-RAS International\n  Conference on Humanoid Robots (Humanoids)"},{"id":"http://arxiv.org/abs/2310.08116v1","updated":"2023-10-12T08:17:57Z","published":"2023-10-12T08:17:57Z","title":"Multimodal Active Measurement for Human Mesh Recovery in Close Proximity","summary":"  For safe and sophisticated physical human-robot interactions (pHRI), a robot\nneeds to estimate the accurate body pose or mesh of the target person. However,\nin these pHRI scenarios, the robot cannot fully observe the target person's\nbody with equipped cameras because the target person is usually close to the\nrobot. This leads to severe truncation and occlusions, and results in poor\naccuracy of human pose estimation. For better accuracy of human pose estimation\nor mesh recovery on this limited information from cameras, we propose an active\nmeasurement and sensor fusion framework of the equipped cameras and other\nsensors such as touch sensors and 2D LiDAR. These touch and LiDAR sensing are\nobtained attendantly through pHRI without additional costs. These sensor\nmeasurements are sparse but reliable and informative cues for human mesh\nrecovery. In our active measurement process, camera viewpoints and sensor\nplacements are optimized based on the uncertainty of the estimated pose, which\nis closely related to the truncated or occluded areas. In our sensor fusion\nprocess, we fuse the sensor measurements to the camera-based estimated pose by\nminimizing the distance between the estimated mesh and measured positions. Our\nmethod is agnostic to robot configurations. Experiments were conducted using\nthe Toyota Human Support Robot, which has a camera, 2D LiDAR, and a touch\nsensor on the robot arm. Our proposed method demonstrated the superiority in\nthe human pose estimation accuracy on the quantitative comparison. Furthermore,\nour proposed method reliably estimated the pose of the target person in\npractical settings such as target people occluded by a blanket and standing aid\nwith the robot arm.\n","authors":["Takahiro Maeda","Keisuke Takeshita","Kazuhito Tanaka"],"pdf_url":"https://arxiv.org/pdf/2310.08116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08114v1","updated":"2023-10-12T08:17:17Z","published":"2023-10-12T08:17:17Z","title":"Multi-Modal Sensor Fusion and Object Tracking for Autonomous Racing","summary":"  Reliable detection and tracking of surrounding objects are indispensable for\ncomprehensive motion prediction and planning of autonomous vehicles. Due to the\nlimitations of individual sensors, the fusion of multiple sensor modalities is\nrequired to improve the overall detection capabilities. Additionally, robust\nmotion tracking is essential for reducing the effect of sensor noise and\nimproving state estimation accuracy. The reliability of the autonomous vehicle\nsoftware becomes even more relevant in complex, adversarial high-speed\nscenarios at the vehicle handling limits in autonomous racing. In this paper,\nwe present a modular multi-modal sensor fusion and tracking method for\nhigh-speed applications. The method is based on the Extended Kalman Filter\n(EKF) and is capable of fusing heterogeneous detection inputs to track\nsurrounding objects consistently. A novel delay compensation approach enables\nto reduce the influence of the perception software latency and to output an\nupdated object list. It is the first fusion and tracking method validated in\nhigh-speed real-world scenarios at the Indy Autonomous Challenge 2021 and the\nAutonomous Challenge at CES (AC@CES) 2022, proving its robustness and\ncomputational efficiency on embedded systems. It does not require any labeled\ndata and achieves position tracking residuals below 0.1 m. The related code is\navailable as open-source software at https://github.com/TUMFTM/FusionTracking.\n","authors":["Phillip Karle","Felix Fent","Sebastian Huch","Florian Sauerbeck","Markus Lienkamp"],"pdf_url":"https://arxiv.org/pdf/2310.08114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.10404v3","updated":"2023-10-12T07:55:10Z","published":"2022-09-21T14:51:42Z","title":"GP-net: Flexible Viewpoint Grasp Proposal","summary":"  We present the Grasp Proposal Network (GP-net), a Convolutional Neural\nNetwork model which can generate 6-DoF grasps from flexible viewpoints, e.g. as\nexperienced by mobile manipulators. To train GP-net, we synthetically generate\na dataset containing depth-images and ground-truth grasp information. In\nreal-world experiments, we use the EGAD evaluation benchmark to evaluate GP-net\nagainst two commonly used algorithms, the Volumetric Grasping Network (VGN) and\nthe Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In\ncontrast to the state-of-the-art methods in robotic grasping, GP-net can be\nused for grasping objects from flexible, unknown viewpoints without the need to\ndefine the workspace and achieves a grasp success of 54.4% compared to 51.6%\nfor VGN and 44.2% for GPD. We provide a ROS package along with our code and\npre-trained models at https://aucoroboticsmu.github.io/GP-net/.\n","authors":["Anna Konrad","John McDonald","Rudi Villing"],"pdf_url":"https://arxiv.org/pdf/2209.10404v3.pdf","comment":"Accepted to ICAR 2023"},{"id":"http://arxiv.org/abs/2303.04466v2","updated":"2023-10-12T06:50:25Z","published":"2023-03-08T09:36:47Z","title":"GRADE: Generating Realistic Animated Dynamic Environments for Robotics\n  Research","summary":"  In recent years, computer vision tasks like target tracking and human pose\nestimation have immensely benefited from synthetic data generation and novel\nrendering techniques. On the other hand, methods in robotics, especially for\nrobot perception, have been slow to leverage these techniques. This is because\nstate-of-the-art simulation frameworks for robotics lack either complete\ncontrol, integration with the Robot Operating System (ROS), realistic physics\nor photorealism. To solve this, we present a fully customizable framework for\ngenerating realistic animated dynamic environments (GRADE) for robotics\nresearch, focused primarily at robot perception. The framework can be used\neither to generate ground truth data for robotic vision-related tasks and\noffline processing, or to experiment with robots online in dynamic\nenvironments. We build upon the Nvidia Isaac Sim to allow control of custom\nrobots. We provide methods to include assets, populate and control the\nsimulation, and process the data. Using autonomous robots in GRADE, we generate\nvideo datasets of an indoor dynamic environment. First, we use it to\ndemonstrate the framework's visual realism by evaluating the sim-to-real gap\nthrough experiments with YOLO and Mask R-CNN. Second, we benchmark dynamic SLAM\nalgorithms with this dataset. This not only shows that GRADE can significantly\nimprove training performance and generalization to real sequences, but also\nhighlights how current dynamic SLAM methods over-rely on known benchmarks,\nfailing to generalize. We also introduce a method to precisely repeat a\npreviously recorded experiment, while allowing changes in the surroundings of\nthe robot. Code and data are provided as open-source at\nhttps://grade.is.tue.mpg.de.\n","authors":["Elia Bonetto","Chenghao Xu","Aamir Ahmad"],"pdf_url":"https://arxiv.org/pdf/2303.04466v2.pdf","comment":"28 pages, 10 tables, 4 figures"},{"id":"http://arxiv.org/abs/2307.07763v2","updated":"2023-10-12T06:24:02Z","published":"2023-07-15T10:06:43Z","title":"Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile\n  Agents","summary":"  The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to\nprovide autonomous navigation and task execution in complex and unknown\nenvironments. However, it is hard to develop a dedicated algorithm for mobile\nrobots due to dynamic and challenging situations, such as poor lighting\nconditions and motion blur. To tackle this issue, we propose a tightly-coupled\nLiDAR-visual SLAM based on geometric features, which includes two sub-systems\n(LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework\nassociates the depth and semantics of the multi-modal geometric features to\ncomplement the visual line landmarks and to add direction optimization in\nBundle Adjustment (BA). This further constrains visual odometry. On the other\nhand, the entire line segment detected by the visual subsystem overcomes the\nlimitation of the LiDAR subsystem, which can only perform the local calculation\nfor geometric features. It adjusts the direction of linear feature points and\nfilters out outliers, leading to a higher accurate odometry system. Finally, we\nemploy a module to detect the subsystem's operation, providing the LiDAR\nsubsystem's output as a complementary trajectory to our system while visual\nsubsystem tracking fails. The evaluation results on the public dataset M2DGR,\ngathered from ground robots across various indoor and outdoor scenarios, show\nthat our system achieves more accurate and robust pose estimation compared to\ncurrent state-of-the-art multi-modal methods.\n","authors":["Ke Cao","Ruiping Liu","Ze Wang","Kunyu Peng","Jiaming Zhang","Junwei Zheng","Zhifeng Teng","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2307.07763v2.pdf","comment":"Accepted to ROBIO 2023"},{"id":"http://arxiv.org/abs/2310.08045v1","updated":"2023-10-12T05:39:13Z","published":"2023-10-12T05:39:13Z","title":"Model Predictive Inferential Control of Neural State-Space Models for\n  Autonomous Vehicle Motion Planning","summary":"  Model predictive control (MPC) has proven useful in enabling safe and optimal\nmotion planning for autonomous vehicles. In this paper, we investigate how to\nachieve MPC-based motion planning when a neural state-space model represents\nthe vehicle dynamics. As the neural state-space model will lead to highly\ncomplex, nonlinear and nonconvex optimization landscapes, mainstream\ngradient-based MPC methods will be computationally too heavy to be a viable\nsolution. In a departure, we propose the idea of model predictive inferential\ncontrol (MPIC), which seeks to infer the best control decisions from the\ncontrol objectives and constraints. Following the idea, we convert the MPC\nproblem for motion planning into a Bayesian state estimation problem. Then, we\ndevelop a new particle filtering/smoothing approach to perform the estimation.\nThis approach is implemented as banks of unscented Kalman filters/smoothers and\noffers high sampling efficiency, fast computation, and estimation accuracy. We\nevaluate the MPIC approach through a simulation study of autonomous driving in\ndifferent scenarios, along with an exhaustive comparison with gradient-based\nMPC. The results show that the MPIC approach has considerable computational\nefficiency, regardless of complex neural network architectures, and shows the\ncapability to solve large-scale MPC problems for neural state-space models.\n","authors":["Iman Askari","Xumein Tu","Shen Zeng","Huazhen Fang"],"pdf_url":"https://arxiv.org/pdf/2310.08045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03410v2","updated":"2023-10-12T04:57:23Z","published":"2023-06-06T05:17:02Z","title":"Learning to Simulate Tree-Branch Dynamics for Manipulation","summary":"  We propose to use a simulation driven inverse inference approach to model the\ndynamics of tree branches under manipulation. Learning branch dynamics and\ngaining the ability to manipulate deformable vegetation can help with\nocclusion-prone tasks, such as fruit picking in dense foliage, as well as\nmoving overhanging vines and branches for navigation in dense vegetation. The\nunderlying deformable tree geometry is encapsulated as coarse spring\nabstractions executed on parallel, non-differentiable simulators. The implicit\nstatistical model defined by the simulator, reference trajectories obtained by\nactively probing the ground truth, and the Bayesian formalism, together guide\nthe spring parameter posterior density estimation. Our non-parametric inference\nalgorithm, based on Stein Variational Gradient Descent, incorporates\nbiologically motivated assumptions into the inference process as neural network\ndriven learnt joint priors; moreover, it leverages the finite difference scheme\nfor gradient approximations. Real and simulated experiments confirm that our\nmodel can predict deformation trajectories, quantify the estimation\nuncertainty, and it can perform better when base-lined against other inference\nalgorithms, particularly from the Monte Carlo family. The model displays strong\nrobustness properties in the presence of heteroscedastic sensor noise;\nfurthermore, it can generalise to unseen grasp locations.\n","authors":["Jayadeep Jacob","Tirthankar Bandyopadhyay","Jason Williams","Paulo Borges","Fabio Ramos"],"pdf_url":"https://arxiv.org/pdf/2306.03410v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.08034v1","updated":"2023-10-12T04:56:01Z","published":"2023-10-12T04:56:01Z","title":"Receive, Reason, and React: Drive as You Say with Large Language Models\n  in Autonomous Vehicles","summary":"  The fusion of human-centric design and artificial intelligence (AI)\ncapabilities has opened up new possibilities for next-generation autonomous\nvehicles that go beyond transportation. These vehicles can dynamically interact\nwith passengers and adapt to their preferences. This paper proposes a novel\nframework that leverages Large Language Models (LLMs) to enhance the\ndecision-making process in autonomous vehicles. By utilizing LLMs' linguistic\nand contextual understanding abilities with specialized tools, we aim to\nintegrate the language and reasoning capabilities of LLMs into autonomous\nvehicles. Our research includes experiments in HighwayEnv, a collection of\nenvironments for autonomous driving and tactical decision-making tasks, to\nexplore LLMs' interpretation, interaction, and reasoning in various scenarios.\nWe also examine real-time personalization, demonstrating how LLMs can influence\ndriving behaviors based on verbal commands. Our empirical results highlight the\nsubstantial advantages of utilizing chain-of-thought prompting, leading to\nimproved driving decisions, and showing the potential for LLMs to enhance\npersonalized driving experiences through ongoing verbal feedback. The proposed\nframework aims to transform autonomous vehicle operations, offering\npersonalized support, transparent decision-making, and continuous learning to\nenhance safety and effectiveness. We achieve user-centric, transparent, and\nadaptive autonomous driving ecosystems supported by the integration of LLMs\ninto autonomous vehicles.\n","authors":["Can Cui","Yunsheng Ma","Xu Cao","Wenqian Ye","Ziran Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08034v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2309.10228"},{"id":"http://arxiv.org/abs/2209.05167v3","updated":"2023-10-12T03:09:25Z","published":"2022-09-12T11:51:20Z","title":"LF-VISLAM: A SLAM Framework for Large Field-of-View Cameras with\n  Negative Imaging Plane on Mobile Agents","summary":"  Simultaneous Localization And Mapping (SLAM) has become a crucial aspect in\nthe fields of autonomous driving and robotics. One crucial component of visual\nSLAM is the Field-of-View (FoV) of the camera, as a larger FoV allows for a\nwider range of surrounding elements and features to be perceived. However, when\nthe FoV of the camera reaches the negative half-plane, traditional methods for\nrepresenting image feature points using [u,v,1]^T become ineffective. While the\npanoramic FoV is advantageous for loop closure, its benefits are not easily\nrealized under large-attitude-angle differences where loop-closure frames\ncannot be easily matched by existing methods. As loop closure on wide-FoV\npanoramic data further comes with a large number of outliers, traditional\noutlier rejection methods are not directly applicable. To address these issues,\nwe propose LF-VISLAM, a Visual Inertial SLAM framework for cameras with\nextremely Large FoV with loop closure. A three-dimensional vector with unit\nlength is introduced to effectively represent feature points even on the\nnegative half-plane. The attitude information of the SLAM system is leveraged\nto guide the feature point detection of the loop closure. Additionally, a new\noutlier rejection method based on the unit length representation is integrated\ninto the loop closure module. We collect the PALVIO dataset using a Panoramic\nAnnular Lens (PAL) system with an entire FoV of 360{\\deg}x(40{\\deg}~120{\\deg})\nand an Inertial Measurement Unit (IMU) for Visual Inertial Odometry (VIO) to\naddress the lack of panoramic SLAM datasets. Experiments on the established\nPALVIO and public datasets show that the proposed LF-VISLAM outperforms\nstate-of-the-art SLAM methods. Our code will be open-sourced at\nhttps://github.com/flysoaryun/LF-VISLAM.\n","authors":["Ze Wang","Kailun Yang","Hao Shi","Peng Li","Fei Gao","Jian Bai","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2209.05167v3.pdf","comment":"Accepted to IEEE Transactions on Automation Science and Engineering\n  (T-ASE). Extended version of IROS2022 paper arXiv:2202.12613. Code and\n  dataset will be open-sourced at https://github.com/flysoaryun/LF-SLAM"},{"id":"http://arxiv.org/abs/2310.07433v2","updated":"2023-10-12T03:04:37Z","published":"2023-10-11T12:34:39Z","title":"Imitation Learning from Observation with Automatic Discount Scheduling","summary":"  Humans often acquire new skills through observation and imitation. For\nrobotic agents, learning from the plethora of unlabeled video demonstration\ndata available on the Internet necessitates imitating the expert without access\nto its action, presenting a challenge known as Imitation Learning from\nObservations (ILfO). A common approach to tackle ILfO problems is to convert\nthem into inverse reinforcement learning problems, utilizing a proxy reward\ncomputed from the agent's and the expert's observations. Nonetheless, we\nidentify that tasks characterized by a progress dependency property pose\nsignificant challenges for such approaches; in these tasks, the agent needs to\ninitially learn the expert's preceding behaviors before mastering the\nsubsequent ones. Our investigation reveals that the main cause is that the\nreward signals assigned to later steps hinder the learning of initial\nbehaviors. To address this challenge, we present a novel ILfO framework that\nenables the agent to master earlier behaviors before advancing to later ones.\nWe introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively\nalters the discount factor in reinforcement learning during the training phase,\nprioritizing earlier rewards initially and gradually engaging later rewards\nonly when the earlier behaviors have been mastered. Our experiments, conducted\non nine Meta-World tasks, demonstrate that our method significantly outperforms\nstate-of-the-art methods across all tasks, including those that are unsolvable\nby them.\n","authors":["Yuyang Liu","Weijun Dong","Yingdong Hu","Chuan Wen","Zhao-Heng Yin","Chongjie Zhang","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2310.07433v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07981v1","updated":"2023-10-12T02:10:29Z","published":"2023-10-12T02:10:29Z","title":"Reinforcement Learning of Display Transfer Robots in Glass Flow Control\n  Systems: A Physical Simulation-Based Approach","summary":"  A flow control system is a critical concept for increasing the production\ncapacity of manufacturing systems. To solve the scheduling optimization problem\nrelated to the flow control with the aim of improving productivity, existing\nmethods depend on a heuristic design by domain human experts. Therefore, the\nmethods require correction, monitoring, and verification by using real\nequipment. As system designs increase in complexity, the monitoring time\nincreases, which decreases the probability of arriving at the optimal design.\nAs an alternative approach to the heuristic design of flow control systems, the\nuse of deep reinforcement learning to solve the scheduling optimization problem\nhas been considered. Although the existing research on reinforcement learning\nhas yielded excellent performance in some areas, the applicability of the\nresults to actual FAB such as display and semiconductor manufacturing processes\nis not evident so far. To this end, we propose a method to implement a physical\nsimulation environment and devise a feasible flow control system design using a\ntransfer robot in display manufacturing through reinforcement learning. We\npresent a model and parameter setting to build a virtual environment for\ndifferent display transfer robots, and training methods of reinforcement\nlearning on the environment to obtain an optimal scheduling of glass flow\ncontrol systems. Its feasibility was verified by using different types of\nrobots used in the actual process.\n","authors":["Hwajong Lee","Chan Kim","Seong-Woo Kim"],"pdf_url":"https://arxiv.org/pdf/2310.07981v1.pdf","comment":"10 pages, 17 figures"},{"id":"http://arxiv.org/abs/2310.07968v1","updated":"2023-10-12T01:17:56Z","published":"2023-10-12T01:17:56Z","title":"Think, Act, and Ask: Open-World Interactive Personalized Robot\n  Navigation","summary":"  Zero-Shot Object Navigation (ZSON) enables agents to navigate towards\nopen-vocabulary objects in unknown environments. The existing works of ZSON\nmainly focus on following individual instructions to find generic object\nclasses, neglecting the utilization of natural language interaction and the\ncomplexities of identifying user-specific objects. To address these\nlimitations, we introduce Zero-shot Interactive Personalized Object Navigation\n(ZIPON), where robots need to navigate to personalized goal objects while\nengaging in conversations with users. To solve ZIPON, we propose a new\nframework termed Open-woRld Interactive persOnalized Navigation (ORION), which\nuses Large Language Models (LLMs) to make sequential decisions to manipulate\ndifferent modules for perception, navigation and communication. Experimental\nresults show that the performance of interactive agents that can leverage user\nfeedback exhibits significant improvement. However, obtaining a good balance\nbetween task completion and the efficiency of navigation and interaction\nremains challenging for all methods. We further provide more findings on the\nimpact of diverse user feedback forms on the agents' performance.\n","authors":["Yinpei Dai","Run Peng","Sikai Li","Joyce Chai"],"pdf_url":"https://arxiv.org/pdf/2310.07968v1.pdf","comment":"Video available at https://www.youtube.com/watch?v=QW6rMHVpxUY"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.08587v1","updated":"2023-10-12T17:59:58Z","published":"2023-10-12T17:59:58Z","title":"Is Generalized Dynamic Novel View Synthesis from Monocular Videos\n  Possible Today?","summary":"  Rendering scenes observed in a monocular video from novel viewpoints is a\nchallenging problem. For static scenes the community has studied both\nscene-specific optimization techniques, which optimize on every test scene, and\ngeneralized techniques, which only run a deep net forward pass on a test scene.\nIn contrast, for dynamic scenes, scene-specific optimization techniques exist,\nbut, to our best knowledge, there is currently no generalized method for\ndynamic novel view synthesis from a given monocular video. To answer whether\ngeneralized dynamic novel view synthesis from monocular videos is possible\ntoday, we establish an analysis framework based on existing techniques and work\ntoward the generalized approach. We find a pseudo-generalized process without\nscene-specific appearance optimization is possible, but geometrically and\ntemporally consistent depth estimates are needed. Despite no scene-specific\nappearance optimization, the pseudo-generalized approach improves upon some\nscene-specific methods.\n","authors":["Xiaoming Zhao","Alex Colburn","Fangchang Ma","Miguel Angel Bautista","Joshua M. Susskind","Alexander G. Schwing"],"pdf_url":"https://arxiv.org/pdf/2310.08587v1.pdf","comment":"Project page: https://xiaoming-zhao.github.io/projects/pgdvs"},{"id":"http://arxiv.org/abs/2310.08588v1","updated":"2023-10-12T17:59:58Z","published":"2023-10-12T17:59:58Z","title":"Octopus: Embodied Vision-Language Programmer from Environmental Feedback","summary":"  Large vision-language models (VLMs) have achieved substantial progress in\nmultimodal perception and reasoning. Furthermore, when seamlessly integrated\ninto an embodied agent, it signifies a crucial stride towards the creation of\nautonomous and context-aware systems capable of formulating plans and executing\ncommands with precision. In this paper, we introduce Octopus, a novel VLM\ndesigned to proficiently decipher an agent's vision and textual task objectives\nand to formulate intricate action sequences and generate executable code. Our\ndesign allows the agent to adeptly handle a wide spectrum of tasks, ranging\nfrom mundane daily chores in simulators to sophisticated interactions in\ncomplex video games. Octopus is trained by leveraging GPT-4 to control an\nexplorative agent to generate training data, i.e., action blueprints and the\ncorresponding executable code, within our experimental environment called\nOctoVerse. We also collect the feedback that allows the enhanced training\nscheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a\nseries of experiments, we illuminate Octopus's functionality and present\ncompelling results, and the proposed RLEF turns out to refine the agent's\ndecision-making. By open-sourcing our model architecture, simulator, and\ndataset, we aspire to ignite further innovation and foster collaborative\napplications within the broader embodied AI community.\n","authors":["Jingkang Yang","Yuhao Dong","Shuai Liu","Bo Li","Ziyue Wang","Chencheng Jiang","Haoran Tan","Jiamu Kang","Yuanhan Zhang","Kaiyang Zhou","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2310.08588v1.pdf","comment":"Project Page: https://choiszt.github.io/Octopus/, Codebase:\n  https://github.com/dongyh20/Octopus"},{"id":"http://arxiv.org/abs/2310.08585v1","updated":"2023-10-12T17:59:57Z","published":"2023-10-12T17:59:57Z","title":"Im4D: High-Fidelity and Real-Time Novel View Synthesis for Dynamic\n  Scenes","summary":"  This paper aims to tackle the challenge of dynamic view synthesis from\nmulti-view videos. The key observation is that while previous grid-based\nmethods offer consistent rendering, they fall short in capturing appearance\ndetails of a complex dynamic scene, a domain where multi-view image-based\nrendering methods demonstrate the opposite properties. To combine the best of\ntwo worlds, we introduce Im4D, a hybrid scene representation that consists of a\ngrid-based geometry representation and a multi-view image-based appearance\nrepresentation. Specifically, the dynamic geometry is encoded as a 4D density\nfunction composed of spatiotemporal feature planes and a small MLP network,\nwhich globally models the scene structure and facilitates the rendering\nconsistency. We represent the scene appearance by the original multi-view\nvideos and a network that learns to predict the color of a 3D point from image\nfeatures, instead of memorizing detailed appearance totally with networks,\nthereby naturally making the learning of networks easier. Our method is\nevaluated on five dynamic view synthesis datasets including DyNeRF, ZJU-MoCap,\nNHR, DNA-Rendering and ENeRF-Outdoor datasets. The results show that Im4D\nexhibits state-of-the-art performance in rendering quality and can be trained\nefficiently, while realizing real-time rendering with a speed of 79.8 FPS for\n512x512 images, on a single RTX 3090 GPU.\n","authors":["Haotong Lin","Sida Peng","Zhen Xu","Tao Xie","Xingyi He","Hujun Bao","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.08585v1.pdf","comment":"SIGGRAPH Asia 2023; Project page: https://zju3dv.github.io/im4d"},{"id":"http://arxiv.org/abs/2310.08586v1","updated":"2023-10-12T17:59:57Z","published":"2023-10-12T17:59:57Z","title":"PonderV2: Pave the Way for 3D Foundataion Model with A Universal\n  Pre-training Paradigm","summary":"  In contrast to numerous NLP and 2D computer vision foundational models, the\nlearning of a robust and highly generalized 3D foundational model poses\nconsiderably greater challenges. This is primarily due to the inherent data\nvariability and the diversity of downstream tasks. In this paper, we introduce\na comprehensive 3D pre-training framework designed to facilitate the\nacquisition of efficient 3D representations, thereby establishing a pathway to\n3D foundational models. Motivated by the fact that informative 3D features\nshould be able to encode rich geometry and appearance cues that can be utilized\nto render realistic images, we propose a novel universal paradigm to learn\npoint cloud representations by differentiable neural rendering, serving as a\nbridge between 3D and 2D worlds. We train a point cloud encoder within a\ndevised volumetric neural renderer by comparing the rendered images with the\nreal images. Notably, our approach demonstrates the seamless integration of the\nlearned 3D encoder into diverse downstream tasks. These tasks encompass not\nonly high-level challenges such as 3D detection and segmentation but also\nlow-level objectives like 3D reconstruction and image synthesis, spanning both\nindoor and outdoor scenarios. Besides, we also illustrate the capability of\npre-training a 2D backbone using the proposed universal methodology, surpassing\nconventional pre-training methods by a large margin. For the first time,\n\\sexyname achieves state-of-the-art performance on 11 indoor and outdoor\nbenchmarks. The consistent improvements in various settings imply the\neffectiveness of the proposed method. Code and models will be made available at\nhttps://github.com/Pointcept/Pointcept.\n","authors":["Haoyi Zhu","Honghui Yang","Xiaoyang Wu","Di Huang","Sha Zhang","Xianglong He","Tong He","Hengshuang Zhao","Chunhua Shen","Yu Qiao","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.08586v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2301.00157"},{"id":"http://arxiv.org/abs/2310.08584v1","updated":"2023-10-12T17:59:55Z","published":"2023-10-12T17:59:55Z","title":"Is ImageNet worth 1 video? Learning strong image encoders from 1 long\n  unlabelled video","summary":"  Self-supervised learning has unlocked the potential of scaling up pretraining\nto billions of images, since annotation is unnecessary. But are we making the\nbest use of data? How more economical can we be? In this work, we attempt to\nanswer this question by making two contributions. First, we investigate\nfirst-person videos and introduce a \"Walking Tours\" dataset. These videos are\nhigh-resolution, hours-long, captured in a single uninterrupted take, depicting\na large number of objects and actions with natural scene transitions. They are\nunlabeled and uncurated, thus realistic for self-supervision and comparable\nwith human learning.\n  Second, we introduce a novel self-supervised image pretraining method\ntailored for learning from continuous videos. Existing methods typically adapt\nimage-based pretraining approaches to incorporate more frames. Instead, we\nadvocate a \"tracking to learn to recognize\" approach. Our method called DoRA,\nleads to attention maps that Discover and tRAck objects over time in an\nend-to-end manner, using transformer cross-attention. We derive multiple views\nfrom the tracks and use them in a classical self-supervised distillation loss.\nUsing our novel approach, a single Walking Tours video remarkably becomes a\nstrong competitor to ImageNet for several image and video downstream tasks.\n","authors":["Shashanka Venkataramanan","Mamshad Nayeem Rizve","João Carreira","Yuki M. Asano","Yannis Avrithis"],"pdf_url":"https://arxiv.org/pdf/2310.08584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08581v1","updated":"2023-10-12T17:59:41Z","published":"2023-10-12T17:59:41Z","title":"Universal Visual Decomposer: Long-Horizon Manipulation Made Easy","summary":"  Real-world robotic tasks stretch over extended horizons and encompass\nmultiple stages. Learning long-horizon manipulation tasks, however, is a\nlong-standing challenge, and demands decomposing the overarching task into\nseveral manageable subtasks to facilitate policy learning and generalization to\nunseen tasks. Prior task decomposition methods require task-specific knowledge,\nare computationally intensive, and cannot readily be applied to new tasks. To\naddress these shortcomings, we propose Universal Visual Decomposer (UVD), an\noff-the-shelf task decomposition method for visual long horizon manipulation\nusing pre-trained visual representations designed for robotic control. At a\nhigh level, UVD discovers subgoals by detecting phase shifts in the embedding\nspace of the pre-trained representation. Operating purely on visual\ndemonstrations without auxiliary information, UVD can effectively extract\nvisual subgoals embedded in the videos, while incurring zero additional\ntraining cost on top of standard visuomotor policy training. Goal-conditioned\npolicies learned with UVD-discovered subgoals exhibit significantly improved\ncompositional generalization at test time to unseen tasks. Furthermore,\nUVD-discovered subgoals can be used to construct goal-based reward shaping that\njump-starts temporally extended exploration for reinforcement learning. We\nextensively evaluate UVD on both simulation and real-world tasks, and in all\ncases, UVD substantially outperforms baselines across imitation and\nreinforcement learning settings on in-domain and out-of-domain task sequences\nalike, validating the clear advantage of automated visual task decomposition\nwithin the simple, compact UVD framework.\n","authors":["Zichen Zhang","Yunshuang Li","Osbert Bastani","Abhishek Gupta","Dinesh Jayaraman","Yecheng Jason Ma","Luca Weihs"],"pdf_url":"https://arxiv.org/pdf/2310.08581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08580v1","updated":"2023-10-12T17:59:38Z","published":"2023-10-12T17:59:38Z","title":"OmniControl: Control Any Joint at Any Time for Human Motion Generation","summary":"  We present a novel approach named OmniControl for incorporating flexible\nspatial control signals into a text-conditioned human motion generation model\nbased on the diffusion process. Unlike previous methods that can only control\nthe pelvis trajectory, OmniControl can incorporate flexible spatial control\nsignals over different joints at different times with only one model.\nSpecifically, we propose analytic spatial guidance that ensures the generated\nmotion can tightly conform to the input control signals. At the same time,\nrealism guidance is introduced to refine all the joints to generate more\ncoherent motion. Both the spatial and realism guidance are essential and they\nare highly complementary for balancing control accuracy and motion realism. By\ncombining them, OmniControl generates motions that are realistic, coherent, and\nconsistent with the spatial constraints. Experiments on HumanML3D and KIT-ML\ndatasets show that OmniControl not only achieves significant improvement over\nstate-of-the-art methods on pelvis control but also shows promising results\nwhen incorporating the constraints over other joints.\n","authors":["Yiming Xie","Varun Jampani","Lei Zhong","Deqing Sun","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2310.08580v1.pdf","comment":"Project page: https://neu-vi.github.io/omnicontrol/"},{"id":"http://arxiv.org/abs/2310.08579v1","updated":"2023-10-12T17:59:34Z","published":"2023-10-12T17:59:34Z","title":"HyperHuman: Hyper-Realistic Human Generation with Latent Structural\n  Diffusion","summary":"  Despite significant advances in large-scale text-to-image models, achieving\nhyper-realistic human image generation remains a desirable yet unsolved task.\nExisting models like Stable Diffusion and DALL-E 2 tend to generate human\nimages with incoherent parts or unnatural poses. To tackle these challenges,\nour key insight is that human image is inherently structural over multiple\ngranularities, from the coarse-level body skeleton to fine-grained spatial\ngeometry. Therefore, capturing such correlations between the explicit\nappearance and latent structure in one model is essential to generate coherent\nand natural human images. To this end, we propose a unified framework,\nHyperHuman, that generates in-the-wild human images of high realism and diverse\nlayouts. Specifically, 1) we first build a large-scale human-centric dataset,\nnamed HumanVerse, which consists of 340M images with comprehensive annotations\nlike human pose, depth, and surface normal. 2) Next, we propose a Latent\nStructural Diffusion Model that simultaneously denoises the depth and surface\nnormal along with the synthesized RGB image. Our model enforces the joint\nlearning of image appearance, spatial relationship, and geometry in a unified\nnetwork, where each branch in the model complements to each other with both\nstructural awareness and textural richness. 3) Finally, to further boost the\nvisual quality, we propose a Structure-Guided Refiner to compose the predicted\nconditions for more detailed generation of higher resolution. Extensive\nexperiments demonstrate that our framework yields the state-of-the-art\nperformance, generating hyper-realistic human images under diverse scenarios.\nProject Page: https://snap-research.github.io/HyperHuman/\n","authors":["Xian Liu","Jian Ren","Aliaksandr Siarohin","Ivan Skorokhodov","Yanyu Li","Dahua Lin","Xihui Liu","Ziwei Liu","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2310.08579v1.pdf","comment":"Project Page: https://snap-research.github.io/HyperHuman/"},{"id":"http://arxiv.org/abs/2310.08577v1","updated":"2023-10-12T17:59:30Z","published":"2023-10-12T17:59:30Z","title":"Visual Data-Type Understanding does not emerge from Scaling\n  Vision-Language Models","summary":"  Recent advances in the development of vision-language models (VLMs) are\nyielding remarkable success in recognizing visual semantic content, including\nimpressive instances of compositional image understanding. Here, we introduce\nthe novel task of \\textit{Visual Data-Type Identification}, a basic perceptual\nskill with implications for data curation (e.g., noisy data-removal from large\ndatasets, domain-specific retrieval) and autonomous vision (e.g.,\ndistinguishing changing weather conditions from camera lens staining). We\ndevelop two datasets consisting of animal images altered across a diverse set\nof 27 visual \\textit{data-types}, spanning four broad categories. An extensive\nzero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a\nnuanced performance landscape. While VLMs are reasonably good at identifying\ncertain stylistic \\textit{data-types}, such as cartoons and sketches, they\nstruggle with simpler \\textit{data-types} arising from basic manipulations like\nimage rotations or additive noise. Our findings reveal that (i) model scaling\nalone yields marginal gains for contrastively-trained models like CLIP, and\n(ii) there is a pronounced drop in performance for the largest\nauto-regressively trained VLMs like OpenFlamingo. This finding points to a\nblind spot in current frontier VLMs: they excel in recognizing semantic content\nbut fail to acquire an understanding of visual \\textit{data-types} through\nscaling. By analyzing the pre-training distributions of these models and\nincorporating \\textit{data-type} information into the captions during\nfine-tuning, we achieve a significant enhancement in performance. By exploring\nthis previously uncharted task, we aim to set the stage for further advancing\nVLMs to equip them with visual data-type understanding. Code and datasets are\nreleased \\href{https://github.com/bethgelab/DataTypeIdentification}{here}.\n","authors":["Vishaal Udandarao","Max F. Burg","Samuel Albanie","Matthias Bethge"],"pdf_url":"https://arxiv.org/pdf/2310.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08576v1","updated":"2023-10-12T17:59:23Z","published":"2023-10-12T17:59:23Z","title":"Learning to Act from Actionless Videos through Dense Correspondences","summary":"  In this work, we present an approach to construct a video-based robot policy\ncapable of reliably executing diverse tasks across different robots and\nenvironments from few video demonstrations without using any action\nannotations. Our method leverages images as a task-agnostic representation,\nencoding both the state and action information, and text as a general\nrepresentation for specifying robot goals. By synthesizing videos that\n``hallucinate'' robot executing actions and in combination with dense\ncorrespondences between frames, our approach can infer the closed-formed action\nto execute to an environment without the need of any explicit action labels.\nThis unique capability allows us to train the policy solely based on RGB videos\nand deploy learned policies to various robotic tasks. We demonstrate the\nefficacy of our approach in learning policies on table-top manipulation and\nnavigation tasks. Additionally, we contribute an open-source framework for\nefficient video modeling, enabling the training of high-fidelity policy models\nwith four GPUs within a single day.\n","authors":["Po-Chen Ko","Jiayuan Mao","Yilun Du","Shao-Hua Sun","Joshua B. Tenenbaum"],"pdf_url":"https://arxiv.org/pdf/2310.08576v1.pdf","comment":"Project page: https://flow-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2112.09726v2","updated":"2023-10-12T17:57:51Z","published":"2021-12-17T19:22:01Z","title":"Soundify: Matching Sound Effects to Video","summary":"  In the art of video editing, sound helps add character to an object and\nimmerse the viewer within a space. Through formative interviews with\nprofessional editors (N=10), we found that the task of adding sounds to video\ncan be challenging. This paper presents Soundify, a system that assists editors\nin matching sounds to video. Given a video, Soundify identifies matching\nsounds, synchronizes the sounds to the video, and dynamically adjusts panning\nand volume to create spatial audio. In a human evaluation study (N=889), we\nshow that Soundify is capable of matching sounds to video out-of-the-box for a\ndiverse range of audio categories. In a within-subjects expert study (N=12), we\ndemonstrate the usefulness of Soundify in helping video editors match sounds to\nvideo with lighter workload, reduced task completion time, and improved\nusability.\n","authors":["David Chuan-En Lin","Anastasis Germanidis","Cristóbal Valenzuela","Yining Shi","Nikolas Martelaro"],"pdf_url":"https://arxiv.org/pdf/2112.09726v2.pdf","comment":"Full paper in UIST 2023; Short paper in NeurIPS 2021 ML4CD Workshop;\n  Online demo: https://soundify.cc"},{"id":"http://arxiv.org/abs/2308.11606v2","updated":"2023-10-12T17:50:38Z","published":"2023-08-22T17:53:55Z","title":"StoryBench: A Multifaceted Benchmark for Continuous Story Visualization","summary":"  Generating video stories from text prompts is a complex task. In addition to\nhaving high visual quality, videos need to realistically adhere to a sequence\nof text prompts whilst being consistent throughout the frames. Creating a\nbenchmark for video generation requires data annotated over time, which\ncontrasts with the single caption used often in video datasets. To fill this\ngap, we collect comprehensive human annotations on three existing datasets, and\nintroduce StoryBench: a new, challenging multi-task benchmark to reliably\nevaluate forthcoming text-to-video models. Our benchmark includes three video\ngeneration tasks of increasing difficulty: action execution, where the next\naction must be generated starting from a conditioning video; story\ncontinuation, where a sequence of actions must be executed starting from a\nconditioning video; and story generation, where a video must be generated from\nonly text prompts. We evaluate small yet strong text-to-video baselines, and\nshow the benefits of training on story-like data algorithmically generated from\nexisting video captions. Finally, we establish guidelines for human evaluation\nof video stories, and reaffirm the need of better automatic metrics for video\ngeneration. StoryBench aims at encouraging future research efforts in this\nexciting new area.\n","authors":["Emanuele Bugliarello","Hernan Moraldo","Ruben Villegas","Mohammad Babaeizadeh","Mohammad Taghi Saffar","Han Zhang","Dumitru Erhan","Vittorio Ferrari","Pieter-Jan Kindermans","Paul Voigtlaender"],"pdf_url":"https://arxiv.org/pdf/2308.11606v2.pdf","comment":"NeurIPS D&B 2023"},{"id":"http://arxiv.org/abs/2310.08541v1","updated":"2023-10-12T17:34:20Z","published":"2023-10-12T17:34:20Z","title":"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic\n  Image Design and Generation","summary":"  We introduce ``Idea to Image,'' a system that enables multimodal iterative\nself-refinement with GPT-4V(ision) for automatic image design and generation.\nHumans can quickly identify the characteristics of different text-to-image\n(T2I) models via iterative explorations. This enables them to efficiently\nconvert their high-level generation ideas into effective T2I prompts that can\nproduce good images. We investigate if systems based on large multimodal models\n(LMMs) can develop analogous multimodal self-refinement abilities that enable\nexploring unknown models or environments via self-refining tries. Idea2Img\ncyclically generates revised T2I prompts to synthesize draft images, and\nprovides directional feedback for prompt revision, both conditioned on its\nmemory of the probed T2I model's characteristics. The iterative self-refinement\nbrings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img\ncan process input ideas with interleaved image-text sequences, follow ideas\nwith design instructions, and generate images of better semantic and visual\nqualities. The user preference study validates the efficacy of multimodal\niterative self-refinement on automatic image design and generation.\n","authors":["Zhengyuan Yang","Jianfeng Wang","Linjie Li","Kevin Lin","Chung-Ching Lin","Zicheng Liu","Lijuan Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08541v1.pdf","comment":"Project page at https://idea2img.github.io/"},{"id":"http://arxiv.org/abs/2310.08538v1","updated":"2023-10-12T17:28:06Z","published":"2023-10-12T17:28:06Z","title":"Image2PCI -- A Multitask Learning Framework for Estimating Pavement\n  Condition Indices Directly from Images","summary":"  The Pavement Condition Index (PCI) is a widely used metric for evaluating\npavement performance based on the type, extent and severity of distresses\ndetected on a pavement surface. In recent times, significant progress has been\nmade in utilizing deep-learning approaches to automate PCI estimation process.\nHowever, the current approaches rely on at least two separate models to\nestimate PCI values -- one model dedicated to determining the type and extent\nand another for estimating their severity. This approach presents several\nchallenges, including complexities, high computational resource demands, and\nmaintenance burdens that necessitate careful consideration and resolution. To\novercome these challenges, the current study develops a unified multi-tasking\nmodel that predicts the PCI directly from a top-down pavement image. The\nproposed architecture is a multi-task model composed of one encoder for feature\nextraction and four decoders to handle specific tasks: two detection heads, one\nsegmentation head and one PCI estimation head. By multitasking, we are able to\nextract features from the detection and segmentation heads for automatically\nestimating the PCI directly from the images. The model performs very well on\nour benchmarked and open pavement distress dataset that is annotated for\nmultitask learning (the first of its kind). To our best knowledge, this is the\nfirst work that can estimate PCI directly from an image at real time speeds\nwhile maintaining excellent accuracy on all related tasks for crack detection\nand segmentation.\n","authors":["Neema Jakisa Owor","Hang Du","Abdulateef Daud","Armstrong Aboah","Yaw Adu-Gyamfi"],"pdf_url":"https://arxiv.org/pdf/2310.08538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08537v1","updated":"2023-10-12T17:26:16Z","published":"2023-10-12T17:26:16Z","title":"XAI Benchmark for Visual Explanation","summary":"  The rise of deep learning algorithms has led to significant advancements in\ncomputer vision tasks, but their \"black box\" nature has raised concerns\nregarding interpretability. Explainable AI (XAI) has emerged as a critical area\nof research aiming to open this \"black box\", and shed light on the\ndecision-making process of AI models. Visual explanations, as a subset of\nExplainable Artificial Intelligence (XAI), provide intuitive insights into the\ndecision-making processes of AI models handling visual data by highlighting\ninfluential areas in an input image. Despite extensive research conducted on\nvisual explanations, most evaluations are model-centered since the availability\nof corresponding real-world datasets with ground truth explanations is scarce\nin the context of image data. To bridge this gap, we introduce an XAI Benchmark\ncomprising a dataset collection from diverse topics that provide both class\nlabels and corresponding explanation annotations for images. We have processed\ndata from diverse domains to align with our unified visual explanation\nframework. We introduce a comprehensive Visual Explanation pipeline, which\nintegrates data loading, preprocessing, experimental setup, and model\nevaluation processes. This structure enables researchers to conduct fair\ncomparisons of various visual explanation techniques. In addition, we provide a\ncomprehensive review of over 10 evaluation methods for visual explanation to\nassist researchers in effectively utilizing our dataset collection. To further\nassess the performance of existing visual explanation methods, we conduct\nexperiments on selected datasets using various model-centered and ground\ntruth-centered evaluation metrics. We envision this benchmark could facilitate\nthe advancement of visual explanation models. The XAI dataset collection and\neasy-to-use code for evaluation are publicly accessible at\nhttps://xaidataset.github.io.\n","authors":["Yifei Zhang","Siyi Gu","James Song","Bo Pan","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.08537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05173v2","updated":"2023-10-12T17:25:44Z","published":"2023-09-11T00:02:05Z","title":"DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning","summary":"  Prompt tuning (PT), where a small amount of trainable soft (continuous)\nprompt vectors is affixed to the input of language models (LM), has shown\npromising results across various tasks and models for parameter-efficient\nfine-tuning (PEFT). PT stands out from other PEFT approaches because it\nmaintains competitive performance with fewer trainable parameters and does not\ndrastically scale up its parameters as the model size expands. However, PT\nintroduces additional soft prompt tokens, leading to longer input sequences,\nwhich significantly impacts training and inference time and memory usage due to\nthe Transformer's quadratic complexity. Particularly concerning for Large\nLanguage Models (LLMs) that face heavy daily querying. To address this issue,\nwe propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt\ninto a shorter soft prompt and a pair of low-rank matrices that are then\noptimised with two different learning rates. This allows DePT to achieve better\nperformance while saving over 20% memory and time costs compared to vanilla PT\nand its variants, without changing trainable parameter sizes. Through extensive\nexperiments on 23 natural language processing (NLP) and vision-language (VL)\ntasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,\nincluding the full fine-tuning baseline in some scenarios. Additionally, we\nempirically show that DEPT grows more efficient as the model size increases.\nOur further study reveals that DePT integrates seamlessly with\nparameter-efficient transfer learning in the few-shot learning setting and\nhighlights its adaptability to various model architectures and sizes.\n","authors":["Zhengxiang Shi","Aldo Lipani"],"pdf_url":"https://arxiv.org/pdf/2309.05173v2.pdf","comment":"Code is available at https://github.com/ZhengxiangShi/DePT"},{"id":"http://arxiv.org/abs/2307.03293v2","updated":"2023-10-12T17:25:26Z","published":"2023-07-06T21:08:03Z","title":"CheXmask: a large-scale dataset of anatomical segmentation masks for\n  multi-center chest x-ray images","summary":"  The development of successful artificial intelligence models for chest X-ray\nanalysis relies on large, diverse datasets with high-quality annotations. While\nseveral databases of chest X-ray images have been released, most include\ndisease diagnosis labels but lack detailed pixel-level anatomical segmentation\nlabels. To address this gap, we introduce an extensive chest X-ray multi-center\nsegmentation dataset with uniform and fine-grain anatomical annotations for\nimages coming from six well-known publicly available databases: CANDID-PTX,\nChestX-ray8, Chexpert, MIMIC-CXR-JPG, Padchest, and VinDr-CXR, resulting in\n676,803 segmentation masks. Our methodology utilizes the HybridGNet model to\nensure consistent and high-quality segmentations across all datasets. Rigorous\nvalidation, including expert physician evaluation and automatic quality\ncontrol, was conducted to validate the resulting masks. Additionally, we\nprovide individualized quality indices per mask and an overall quality\nestimation per dataset. This dataset serves as a valuable resource for the\nbroader scientific community, streamlining the development and assessment of\ninnovative methodologies in chest X-ray analysis. The CheXmask dataset is\npublicly available at:\nhttps://physionet.org/content/chexmask-cxr-segmentation-data/\n","authors":["Nicolás Gaggion","Candelaria Mosquera","Lucas Mansilla","Martina Aineseder","Diego H. Milone","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2307.03293v2.pdf","comment":"The CheXmask dataset is publicly available at\n  https://physionet.org/content/chexmask-cxr-segmentation-data/"},{"id":"http://arxiv.org/abs/2310.08534v1","updated":"2023-10-12T17:24:05Z","published":"2023-10-12T17:24:05Z","title":"Animating Street View","summary":"  We present a system that automatically brings street view imagery to life by\npopulating it with naturally behaving, animated pedestrians and vehicles. Our\napproach is to remove existing people and vehicles from the input image, insert\nmoving objects with proper scale, angle, motion, and appearance, plan paths and\ntraffic behavior, as well as render the scene with plausible occlusion and\nshadowing effects. The system achieves these by reconstructing the still image\nstreet scene, simulating crowd behavior, and rendering with consistent\nlighting, visibility, occlusions, and shadows. We demonstrate results on a\ndiverse range of street scenes including regular still images and panoramas.\n","authors":["Mengyi Shan","Brian Curless","Ira Kemelmacher-Shlizerman","Steve Seitz"],"pdf_url":"https://arxiv.org/pdf/2310.08534v1.pdf","comment":"SIGGRAPH Asia 2023 Conference Track"},{"id":"http://arxiv.org/abs/2310.08530v1","updated":"2023-10-12T17:22:58Z","published":"2023-10-12T17:22:58Z","title":"UniPose: Detecting Any Keypoints","summary":"  This work proposes a unified framework called UniPose to detect keypoints of\nany articulated (e.g., human and animal), rigid, and soft objects via visual or\ntextual prompts for fine-grained vision understanding and manipulation.\nKeypoint is a structure-aware, pixel-level, and compact representation of any\nobject, especially articulated objects. Existing fine-grained promptable tasks\nmainly focus on object instance detection and segmentation but often fail to\nidentify fine-grained granularity and structured information of image and\ninstance, such as eyes, leg, paw, etc. Meanwhile, prompt-based keypoint\ndetection is still under-explored. To bridge the gap, we make the first attempt\nto develop an end-to-end prompt-based keypoint detection framework called\nUniPose to detect keypoints of any objects. As keypoint detection tasks are\nunified in this framework, we can leverage 13 keypoint detection datasets with\n338 keypoints across 1,237 categories over 400K instances to train a generic\nkeypoint detection model. UniPose can effectively align text-to-keypoint and\nimage-to-keypoint due to the mutual enhancement of textual and visual prompts\nbased on the cross-modality contrastive learning optimization objectives. Our\nexperimental results show that UniPose has strong fine-grained localization and\ngeneralization abilities across image styles, categories, and poses. Based on\nUniPose as a generalist keypoint detector, we hope it could serve fine-grained\nvisual perception, understanding, and generation.\n","authors":["Jie Yang","Ailing Zeng","Ruimao Zhang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08529v1","updated":"2023-10-12T17:22:24Z","published":"2023-10-12T17:22:24Z","title":"GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with\n  Point Cloud Priors","summary":"  In recent times, the generation of 3D assets from text prompts has shown\nimpressive results. Both 2D and 3D diffusion models can generate decent 3D\nobjects based on prompts. 3D diffusion models have good 3D consistency, but\ntheir quality and generalization are limited as trainable 3D data is expensive\nand hard to obtain. 2D diffusion models enjoy strong abilities of\ngeneralization and fine generation, but the 3D consistency is hard to\nguarantee. This paper attempts to bridge the power from the two types of\ndiffusion models via the recent explicit and efficient 3D Gaussian splatting\nrepresentation. A fast 3D generation framework, named as \\name, is proposed,\nwhere the 3D diffusion model provides point cloud priors for initialization and\nthe 2D diffusion model enriches the geometry and appearance. Operations of\nnoisy point growing and color perturbation are introduced to enhance the\ninitialized Gaussians. Our \\name can generate a high-quality 3D instance within\n25 minutes on one GPU, much faster than previous methods, while the generated\ninstances can be directly rendered in real time. Demos and code are available\nat https://taoranyi.com/gaussiandreamer/.\n","authors":["Taoran Yi","Jiemin Fang","Guanjun Wu","Lingxi Xie","Xiaopeng Zhang","Wenyu Liu","Qi Tian","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08529v1.pdf","comment":"Work in progress. Project page: https://taoranyi.com/gaussiandreamer/"},{"id":"http://arxiv.org/abs/2310.08528v1","updated":"2023-10-12T17:21:41Z","published":"2023-10-12T17:21:41Z","title":"4D Gaussian Splatting for Real-Time Dynamic Scene Rendering","summary":"  Representing and rendering dynamic scenes has been an important but\nchallenging task. Especially, to accurately model complex motions, high\nefficiency is usually hard to maintain. We introduce the 4D Gaussian Splatting\n(4D-GS) to achieve real-time dynamic scene rendering while also enjoying high\ntraining and storage efficiency. An efficient deformation field is constructed\nto model both Gaussian motions and shape deformations. Different adjacent\nGaussians are connected via a HexPlane to produce more accurate position and\nshape deformations. Our 4D-GS method achieves real-time rendering under high\nresolutions, 70 FPS at a 800$\\times$800 resolution on an RTX 3090 GPU, while\nmaintaining comparable or higher quality than previous state-of-the-art\nmethods. More demos and code are available at\nhttps://guanjunwu.github.io/4dgs/.\n","authors":["Guanjun Wu","Taoran Yi","Jiemin Fang","Lingxi Xie","Xiaopeng Zhang","Wei Wei","Wenyu Liu","Qi Tian","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2310.08528v1.pdf","comment":"Work in progress. Project page: https://guanjunwu.github.io/4dgs/"},{"id":"http://arxiv.org/abs/2310.08501v1","updated":"2023-10-12T16:59:50Z","published":"2023-10-12T16:59:50Z","title":"Unsupervised Learning of Object-Centric Embeddings for Cell Instance\n  Segmentation in Microscopy Images","summary":"  Segmentation of objects in microscopy images is required for many biomedical\napplications. We introduce object-centric embeddings (OCEs), which embed image\npatches such that the spatial offsets between patches cropped from the same\nobject are preserved. Those learnt embeddings can be used to delineate\nindividual objects and thus obtain instance segmentations. Here, we show\ntheoretically that, under assumptions commonly found in microscopy images, OCEs\ncan be learnt through a self-supervised task that predicts the spatial offset\nbetween image patches. Together, this forms an unsupervised cell instance\nsegmentation method which we evaluate on nine diverse large-scale microscopy\ndatasets. Segmentations obtained with our method lead to substantially improved\nresults, compared to state-of-the-art baselines on six out of nine datasets,\nand perform on par on the remaining three datasets. If ground-truth annotations\nare available, our method serves as an excellent starting point for supervised\ntraining, reducing the required amount of ground-truth needed by one order of\nmagnitude, thus substantially increasing the practical applicability of our\nmethod. Source code is available at https://github.com/funkelab/cellulus.\n","authors":["Steffen Wolf","Manan Lalit","Henry Westmacott","Katie McDole","Jan Funke"],"pdf_url":"https://arxiv.org/pdf/2310.08501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.06823v2","updated":"2023-10-12T16:42:55Z","published":"2023-10-10T17:53:36Z","title":"NECO: NEural Collapse Based Out-of-distribution detection","summary":"  Detecting out-of-distribution (OOD) data is a critical challenge in machine\nlearning due to model overconfidence, often without awareness of their\nepistemological limits. We hypothesize that ``neural collapse'', a phenomenon\naffecting in-distribution data for models trained beyond loss convergence, also\ninfluences OOD data. To benefit from this interplay, we introduce NECO, a novel\npost-hoc method for OOD detection, which leverages the geometric properties of\n``neural collapse'' and of principal component spaces to identify OOD data. Our\nextensive experiments demonstrate that NECO achieves state-of-the-art results\non both small and large-scale OOD detection tasks while exhibiting strong\ngeneralization capabilities across different network architectures.\nFurthermore, we provide a theoretical explanation for the effectiveness of our\nmethod in OOD detection. We plan to release the code after the anonymity\nperiod.\n","authors":["Mouïn Ben Ammar","Nacim Belkhir","Sebastian Popescu","Antoine Manzanera","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2310.06823v2.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2310.08475v1","updated":"2023-10-12T16:32:44Z","published":"2023-10-12T16:32:44Z","title":"Can We Edit Multimodal Large Language Models?","summary":"  In this paper, we focus on editing Multimodal Large Language Models (MLLMs).\nCompared to editing single-modal LLMs, multimodal model editing is more\nchallenging, which demands a higher level of scrutiny and careful consideration\nin the editing process. To facilitate research in this area, we construct a new\nbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite\nof innovative metrics for evaluation. We conduct comprehensive experiments\ninvolving various model editing baselines and analyze the impact of editing\ndifferent components for multimodal LLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal LLMs to some extent, but the effect\nis still barely satisfactory, indicating the potential difficulty of this task.\nWe hope that our work can provide the NLP community with insights\\footnote{Code\nand dataset are available in https://github.com/zjunlp/EasyEdit.\n","authors":["Siyuan Cheng","Bozhong Tian","Qingbin Liu","Xi Chen","Yongheng Wang","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08475v1.pdf","comment":"EMNLP 2023"},{"id":"http://arxiv.org/abs/2310.08465v1","updated":"2023-10-12T16:26:18Z","published":"2023-10-12T16:26:18Z","title":"MotionDirector: Motion Customization of Text-to-Video Diffusion Models","summary":"  Large-scale pre-trained diffusion models have exhibited remarkable\ncapabilities in diverse video generations. Given a set of video clips of the\nsame motion concept, the task of Motion Customization is to adapt existing\ntext-to-video diffusion models to generate videos with this motion. For\nexample, generating a video with a car moving in a prescribed manner under\nspecific camera movements to make a movie, or a video illustrating how a bear\nwould lift weights to inspire creators. Adaptation methods have been developed\nfor customizing appearance like subject or style, yet unexplored for motion. It\nis straightforward to extend mainstream adaption methods for motion\ncustomization, including full model tuning, parameter-efficient tuning of\nadditional layers, and Low-Rank Adaptions (LoRAs). However, the motion concept\nlearned by these methods is often coupled with the limited appearances in the\ntraining videos, making it difficult to generalize the customized motion to\nother appearances. To overcome this challenge, we propose MotionDirector, with\na dual-path LoRAs architecture to decouple the learning of appearance and\nmotion. Further, we design a novel appearance-debiased temporal loss to\nmitigate the influence of appearance on the temporal training objective.\nExperimental results show the proposed method can generate videos of diverse\nappearances for the customized motions. Our method also supports various\ndownstream applications, such as the mixing of different videos with their\nappearance and motion respectively, and animating a single image with\ncustomized motions. Our code and model weights will be released.\n","authors":["Rui Zhao","Yuchao Gu","Jay Zhangjie Wu","David Junhao Zhang","Jiawei Liu","Weijia Wu","Jussi Keppo","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2310.08465v1.pdf","comment":"Project Page: https://showlab.github.io/MotionDirector/"},{"id":"http://arxiv.org/abs/2310.08451v1","updated":"2023-10-12T16:11:13Z","published":"2023-10-12T16:11:13Z","title":"Proving the Potential of Skeleton Based Action Recognition to Automate\n  the Analysis of Manual Processes","summary":"  In manufacturing sectors such as textiles and electronics, manual processes\nare a fundamental part of production. The analysis and monitoring of the\nprocesses is necessary for efficient production design. Traditional methods for\nanalyzing manual processes are complex, expensive, and inflexible. Compared to\nestablished approaches such as Methods-Time-Measurement (MTM), machine learning\n(ML) methods promise: Higher flexibility, self-sufficient & permanent use,\nlower costs. In this work, based on a video stream, the current motion class in\na manual assembly process is detected. With information on the current motion,\nKey-Performance-Indicators (KPIs) can be derived easily. A skeleton-based\naction recognition approach is taken, as this field recently shows major\nsuccess in machine vision tasks. For skeleton-based action recognition in\nmanual assembly, no sufficient pre-work could be found. Therefore, a ML\npipeline is developed, to enable extensive research on different (pre-)\nprocessing methods and neural nets. Suitable well generalizing approaches are\nfound, proving the potential of ML to enhance analyzation of manual processes.\nModels detect the current motion, performed by an operator in manual assembly,\nbut the results can be transferred to all kinds of manual processes.\n","authors":["Marlin Berger","Frederik Cloppenburg","Jens Eufinger","Thomas Gries"],"pdf_url":"https://arxiv.org/pdf/2310.08451v1.pdf","comment":"16 pages, 6 figures. Find peer-reviewed version in Proceedings of\n  IntelliSys 2023"},{"id":"http://arxiv.org/abs/2310.08442v1","updated":"2023-10-12T16:04:41Z","published":"2023-10-12T16:04:41Z","title":"Debias the Training of Diffusion Models","summary":"  Diffusion models have demonstrated compelling generation quality by\noptimizing the variational lower bound through a simple denoising score\nmatching loss. In this paper, we provide theoretical evidence that the\nprevailing practice of using a constant loss weight strategy in diffusion\nmodels leads to biased estimation during the training phase. Simply optimizing\nthe denoising network to predict Gaussian noise with constant weighting may\nhinder precise estimations of original images. To address the issue, we propose\nan elegant and effective weighting strategy grounded in the theoretically\nunbiased principle. Moreover, we conduct a comprehensive and systematic\nexploration to dissect the inherent bias problem deriving from constant\nweighting loss from the perspectives of its existence, impact and reasons.\nThese analyses are expected to advance our understanding and demystify the\ninner workings of diffusion models. Through empirical evaluation, we\ndemonstrate that our proposed debiased estimation method significantly enhances\nsample quality without the reliance on complex techniques, and exhibits\nimproved efficiency compared to the baseline method both in training and\nsampling processes.\n","authors":["Hu Yu","Li Shen","Jie Huang","Man Zhou","Hongsheng Li","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.08442v1.pdf","comment":"University of Science and Technology of China, Alibaba Group, The\n  Chinese University of Hong Kong"},{"id":"http://arxiv.org/abs/2310.08430v1","updated":"2023-10-12T15:53:47Z","published":"2023-10-12T15:53:47Z","title":"Assessing of Soil Erosion Risk Through Geoinformation Sciences and\n  Remote Sensing -- A Review","summary":"  During past decades a marked manifestation of widespread erosion phenomena\nwas studied worldwide. Global conservation community has launched campaigns at\nlocal, regional and continental level in developing countries for preservation\nof soil resources in order not only to stop or mitigate human impact on nature\nbut also to improve life in rural areas introducing new approaches for soil\ncultivation. After the adoption of Sustainable Development Goals of UNs and\nlaunching several world initiatives such as the Land Degradation Neutrality\n(LDN) the world came to realize the very importance of the soil resources on\nwhich the biosphere relies for its existence. The main goal of the chapter is\nto review different types and structures erosion models as well as their\napplications. Several methods using spatial analysis capabilities of geographic\ninformation systems (GIS) are in operation for soil erosion risk assessment,\nsuch as Universal Soil Loss Equation (USLE), Revised Universal Soil Loss\nEquation (RUSLE) in operation worldwide and in the USA and MESALES model. These\nand more models are being discussed in the present work alongside more\nexperimental models and methods for assessing soil erosion risk such as\nArtificial Intelligence (AI), Machine and Deep Learning, etc. At the end of\nthis work, a prospectus for the future development of soil erosion risk\nassessment is drawn.\n","authors":["Lachezar Filchev","Vasil Kolev"],"pdf_url":"https://arxiv.org/pdf/2310.08430v1.pdf","comment":"Chapter 21 (pages 54)"},{"id":"http://arxiv.org/abs/2310.08429v1","updated":"2023-10-12T15:53:24Z","published":"2023-10-12T15:53:24Z","title":"Revisiting Data Augmentation for Rotational Invariance in Convolutional\n  Neural Networks","summary":"  Convolutional Neural Networks (CNN) offer state of the art performance in\nvarious computer vision tasks. Many of those tasks require different subtypes\nof affine invariances (scale, rotational, translational) to image\ntransformations. Convolutional layers are translation equivariant by design,\nbut in their basic form lack invariances. In this work we investigate how best\nto include rotational invariance in a CNN for image classification. Our\nexperiments show that networks trained with data augmentation alone can\nclassify rotated images nearly as well as in the normal unrotated case; this\nincrease in representational power comes only at the cost of training time. We\nalso compare data augmentation versus two modified CNN models for achieving\nrotational invariance or equivariance, Spatial Transformer Networks and Group\nEquivariant CNNs, finding no significant accuracy increase with these\nspecialized methods. In the case of data augmented networks, we also analyze\nwhich layers help the network to encode the rotational invariance, which is\nimportant for understanding its limitations and how to best retrain a network\nwith data augmentation to achieve invariance to rotation.\n","authors":["Facundo Manuel Quiroga","Franco Ronchetti","Laura Lanzarini","Aurelio Fernandez-Bariviera"],"pdf_url":"https://arxiv.org/pdf/2310.08429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.05188v3","updated":"2023-10-12T15:43:01Z","published":"2022-05-10T21:55:26Z","title":"On Scale Space Radon Transform, Properties and Application in CT Image\n  Reconstruction","summary":"  Since the Radon transform (RT) consists in a line integral function, some\nmodeling assumptions are made on Computed Tomography (CT) system, making image\nreconstruction analytical methods, such as Filtered Backprojection (FBP),\nsensitive to artifacts and noise. In the other hand, recently, a new integral\ntransform, called Scale Space Radon Transform (SSRT), is introduced where, RT\nis a particular case. Thanks to its interesting properties, such as good scale\nspace behavior, the SSRT has known number of new applications. In this paper,\nwith the aim to improve the reconstructed image quality for these methods, we\npropose to model the X-ray beam with the Scale Space Radon Transform (SSRT)\nwhere, the assumptions done on the physical dimensions of the CT system\nelements reflect better the reality. After depicting the basic properties and\nthe inversion of SSRT, the FBP algorithm is used to reconstruct the image from\nthe SSRT sinogram where the RT spectrum used in FBP is replaced by SSRT and the\nGaussian kernel, expressed in their frequency domain. PSNR and SSIM, as quality\nmeasures, are used to compare RT and SSRT-based image reconstruction on\nShepp-Logan head and anthropomorphic abdominal phantoms. The first findings\nshow that the SSRT-based method outperforms the methods based on RT,\nespecially, when the number of projections is reduced, making it more\nappropriate for applications requiring low-dose radiation, such as medical\nX-ray CT. While SSRT-FBP and RT-FBP have utmost the same runtime, the\nexperiments show that SSRT-FBP is more robust to Poisson-Gaussian noise\ncorrupting CT data.\n","authors":["Nafaa Nacereddine","Djemel Ziou","Aicha Baya Goumeidane"],"pdf_url":"https://arxiv.org/pdf/2205.05188v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08421v1","updated":"2023-10-12T15:42:17Z","published":"2023-10-12T15:42:17Z","title":"\"SegLoc\": Study on Novel Visual Self-supervised Learning Scheme (Segment\n  Localization) Tailored for Dense Prediction Tasks of Security Inspection\n  X-ray Images","summary":"  Lately, remarkable advancements of artificial intelligence have been\nattributed to the integration of self-supervised learning scheme. Despite\nimpressive achievements within NLP, yet SSL in computer vision has not been\nable to stay on track comparatively. Recently, integration of contrastive\nlearning on top of existing SSL models has established considerable progress in\ncomputer vision through which visual SSL models have outperformed their\nsupervised counterparts. Nevertheless, most of these improvements were limited\nto classification tasks, and also, few works have been dedicated to evaluation\nof SSL models in real-world scenarios of computer vision, while the majority of\nworks are centered around datasets containing class-wise portrait images, most\nnotably, ImageNet. Consequently, in this work, we have considered dense\nprediction task of semantic segmentation in security inspection x-ray images to\nevaluate our proposed model Segmentation Localization. Based upon the model\nInstance Localization, our model SegLoc has managed to address one of the most\nchallenging downsides of contrastive learning, i.e., false negative pairs of\nquery embeddings. In order to do so, in contrast to baseline model InsLoc, our\npretraining dataset is synthesized by cropping, transforming, then pasting\nalready labeled segments from an available labeled dataset, foregrounds, onto\ninstances of an unlabeled dataset, backgrounds. In our case, PIDray and SIXray\ndatasets are considered as labeled and unlabeled datasets, respectively.\nMoreover, we fully harness labels by avoiding false negative pairs through\nimplementing the idea, one queue per class, in MoCo-v2 whereby negative pairs\ncorresponding to each query are extracted from its corresponding queue within\nthe memory bank. Our approach has outperformed random initialization by 3% to\n6%, while having underperformed supervised initialization.\n","authors":["Shervin Halat","Mohammad Rahmati","Ehsan Nazerfard"],"pdf_url":"https://arxiv.org/pdf/2310.08421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08420v1","updated":"2023-10-12T15:39:54Z","published":"2023-10-12T15:39:54Z","title":"Visual Attention-Prompted Prediction and Learning","summary":"  Explanation(attention)-guided learning is a method that enhances a model's\npredictive power by incorporating human understanding during the training\nphase. While attention-guided learning has shown promising results, it often\ninvolves time-consuming and computationally expensive model retraining. To\naddress this issue, we introduce the attention-prompted prediction technique,\nwhich enables direct prediction guided by the attention prompt without the need\nfor model retraining. However, this approach presents several challenges,\nincluding: 1) How to incorporate the visual attention prompt into the model's\ndecision-making process and leverage it for future predictions even in the\nabsence of a prompt? and 2) How to handle the incomplete information from the\nvisual attention prompt? To tackle these challenges, we propose a novel\nframework called Visual Attention-Prompted Prediction and Learning, which\nseamlessly integrates visual attention prompts into the model's decision-making\nprocess and adapts to images both with and without attention prompts for\nprediction. To address the incomplete information of the visual attention\nprompt, we introduce a perturbation-based attention map modification method.\nAdditionally, we propose an optimization-based mask aggregation method with a\nnew weight learning function for adaptive perturbed annotation aggregation in\nthe attention map modification process. Our overall framework is designed to\nlearn in an attention-prompt guided multi-task manner to enhance future\npredictions even for samples without attention prompts and trained in an\nalternating manner for better convergence. Extensive experiments conducted on\ntwo datasets demonstrate the effectiveness of our proposed framework in\nenhancing predictions for samples, both with and without provided prompts.\n","authors":["Yifei Zhang","Siyi Gu","Bo Pan","Guangji Bai","Xiaofeng Yang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.08420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.11713v4","updated":"2023-10-12T15:30:41Z","published":"2023-02-23T00:33:54Z","title":"Can Pre-trained Vision and Language Models Answer Visual\n  Information-Seeking Questions?","summary":"  Pre-trained vision and language models have demonstrated state-of-the-art\ncapabilities over existing tasks involving images and texts, including visual\nquestion answering. However, it remains unclear whether these models possess\nthe capability to answer questions that are not only querying visual content\nbut knowledge-intensive and information-seeking. In this study, we introduce\nInfoSeek, a visual question answering dataset tailored for information-seeking\nquestions that cannot be answered with only common sense knowledge. Using\nInfoSeek, we analyze various pre-trained visual question answering models and\ngain insights into their characteristics. Our findings reveal that\nstate-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)\nface challenges in answering visual information-seeking questions, but\nfine-tuning on the InfoSeek dataset elicits models to use fine-grained\nknowledge that was learned during their pre-training. Furthermore, we show that\naccurate visual entity recognition can be used to improve performance on\nInfoSeek by retrieving relevant documents, showing a significant space for\nimprovement.\n","authors":["Yang Chen","Hexiang Hu","Yi Luan","Haitian Sun","Soravit Changpinyo","Alan Ritter","Ming-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2302.11713v4.pdf","comment":"EMNLP 2023 (main conference); Our dataset and evaluation is available\n  at https://open-vision-language.github.io/infoseek/"},{"id":"http://arxiv.org/abs/2310.08398v1","updated":"2023-10-12T15:09:12Z","published":"2023-10-12T15:09:12Z","title":"Towards Design and Development of an ArUco Markers-Based Quantitative\n  Surface Tactile Sensor","summary":"  In this paper, with the goal of quantifying the qualitative image outputs of\na Vision-based Tactile Sensor (VTS), we present the design, fabrication, and\ncharacterization of a novel Quantitative Surface Tactile Sensor (called QS-TS).\nQS-TS directly estimates the sensor's gel layer deformation in real-time\nenabling safe and autonomous tactile manipulation and servoing of delicate\nobjects using robotic manipulators. The core of the proposed sensor is the\nutilization of miniature 1.5 mm x 1.5 mm synthetic square markers with inner\nbinary patterns and a broad black border, called ArUco Markers. Each ArUco\nmarker can provide real-time camera pose estimation that, in our design, is\nused as a quantitative measure for obtaining deformation of the QS-TS gel\nlayer. Moreover, thanks to the use of ArUco markers, we propose a unique\nfabrication procedure that mitigates various challenges associated with the\nfabrication of the existing marker-based VTSs and offers an intuitive and\nless-arduous method for the construction of the VTS. Remarkably, the proposed\nfabrication facilitates the integration and adherence of markers with the gel\nlayer to robustly and reliably obtain a quantitative measure of deformation in\nreal-time regardless of the orientation of ArUco Markers. The performance and\nefficacy of the proposed QS-TS in estimating the deformation of the sensor's\ngel layer were experimentally evaluated and verified. Results demonstrate the\nphenomenal performance of the QS-TS in estimating the deformation of the gel\nlayer with a relative error of <5%.\n","authors":["Ozdemir Can Kara","Charles Everson","Farshid Alambeigi"],"pdf_url":"https://arxiv.org/pdf/2310.08398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02621v2","updated":"2023-10-12T15:05:15Z","published":"2023-04-05T17:43:57Z","title":"High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation","summary":"  Image-level weakly-supervised semantic segmentation (WSSS) reduces the\nusually vast data annotation cost by surrogate segmentation masks during\ntraining. The typical approach involves training an image classification\nnetwork using global average pooling (GAP) on convolutional feature maps. This\nenables the estimation of object locations based on class activation maps\n(CAMs), which identify the importance of image regions. The CAMs are then used\nto generate pseudo-labels, in the form of segmentation masks, to supervise a\nsegmentation model in the absence of pixel-level ground truth. Our work is\nbased on two techniques for improving CAMs; importance sampling, which is a\nsubstitute for GAP, and the feature similarity loss, which utilizes a heuristic\nthat object contours almost always align with color edges in images. However,\nboth are based on the multinomial posterior with softmax, and implicitly assume\nthat classes are mutually exclusive, which turns out suboptimal in our\nexperiments. Thus, we reformulate both techniques based on binomial posteriors\nof multiple independent binary problems. This has two benefits; their\nperformance is improved and they become more general, resulting in an add-on\nmethod that can boost virtually any WSSS method. This is demonstrated on a wide\nvariety of baselines on the PASCAL VOC dataset, improving the region similarity\nand contour quality of all implemented state-of-the-art methods. Experiments on\nthe MS COCO dataset show that our proposed add-on is well-suited for\nlarge-scale settings. Our code is available at https://github.com/arvijj/hfpl.\n","authors":["Arvi Jonnarth","Yushan Zhang","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2304.02621v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04946v2","updated":"2023-10-12T15:04:30Z","published":"2023-09-10T06:33:17Z","title":"Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation","summary":"  Audio-driven talking-head synthesis is a popular research topic for virtual\nhuman-related applications. However, the inflexibility and inefficiency of\nexisting methods, which necessitate expensive end-to-end training to transfer\nemotions from guidance videos to talking-head predictions, are significant\nlimitations. In this work, we propose the Emotional Adaptation for Audio-driven\nTalking-head (EAT) method, which transforms emotion-agnostic talking-head\nmodels into emotion-controllable ones in a cost-effective and efficient manner\nthrough parameter-efficient adaptations. Our approach utilizes a pretrained\nemotion-agnostic talking-head transformer and introduces three lightweight\nadaptations (the Deep Emotional Prompts, Emotional Deformation Network, and\nEmotional Adaptation Module) from different perspectives to enable precise and\nrealistic emotion controls. Our experiments demonstrate that our approach\nachieves state-of-the-art performance on widely-used benchmarks, including LRW\nand MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable\ngeneralization ability, even in scenarios where emotional training videos are\nscarce or nonexistent. Project website: https://yuangan.github.io/eat/\n","authors":["Yuan Gan","Zongxin Yang","Xihang Yue","Lingyun Sun","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2309.04946v2.pdf","comment":"Accepted to ICCV 2023. Project page: https://yuangan.github.io/eat/"},{"id":"http://arxiv.org/abs/2310.08390v1","updated":"2023-10-12T15:00:06Z","published":"2023-10-12T15:00:06Z","title":"Hyp-UML: Hyperbolic Image Retrieval with Uncertainty-aware Metric\n  Learning","summary":"  Metric learning plays a critical role in training image retrieval and\nclassification. It is also a key algorithm in representation learning, e.g.,\nfor feature learning and its alignment in metric space. Hyperbolic embedding\nhas been recently developed, compared to the conventional Euclidean embedding\nin most of the previously developed models, and can be more effective in\nrepresenting the hierarchical data structure. Second, uncertainty\nestimation/measurement is a long-lasting challenge in artificial intelligence.\nSuccessful uncertainty estimation can improve a machine learning model's\nperformance, robustness, and security. In Hyperbolic space, uncertainty\nmeasurement is at least with equivalent, if not more, critical importance. In\nthis paper, we develop a Hyperbolic image embedding with uncertainty-aware\nmetric learning for image retrieval. We call our method Hyp-UML: Hyperbolic\nUncertainty-aware Metric Learning. Our contribution are threefold: we propose\nan image embedding algorithm based on Hyperbolic space, with their\ncorresponding uncertainty value; we propose two types of uncertainty-aware\nmetric learning, for the popular Contrastive learning and conventional\nmargin-based metric learning, respectively. We perform extensive experimental\nvalidations to prove that the proposed algorithm can achieve state-of-the-art\nresults among related methods. The comprehensive ablation study validates the\neffectiveness of each component of the proposed algorithm.\n","authors":["Shiyang Yan","Zongxuan Liu","Lin Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08387v1","updated":"2023-10-12T14:59:22Z","published":"2023-10-12T14:59:22Z","title":"MeanAP-Guided Reinforced Active Learning for Object Detection","summary":"  Active learning presents a promising avenue for training high-performance\nmodels with minimal labeled data, achieved by judiciously selecting the most\ninformative instances to label and incorporating them into the task learner.\nDespite notable advancements in active learning for image recognition, metrics\ndevised or learned to gauge the information gain of data, crucial for query\nstrategy design, do not consistently align with task model performance metrics,\nsuch as Mean Average Precision (MeanAP) in object detection tasks. This paper\nintroduces MeanAP-Guided Reinforced Active Learning for Object Detection\n(MAGRAL), a novel approach that directly utilizes the MeanAP metric of the task\nmodel to devise a sampling strategy employing a reinforcement learning-based\nsampling agent. Built upon LSTM architecture, the agent efficiently explores\nand selects subsequent training instances, and optimizes the process through\npolicy gradient with MeanAP serving as reward. Recognizing the time-intensive\nnature of MeanAP computation at each step, we propose fast look-up tables to\nexpedite agent training. We assess MAGRAL's efficacy across popular benchmarks,\nPASCAL VOC and MS COCO, utilizing different backbone architectures. Empirical\nfindings substantiate MAGRAL's superiority over recent state-of-the-art\nmethods, showcasing substantial performance gains. MAGRAL establishes a robust\nbaseline for reinforced active object detection, signifying its potential in\nadvancing the field.\n","authors":["Zhixuan Liang","Xingyu Zeng","Rui Zhao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2310.08387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08381v1","updated":"2023-10-12T14:55:31Z","published":"2023-10-12T14:55:31Z","title":"AutoVP: An Automated Visual Prompting Framework and Benchmark","summary":"  Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach\nto adapting pre-trained vision models to solve various downstream\nimage-classification tasks. However, there has hitherto been little systematic\nstudy of the design space of VP and no clear benchmark for evaluating its\nperformance. To bridge this gap, we propose AutoVP, an end-to-end expandable\nframework for automating VP design choices, along with 12 downstream\nimage-classification tasks that can serve as a holistic VP-performance\nbenchmark. Our design space covers 1) the joint optimization of the prompts; 2)\nthe selection of pre-trained models, including image classifiers and text-image\nencoders; and 3) model output mapping strategies, including nonparametric and\ntrainable label mapping. Our extensive experimental results show that AutoVP\noutperforms the best-known current VP methods by a substantial margin, having\nup to 6.7% improvement in accuracy; and attains a maximum performance increase\nof 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold\ncontribution: serving both as an efficient tool for hyperparameter tuning on VP\ndesign choices, and as a comprehensive benchmark that can reasonably be\nexpected to accelerate VP's development. The source code is available at\nhttps://github.com/IBM/AutoVP.\n","authors":["Hsi-Ai Tsao","Lei Hsiung","Pin-Yu Chen","Sijia Liu","Tsung-Yi Ho"],"pdf_url":"https://arxiv.org/pdf/2310.08381v1.pdf","comment":"Preprint. The code is available at https://github.com/IBM/AutoVP"},{"id":"http://arxiv.org/abs/2310.08371v1","updated":"2023-10-12T14:40:24Z","published":"2023-10-12T14:40:24Z","title":"Worst-Case Morphs using Wasserstein ALI and Improved MIPGAN","summary":"  A lot of progress has been made in the last years on using Generative\nAdversarial Networks (GAN) to create realistic images. However, to be able\nreconstruct images or to generate images using real data as input, an Encoder\nis needed that reverses the mapping from the GAN's latent space to image space.\nThis means that three networks are needed: an Encoder, a Decoder (called\nGenerator in a normal GAN) and a Discriminator. These three networks can be\ntrained from scratch simultaneously (Adversarially Learned Inference), or\nalternatively an Encoder network can be trained that maps images into the\nlatent space of a \\textit{pretrained} GAN model (Inverse GAN). In the latter\ncase, the networks are trained consecutively, so the Encoder has to make do\nwith whatever model the Decoder learned during GAN training. Training three\nnetworks simultaneously is more unstable and therefore more challenging, but it\nis possible that the Encoder and Decoder benefit from interacting with each\nother during training. We compare the two different approaches and discuss\nwhether it is worth the extra effort to train all three networks\nsimultaneously.\n","authors":["Una M. Kelly","Meike Nauta","Lu Liu","Luuk J. Spreeuwers","Raymond N. J. Veldhuis"],"pdf_url":"https://arxiv.org/pdf/2310.08371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08370v1","updated":"2023-10-12T14:39:58Z","published":"2023-10-12T14:39:58Z","title":"UniPAD: A Universal Pre-training Paradigm for Autonomous Driving","summary":"  In the context of autonomous driving, the significance of effective feature\nlearning is widely acknowledged. While conventional 3D self-supervised\npre-training methods have shown widespread success, most methods follow the\nideas originally designed for 2D images. In this paper, we present UniPAD, a\nnovel self-supervised learning paradigm applying 3D volumetric differentiable\nrendering. UniPAD implicitly encodes 3D space, facilitating the reconstruction\nof continuous 3D shape structures and the intricate appearance characteristics\nof their 2D projections. The flexibility of our method enables seamless\nintegration into both 2D and 3D frameworks, enabling a more holistic\ncomprehension of the scenes. We manifest the feasibility and effectiveness of\nUniPAD by conducting extensive experiments on various downstream 3D tasks. Our\nmethod significantly improves lidar-, camera-, and lidar-camera-based baseline\nby 9.1, 7.7, and 6.9 NDS, respectively. Notably, our pre-training pipeline\nachieves 73.2 NDS for 3D object detection and 79.4 mIoU for 3D semantic\nsegmentation on the nuScenes validation set, achieving state-of-the-art results\nin comparison with previous methods. The code will be available at\nhttps://github.com/Nightmare-n/UniPAD.\n","authors":["Honghui Yang","Sha Zhang","Di Huang","Xiaoyang Wu","Haoyi Zhu","Tong He","Shixiang Tang","Hengshuang Zhao","Qibo Qiu","Binbin Lin","Xiaofei He","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2310.08370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08368v1","updated":"2023-10-12T14:38:52Z","published":"2023-10-12T14:38:52Z","title":"Mapping Memes to Words for Multimodal Hateful Meme Classification","summary":"  Multimodal image-text memes are prevalent on the internet, serving as a\nunique form of communication that combines visual and textual elements to\nconvey humor, ideas, or emotions. However, some memes take a malicious turn,\npromoting hateful content and perpetuating discrimination. Detecting hateful\nmemes within this multimodal context is a challenging task that requires\nunderstanding the intertwined meaning of text and images. In this work, we\naddress this issue by proposing a novel approach named ISSUES for multimodal\nhateful meme classification. ISSUES leverages a pre-trained CLIP\nvision-language model and the textual inversion technique to effectively\ncapture the multimodal semantic content of the memes. The experiments show that\nour method achieves state-of-the-art results on the Hateful Memes Challenge and\nHarMeme datasets. The code and the pre-trained models are publicly available at\nhttps://github.com/miccunifi/ISSUES.\n","authors":["Giovanni Burbi","Alberto Baldrati","Lorenzo Agnolucci","Marco Bertini","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2310.08368v1.pdf","comment":"ICCV2023 CLVL Workshop"},{"id":"http://arxiv.org/abs/2310.08367v1","updated":"2023-10-12T14:38:25Z","published":"2023-10-12T14:38:25Z","title":"MCU: A Task-centric Framework for Open-ended Agent Evaluation in\n  Minecraft","summary":"  To pursue the goal of creating an open-ended agent in Minecraft, an\nopen-ended game environment with unlimited possibilities, this paper introduces\na task-centric framework named MCU for Minecraft agent evaluation. The MCU\nframework leverages the concept of atom tasks as fundamental building blocks,\nenabling the generation of diverse or even arbitrary tasks. Within the MCU\nframework, each task is measured with six distinct difficulty scores (time\nconsumption, operational effort, planning complexity, intricacy, creativity,\nnovelty). These scores offer a multi-dimensional assessment of a task from\ndifferent angles, and thus can reveal an agent's capability on specific facets.\nThe difficulty scores also serve as the feature of each task, which creates a\nmeaningful task space and unveils the relationship between tasks. For efficient\nevaluation of Minecraft agents employing the MCU framework, we maintain a\nunified benchmark, namely SkillForge, which comprises representative tasks with\ndiverse categories and difficulty distribution. We also provide convenient\nfilters for users to select tasks to assess specific capabilities of agents. We\nshow that MCU has the high expressivity to cover all tasks used in recent\nliterature on Minecraft agent, and underscores the need for advancements in\nareas such as creativity, precise control, and out-of-distribution\ngeneralization under the goal of open-ended Minecraft agent development.\n","authors":["Haowei Lin","Zihao Wang","Jianzhu Ma","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2310.08367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04099v2","updated":"2023-10-12T14:18:52Z","published":"2023-10-06T09:01:15Z","title":"ClusVPR: Efficient Visual Place Recognition with Clustering-based\n  Weighted Transformer","summary":"  Visual place recognition (VPR) is a highly challenging task that has a wide\nrange of applications, including robot navigation and self-driving vehicles.\nVPR is particularly difficult due to the presence of duplicate regions and the\nlack of attention to small objects in complex scenes, resulting in recognition\ndeviations. In this paper, we present ClusVPR, a novel approach that tackles\nthe specific issues of redundant information in duplicate regions and\nrepresentations of small objects. Different from existing methods that rely on\nConvolutional Neural Networks (CNNs) for feature map generation, ClusVPR\nintroduces a unique paradigm called Clustering-based Weighted Transformer\nNetwork (CWTNet). CWTNet leverages the power of clustering-based weighted\nfeature maps and integrates global dependencies to effectively address visual\ndeviations encountered in large-scale VPR problems. We also introduce the\noptimized-VLAD (OptLAD) layer that significantly reduces the number of\nparameters and enhances model efficiency. This layer is specifically designed\nto aggregate the information obtained from scale-wise image patches.\nAdditionally, our pyramid self-supervised strategy focuses on extracting\nrepresentative and diverse information from scale-wise image patches instead of\nentire images, which is crucial for capturing representative and diverse\ninformation in VPR. Extensive experiments on four VPR datasets show our model's\nsuperior performance compared to existing models while being less complex.\n","authors":["Yifan Xu","Pourya Shamsolmoali","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2310.04099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08339v1","updated":"2023-10-12T13:57:32Z","published":"2023-10-12T13:57:32Z","title":"A Generic Software Framework for Distributed Topological Analysis\n  Pipelines","summary":"  This system paper presents a software framework for the support of\ntopological analysis pipelines in a distributed-memory model. While several\nrecent papers introduced topology-based approaches for distributed-memory\nenvironments, these were reporting experiments obtained with tailored,\nmono-algorithm implementations. In contrast, we describe in this paper a\ngeneral-purpose, generic framework for topological analysis pipelines, i.e. a\nsequence of topological algorithms interacting together, possibly on distinct\nnumbers of processes. Specifically, we instantiated our framework with the MPI\nmodel, within the Topology ToolKit (TTK). While developing this framework, we\nfaced several algorithmic and software engineering challenges, which we\ndocument in this paper. We provide a taxonomy for the distributed-memory\ntopological algorithms supported by TTK, depending on their communication needs\nand provide examples of hybrid MPI+thread parallelizations. Detailed\nperformance analyses show that parallel efficiencies range from $20\\%$ to\n$80\\%$ (depending on the algorithms), and that the MPI-specific preconditioning\nintroduced by our framework induces a negligible computation time overhead. We\nillustrate the new distributed-memory capabilities of TTK with an example of\nadvanced analysis pipeline, combining multiple algorithms, run on the largest\npublicly available dataset we have found (120 billion vertices) on a standard\ncluster with 64 nodes (for a total of 1,536 cores). Finally, we provide a\nroadmap for the completion of TTK's MPI extension, along with generic\nrecommendations for each algorithm communication category.\n","authors":["Eve Le Guillou","Michael Will","Pierre Guillou","Jonas Lukasczyk","Pierre Fortin","Christoph Garth","Julien Tierny"],"pdf_url":"https://arxiv.org/pdf/2310.08339v1.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2206.02136v3","updated":"2023-10-12T13:55:06Z","published":"2022-06-05T09:39:12Z","title":"LDRNet: Enabling Real-time Document Localization on Mobile Devices","summary":"  While Identity Document Verification (IDV) technology on mobile devices\nbecomes ubiquitous in modern business operations, the risk of identity theft\nand fraud is increasing. The identity document holder is normally required to\nparticipate in an online video interview to circumvent impostors. However, the\ncurrent IDV process depends on an additional human workforce to support online\nstep-by-step guidance which is inefficient and expensive. The performance of\nexisting AI-based approaches cannot meet the real-time and lightweight demands\nof mobile devices. In this paper, we address those challenges by designing an\nedge intelligence-assisted approach for real-time IDV. Aiming at improving the\nresponsiveness of the IDV process, we propose a new document localization model\nfor mobile devices, LDRNet, to Localize the identity Document in Real-time. On\nthe basis of a lightweight backbone network, we build three prediction branches\nfor LDRNet, the corner points prediction, the line borders prediction and the\ndocument classification. We design novel supplementary targets, the\nequal-division points, and use a new loss function named Line Loss, to improve\nthe speed and accuracy of our approach. In addition to the IDV process, LDRNet\nis an efficient and reliable document localization alternative for all kinds of\nmobile applications. As a matter of proof, we compare the performance of LDRNet\nwith other popular approaches on localizing general document datasets. The\nexperimental results show that LDRNet runs at a speed up to 790 FPS which is\n47x faster, while still achieving comparable Jaccard Index(JI) in single-model\nand single-scale tests.\n","authors":["Han Wu","Holland Qian","Huaming Wu","Aad van Moorsel"],"pdf_url":"https://arxiv.org/pdf/2206.02136v3.pdf","comment":"ECML-PKDD 2022 https://doi.org/10.1007/978-3-031-23618-1_42"},{"id":"http://arxiv.org/abs/2310.08332v1","updated":"2023-10-12T13:46:36Z","published":"2023-10-12T13:46:36Z","title":"Real-Time Neural BRDF with Spherically Distributed Primitives","summary":"  We propose a novel compact and efficient neural BRDF offering highly\nversatile material representation, yet with very-light memory and neural\ncomputation consumption towards achieving real-time rendering. The results in\nFigure 1, rendered at full HD resolution on a current desktop machine, show\nthat our system achieves real-time rendering with a wide variety of\nappearances, which is approached by the following two designs. On the one hand,\nnoting that bidirectional reflectance is distributed in a very sparse\nhigh-dimensional subspace, we propose to project the BRDF into two\nlow-dimensional components, i.e., two hemisphere feature-grids for incoming and\noutgoing directions, respectively. On the other hand, learnable neural\nreflectance primitives are distributed on our highly-tailored spherical surface\ngrid, which offer informative features for each component and alleviate the\nconventional heavy feature learning network to a much smaller one, leading to\nvery fast evaluation. These primitives are centrally stored in a codebook and\ncan be shared across multiple grids and even across materials, based on the\nlow-cost indices stored in material-specific spherical surface grids. Our\nneural BRDF, which is agnostic to the material, provides a unified framework\nthat can represent a variety of materials in consistent manner. Comprehensive\nexperimental results on measured BRDF compression, Monte Carlo simulated BRDF\nacceleration, and extension to spatially varying effect demonstrate the\nsuperior quality and generalizability achieved by the proposed scheme.\n","authors":["Yishun Dou","Zhong Zheng","Qiaoqiao Jin","Bingbing Ni","Yugang Chen","Junxiang Ke"],"pdf_url":"https://arxiv.org/pdf/2310.08332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08326v1","updated":"2023-10-12T13:42:49Z","published":"2023-10-12T13:42:49Z","title":"NSM4D: Neural Scene Model Based Online 4D Point Cloud Sequence\n  Understanding","summary":"  Understanding 4D point cloud sequences online is of significant practical\nvalue in various scenarios such as VR/AR, robotics, and autonomous driving. The\nkey goal is to continuously analyze the geometry and dynamics of a 3D scene as\nunstructured and redundant point cloud sequences arrive. And the main challenge\nis to effectively model the long-term history while keeping computational costs\nmanageable. To tackle these challenges, we introduce a generic online 4D\nperception paradigm called NSM4D. NSM4D serves as a plug-and-play strategy that\ncan be adapted to existing 4D backbones, significantly enhancing their online\nperception capabilities for both indoor and outdoor scenarios. To efficiently\ncapture the redundant 4D history, we propose a neural scene model that\nfactorizes geometry and motion information by constructing geometry tokens\nseparately storing geometry and motion features. Exploiting the history becomes\nas straightforward as querying the neural scene model. As the sequence\nprogresses, the neural scene model dynamically deforms to align with new\nobservations, effectively providing the historical context and updating itself\nwith the new observations. By employing token representation, NSM4D also\nexhibits robustness to low-level sensor noise and maintains a compact size\nthrough a geometric sampling scheme. We integrate NSM4D with state-of-the-art\n4D perception backbones, demonstrating significant improvements on various\nonline perception benchmarks in indoor and outdoor settings. Notably, we\nachieve a 9.6% accuracy improvement for HOI4D online action segmentation and a\n3.4% mIoU improvement for SemanticKITTI online semantic segmentation.\nFurthermore, we show that NSM4D inherently offers excellent scalability to\nlonger sequences beyond the training set, which is crucial for real-world\napplications.\n","authors":["Yuhao Dong","Zhuoyang Zhang","Yunze Liu","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2310.08326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08320v1","updated":"2023-10-12T13:33:04Z","published":"2023-10-12T13:33:04Z","title":"Defending Our Privacy With Backdoors","summary":"  The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information such as names of individuals\nfrom models, and focus in this work on text encoders. Specifically, through\nstrategic insertion of backdoors, we align the embeddings of sensitive phrases\nwith those of neutral terms-\"a person\" instead of the person's name. Our\nempirical results demonstrate the effectiveness of our backdoor-based defense\non CLIP by assessing its performance using a specialized privacy attack for\nzero-shot classifiers. Our approach provides not only a new \"dual-use\"\nperspective on backdoor attacks, but also presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n","authors":["Dominik Hintersdorf","Lukas Struppek","Daniel Neider","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2310.08320v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2310.08316v1","updated":"2023-10-12T13:27:21Z","published":"2023-10-12T13:27:21Z","title":"Extended target tracking utilizing machine-learning software -- with\n  applications to animal classification","summary":"  This paper considers the problem of detecting and tracking objects in a\nsequence of images. The problem is formulated in a filtering framework, using\nthe output of object-detection algorithms as measurements. An extension to the\nfiltering formulation is proposed that incorporates class information from the\nprevious frame to robustify the classification, even if the object-detection\nalgorithm outputs an incorrect prediction. Further, the properties of the\nobject-detection algorithm are exploited to quantify the uncertainty of the\nbounding box detection in each frame. The complete filtering method is\nevaluated on camera trap images of the four large Swedish carnivores, bear,\nlynx, wolf, and wolverine. The experiments show that the class tracking\nformulation leads to a more robust classification.\n","authors":["Magnus Malmström","Anton Kullberg","Isaac Skog","Daniel Axehill","Fredrik Gustafsson"],"pdf_url":"https://arxiv.org/pdf/2310.08316v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.08312v1","updated":"2023-10-12T13:20:17Z","published":"2023-10-12T13:20:17Z","title":"GePSAn: Generative Procedure Step Anticipation in Cooking Videos","summary":"  We study the problem of future step anticipation in procedural videos. Given\na video of an ongoing procedural activity, we predict a plausible next\nprocedure step described in rich natural language. While most previous work\nfocus on the problem of data scarcity in procedural video datasets, another\ncore challenge of future anticipation is how to account for multiple plausible\nfuture realizations in natural settings. This problem has been largely\noverlooked in previous work. To address this challenge, we frame future step\nprediction as modelling the distribution of all possible candidates for the\nnext step. Specifically, we design a generative model that takes a series of\nvideo clips as input, and generates multiple plausible and diverse candidates\n(in natural language) for the next step. Following previous work, we side-step\nthe video annotation scarcity by pretraining our model on a large text-based\ncorpus of procedural activities, and then transfer the model to the video\ndomain. Our experiments, both in textual and video domains, show that our model\ncaptures diversity in the next step prediction and generates multiple plausible\nfuture predictions. Moreover, our model establishes new state-of-the-art\nresults on YouCookII, where it outperforms existing baselines on the next step\nanticipation. Finally, we also show that our model can successfully transfer\nfrom text to the video domain zero-shot, ie, without fine-tuning or adaptation,\nand produces good-quality future step predictions from video.\n","authors":["Mohamed Ashraf Abdelsalam","Samrudhdhi B. Rangrej","Isma Hadji","Nikita Dvornik","Konstantinos G. Derpanis","Afsaneh Fazly"],"pdf_url":"https://arxiv.org/pdf/2310.08312v1.pdf","comment":"published at ICCV 2023"},{"id":"http://arxiv.org/abs/2310.08304v1","updated":"2023-10-12T13:11:38Z","published":"2023-10-12T13:11:38Z","title":"CHIP: Contrastive Hierarchical Image Pretraining","summary":"  Few-shot object classification is the task of classifying objects in an image\nwith limited number of examples as supervision. We propose a one-shot/few-shot\nclassification model that can classify an object of any unseen class into a\nrelatively general category in an hierarchically based classification. Our\nmodel uses a three-level hierarchical contrastive loss based ResNet152\nclassifier for classifying an object based on its features extracted from Image\nembedding, not used during the training phase. For our experimentation, we have\nused a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal\nclasses for training our model and created our own dataset of unseen classes\nfor evaluating our trained model. Our model provides satisfactory results in\nclassifying the unknown objects into a generic category which has been later\ndiscussed in greater detail.\n","authors":["Arpit Mittal","Harshil Jhaveri","Swapnil Mallick","Abhishek Ajmera"],"pdf_url":"https://arxiv.org/pdf/2310.08304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08303v1","updated":"2023-10-12T13:09:40Z","published":"2023-10-12T13:09:40Z","title":"Multimodal Variational Auto-encoder based Audio-Visual Segmentation","summary":"  We propose an Explicit Conditional Multimodal Variational Auto-Encoder\n(ECMVAE) for audio-visual segmentation (AVS), aiming to segment sound sources\nin the video sequence. Existing AVS methods focus on implicit feature fusion\nstrategies, where models are trained to fit the discrete samples in the\ndataset. With a limited and less diverse dataset, the resulting performance is\nusually unsatisfactory. In contrast, we address this problem from an effective\nrepresentation learning perspective, aiming to model the contribution of each\nmodality explicitly. Specifically, we find that audio contains critical\ncategory information of the sound producers, and visual data provides candidate\nsound producer(s). Their shared information corresponds to the target sound\nproducer(s) shown in the visual data. In this case, cross-modal shared\nrepresentation learning is especially important for AVS. To achieve this, our\nECMVAE factorizes the representations of each modality with a modality-shared\nrepresentation and a modality-specific representation. An orthogonality\nconstraint is applied between the shared and specific representations to\nmaintain the exclusive attribute of the factorized latent code. Further, a\nmutual information maximization regularizer is introduced to achieve extensive\nexploration of each modality. Quantitative and qualitative evaluations on the\nAVSBench demonstrate the effectiveness of our approach, leading to a new\nstate-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging\nMS3 subset for multiple sound source segmentation.\n","authors":["Yuxin Mao","Jing Zhang","Mochu Xiang","Yiran Zhong","Yuchao Dai"],"pdf_url":"https://arxiv.org/pdf/2310.08303v1.pdf","comment":"Accepted by ICCV2023,Project\n  page(https://npucvr.github.io/MMVAE-AVS),Code(https://github.com/OpenNLPLab/MMVAE-AVS)"},{"id":"http://arxiv.org/abs/2308.12435v2","updated":"2023-10-12T12:57:55Z","published":"2023-08-23T21:36:35Z","title":"Characterising representation dynamics in recurrent neural networks for\n  object recognition","summary":"  Recurrent neural networks (RNNs) have yielded promising results for both\nrecognizing objects in challenging conditions and modeling aspects of primate\nvision. However, the representational dynamics of recurrent computations remain\npoorly understood, especially in large-scale visual models. Here, we studied\nsuch dynamics in RNNs trained for object classification on MiniEcoset, a novel\nsubset of ecoset. We report two main insights. First, upon inference,\nrepresentations continued to evolve after correct classification, suggesting a\nlack of the notion of being ``done with classification''. Second, focusing on\n``readout zones'' as a way to characterize the activation trajectories, we\nobserve that misclassified representations exhibit activation patterns with\nlower L2 norm, and are positioned more peripherally in the readout zones. Such\narrangements help the misclassified representations move into the correct zones\nas time progresses. Our findings generalize to networks with lateral and\ntop-down connections, and include both additive and multiplicative interactions\nwith the bottom-up sweep. The results therefore contribute to a general\nunderstanding of RNN dynamics in naturalistic tasks. We hope that the analysis\nframework will aid future investigations of other types of RNNs, including\nunderstanding of representational dynamics in primate vision.\n","authors":["Sushrut Thorat","Adrien Doerig","Tim C. Kietzmann"],"pdf_url":"https://arxiv.org/pdf/2308.12435v2.pdf","comment":"8 pages, 7 figures; revision of our Conference on Cognitive\n  Computational Neuroscience (CCN) 2023 paper"},{"id":"http://arxiv.org/abs/2202.09348v2","updated":"2023-10-12T12:56:28Z","published":"2022-02-18T18:36:01Z","title":"A Machine Learning Paradigm for Studying Pictorial Realism: Are\n  Constable's Clouds More Real than His Contemporaries?","summary":"  The British landscape painter John Constable is considered foundational for\nthe Realist movement in 19th-century European painting. Constable's painted\nskies, in particular, were seen as remarkably accurate by his contemporaries,\nan impression shared by many viewers today. Yet, assessing the accuracy of\nrealist paintings like Constable's is subjective or intuitive, even for\nprofessional art historians, making it difficult to say with certainty what set\nConstable's skies apart from those of his contemporaries. Our goal is to\ncontribute to a more objective understanding of Constable's realism. We propose\na new machine-learning-based paradigm for studying pictorial realism in an\nexplainable way. Our framework assesses realism by measuring the similarity\nbetween clouds painted by artists noted for their skies, like Constable, and\nphotographs of clouds. The experimental results of cloud classification show\nthat Constable approximates more consistently than his contemporaries the\nformal features of actual clouds in his paintings. The study, as a novel\ninterdisciplinary approach that combines computer vision and machine learning,\nmeteorology, and art history, is a springboard for broader and deeper analyses\nof pictorial realism.\n","authors":["Zhuomin Zhang","Elizabeth C. Mansfield","Jia Li","John Russell","George S. Young","Catherine Adams","James Z. Wang"],"pdf_url":"https://arxiv.org/pdf/2202.09348v2.pdf","comment":"Supplementary materials are available from the authors or\n  http://wang.ist.psu.edu"},{"id":"http://arxiv.org/abs/2303.07189v3","updated":"2023-10-12T12:47:22Z","published":"2023-03-13T15:30:28Z","title":"Optimizing Convolutional Neural Networks for Chronic Obstructive\n  Pulmonary Disease Detection in Clinical Computed Tomography Imaging","summary":"  We aim to optimize the binary detection of Chronic Obstructive Pulmonary\nDisease (COPD) based on emphysema presence in the lung with convolutional\nneural networks (CNN) by exploring manually adjusted versus automated\nwindow-setting optimization (WSO) on computed tomography (CT) images. 7,194 CT\nimages (3,597 with COPD; 3,597 healthy controls) from 78 subjects (43 with\nCOPD; 35 healthy controls) were selected retrospectively (10.2018-12.2019) and\npreprocessed. For each image, intensity values were manually clipped to the\nemphysema window setting and a baseline 'full-range' window setting.\nClass-balanced train, validation, and test sets contained 3,392, 1,114, and\n2,688 images. The network backbone was optimized by comparing various CNN\narchitectures. Furthermore, automated WSO was implemented by adding a\ncustomized layer to the model. The image-level area under the Receiver\nOperating Characteristics curve (AUC) [lower, upper limit 95% confidence] was\nutilized to compare model variations. Repeated inference (n=7) on the test set\nshowed that the DenseNet was the most efficient backbone and achieved a mean\nAUC of 0.80 [0.76, 0.85] without WSO. Comparably, with input images manually\nadjusted to the emphysema window, the DenseNet model predicted COPD with a mean\nAUC of 0.86 [0.82, 0.89]. By adding a customized WSO layer to the DenseNet, an\noptimal window in the proximity of the emphysema window setting was learned\nautomatically, and a mean AUC of 0.82 [0.78, 0.86] was achieved. Detection of\nCOPD with DenseNet models was improved by WSO of CT data to the emphysema\nwindow setting range.\n","authors":["Tina Dorosti","Manuel Schultheiss","Felix Hofmann","Johannes Thalhammer","Luisa Kirchner","Theresa Urban","Franz Pfeiffer","Florian Schaff","Tobias Lasser","Daniela Pfeiffer"],"pdf_url":"https://arxiv.org/pdf/2303.07189v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.01601v3","updated":"2023-10-12T12:43:39Z","published":"2023-04-04T07:43:56Z","title":"Primitive Simultaneous Optimization of Similarity Metrics for Image\n  Registration","summary":"  Even though simultaneous optimization of similarity metrics is a standard\nprocedure in the field of semantic segmentation, surprisingly, this is much\nless established for image registration. To help closing this gap in the\nliterature, we investigate in a complex multi-modal 3D setting whether\nsimultaneous optimization of registration metrics, here implemented by means of\nprimitive summation, can benefit image registration. We evaluate two\nchallenging datasets containing collections of pre- to post-operative and pre-\nto intra-operative MR images of glioma. Employing the proposed optimization, we\ndemonstrate improved registration accuracy in terms of TRE on expert\nneuroradiologists' landmark annotations.\n","authors":["Diana Waldmannstetter","Benedikt Wiestler","Julian Schwarting","Ivan Ezhov","Marie Metz","Spyridon Bakas","Bhakti Baheti","Satrajit Chakrabarty","Daniel Rueckert","Jan S. Kirschke","Rolf A. Heckemann","Marie Piraud","Bjoern H. Menze","Florian Kofler"],"pdf_url":"https://arxiv.org/pdf/2304.01601v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08276v1","updated":"2023-10-12T12:28:47Z","published":"2023-10-12T12:28:47Z","title":"Direction-Oriented Visual-semantic Embedding Model for Remote Sensing\n  Image-text Retrieval","summary":"  Image-text retrieval has developed rapidly in recent years. However, it is\nstill a challenge in remote sensing due to visual-semantic imbalance, which\nleads to incorrect matching of non-semantic visual and textual features. To\nsolve this problem, we propose a novel Direction-Oriented Visual-semantic\nEmbedding Model (DOVE) to mine the relationship between vision and language.\nConcretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the\ndistance between the final visual and textual embeddings in the latent semantic\nspace, oriented by regional visual features. Meanwhile, a lightweight Digging\nText Genome Assistant (DTGA) is designed to expand the range of tractable\ntextual representation and enhance global word-level semantic connections using\nless attention operations. Ultimately, we exploit a global visual-semantic\nconstraint to reduce single visual dependency and serve as an external\nconstraint for the final visual and textual representations. The effectiveness\nand superiority of our method are verified by extensive experiments including\nparameter evaluation, quantitative comparison, ablation studies and visual\nanalysis, on two benchmark datasets, RSICD and RSITMD.\n","authors":["Qing Ma","Jiancheng Pan","Cong Bai"],"pdf_url":"https://arxiv.org/pdf/2310.08276v1.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2310.08261v1","updated":"2023-10-12T12:06:31Z","published":"2023-10-12T12:06:31Z","title":"GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for\n  Multi-Modal 3D Object Detection","summary":"  LiDAR and cameras are complementary sensors for 3D object detection in\nautonomous driving. However, it is challenging to explore the unnatural\ninteraction between point clouds and images, and the critical factor is how to\nconduct feature alignment of heterogeneous modalities. Currently, many methods\nachieve feature alignment by projection calibration only, without considering\nthe problem of coordinate conversion accuracy errors between sensors, leading\nto sub-optimal performance. In this paper, we present GraphAlign, a more\naccurate feature alignment strategy for 3D object detection by graph matching.\nSpecifically, we fuse image features from a semantic segmentation encoder in\nthe image branch and point cloud features from a 3D Sparse CNN in the LiDAR\nbranch. To save computation, we construct the nearest neighbor relationship by\ncalculating Euclidean distance within the subspaces that are divided into the\npoint cloud features. Through the projection calibration between the image and\npoint cloud, we project the nearest neighbors of point cloud features onto the\nimage features. Then by matching the nearest neighbors with a single point\ncloud to multiple images, we search for a more appropriate feature alignment.\nIn addition, we provide a self-attention module to enhance the weights of\nsignificant relations to fine-tune the feature alignment between heterogeneous\nmodalities. Extensive experiments on nuScenes benchmark demonstrate the\neffectiveness and efficiency of our GraphAlign.\n","authors":["Ziying Song","Haiyue Wei","Lin Bai","Lei Yang","Caiyan Jia"],"pdf_url":"https://arxiv.org/pdf/2310.08261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08259v1","updated":"2023-10-12T12:05:51Z","published":"2023-10-12T12:05:51Z","title":"Invisible Threats: Backdoor Attack in OCR Systems","summary":"  Optical Character Recognition (OCR) is a widely used tool to extract text\nfrom scanned documents. Today, the state-of-the-art is achieved by exploiting\ndeep neural networks. However, the cost of this performance is paid at the\nprice of system vulnerability. For instance, in backdoor attacks, attackers\ncompromise the training phase by inserting a backdoor in the victim's model\nthat will be activated at testing time by specific patterns while leaving the\noverall model performance intact. This work proposes a backdoor attack for OCR\nresulting in the injection of non-readable characters from malicious input\nimages. This simple but effective attack exposes the state-of-the-art OCR\nweakness, making the extracted text correct to human eyes but simultaneously\nunusable for the NLP application that uses OCR as a preprocessing step.\nExperimental results show that the attacked models successfully output\nnon-readable characters for around 90% of the poisoned instances without\nharming their performance for the remaining instances.\n","authors":["Mauro Conti","Nicola Farronato","Stefanos Koffas","Luca Pajola","Stjepan Picek"],"pdf_url":"https://arxiv.org/pdf/2310.08259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04825v2","updated":"2023-10-12T12:05:15Z","published":"2023-10-07T14:29:57Z","title":"Comparative study of multi-person tracking methods","summary":"  This paper presents a study of two tracking algorithms (SORT~\\cite{7533003}\nand Tracktor++~\\cite{2019}) that were ranked first positions on the MOT\nChallenge leaderboard (The MOTChallenge web page: https://motchallenge.net ).\nThe purpose of this study is to discover the techniques used and to provide\nuseful insights about these algorithms in the tracking pipeline that could\nimprove the performance of MOT tracking algorithms. To this end, we adopted the\npopular tracking-by-detection approach. We trained our own Pedestrian Detection\nmodel using the MOT17Det dataset (MOT17Det :\nhttps://motchallenge.net/data/MOT17Det/ ). We also used a re-identification\nmodel trained on MOT17 dataset (MOT17 : https://motchallenge.net/data/MOT17/ )\nfor Tracktor++ to reduce the false re-identification alarms. We then present\nexperimental results which shows that Tracktor++ is a better multi-person\ntracking algorithm than SORT. We also performed ablation studies to discover\nthe contribution of re-identification(RE-ID) network and motion to the results\nof Tracktor++. We finally conclude by providing some recommendations for future\nresearch.\n","authors":["Denis Mbey Akola"],"pdf_url":"https://arxiv.org/pdf/2310.04825v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04829v2","updated":"2023-10-12T12:04:16Z","published":"2023-10-07T14:38:16Z","title":"How to effectively train an ensemble of Faster R-CNN object detectors to\n  quantify uncertainty","summary":"  This paper presents a new approach for training two-stage object detection\nensemble models, more specifically, Faster R-CNN models to estimate\nuncertainty. We propose training one Region Proposal\nNetwork(RPN)~\\cite{https://doi.org/10.48550/arxiv.1506.01497} and multiple Fast\nR-CNN prediction heads is all you need to build a robust deep ensemble network\nfor estimating uncertainty in object detection. We present this approach and\nprovide experiments to show that this approach is much faster than the naive\nmethod of fully training all $n$ models in an ensemble. We also estimate the\nuncertainty by measuring this ensemble model's Expected Calibration Error\n(ECE). We then further compare the performance of this model with that of\nGaussian YOLOv3, a variant of YOLOv3 that models uncertainty using predicted\nbounding box coordinates. The source code is released at\n\\url{https://github.com/Akola-Mbey-Denis/EfficientEnsemble}\n","authors":["Denis Mbey Akola","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2310.04829v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19443v2","updated":"2023-10-12T12:02:34Z","published":"2023-05-30T22:34:48Z","title":"OWAdapt: An adaptive loss function for deep learning using OWA operators","summary":"  In this paper, we propose a fuzzy adaptive loss function for enhancing deep\nlearning performance in classification tasks. Specifically, we redefine the\ncross-entropy loss to effectively address class-level noise conditions,\nincluding the challenging problem of class imbalance. Our approach introduces\naggregation operators, leveraging the power of fuzzy logic to improve\nclassification accuracy. The rationale behind our proposed method lies in the\niterative up-weighting of class-level components within the loss function,\nfocusing on those with larger errors. To achieve this, we employ the ordered\nweighted average (OWA) operator and combine it with an adaptive scheme for\ngradient-based learning. Through extensive experimentation, our method\noutperforms other commonly used loss functions, such as the standard\ncross-entropy or focal loss, across various binary and multiclass\nclassification tasks. Furthermore, we explore the influence of hyperparameters\nassociated with the OWA operators and present a default configuration that\nperforms well across different experimental settings.\n","authors":["Sebastián Maldonado","Carla Vairetti","Katherine Jara","Miguel Carrasco","Julio López"],"pdf_url":"https://arxiv.org/pdf/2305.19443v2.pdf","comment":"15 pages, 1 figure, published"},{"id":"http://arxiv.org/abs/2310.08255v1","updated":"2023-10-12T11:59:54Z","published":"2023-10-12T11:59:54Z","title":"Distilling from Vision-Language Models for Improved OOD Generalization\n  in Vision Tasks","summary":"  Vision-Language Models (VLMs) such as CLIP are trained on large amounts of\nimage-text pairs, resulting in remarkable generalization across several data\ndistributions. The prohibitively expensive training and data\ncollection/curation costs of these models make them valuable Intellectual\nProperty (IP) for organizations. This motivates a vendor-client paradigm, where\na vendor trains a large-scale VLM and grants only input-output access to\nclients on a pay-per-query basis in a black-box setting. The client aims to\nminimize inference cost by distilling the VLM to a student model using the\nlimited available task-specific data, and further deploying this student model\nin the downstream application. While naive distillation largely improves the\nIn-Domain (ID) accuracy of the student, it fails to transfer the superior\nout-of-distribution (OOD) generalization of the VLM teacher using the limited\navailable labeled images. To mitigate this, we propose Vision-Language to\nVision-Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and\nlanguage modalities of the teacher model with the vision modality of a\npre-trained student model, and further distills the aligned VLM embeddings to\nthe student. This maximally retains the pre-trained features of the student,\nwhile also incorporating the rich representations of the VLM image encoder and\nthe superior generalization of the text embeddings. The proposed approach\nachieves state-of-the-art results on the standard Domain Generalization\nbenchmarks in a black-box teacher setting, and also when weights of the VLM are\naccessible.\n","authors":["Sravanti Addepalli","Ashish Ramayee Asokan","Lakshay Sharma","R. Venkatesh Babu"],"pdf_url":"https://arxiv.org/pdf/2310.08255v1.pdf","comment":"Code is available at https://github.com/val-iisc/VL2V-ADiP.git"},{"id":"http://arxiv.org/abs/2309.06188v2","updated":"2023-10-12T11:51:21Z","published":"2023-09-12T12:54:12Z","title":"Computer Vision Pipeline for Automated Antarctic Krill Analysis","summary":"  British Antarctic Survey (BAS) researchers launch annual expeditions to the\nAntarctic in order to estimate Antarctic Krill biomass and assess the change\nfrom previous years. These comparisons provide insight into the effects of the\ncurrent environment on this key component of the marine food chain. In this\nwork we have developed tools for automating the data collection and analysis\nprocess, using web-based image annotation tools and deep learning image\nclassification and regression models. We achieve highly accurate krill instance\nsegmentation results with an average 77.28% AP score, as well as separate\nmaturity stage and length estimation of krill specimens with 62.99% accuracy\nand a 1.98mm length error respectively.\n","authors":["Mazvydas Gudelis","Michal Mackiewicz","Julie Bremner","Sophie Fielding"],"pdf_url":"https://arxiv.org/pdf/2309.06188v2.pdf","comment":"Accepted to MVEO @ BMVC 2023"},{"id":"http://arxiv.org/abs/2308.03998v4","updated":"2023-10-12T11:49:34Z","published":"2023-08-08T02:28:48Z","title":"Real-time Strawberry Detection Based on Improved YOLOv5s Architecture\n  for Robotic Harvesting in open-field environment","summary":"  This study proposed a YOLOv5-based custom object detection model to detect\nstrawberries in an outdoor environment. The original architecture of the\nYOLOv5s was modified by replacing the C3 module with the C2f module in the\nbackbone network, which provided a better feature gradient flow. Secondly, the\nSpatial Pyramid Pooling Fast in the final layer of the backbone network of\nYOLOv5s was combined with Cross Stage Partial Net to improve the generalization\nability over the strawberry dataset in this study. The proposed architecture\nwas named YOLOv5s-Straw. The RGB images dataset of the strawberry canopy with\nthree maturity classes (immature, nearly mature, and mature) was collected in\nopen-field environment and augmented through a series of operations including\nbrightness reduction, brightness increase, and noise adding. To verify the\nsuperiority of the proposed method for strawberry detection in open-field\nenvironment, four competitive detection models (YOLOv3-tiny, YOLOv5s,\nYOLOv5s-C2f, and YOLOv8s) were trained, and tested under the same computational\nenvironment and compared with YOLOv5s-Straw. The results showed that the\nhighest mean average precision of 80.3% was achieved using the proposed\narchitecture whereas the same was achieved with YOLOv3-tiny, YOLOv5s,\nYOLOv5s-C2f, and YOLOv8s were 73.4%, 77.8%, 79.8%, 79.3%, respectively.\nSpecifically, the average precision of YOLOv5s-Straw was 82.1% in the immature\nclass, 73.5% in the nearly mature class, and 86.6% in the mature class, which\nwere 2.3% and 3.7%, respectively, higher than that of the latest YOLOv8s. The\nmodel included 8.6*10^6 network parameters with an inference speed of 18ms per\nimage while the inference speed of YOLOv8s had a slower inference speed of\n21.0ms and heavy parameters of 11.1*10^6, which indicates that the proposed\nmodel is fast enough for real time strawberry detection and localization for\nthe robotic picking.\n","authors":["Zixuan He","Salik Ram Khanal","Xin Zhang","Manoj Karkee","Qin Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.03998v4.pdf","comment":"20 pages; 15 figures"},{"id":"http://arxiv.org/abs/2310.08230v1","updated":"2023-10-12T11:23:07Z","published":"2023-10-12T11:23:07Z","title":"Fast Discrete Optimisation for Geometrically Consistent 3D Shape\n  Matching","summary":"  In this work we propose to combine the advantages of learning-based and\ncombinatorial formalisms for 3D shape matching. While learning-based shape\nmatching solutions lead to state-of-the-art matching performance, they do not\nensure geometric consistency, so that obtained matchings are locally unsmooth.\nOn the contrary, axiomatic methods allow to take geometric consistency into\naccount by explicitly constraining the space of valid matchings. However,\nexisting axiomatic formalisms are impractical since they do not scale to\npractically relevant problem sizes, or they require user input for the\ninitialisation of non-convex optimisation problems. In this work we aim to\nclose this gap by proposing a novel combinatorial solver that combines a unique\nset of favourable properties: our approach is (i) initialisation free, (ii)\nmassively parallelisable powered by a quasi-Newton method, (iii) provides\noptimality gaps, and (iv) delivers decreased runtime and globally optimal\nresults for many instances.\n","authors":["Paul Roetzer","Ahmed Abbas","Dongliang Cao","Florian Bernard","Paul Swoboda"],"pdf_url":"https://arxiv.org/pdf/2310.08230v1.pdf","comment":"Paul Roetzer and Ahmed Abbas contributed equally"},{"id":"http://arxiv.org/abs/2310.08222v1","updated":"2023-10-12T11:14:27Z","published":"2023-10-12T11:14:27Z","title":"Structural analysis of Hindi online handwritten characters for character\n  recognition","summary":"  Direction properties of online strokes are used to analyze them in terms of\nhomogeneous regions or sub-strokes with points satisfying common geometric\nproperties. Such sub-strokes are called sub-units. These properties are used to\nextract sub-units from Hindi ideal online characters. These properties along\nwith some heuristics are used to extract sub-units from Hindi online\nhandwritten characters.\\\\ A method is developed to extract point stroke,\nclockwise curve stroke, counter-clockwise curve stroke and loop stroke segments\nas sub-units from Hindi online handwritten characters. These extracted\nsub-units are close in structure to the sub-units of the corresponding Hindi\nonline ideal characters.\\\\ Importance of local representation of online\nhandwritten characters in terms of sub-units is assessed by training a\nclassifier with sub-unit level local and character level global features\nextracted from characters for character recognition. The classifier has the\nrecognition accuracy of 93.5\\% on the testing set. This accuracy is the highest\nwhen compared with that of the classifiers trained only with global features\nextracted from characters in the same training set and evaluated on the same\ntesting set.\\\\ Sub-unit extraction algorithm and the sub-unit based character\nclassifier are tested on Hindi online handwritten character dataset. This\ndataset consists of samples from 96 different characters. There are 12832 and\n2821 samples in the training and testing sets, respectively.\n","authors":["Anand Sharma","A. G. Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2310.08222v1.pdf","comment":"34 pages, 36 jpg figures"},{"id":"http://arxiv.org/abs/2309.00848v2","updated":"2023-10-12T11:11:23Z","published":"2023-09-02T07:17:43Z","title":"Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach","summary":"  This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using\nthe YOLOv8 model and innovative post-processing techniques. We tackle\nchallenges unique to the complex Bengali script by employing data augmentation\nfor model robustness. After meticulous validation set evaluation, we fine-tune\nour approach on the complete dataset, leading to a two-stage prediction\nstrategy for accurate element segmentation. Our ensemble model, combined with\npost-processing, outperforms individual base architectures, addressing issues\nidentified in the BaDLAD dataset. By leveraging this approach, we aim to\nadvance Bengali document analysis, contributing to improved OCR and document\ncomprehension and BaDLAD serves as a foundational resource for this endeavor,\naiding future research in the field. Furthermore, our experiments provided key\ninsights to incorporate new strategies into the established solution.\n","authors":["Nazmus Sakib Ahmed","Saad Sakib Noor","Ashraful Islam Shanto Sikder","Abhijit Paul"],"pdf_url":"https://arxiv.org/pdf/2309.00848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08217v1","updated":"2023-10-12T11:05:34Z","published":"2023-10-12T11:05:34Z","title":"TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge\n  Retention and Promotion","summary":"  Continual learning (CL) has remained a persistent challenge for deep neural\nnetworks due to catastrophic forgetting (CF) of previously learned tasks.\nSeveral techniques such as weight regularization, experience rehearsal, and\nparameter isolation have been proposed to alleviate CF. Despite their relative\nsuccess, these research directions have predominantly remained orthogonal and\nsuffer from several shortcomings, while missing out on the advantages of\ncompeting strategies. On the contrary, the brain continually learns,\naccommodates, and transfers knowledge across tasks by simultaneously leveraging\nseveral neurophysiological processes, including neurogenesis, active\nforgetting, neuromodulation, metaplasticity, experience rehearsal, and\ncontext-dependent gating, rarely resulting in CF. Inspired by how the brain\nexploits multiple mechanisms concurrently, we propose TriRE, a novel CL\nparadigm that encompasses retaining the most prominent neurons for each task,\nrevising and solidifying the extracted knowledge of current and past tasks, and\nactively promoting less active neurons for subsequent tasks through rewinding\nand relearning. Across CL settings, TriRE significantly reduces task\ninterference and surpasses different CL approaches considered in isolation.\n","authors":["Preetha Vijayan","Prashant Bhat","Elahe Arani","Bahram Zonooz"],"pdf_url":"https://arxiv.org/pdf/2310.08217v1.pdf","comment":"Accepted at 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2310.08206v1","updated":"2023-10-12T10:51:23Z","published":"2023-10-12T10:51:23Z","title":"Long-Tailed Classification Based on Coarse-Grained Leading Forest and\n  Multi-Center Loss","summary":"  Long-tailed(LT) classification is an unavoidable and challenging problem in\nthe real world. Most of the existing long-tailed classification methods focus\nonly on solving the inter-class imbalance in which there are more samples in\nthe head class than in the tail class, while ignoring the intra-lass imbalance\nin which the number of samples of the head attribute within the same class is\nmuch larger than the number of samples of the tail attribute. The deviation in\nthe model is caused by both of these factors, and due to the fact that\nattributes are implicit in most datasets and the combination of attributes is\nvery complex, the intra-class imbalance is more difficult to handle. For this\npurpose, we proposed a long-tailed classification framework, known as\n\\textbf{\\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest\n(CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint\nsolution model by means of invariant feature learning. In this method, we\ndesigned an unsupervised learning method, i.e., CLF, to better characterize the\ndistribution of attributes within a class. Depending on the distribution of\nattributes, we can flexibly construct sampling strategies suitable for\ndifferent environments. In addition, we introduce a new metric learning loss\n(MCL), which aims to gradually eliminate confusing attributes during the\nfeature learning process. More importantly, this approach does not depend on a\nspecific model structure and can be integrated with existing LT methods as an\nindependent component. We have conducted extensive experiments and our approach\nhas state-of-the-art performance in both existing benchmarks ImageNet-GLT and\nMSCOCO-GLT, and can improve the performance of existing LT methods. Our codes\nare available on GitHub: \\url{https://github.com/jinyery/cognisance}\n","authors":["Jinye Yang","Ji Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08206v1.pdf","comment":"This is another research work to apply leading tree structure along\n  with deep learning architecture"},{"id":"http://arxiv.org/abs/2310.08204v1","updated":"2023-10-12T10:50:21Z","published":"2023-10-12T10:50:21Z","title":"Lifelong Audio-video Masked Autoencoder with Forget-robust Localized\n  Alignments","summary":"  We present a lifelong audio-video masked autoencoder that continually learns\nthe multimodal representations from a video stream containing audio-video\npairs, while its distribution continually shifts over time. Specifically, we\npropose two novel ideas to tackle the problem: (1) Localized Alignment: We\nintroduce a small trainable multimodal encoder that predicts the audio and\nvideo tokens that are well-aligned with each other. This allows the model to\nlearn only the highly correlated audiovisual patches with accurate multimodal\nrelationships. (2) Forget-robust multimodal patch selection: We compare the\nrelative importance of each audio-video patch between the current and past data\npair to mitigate unintended drift of the previously learned audio-video\nrepresentations. Our proposed method, FLAVA (Forget-robust Localized\nAudio-Video Alignment), therefore, captures the complex relationships between\nthe audio and video modalities during training on a sequence of pre-training\ntasks while alleviating the forgetting of learned audiovisual correlations. Our\nexperiments validate that FLAVA outperforms the state-of-the-art continual\nlearning methods on several benchmark datasets under continual audio-video\nrepresentation learning scenarios.\n","authors":["Jaewoo Lee","Jaehong Yoon","Wonjae Kim","Yunji Kim","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2310.08204v1.pdf","comment":"Preprint, project page: https://g-jwlee.github.io/FLAVA/"},{"id":"http://arxiv.org/abs/2310.05969v2","updated":"2023-10-12T10:26:17Z","published":"2023-09-28T07:57:03Z","title":"Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning\n  Approach","summary":"  Reading and interpreting chest X-ray images is one of the most radiologist's\nroutines. However, it still can be challenging, even for the most experienced\nones. Therefore, we proposed a multi-model deep learning-based automated chest\nX-ray report generator system designed to assist radiologists in their work.\nThe basic idea of the proposed system is by utilizing multi\nbinary-classification models for detecting multi abnormalities, with each model\nresponsible for detecting one abnormality, in a single image. In this study, we\nlimited the radiology abnormalities detection to only cardiomegaly, lung\neffusion, and consolidation. The system generates a radiology report by\nperforming the following three steps: image pre-processing, utilizing deep\nlearning models to detect abnormalities, and producing a report. The aim of the\nimage pre-processing step is to standardize the input by scaling it to 128x128\npixels and slicing it into three segments, which covers the upper, lower, and\nmiddle parts of the lung. After pre-processing, each corresponding model\nclassifies the image, resulting in a 0 (zero) for no abnormality detected and a\n1 (one) for the presence of an abnormality. The prediction outputs of each\nmodel are then concatenated to form a 'result code'. The 'result code' is used\nto construct a report by selecting the appropriate pre-determined sentence for\neach detected abnormality in the report generation step. The proposed system is\nexpected to reduce the workload of radiologists and increase the accuracy of\nchest X-ray diagnosis.\n","authors":["Arief Purnama Muharram","Hollyana Puteri Haryono","Abassi Haji Juma","Ira Puspasari","Nugraha Priya Utama"],"pdf_url":"https://arxiv.org/pdf/2310.05969v2.pdf","comment":"Presented in the 2023 IEEE International Conference on Data and\n  Software Engineering (ICoDSE 2023)"},{"id":"http://arxiv.org/abs/2309.13336v2","updated":"2023-10-12T10:24:42Z","published":"2023-09-23T10:58:08Z","title":"FedDrive v2: an Analysis of the Impact of Label Skewness in Federated\n  Semantic Segmentation for Autonomous Driving","summary":"  We propose FedDrive v2, an extension of the Federated Learning benchmark for\nSemantic Segmentation in Autonomous Driving. While the first version aims at\nstudying the effect of domain shift of the visual features across clients, in\nthis work, we focus on the distribution skewness of the labels. We propose six\nnew federated scenarios to investigate how label skewness affects the\nperformance of segmentation models and compare it with the effect of domain\nshift. Finally, we study the impact of using the domain information during\ntesting. Official website: https://feddrive.github.io\n","authors":["Eros Fanì","Marco Ciccone","Barbara Caputo"],"pdf_url":"https://arxiv.org/pdf/2309.13336v2.pdf","comment":"5th Italian Conference on Robotics and Intelligent Machines (I-RIM)\n  2023"},{"id":"http://arxiv.org/abs/2310.08182v1","updated":"2023-10-12T10:17:40Z","published":"2023-10-12T10:17:40Z","title":"XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness\n  Evaluation","summary":"  The lack of standardized robustness metrics and the widespread reliance on\nnumerous unrelated benchmark datasets for testing have created a gap between\nacademically validated robust models and their often problematic practical\nadoption. To address this, we introduce XIMAGENET-12, an explainable benchmark\ndataset with over 200K images and 15,600 manual semantic annotations. Covering\n12 categories from ImageNet to represent objects commonly encountered in\npractical life and simulating six diverse scenarios, including overexposure,\nblurring, color changing, etc., we further propose a novel robustness criterion\nthat extends beyond model generation ability assessment. This benchmark\ndataset, along with related code, is available at\nhttps://sites.google.com/view/ximagenet-12/home. Researchers and practitioners\ncan leverage this resource to evaluate the robustness of their visual models\nunder challenging conditions and ultimately benefit from the demands of\npractical computer vision systems.\n","authors":["Qiang Li","Dan Zhang","Shengzhao Lei","Xun Zhao","Shuyan Li","Porawit Kamnoedboon","WeiWei Li"],"pdf_url":"https://arxiv.org/pdf/2310.08182v1.pdf","comment":"UnderSubmission"},{"id":"http://arxiv.org/abs/2310.07449v2","updated":"2023-10-12T10:14:39Z","published":"2023-10-11T12:51:16Z","title":"PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction","summary":"  Neural surface reconstruction is sensitive to the camera pose noise, even if\nstate-of-the-art pose estimators like COLMAP or ARKit are used. More\nimportantly, existing Pose-NeRF joint optimisation methods have struggled to\nimprove pose accuracy in challenging real-world scenarios. To overcome the\nchallenges, we introduce the pose residual field (\\textbf{PoRF}), a novel\nimplicit representation that uses an MLP for regressing pose updates. This is\nmore robust than the conventional pose parameter optimisation due to parameter\nsharing that leverages global information over the entire sequence.\nFurthermore, we propose an epipolar geometry loss to enhance the supervision\nthat leverages the correspondences exported from COLMAP results without the\nextra computational overhead. Our method yields promising results. On the DTU\ndataset, we reduce the rotation error by 78\\% for COLMAP poses, leading to the\ndecreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the\nMobileBrick dataset that contains casually captured unbounded 360-degree\nvideos, our method refines ARKit poses and improves the reconstruction F1 score\nfrom 69.18 to 75.67, outperforming that with the dataset provided ground-truth\npose (75.14). These achievements demonstrate the efficacy of our approach in\nrefining camera poses and improving the accuracy of neural surface\nreconstruction in real-world scenarios.\n","authors":["Jia-Wang Bian","Wenjing Bian","Victor Adrian Prisacariu","Philip Torr"],"pdf_url":"https://arxiv.org/pdf/2310.07449v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.08177v1","updated":"2023-10-12T10:03:25Z","published":"2023-10-12T10:03:25Z","title":"Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization","summary":"  Evaluating the adversarial robustness of machine learning models using\ngradient-based attacks is challenging. In this work, we show that\nhyperparameter optimization can improve fast minimum-norm attacks by automating\nthe selection of the loss function, the optimizer and the step-size scheduler,\nalong with the corresponding hyperparameters. Our extensive evaluation\ninvolving several robust models demonstrates the improved efficacy of fast\nminimum-norm attacks when hyper-up with hyperparameter optimization. We release\nour open-source code at https://github.com/pralab/HO-FMN.\n","authors":["Giuseppe Floris","Raffaele Mura","Luca Scionis","Giorgio Piras","Maura Pintor","Ambra Demontis","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2310.08177v1.pdf","comment":"Accepted at ESANN23"},{"id":"http://arxiv.org/abs/2310.08165v1","updated":"2023-10-12T09:37:56Z","published":"2023-10-12T09:37:56Z","title":"COVID-19 Detection Using Swin Transformer Approach from Computed\n  Tomography Images","summary":"  The accurate and efficient diagnosis of COVID-19 is of paramount importance,\nparticularly in the context of large-scale medical imaging datasets. In this\npreprint paper, we propose a novel approach for COVID-19 diagnosis using CT\nimages that leverages the power of Swin Transformer models, state-of-the-art\nsolutions in computer vision tasks. Our method includes a systematic approach\nfor patient-level predictions, where individual CT slices are classified as\nCOVID-19 or non-COVID, and the patient's overall diagnosis is determined\nthrough majority voting. The application of the Swin Transformer in this\ncontext results in patient-level predictions that demonstrate exceptional\ndiagnostic accuracy. In terms of evaluation metrics, our approach consistently\noutperforms the baseline, as well as numerous competing methods, showcasing its\neffectiveness in COVID-19 diagnosis. The macro F1 score achieved by our model\nexceeds the baseline and offers a robust solution for accurate diagnosis.\n","authors":["Kenan Morani"],"pdf_url":"https://arxiv.org/pdf/2310.08165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17046v2","updated":"2023-10-12T09:22:10Z","published":"2023-06-29T15:43:06Z","title":"Spiking Denoising Diffusion Probabilistic Models","summary":"  Spiking neural networks (SNNs) have ultra-low energy consumption and high\nbiological plausibility due to their binary and bio-driven nature compared with\nartificial neural networks (ANNs). While previous research has primarily\nfocused on enhancing the performance of SNNs in classification tasks, the\ngenerative potential of SNNs remains relatively unexplored. In our paper, we\nput forward Spiking Denoising Diffusion Probabilistic Models (SDDPM), a new\nclass of SNN-based generative models that achieve high sample quality. To fully\nexploit the energy efficiency of SNNs, we propose a purely Spiking U-Net\narchitecture, which achieves comparable performance to its ANN counterpart\nusing only 4 time steps, resulting in significantly reduced energy consumption.\nExtensive experimental results reveal that our approach achieves\nstate-of-the-art on the generative tasks and substantially outperforms other\nSNN-based generative models, achieving up to $12\\times$ and $6\\times$\nimprovement on the CIFAR-10 and the CelebA datasets, respectively. Moreover, we\npropose a threshold-guided strategy that can further improve the performances\nby 16.7% in a training-free manner. The SDDPM symbolizes a significant\nadvancement in the field of SNN generation, injecting new perspectives and\npotential avenues of exploration.\n","authors":["Jiahang Cao","Ziqing Wang","Hanzhong Guo","Hao Cheng","Qiang Zhang","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2306.17046v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2207.09339v3","updated":"2023-10-12T09:13:37Z","published":"2022-07-19T15:49:35Z","title":"Vision Transformers: From Semantic Segmentation to Dense Prediction","summary":"  The emergence of vision transformers (ViTs) in image classification has\nshifted the methodologies for visual representation learning. In particular,\nViTs learn visual representation at full receptive field per layer across all\nthe image patches, in comparison to the increasing receptive fields of CNNs\nacross layers and other alternatives (e.g., large kernels and atrous\nconvolution). In this work, for the first time we explore the global context\nlearning potentials of ViTs for dense visual prediction (e.g., semantic\nsegmentation). Our motivation is that through learning global context at full\nreceptive field layer by layer, ViTs may capture stronger long-range dependency\ninformation, critical for dense prediction tasks. We first demonstrate that\nencoding an image as a sequence of patches, a vanilla ViT without local\nconvolution and resolution reduction can yield stronger visual representation\nfor semantic segmentation. For example, our model, termed as SEgmentation\nTRansformer (SETR), excels on ADE20K (50.28% mIoU, the first position in the\ntest leaderboard on the day of submission) and Pascal Context (55.83% mIoU),\nand performs competitively on Cityscapes. For tackling general dense visual\nprediction tasks in a cost-effective manner, we further formulate a family of\nHierarchical Local-Global (HLG) Transformers, characterized by local attention\nwithin windows and global-attention across windows in a pyramidal architecture.\nExtensive experiments show that our methods achieve appealing performance on a\nvariety of dense prediction tasks (e.g., object detection and instance\nsegmentation and semantic segmentation) as well as image classification. Our\ncode and models are available at https://github.com/fudan-zvg/SETR.\n","authors":["Li Zhang","Jiachen Lu","Sixiao Zheng","Xinxuan Zhao","Xiatian Zhu","Yanwei Fu","Tao Xiang","Jianfeng Feng","Philip H. S. Torr"],"pdf_url":"https://arxiv.org/pdf/2207.09339v3.pdf","comment":"Extended version of CVPR 2021 paper arXiv:2012.15840"},{"id":"http://arxiv.org/abs/2301.12082v3","updated":"2023-10-12T09:01:04Z","published":"2023-01-28T03:58:32Z","title":"Pushing the Limits of Fewshot Anomaly Detection in Industry Vision:\n  Graphcore","summary":"  In the area of fewshot anomaly detection (FSAD), efficient visual feature\nplays an essential role in memory bank M-based methods. However, these methods\ndo not account for the relationship between the visual feature and its rotated\nvisual feature, drastically limiting the anomaly detection performance. To push\nthe limits, we reveal that rotation-invariant feature property has a\nsignificant impact in industrial-based FSAD. Specifically, we utilize graph\nrepresentation in FSAD and provide a novel visual isometric invariant feature\n(VIIF) as anomaly measurement feature. As a result, VIIF can robustly improve\nthe anomaly discriminating ability and can further reduce the size of redundant\nfeatures stored in M by a large amount. Besides, we provide a novel model\nGraphCore via VIIFs that can fast implement unsupervised FSAD training and can\nimprove the performance of anomaly detection. A comprehensive evaluation is\nprovided for comparing GraphCore and other SOTA anomaly detection models under\nour proposed fewshot anomaly detection setting, which shows GraphCore can\nincrease average AUC by 5.8%, 4.1%, 3.4%, and 1.6% on MVTec AD and by 25.5%,\n22.0%, 16.9%, and 14.1% on MPDD for 1, 2, 4, and 8-shot cases, respectively.\n","authors":["Guoyang Xie","Jinbao Wang","Jiaqi Liu","Feng Zheng","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2301.12082v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.02081v2","updated":"2023-10-12T09:00:34Z","published":"2022-10-05T08:19:16Z","title":"Locate before Answering: Answer Guided Question Localization for Video\n  Question Answering","summary":"  Video question answering (VideoQA) is an essential task in vision-language\nunderstanding, which has attracted numerous research attention recently.\nNevertheless, existing works mostly achieve promising performances on short\nvideos of duration within 15 seconds. For VideoQA on minute-level long-term\nvideos, those methods are likely to fail because of lacking the ability to deal\nwith noise and redundancy caused by scene changes and multiple actions in the\nvideo. Considering the fact that the question often remains concentrated in a\nshort temporal range, we propose to first locate the question to a segment in\nthe video and then infer the answer using the located segment only. Under this\nscheme, we propose \"Locate before Answering\" (LocAns), a novel approach that\nintegrates a question locator and an answer predictor into an end-to-end model.\nDuring the training phase, the available answer label not only serves as the\nsupervision signal of the answer predictor, but also is used to generate pseudo\ntemporal labels for the question locator. Moreover, we design a decoupled\nalternative training strategy to update the two modules separately. In the\nexperiments, LocAns achieves state-of-the-art performance on two modern\nlong-term VideoQA datasets NExT-QA and ActivityNet-QA, and its qualitative\nexamples show the reliable performance of the question localization.\n","authors":["Tianwen Qian","Ran Cui","Jingjing Chen","Pai Peng","Xiaowei Guo","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2210.02081v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08143v1","updated":"2023-10-12T08:58:01Z","published":"2023-10-12T08:58:01Z","title":"A Deep Learning Framework for Spatiotemporal Ultrasound Localization\n  Microscopy","summary":"  Ultrasound Localization Microscopy can resolve the microvascular bed down to\na few micrometers. To achieve such performance microbubble contrast agents must\nperfuse the entire microvascular network. Microbubbles are then located\nindividually and tracked over time to sample individual vessels, typically over\nhundreds of thousands of images. To overcome the fundamental limit of\ndiffraction and achieve a dense reconstruction of the network, low microbubble\nconcentrations must be used, which lead to acquisitions lasting several\nminutes. Conventional processing pipelines are currently unable to deal with\ninterference from multiple nearby microbubbles, further reducing achievable\nconcentrations. This work overcomes this problem by proposing a Deep Learning\napproach to recover dense vascular networks from ultrasound acquisitions with\nhigh microbubble concentrations. A realistic mouse brain microvascular network,\nsegmented from 2-photon microscopy, was used to train a three-dimensional\nconvolutional neural network based on a V-net architecture. Ultrasound data\nsets from multiple microbubbles flowing through the microvascular network were\nsimulated and used as ground truth to train the 3D CNN to track microbubbles.\nThe 3D-CNN approach was validated in silico using a subset of the data and in\nvivo on a rat brain acquisition. In silico, the CNN reconstructed vascular\nnetworks with higher precision (81%) than a conventional ULM framework (70%).\nIn vivo, the CNN could resolve micro vessels as small as 10 $\\mu$m with an\nincrease in resolution when compared against a conventional approach.\n","authors":["Léo Milecki","Jonathan Porée","Hatim Belgharbi","Chloé Bourquin","Rafat Damseh","Patrick Delafontaine-Martel","Frédéric Lesage","Maxime Gasse","Jean Provost"],"pdf_url":"https://arxiv.org/pdf/2310.08143v1.pdf","comment":"Copyright 2021 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2310.08142v1","updated":"2023-10-12T08:57:33Z","published":"2023-10-12T08:57:33Z","title":"Fine-Grained Annotation for Face Anti-Spoofing","summary":"  Face anti-spoofing plays a critical role in safeguarding facial recognition\nsystems against presentation attacks. While existing deep learning methods show\npromising results, they still suffer from the lack of fine-grained annotations,\nwhich lead models to learn task-irrelevant or unfaithful features. In this\npaper, we propose a fine-grained annotation method for face anti-spoofing.\nSpecifically, we first leverage the Segment Anything Model (SAM) to obtain\npixel-wise segmentation masks by utilizing face landmarks as point prompts. The\nface landmarks provide segmentation semantics, which segments the face into\nregions. We then adopt these regions as masks and assemble them into three\nseparate annotation maps: spoof, living, and background maps. Finally, we\ncombine three separate maps into a three-channel map as annotations for model\ntraining. Furthermore, we introduce the Multi-Channel Region Exchange\nAugmentation (MCREA) to diversify training data and reduce overfitting.\nExperimental results demonstrate that our method outperforms existing\nstate-of-the-art approaches in both intra-dataset and cross-dataset\nevaluations.\n","authors":["Xu Chen","Yunde Jia","Yuwei Wu"],"pdf_url":"https://arxiv.org/pdf/2310.08142v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.08139v1","updated":"2023-10-12T08:55:10Z","published":"2023-10-12T08:55:10Z","title":"DualAug: Exploiting Additional Heavy Augmentation with OOD Data\n  Rejection","summary":"  Data augmentation is a dominant method for reducing model overfitting and\nimproving generalization. Most existing data augmentation methods tend to find\na compromise in augmenting the data, \\textit{i.e.}, increasing the amplitude of\naugmentation carefully to avoid degrading some data too much and doing harm to\nthe model performance. We delve into the relationship between data augmentation\nand model performance, revealing that the performance drop with heavy\naugmentation comes from the presence of out-of-distribution (OOD) data.\nNonetheless, as the same data transformation has different effects for\ndifferent training samples, even for heavy augmentation, there remains part of\nin-distribution data which is beneficial to model training. Based on the\nobservation, we propose a novel data augmentation method, named\n\\textbf{DualAug}, to keep the augmentation in distribution as much as possible\nat a reasonable time and computational cost. We design a data mixing strategy\nto fuse augmented data from both the basic- and the heavy-augmentation\nbranches. Extensive experiments on supervised image classification benchmarks\nshow that DualAug improve various automated data augmentation method. Moreover,\nthe experiments on semi-supervised learning and contrastive self-supervised\nlearning demonstrate that our DualAug can also improve related method. Code is\navailable at\n\\href{https://github.com/shuguang99/DualAug}{https://github.com/shuguang99/DualAug}.\n","authors":["Zehao Wang","Yiwen Guo","Qizhang Li","Guanglei Yang","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2310.08139v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2301.11514v5","updated":"2023-10-12T08:49:31Z","published":"2023-01-27T03:18:09Z","title":"Deep Industrial Image Anomaly Detection: A Survey","summary":"  The recent rapid development of deep learning has laid a milestone in\nindustrial Image Anomaly Detection (IAD). In this paper, we provide a\ncomprehensive review of deep learning-based image anomaly detection techniques,\nfrom the perspectives of neural network architectures, levels of supervision,\nloss functions, metrics and datasets. In addition, we extract the new setting\nfrom industrial manufacturing and review the current IAD approaches under our\nproposed our new setting. Moreover, we highlight several opening challenges for\nimage anomaly detection. The merits and downsides of representative network\narchitectures under varying supervision are discussed. Finally, we summarize\nthe research findings and point out future research directions. More resources\nare available at\nhttps://github.com/M-3LAB/awesome-industrial-anomaly-detection.\n","authors":["Jiaqi Liu","Guoyang Xie","Jinbao Wang","Shangnian Li","Chengjie Wang","Feng Zheng","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2301.11514v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.13359v3","updated":"2023-10-12T08:47:19Z","published":"2023-01-31T01:24:45Z","title":"IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing","summary":"  Image anomaly detection (IAD) is an urgent issue that needs to be addressed\nin modern industrial manufacturing (IM). Recently, many advanced algorithms\nhave been released, but their performance varies greatly due to non-uniformed\nsettings. That is, researchers find it difficult to analyze because they are\ndesigned for different or specific cases in IM. To eliminate this problem, we\nfirst propose a uniform IAD setting to systematically assess the effectiveness\nof these algorithms, mainly considering three aspects of supervision level\n(unsupervised, fully supervised), learning paradigm (few-shot, continual, noisy\nlabel), and efficiency (memory usage, inference speed). Then, we skillfully\nconstruct a comprehensive image anomaly detection benchmark (IM-IAD), which\nincludes 19 algorithms on 7 major datasets with the same setting. Our extensive\nexperiments (17,017 total) provide new insights into the redesign or selection\nof the IAD algorithm under uniform conditions. Importantly, the proposed IM-IAD\npresents feasible challenges and future directions for further work. We believe\nthat this work can have a significant impact on the IAD field. To foster\nreproducibility and accessibility, the source code of IM-IAD is uploaded on the\nwebsite, https://github.com/M-3LAB/IM-IAD.\n","authors":["Guoyang Xie","Jinbao Wang","Jiaqi Liu","Jiayi Lyu","Yong Liu","Chengjie Wang","Feng Zheng","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2301.13359v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08129v1","updated":"2023-10-12T08:36:25Z","published":"2023-10-12T08:36:25Z","title":"Tailored Visions: Enhancing Text-to-Image Generation with Personalized\n  Prompt Rewriting","summary":"  We propose a novel perspective of viewing large pretrained models as search\nengines, thereby enabling the repurposing of techniques previously used to\nenhance search engine performance. As an illustration, we employ a personalized\nquery rewriting technique in the realm of text-to-image generation. Despite\nsignificant progress in the field, it is still challenging to create\npersonalized visual representations that align closely with the desires and\npreferences of individual users. This process requires users to articulate\ntheir ideas in words that are both comprehensible to the models and accurately\ncapture their vision, posing difficulties for many users. In this paper, we\ntackle this challenge by leveraging historical user interactions with the\nsystem to enhance user prompts. We propose a novel approach that involves\nrewriting user prompts based a new large-scale text-to-image dataset with over\n300k prompts from 3115 users. Our rewriting model enhances the expressiveness\nand alignment of user prompts with their intended visual outputs. Experimental\nresults demonstrate the superiority of our methods over baseline approaches, as\nevidenced in our new offline evaluation method and online tests. Our approach\nopens up exciting possibilities of applying more search engine techniques to\nbuild truly personalized large pretrained models.\n","authors":["Zijie Chen","Lichao Zhang","Fangsheng Weng","Lili Pan","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2310.08129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.03198v5","updated":"2023-10-12T08:31:50Z","published":"2023-04-06T16:21:56Z","title":"RFAConv: Innovating Spatial Attention and Standard Convolutional\n  Operation","summary":"  Spatial attention has been widely used to improve the performance of\nconvolutional neural networks. However, it has certain limitations. In this\npaper, we propose a new perspective on the effectiveness of spatial attention,\nwhich is that the spatial attention mechanism essentially solves the problem of\nconvolutional kernel parameter sharing. However, the information contained in\nthe attention map generated by spatial attention is not sufficient for\nlarge-size convolutional kernels. Therefore, we propose a novel attention\nmechanism called Receptive-Field Attention (RFA). Existing spatial attention,\nsuch as Convolutional Block Attention Module (CBAM) and Coordinated Attention\n(CA) focus only on spatial features, which does not fully address the problem\nof convolutional kernel parameter sharing. In contrast, RFA not only focuses on\nthe receptive-field spatial feature but also provides effective attention\nweights for large-size convolutional kernels. The Receptive-Field Attention\nconvolutional operation (RFAConv), developed by RFA, represents a new approach\nto replace the standard convolution operation. It offers nearly negligible\nincrement of computational cost and parameters, while significantly improving\nnetwork performance. We conducted a series of experiments on ImageNet-1k, COCO,\nand VOC datasets to demonstrate the superiority of our approach. Of particular\nimportance, we believe that it is time to shift focus from spatial features to\nreceptive-field spatial features for current spatial attention mechanisms. In\nthis way, we can further improve network performance and achieve even better\nresults. The code and pre-trained models for the relevant tasks can be found at\nhttps://github.com/Liuchen1997/RFAConv.\n","authors":["Xin Zhang","Chen Liu","Degang Yang","Tingting Song","Yichen Ye","Ke Li","Yingze Song"],"pdf_url":"https://arxiv.org/pdf/2304.03198v5.pdf","comment":"12 pages, 11figures"},{"id":"http://arxiv.org/abs/2310.08117v1","updated":"2023-10-12T08:21:17Z","published":"2023-10-12T08:21:17Z","title":"DUSA: Decoupled Unsupervised Sim2Real Adaptation for\n  Vehicle-to-Everything Collaborative Perception","summary":"  Vehicle-to-Everything (V2X) collaborative perception is crucial for\nautonomous driving. However, achieving high-precision V2X perception requires a\nsignificant amount of annotated real-world data, which can always be expensive\nand hard to acquire. Simulated data have raised much attention since they can\nbe massively produced at an extremely low cost. Nevertheless, the significant\ndomain gap between simulated and real-world data, including differences in\nsensor type, reflectance patterns, and road surroundings, often leads to poor\nperformance of models trained on simulated data when evaluated on real-world\ndata. In addition, there remains a domain gap between real-world collaborative\nagents, e.g. different types of sensors may be installed on autonomous vehicles\nand roadside infrastructures with different extrinsics, further increasing the\ndifficulty of sim2real generalization. To take full advantage of simulated\ndata, we present a new unsupervised sim2real domain adaptation method for V2X\ncollaborative detection named Decoupled Unsupervised Sim2Real Adaptation\n(DUSA). Our new method decouples the V2X collaborative sim2real domain\nadaptation problem into two sub-problems: sim2real adaptation and inter-agent\nadaptation. For sim2real adaptation, we design a Location-adaptive Sim2Real\nAdapter (LSA) module to adaptively aggregate features from critical locations\nof the feature map and align the features between simulated data and real-world\ndata via a sim/real discriminator on the aggregated global feature. For\ninter-agent adaptation, we further devise a Confidence-aware Inter-agent\nAdapter (CIA) module to align the fine-grained features from heterogeneous\nagents under the guidance of agent-wise confidence maps. Experiments\ndemonstrate the effectiveness of the proposed DUSA approach on unsupervised\nsim2real adaptation from the simulated V2XSet dataset to the real-world\nDAIR-V2X-C dataset.\n","authors":["Xianghao Kong","Wentao Jiang","Jinrang Jia","Yifeng Shi","Runsheng Xu","Si Liu"],"pdf_url":"https://arxiv.org/pdf/2310.08117v1.pdf","comment":"ACM MM 2023"},{"id":"http://arxiv.org/abs/2310.08116v1","updated":"2023-10-12T08:17:57Z","published":"2023-10-12T08:17:57Z","title":"Multimodal Active Measurement for Human Mesh Recovery in Close Proximity","summary":"  For safe and sophisticated physical human-robot interactions (pHRI), a robot\nneeds to estimate the accurate body pose or mesh of the target person. However,\nin these pHRI scenarios, the robot cannot fully observe the target person's\nbody with equipped cameras because the target person is usually close to the\nrobot. This leads to severe truncation and occlusions, and results in poor\naccuracy of human pose estimation. For better accuracy of human pose estimation\nor mesh recovery on this limited information from cameras, we propose an active\nmeasurement and sensor fusion framework of the equipped cameras and other\nsensors such as touch sensors and 2D LiDAR. These touch and LiDAR sensing are\nobtained attendantly through pHRI without additional costs. These sensor\nmeasurements are sparse but reliable and informative cues for human mesh\nrecovery. In our active measurement process, camera viewpoints and sensor\nplacements are optimized based on the uncertainty of the estimated pose, which\nis closely related to the truncated or occluded areas. In our sensor fusion\nprocess, we fuse the sensor measurements to the camera-based estimated pose by\nminimizing the distance between the estimated mesh and measured positions. Our\nmethod is agnostic to robot configurations. Experiments were conducted using\nthe Toyota Human Support Robot, which has a camera, 2D LiDAR, and a touch\nsensor on the robot arm. Our proposed method demonstrated the superiority in\nthe human pose estimation accuracy on the quantitative comparison. Furthermore,\nour proposed method reliably estimated the pose of the target person in\npractical settings such as target people occluded by a blanket and standing aid\nwith the robot arm.\n","authors":["Takahiro Maeda","Keisuke Takeshita","Kazuhito Tanaka"],"pdf_url":"https://arxiv.org/pdf/2310.08116v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14108v3","updated":"2023-10-12T08:05:18Z","published":"2022-11-25T13:50:00Z","title":"3DDesigner: Towards Photorealistic 3D Object Generation and Editing with\n  Text-guided Diffusion Models","summary":"  Text-guided diffusion models have shown superior performance in image/video\ngeneration and editing. While few explorations have been performed in 3D\nscenarios. In this paper, we discuss three fundamental and interesting problems\non this topic. First, we equip text-guided diffusion models to achieve\n3D-consistent generation. Specifically, we integrate a NeRF-like neural field\nto generate low-resolution coarse results for a given camera view. Such results\ncan provide 3D priors as condition information for the following diffusion\nprocess. During denoising diffusion, we further enhance the 3D consistency by\nmodeling cross-view correspondences with a novel two-stream (corresponding to\ntwo different views) asynchronous diffusion process. Second, we study 3D local\nediting and propose a two-step solution that can generate 360-degree\nmanipulated results by editing an object from a single view. Step 1, we propose\nto perform 2D local editing by blending the predicted noises. Step 2, we\nconduct a noise-to-text inversion process that maps 2D blended noises into the\nview-independent text embedding space. Once the corresponding text embedding is\nobtained, 360-degree images can be generated. Last but not least, we extend our\nmodel to perform one-shot novel view synthesis by fine-tuning on a single\nimage, firstly showing the potential of leveraging text guidance for novel view\nsynthesis. Extensive experiments and various applications show the prowess of\nour 3DDesigner. The project page is available at\nhttps://3ddesigner-diffusion.github.io/.\n","authors":["Gang Li","Heliang Zheng","Chaoyue Wang","Chang Li","Changwen Zheng","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2211.14108v3.pdf","comment":"Submitted to IJCV"},{"id":"http://arxiv.org/abs/2310.00847v2","updated":"2023-10-12T08:04:14Z","published":"2023-10-02T02:01:00Z","title":"Can Pre-trained Networks Detect Familiar Out-of-Distribution Data?","summary":"  Out-of-distribution (OOD) detection is critical for safety-sensitive machine\nlearning applications and has been extensively studied, yielding a plethora of\nmethods developed in the literature. However, most studies for OOD detection\ndid not use pre-trained models and trained a backbone from scratch. In recent\nyears, transferring knowledge from large pre-trained models to downstream tasks\nby lightweight tuning has become mainstream for training in-distribution (ID)\nclassifiers. To bridge the gap between the practice of OOD detection and\ncurrent classifiers, the unique and crucial problem is that the samples whose\ninformation networks know often come as OOD input. We consider that such data\nmay significantly affect the performance of large pre-trained networks because\nthe discriminability of these OOD data depends on the pre-training algorithm.\nHere, we define such OOD data as PT-OOD (Pre-Trained OOD) data. In this paper,\nwe aim to reveal the effect of PT-OOD on the OOD detection performance of\npre-trained networks from the perspective of pre-training algorithms. To\nachieve this, we explore the PT-OOD detection performance of supervised and\nself-supervised pre-training algorithms with linear-probing tuning, the most\ncommon efficient tuning method. Through our experiments and analysis, we find\nthat the low linear separability of PT-OOD in the feature space heavily\ndegrades the PT-OOD detection performance, and self-supervised models are more\nvulnerable to PT-OOD than supervised pre-trained models, even with\nstate-of-the-art detection methods. To solve this vulnerability, we further\npropose a unique solution to large-scale pre-trained models: Leveraging\npowerful instance-by-instance discriminative representations of pre-trained\nmodels and detecting OOD in the feature space independent of the ID decision\nboundaries. The code will be available via https://github.com/AtsuMiyai/PT-OOD.\n","authors":["Atsuyuki Miyai","Qing Yu","Go Irie","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2310.00847v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07522v2","updated":"2023-10-12T08:02:18Z","published":"2023-10-11T14:19:05Z","title":"S4C: Self-Supervised Semantic Scene Completion with Neural Fields","summary":"  3D semantic scene understanding is a fundamental challenge in computer\nvision. It enables mobile agents to autonomously plan and navigate arbitrary\nenvironments. SSC formalizes this challenge as jointly estimating dense\ngeometry and semantic information from sparse observations of a scene. Current\nmethods for SSC are generally trained on 3D ground truth based on aggregated\nLiDAR scans. This process relies on special sensors and annotation by hand\nwhich are costly and do not scale well. To overcome this issue, our work\npresents the first self-supervised approach to SSC called S4C that does not\nrely on 3D ground truth data. Our proposed method can reconstruct a scene from\na single image and only relies on videos and pseudo segmentation ground truth\ngenerated from off-the-shelf image segmentation network during training. Unlike\nexisting methods, which use discrete voxel grids, we represent scenes as\nimplicit semantic fields. This formulation allows querying any point within the\ncamera frustum for occupancy and semantic class. Our architecture is trained\nthrough rendering-based self-supervised losses. Nonetheless, our method\nachieves performance close to fully supervised state-of-the-art methods.\nAdditionally, our method demonstrates strong generalization capabilities and\ncan synthesize accurate segmentation maps for far away viewpoints.\n","authors":["Adrian Hayler","Felix Wimbauer","Dominik Muhle","Christian Rupprecht","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2310.07522v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08106v1","updated":"2023-10-12T08:01:11Z","published":"2023-10-12T08:01:11Z","title":"Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing\n  Label Bias in Foundation Models","summary":"  Foundation models like CLIP allow zero-shot transfer on various tasks without\nadditional training data. Yet, the zero-shot performance is less competitive\nthan a fully supervised one. Thus, to enhance the performance, fine-tuning and\nensembling are also commonly adopted to better fit the downstream tasks.\nHowever, we argue that such prior work has overlooked the inherent biases in\nfoundation models. Due to the highly imbalanced Web-scale training set, these\nfoundation models are inevitably skewed toward frequent semantics, and thus the\nsubsequent fine-tuning or ensembling is still biased. In this study, we\nsystematically examine the biases in foundation models and demonstrate the\nefficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that\nbias estimation in foundation models is challenging, as most pre-train data\ncannot be explicitly accessed like in traditional long-tailed classification\ntasks. To this end, GLA has an optimization-based bias estimation approach for\ndebiasing foundation models. As our work resolves a fundamental flaw in the\npre-training, the proposed GLA demonstrates significant improvements across a\ndiverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large\naverage improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on\nlong-tailed classification. Codes are in \\url{https://github.com/BeierZhu/GLA}.\n","authors":["Beier Zhu","Kaihua Tang","Qianru Sun","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08106v1.pdf","comment":"Accepted by NeurIPS2023"},{"id":"http://arxiv.org/abs/2212.09408v3","updated":"2023-10-12T07:55:38Z","published":"2022-12-19T12:40:13Z","title":"Universal Object Detection with Large Vision Model","summary":"  Over the past few years, there has been growing interest in developing a\nbroad, universal, and general-purpose computer vision system. Such systems have\nthe potential to address a wide range of vision tasks simultaneously, without\nbeing limited to specific problems or data domains. This universality is\ncrucial for practical, real-world computer vision applications. In this study,\nour focus is on a specific challenge: the large-scale, multi-domain universal\nobject detection problem, which contributes to the broader goal of achieving a\nuniversal vision system. This problem presents several intricate challenges,\nincluding cross-dataset category label duplication, label conflicts, and the\nnecessity to handle hierarchical taxonomies. To address these challenges, we\nintroduce our approach to label handling, hierarchy-aware loss design, and\nresource-efficient model training utilizing a pre-trained large vision model.\nOur method has demonstrated remarkable performance, securing a prestigious\nsecond-place ranking in the object detection track of the Robust Vision\nChallenge 2022 (RVC 2022) on a million-scale cross-dataset object detection\nbenchmark. We believe that our comprehensive study will serve as a valuable\nreference and offer an alternative approach for addressing similar challenges\nwithin the computer vision community. The source code for our work is openly\navailable at https://github.com/linfeng93/Large-UniDet.\n","authors":["Feng Lin","Wenze Hu","Yaowei Wang","Yonghong Tian","Guangming Lu","Fanglin Chen","Yong Xu","Xiaoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2212.09408v3.pdf","comment":"Accepted by International Journal of Computer Vision (IJCV). The 2nd\n  place in the object detection track of the Robust Vision Challenge (RVC 2022)"},{"id":"http://arxiv.org/abs/2309.15505v2","updated":"2023-10-12T07:55:05Z","published":"2023-09-27T09:13:40Z","title":"Finite Scalar Quantization: VQ-VAE Made Simple","summary":"  We propose to replace vector quantization (VQ) in the latent representation\nof VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where\nwe project the VAE representation down to a few dimensions (typically less than\n10). Each dimension is quantized to a small set of fixed values, leading to an\n(implicit) codebook given by the product of these sets. By appropriately\nchoosing the number of dimensions and values each dimension can take, we obtain\nthe same codebook size as in VQ. On top of such discrete representations, we\ncan train the same models that have been trained on VQ-VAE representations. For\nexample, autoregressive and masked transformer models for image generation,\nmultimodal generation, and dense prediction computer vision tasks. Concretely,\nwe employ FSQ with MaskGIT for image generation, and with UViM for depth\nestimation, colorization, and panoptic segmentation. Despite the much simpler\ndesign of FSQ, we obtain competitive performance in all these tasks. We\nemphasize that FSQ does not suffer from codebook collapse and does not need the\ncomplex machinery employed in VQ (commitment losses, codebook reseeding, code\nsplitting, entropy penalties, etc.) to learn expressive discrete\nrepresentations.\n","authors":["Fabian Mentzer","David Minnen","Eirikur Agustsson","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2309.15505v2.pdf","comment":"Code:\n  https://github.com/google-research/google-research/tree/master/fsq"},{"id":"http://arxiv.org/abs/2310.08094v1","updated":"2023-10-12T07:40:39Z","published":"2023-10-12T07:40:39Z","title":"SingleInsert: Inserting New Concepts from a Single Image into\n  Text-to-Image Models for Flexible Editing","summary":"  Recent progress in text-to-image (T2I) models enables high-quality image\ngeneration with flexible textual control. To utilize the abundant visual priors\nin the off-the-shelf T2I models, a series of methods try to invert an image to\nproper embedding that aligns with the semantic space of the T2I model. However,\nthese image-to-text (I2T) inversion methods typically need multiple source\nimages containing the same concept or struggle with the imbalance between\nediting flexibility and visual fidelity. In this work, we point out that the\ncritical problem lies in the foreground-background entanglement when learning\nan intended concept, and propose a simple and effective baseline for\nsingle-image I2T inversion, named SingleInsert. SingleInsert adopts a two-stage\nscheme. In the first stage, we regulate the learned embedding to concentrate on\nthe foreground area without being associated with the irrelevant background. In\nthe second stage, we finetune the T2I model for better visual resemblance and\ndevise a semantic loss to prevent the language drift problem. With the proposed\ntechniques, SingleInsert excels in single concept generation with high visual\nfidelity while allowing flexible editing. Additionally, SingleInsert can\nperform single-image novel view synthesis and multiple concepts composition\nwithout requiring joint training. To facilitate evaluation, we design an\nediting prompt list and introduce a metric named Editing Success Rate (ESR) for\nquantitative assessment of editing flexibility. Our project page is:\nhttps://jarrentwu1031.github.io/SingleInsert-web/\n","authors":["Zijie Wu","Chaohui Yu","Zhen Zhu","Fan Wang","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2310.08094v1.pdf","comment":"Project page: https://jarrentwu1031.github.io/SingleInsert-web/"},{"id":"http://arxiv.org/abs/2310.08092v1","updated":"2023-10-12T07:38:28Z","published":"2023-10-12T07:38:28Z","title":"Consistent123: Improve Consistency for One Image to 3D Object Synthesis","summary":"  Large image diffusion models enable novel view synthesis with high quality\nand excellent zero-shot capability. However, such models based on\nimage-to-image translation have no guarantee of view consistency, limiting the\nperformance for downstream tasks like 3D reconstruction and image-to-3D\ngeneration. To empower consistency, we propose Consistent123 to synthesize\nnovel views simultaneously by incorporating additional cross-view attention\nlayers and the shared self-attention mechanism. The proposed attention\nmechanism improves the interaction across all synthesized views, as well as the\nalignment between the condition view and novel views. In the sampling stage,\nsuch architecture supports simultaneously generating an arbitrary number of\nviews while training at a fixed length. We also introduce a progressive\nclassifier-free guidance strategy to achieve the trade-off between texture and\ngeometry for synthesized object views. Qualitative and quantitative experiments\nshow that Consistent123 outperforms baselines in view consistency by a large\nmargin. Furthermore, we demonstrate a significant improvement of Consistent123\non varying downstream tasks, showing its great potential in the 3D generation\nfield. The project page is available at consistent-123.github.io.\n","authors":["Haohan Weng","Tianyu Yang","Jianan Wang","Yu Li","Tong Zhang","C. L. Philip Chen","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08092v1.pdf","comment":"For more qualitative results, please see\n  https://consistent-123.github.io/"},{"id":"http://arxiv.org/abs/2212.00564v2","updated":"2023-10-12T07:21:23Z","published":"2022-12-01T15:11:21Z","title":"Leveraging Single-View Images for Unsupervised 3D Point Cloud Completion","summary":"  Point clouds captured by scanning devices are often incomplete due to\nocclusion. To overcome this limitation, point cloud completion methods have\nbeen developed to predict the complete shape of an object based on its partial\ninput. These methods can be broadly classified as supervised or unsupervised.\nHowever, both categories require a large number of 3D complete point clouds,\nwhich may be difficult to capture. In this paper, we propose Cross-PCC, an\nunsupervised point cloud completion method without requiring any 3D complete\npoint clouds. We only utilize 2D images of the complete objects, which are\neasier to capture than 3D complete and clean point clouds. Specifically, to\ntake advantage of the complementary information from 2D images, we use a\nsingle-view RGB image to extract 2D features and design a fusion module to fuse\nthe 2D and 3D features extracted from the partial point cloud. To guide the\nshape of predicted point clouds, we project the predicted points of the object\nto the 2D plane and use the foreground pixels of its silhouette maps to\nconstrain the position of the projected points. To reduce the outliers of the\npredicted point clouds, we propose a view calibrator to move the points\nprojected to the background into the foreground by the single-view silhouette\nimage. To the best of our knowledge, our approach is the first point cloud\ncompletion method that does not require any 3D supervision. The experimental\nresults of our method are superior to those of the state-of-the-art\nunsupervised methods by a large margin. Moreover, our method even achieves\ncomparable performance to some supervised methods. We will make the source code\npublicly available at https://github.com/ltwu6/cross-pcc.\n","authors":["Lintai Wu","Qijian Zhang","Junhui Hou","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2212.00564v2.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2310.08084v1","updated":"2023-10-12T07:17:14Z","published":"2023-10-12T07:17:14Z","title":"Volumetric Medical Image Segmentation via Scribble Annotations and Shape\n  Priors","summary":"  Recently, weakly-supervised image segmentation using weak annotations like\nscribbles has gained great attention in computer vision and medical image\nanalysis, since such annotations are much easier to obtain compared to\ntime-consuming and labor-intensive labeling at the pixel/voxel level. However,\ndue to a lack of structure supervision on regions of interest (ROIs), existing\nscribble-based methods suffer from poor boundary localization. Furthermore,\nmost current methods are designed for 2D image segmentation, which do not fully\nleverage the volumetric information if directly applied to each image slice. In\nthis paper, we propose a scribble-based volumetric image segmentation,\nScribble2D5, which tackles 3D anisotropic image segmentation and aims to its\nimprove boundary prediction. To achieve this, we augment a 2.5D attention UNet\nwith a proposed label propagation module to extend semantic information from\nscribbles and use a combination of static and active boundary prediction to\nlearn ROI's boundary and regularize its shape. Also, we propose an optional\nadd-on component, which incorporates the shape prior information from unpaired\nsegmentation masks to further improve model accuracy. Extensive experiments on\nthree public datasets and one private dataset demonstrate our Scribble2D5\nachieves state-of-the-art performance on volumetric image segmentation using\nscribbles and shape prior if available.\n","authors":["Qiuhui Chen","Haiying Lyu","Xinyue Hu","Yong Lu","Yi Hong"],"pdf_url":"https://arxiv.org/pdf/2310.08084v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2205.06779"},{"id":"http://arxiv.org/abs/2310.08082v1","updated":"2023-10-12T07:12:20Z","published":"2023-10-12T07:12:20Z","title":"Jointly Optimized Global-Local Visual Localization of UAVs","summary":"  Navigation and localization of UAVs present a challenge when global\nnavigation satellite systems (GNSS) are disrupted and unreliable. Traditional\ntechniques, such as simultaneous localization and mapping (SLAM) and visual\nodometry (VO), exhibit certain limitations in furnishing absolute coordinates\nand mitigating error accumulation. Existing visual localization methods achieve\nautonomous visual localization without error accumulation by matching with\northo satellite images. However, doing so cannot guarantee real-time\nperformance due to the complex matching process. To address these challenges,\nwe propose a novel Global-Local Visual Localization (GLVL) network. Our GLVL\nnetwork is a two-stage visual localization approach, combining a large-scale\nretrieval module that finds similar regions with the UAV flight scene, and a\nfine-grained matching module that localizes the precise UAV coordinate,\nenabling real-time and precise localization. The training process is jointly\noptimized in an end-to-end manner to further enhance the model capability.\nExperiments on six UAV flight scenes encompassing both texture-rich and\ntexture-sparse regions demonstrate the ability of our model to achieve the\nreal-time precise localization requirements of UAVs. Particularly, our method\nachieves a localization error of only 2.39 meters in 0.48 seconds in a village\nscene with sparse texture features.\n","authors":["Haoling Li","Jiuniu Wang","Zhiwei Wei","Wenjia Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08080v1","updated":"2023-10-12T07:10:12Z","published":"2023-10-12T07:10:12Z","title":"RT-SRTS: Angle-Agnostic Real-Time Simultaneous 3D Reconstruction and\n  Tumor Segmentation from Single X-Ray Projection","summary":"  Radiotherapy is one of the primary treatment methods for tumors, but the\norgan movement caused by respiratory motion limits its accuracy. Recently, 3D\nimaging from single X-ray projection receives extensive attentions as a\npromising way to address this issue. However, current methods can only\nreconstruct 3D image without direct location of the tumor and are only\nvalidated for fixed-angle imaging, which fails to fully meet the requirement of\nmotion control in radiotherapy. In this study, we propose a novel imaging\nmethod RT-SRTS which integrates 3D imaging and tumor segmentation into one\nnetwork based on the multi-task learning (MTL) and achieves real-time\nsimultaneous 3D reconstruction and tumor segmentation from single X-ray\nprojection at any angle. Futhermore, we propose the attention enhanced\ncalibrator (AEC) and uncertain-region elaboration (URE) modules to aid feature\nextraction and improve segmentation accuracy. We evaluated the proposed method\non ten patient cases and compared it with two state-of-the-art methods. Our\napproach not only delivered superior 3D reconstruction but also demonstrated\ncommendable tumor segmentation results. The simultaneous reconstruction and\nsegmentation could be completed in approximately 70 ms, significantly faster\nthan the required time threshold for real-time tumor tracking. The efficacy of\nboth AEC and URE was also validated through ablation studies.\n","authors":["Miao Zhu","Qiming Fu","Bo Liu","Mengxi Zhang","Bojian Li","Xiaoyan Luo","Fugen Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.08080v1.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2210.11318v2","updated":"2023-10-12T06:52:18Z","published":"2022-10-20T14:51:01Z","title":"A Survey of Computer Vision Technologies In Urban and\n  Controlled-environment Agriculture","summary":"  In the evolution of agriculture to its next stage, Agriculture 5.0,\nartificial intelligence will play a central role. Controlled-environment\nagriculture, or CEA, is a special form of urban and suburban agricultural\npractice that offers numerous economic, environmental, and social benefits,\nincluding shorter transportation routes to population centers, reduced\nenvironmental impact, and increased productivity. Due to its ability to control\nenvironmental factors, CEA couples well with computer vision (CV) in the\nadoption of real-time monitoring of the plant conditions and autonomous\ncultivation and harvesting. The objective of this paper is to familiarize CV\nresearchers with agricultural applications and agricultural practitioners with\nthe solutions offered by CV. We identify five major CV applications in CEA,\nanalyze their requirements and motivation, and survey the state of the art as\nreflected in 68 technical papers using deep learning methods. In addition, we\ndiscuss five key subareas of computer vision and how they related to these CEA\nproblems, as well as eleven vision-based CEA datasets. We hope the survey will\nhelp researchers quickly gain a bird-eye view of the striving research area and\nwill spark inspiration for new research and development.\n","authors":["Jiayun Luo","Boyang Li","Cyril Leung"],"pdf_url":"https://arxiv.org/pdf/2210.11318v2.pdf","comment":"1 overview figures, 37 pages, 8 tables, accepted by ACM Computing\n  Surveys"},{"id":"http://arxiv.org/abs/2310.08073v1","updated":"2023-10-12T06:50:43Z","published":"2023-10-12T06:50:43Z","title":"Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural\n  Networks","summary":"  Neural network pruning has shown to be an effective technique for reducing\nthe network size, trading desirable properties like generalization and\nrobustness to adversarial attacks for higher sparsity. Recent work has claimed\nthat adversarial pruning methods can produce sparse networks while also\npreserving robustness to adversarial examples. In this work, we first\nre-evaluate three state-of-the-art adversarial pruning methods, showing that\ntheir robustness was indeed overestimated. We then compare pruned and dense\nversions of the same models, discovering that samples on thin ice, i.e., closer\nto the unpruned model's decision boundary, are typically misclassified after\npruning. We conclude by discussing how this intuition may lead to designing\nmore effective adversarial pruning methods in future work.\n","authors":["Giorgio Piras","Maura Pintor","Ambra Demontis","Battista Biggio"],"pdf_url":"https://arxiv.org/pdf/2310.08073v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08071v1","updated":"2023-10-12T06:36:41Z","published":"2023-10-12T06:36:41Z","title":"Learning Transferable Conceptual Prototypes for Interpretable\n  Unsupervised Domain Adaptation","summary":"  Despite the great progress of unsupervised domain adaptation (UDA) with the\ndeep neural networks, current UDA models are opaque and cannot provide\npromising explanations, limiting their applications in the scenarios that\nrequire safe and controllable model decisions. At present, a surge of work\nfocuses on designing deep interpretable methods with adequate data annotations\nand only a few methods consider the distributional shift problem. Most existing\ninterpretable UDA methods are post-hoc ones, which cannot facilitate the model\nlearning process for performance enhancement. In this paper, we propose an\ninherently interpretable method, named Transferable Conceptual Prototype\nLearning (TCPL), which could simultaneously interpret and improve the processes\nof knowledge transfer and decision-making in UDA. To achieve this goal, we\ndesign a hierarchically prototypical module that transfers categorical basic\nconcepts from the source domain to the target domain and learns domain-shared\nprototypes for explaining the underlying reasoning process. With the learned\ntransferable prototypes, a self-predictive consistent pseudo-label strategy\nthat fuses confidence, predictions, and prototype information, is designed for\nselecting suitable target samples for pseudo annotations and gradually\nnarrowing down the domain gap. Comprehensive experiments show that the proposed\nmethod can not only provide effective and intuitive explanations but also\noutperform previous state-of-the-arts.\n","authors":["Junyu Gao","Xinhong Ma","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08071v1.pdf","comment":"Submitted to IEEE TIP"},{"id":"http://arxiv.org/abs/2310.08068v1","updated":"2023-10-12T06:32:12Z","published":"2023-10-12T06:32:12Z","title":"Frequency-Aware Re-Parameterization for Over-Fitting Based Image\n  Compression","summary":"  Over-fitting-based image compression requires weights compactness for\ncompression and fast convergence for practical use, posing challenges for deep\nconvolutional neural networks (CNNs) based methods. This paper presents a\nsimple re-parameterization method to train CNNs with reduced weights storage\nand accelerated convergence. The convolution kernels are re-parameterized as a\nweighted sum of discrete cosine transform (DCT) kernels enabling direct\noptimization in the frequency domain. Combined with L1 regularization, the\nproposed method surpasses vanilla convolutions by achieving a significantly\nimproved rate-distortion with low computational cost. The proposed method is\nverified with extensive experiments of over-fitting-based image restoration on\nvarious datasets, achieving up to -46.12% BD-rate on top of HEIF with only 200\niterations.\n","authors":["Yun Ye","Yanjie Pan","Qually Jiang","Ming Lu","Xiaoran Fang","Beryl Xu"],"pdf_url":"https://arxiv.org/pdf/2310.08068v1.pdf","comment":"to be published at ICIP 2023, this version fixed a mistake in Eq. (1)\n  in the proceeding version"},{"id":"http://arxiv.org/abs/2310.08064v1","updated":"2023-10-12T06:26:39Z","published":"2023-10-12T06:26:39Z","title":"Age Estimation Based on Graph Convolutional Networks and Multi-head\n  Attention Mechanisms","summary":"  Age estimation technology is a part of facial recognition and has been\napplied to identity authentication. This technology achieves the development\nand application of a juvenile anti-addiction system by authenticating users in\nthe game. Convolutional Neural Network (CNN) and Transformer algorithms are\nwidely used in this application scenario. However, these two models cannot\nflexibly extract and model features of faces with irregular shapes, and they\nare ineffective in capturing key information. Furthermore, the above methods\nwill contain a lot of background information while extracting features, which\nwill interfere with the model. In consequence, it is easy to extract redundant\ninformation from images. In this paper, a new modeling idea is proposed to\nsolve this problem, which can flexibly model irregular objects. The Graph\nConvolutional Network (GCN) is used to extract features from irregular face\nimages effectively, and multi-head attention mechanisms are added to avoid\nredundant features and capture key region information in the image. This model\ncan effectively improve the accuracy of age estimation and reduce the MAE error\nvalue to about 3.64, which is better than the effect of today's age estimation\nmodel, to improve the accuracy of face recognition and identity authentication.\n","authors":["Miaomiao Yang","Changwei Yao","Shijin Yan"],"pdf_url":"https://arxiv.org/pdf/2310.08064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07763v2","updated":"2023-10-12T06:24:02Z","published":"2023-07-15T10:06:43Z","title":"Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile\n  Agents","summary":"  The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to\nprovide autonomous navigation and task execution in complex and unknown\nenvironments. However, it is hard to develop a dedicated algorithm for mobile\nrobots due to dynamic and challenging situations, such as poor lighting\nconditions and motion blur. To tackle this issue, we propose a tightly-coupled\nLiDAR-visual SLAM based on geometric features, which includes two sub-systems\n(LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework\nassociates the depth and semantics of the multi-modal geometric features to\ncomplement the visual line landmarks and to add direction optimization in\nBundle Adjustment (BA). This further constrains visual odometry. On the other\nhand, the entire line segment detected by the visual subsystem overcomes the\nlimitation of the LiDAR subsystem, which can only perform the local calculation\nfor geometric features. It adjusts the direction of linear feature points and\nfilters out outliers, leading to a higher accurate odometry system. Finally, we\nemploy a module to detect the subsystem's operation, providing the LiDAR\nsubsystem's output as a complementary trajectory to our system while visual\nsubsystem tracking fails. The evaluation results on the public dataset M2DGR,\ngathered from ground robots across various indoor and outdoor scenarios, show\nthat our system achieves more accurate and robust pose estimation compared to\ncurrent state-of-the-art multi-modal methods.\n","authors":["Ke Cao","Ruiping Liu","Ze Wang","Kunyu Peng","Jiaming Zhang","Junwei Zheng","Zhifeng Teng","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2307.07763v2.pdf","comment":"Accepted to ROBIO 2023"},{"id":"http://arxiv.org/abs/2306.06599v4","updated":"2023-10-12T05:51:30Z","published":"2023-06-11T06:27:06Z","title":"Variational Imbalanced Regression: Fair Uncertainty Quantification via\n  Probabilistic Smoothing","summary":"  Existing regression models tend to fall short in both accuracy and\nuncertainty estimation when the label distribution is imbalanced. In this\npaper, we propose a probabilistic deep learning model, dubbed variational\nimbalanced regression (VIR), which not only performs well in imbalanced\nregression but naturally produces reasonable uncertainty estimation as a\nbyproduct. Different from typical variational autoencoders assuming I.I.D.\nrepresentations (a data point's representation is not directly affected by\nother data points), our VIR borrows data with similar regression labels to\ncompute the latent representation's variational distribution; furthermore,\ndifferent from deterministic regression models producing point estimates, VIR\npredicts the entire normal-inverse-gamma distributions and modulates the\nassociated conjugate distributions to impose probabilistic reweighting on the\nimbalanced data, thereby providing better uncertainty estimation. Experiments\nin several real-world datasets show that our VIR can outperform\nstate-of-the-art imbalanced regression models in terms of both accuracy and\nuncertainty estimation. Code will soon be available at\n\\url{https://github.com/Wang-ML-Lab/variational-imbalanced-regression}.\n","authors":["Ziyan Wang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2306.06599v4.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2302.08594v3","updated":"2023-10-12T05:43:45Z","published":"2023-02-16T21:38:36Z","title":"TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point\n  Cloud Semantic Segmentation","summary":"  Common image-based LiDAR point cloud semantic segmentation (LiDAR PCSS)\napproaches have bottlenecks resulting from the boundary-blurring problem of\nconvolution neural networks (CNNs) and quantitation loss of spherical\nprojection. In this work, we propose a transformer-based plug-and-play\nuncertain point refiner, i.e., TransUPR, to refine selected uncertain points in\na learnable manner, which leads to an improved segmentation performance.\nUncertain points are sampled from coarse semantic segmentation results of 2D\nimage segmentation where uncertain points are located close to the object\nboundaries in the 2D range image representation and 3D spherical projection\nbackground points. Following that, the geometry and coarse semantic features of\nuncertain points are aggregated by neighbor points in 3D space without adding\nexpensive computation and memory footprint. Finally, the transformer-based\nrefiner, which contains four stacked self-attention layers, along with an MLP\nmodule, is utilized for uncertain point classification on the concatenated\nfeatures of self-attention layers. As the proposed refiner is independent of 2D\nCNNs, our TransUPR can be easily integrated into any existing image-based LiDAR\nPCSS approaches, e.g., CENet. Our TransUPR with the CENet achieves\nstate-of-the-art performance, i.e., 68.2% mean Intersection over Union (mIoU)\non the Semantic KITTI benchmark, which provides a performance improvement of\n0.6% on the mIoU compared to the original CENet.\n","authors":["Zifan Yu","Meida Chen","Zhikang Zhang","Suya You","Raghuveer Rao","Sanjeev Agarwal","Fengbo Ren"],"pdf_url":"https://arxiv.org/pdf/2302.08594v3.pdf","comment":"6 pages; Accepted by 2023 IROS"},{"id":"http://arxiv.org/abs/2310.08044v1","updated":"2023-10-12T05:34:45Z","published":"2023-10-12T05:34:45Z","title":"EC-Depth: Exploring the consistency of self-supervised monocular depth\n  estimation under challenging scenes","summary":"  Self-supervised monocular depth estimation holds significant importance in\nthe fields of autonomous driving and robotics. However, existing methods are\ntypically designed to train and test on clear and pristine datasets,\noverlooking the impact of various adverse conditions prevalent in real-world\nscenarios. As a result, it is commonly observed that most self-supervised\nmonocular depth estimation methods struggle to perform adequately under\nchallenging conditions. To address this issue, we present EC-Depth, a novel\nself-supervised two-stage training framework to achieve a robust depth\nestimation, starting from the foundation of depth prediction consistency under\ndifferent perturbations. Leveraging the proposed perturbation-invariant depth\nconsistency constraint module and the consistency-based pseudo-label selection\nmodule, our model attains accurate and consistent depth predictions in both\nstandard and challenging scenarios. Extensive experiments substantiate the\neffectiveness of the proposed method. Moreover, our method surpasses existing\nstate-of-the-art methods on KITTI, KITTI-C and DrivingStereo benchmarks,\ndemonstrating its potential for enhancing the reliability of self-supervised\nmonocular depth estimation models in real-world applications.\n","authors":["Ruijie Zhu","Ziyang Song","Chuxin Wang","Jianfeng He","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08042v1","updated":"2023-10-12T05:33:25Z","published":"2023-10-12T05:33:25Z","title":"X-HRNet: Towards Lightweight Human Pose Estimation with Spatially\n  Unidimensional Self-Attention","summary":"  High-resolution representation is necessary for human pose estimation to\nachieve high performance, and the ensuing problem is high computational\ncomplexity. In particular, predominant pose estimation methods estimate human\njoints by 2D single-peak heatmaps. Each 2D heatmap can be horizontally and\nvertically projected to and reconstructed by a pair of 1D heat vectors.\nInspired by this observation, we introduce a lightweight and powerful\nalternative, Spatially Unidimensional Self-Attention (SUSA), to the pointwise\n(1x1) convolution that is the main computational bottleneck in the depthwise\nseparable 3c3 convolution. Our SUSA reduces the computational complexity of the\npointwise (1x1) convolution by 96% without sacrificing accuracy. Furthermore,\nwe use the SUSA as the main module to build our lightweight pose estimation\nbackbone X-HRNet, where `X' represents the estimated cross-shape attention\nvectors. Extensive experiments on the COCO benchmark demonstrate the\nsuperiority of our X-HRNet, and comprehensive ablation studies show the\neffectiveness of the SUSA modules. The code is publicly available at\nhttps://github.com/cool-xuan/x-hrnet.\n","authors":["Yixuan Zhou","Xuanhan Wang","Xing Xu","Lei Zhao","Jingkuan Song"],"pdf_url":"https://arxiv.org/pdf/2310.08042v1.pdf","comment":"Accepted by ICME 2022"},{"id":"http://arxiv.org/abs/2310.05624v2","updated":"2023-10-12T05:33:19Z","published":"2023-10-09T11:26:58Z","title":"Locality-Aware Generalizable Implicit Neural Representation","summary":"  Generalizable implicit neural representation (INR) enables a single\ncontinuous function, i.e., a coordinate-based neural network, to represent\nmultiple data instances by modulating its weights or intermediate features\nusing latent codes. However, the expressive power of the state-of-the-art\nmodulation is limited due to its inability to localize and capture fine-grained\ndetails of data entities such as specific pixels and rays. To address this\nissue, we propose a novel framework for generalizable INR that combines a\ntransformer encoder with a locality-aware INR decoder. The transformer encoder\npredicts a set of latent tokens from a data instance to encode local\ninformation into each latent token. The locality-aware INR decoder extracts a\nmodulation vector by selectively aggregating the latent tokens via\ncross-attention for a coordinate input and then predicts the output by\nprogressively decoding with coarse-to-fine modulation through multiple\nfrequency bandwidths. The selective token aggregation and the multi-band\nfeature modulation enable us to learn locality-aware representation in spatial\nand spectral aspects, respectively. Our framework significantly outperforms\nprevious generalizable INRs and validates the usefulness of the locality-aware\nlatents for downstream tasks such as image generation.\n","authors":["Doyup Lee","Chiheon Kim","Minsu Cho","Wook-Shin Han"],"pdf_url":"https://arxiv.org/pdf/2310.05624v2.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2310.08038v1","updated":"2023-10-12T05:09:27Z","published":"2023-10-12T05:09:27Z","title":"Continual Learning via Manifold Expansion Replay","summary":"  In continual learning, the learner learns multiple tasks in sequence, with\ndata being acquired only once for each task. Catastrophic forgetting is a major\nchallenge to continual learning. To reduce forgetting, some existing\nrehearsal-based methods use episodic memory to replay samples of previous\ntasks. However, in the process of knowledge integration when learning a new\ntask, this strategy also suffers from catastrophic forgetting due to an\nimbalance between old and new knowledge. To address this problem, we propose a\nnovel replay strategy called Manifold Expansion Replay (MaER). We argue that\nexpanding the implicit manifold of the knowledge representation in the episodic\nmemory helps to improve the robustness and expressiveness of the model. To this\nend, we propose a greedy strategy to keep increasing the diameter of the\nimplicit manifold represented by the knowledge in the buffer during memory\nmanagement. In addition, we introduce Wasserstein distance instead of cross\nentropy as distillation loss to preserve previous knowledge. With extensive\nexperimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show\nthat the proposed method significantly improves the accuracy in continual\nlearning setup, outperforming the state of the arts.\n","authors":["Zihao Xu","Xuan Tang","Yufei Shi","Jianfeng Zhang","Jian Yang","Mingsong Chen","Xian Wei"],"pdf_url":"https://arxiv.org/pdf/2310.08038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08035v1","updated":"2023-10-12T05:03:19Z","published":"2023-10-12T05:03:19Z","title":"BaSAL: Size Balanced Warm Start Active Learning for LiDAR Semantic\n  Segmentation","summary":"  Active learning strives to reduce the need for costly data annotation, by\nrepeatedly querying an annotator to label the most informative samples from a\npool of unlabeled data and retraining a model from these samples. We identify\ntwo problems with existing active learning methods for LiDAR semantic\nsegmentation. First, they ignore the severe class imbalance inherent in LiDAR\nsemantic segmentation datasets. Second, to bootstrap the active learning loop,\nthey train their initial model from randomly selected data samples, which leads\nto low performance and is referred to as the cold start problem. To address\nthese problems we propose BaSAL, a size-balanced warm start active learning\nmodel, based on the observation that each object class has a characteristic\nsize. By sampling object clusters according to their size, we can thus create a\nsize-balanced dataset that is also more class-balanced. Furthermore, in\ncontrast to existing information measures like entropy or CoreSet, size-based\nsampling does not require an already trained model and thus can be used to\naddress the cold start problem. Results show that we are able to improve the\nperformance of the initial model by a large margin. Combining size-balanced\nsampling and warm start with established information measures, our approach\nachieves a comparable performance to training on the entire SemanticKITTI\ndataset, despite using only 5% of the annotations, which outperforms existing\nactive learning methods. We also match the existing state-of-the-art in active\nlearning on nuScenes. Our code will be made available upon paper acceptance.\n","authors":["Jiarong Wei","Yancong Lin","Holger Caesar"],"pdf_url":"https://arxiv.org/pdf/2310.08035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08027v1","updated":"2023-10-12T04:14:28Z","published":"2023-10-12T04:14:28Z","title":"Exploring Large Language Models for Multi-Modal Out-of-Distribution\n  Detection","summary":"  Out-of-distribution (OOD) detection is essential for reliable and trustworthy\nmachine learning. Recent multi-modal OOD detection leverages textual\ninformation from in-distribution (ID) class names for visual OOD detection, yet\nit currently neglects the rich contextual information of ID classes. Large\nlanguage models (LLMs) encode a wealth of world knowledge and can be prompted\nto generate descriptive features for each class. Indiscriminately using such\nknowledge causes catastrophic damage to OOD detection due to LLMs'\nhallucinations, as is observed by our analysis. In this paper, we propose to\napply world knowledge to enhance OOD detection performance through selective\ngeneration from LLMs. Specifically, we introduce a consistency-based\nuncertainty calibration method to estimate the confidence score of each\ngeneration. We further extract visual objects from each image to fully\ncapitalize on the aforementioned world knowledge. Extensive experiments\ndemonstrate that our method consistently outperforms the state-of-the-art.\n","authors":["Yi Dai","Hao Lang","Kaisheng Zeng","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2310.08027v1.pdf","comment":"EMNLP2023 Findings Long Paper"},{"id":"http://arxiv.org/abs/2310.08026v1","updated":"2023-10-12T04:12:43Z","published":"2023-10-12T04:12:43Z","title":"Beyond Sharing Weights in Decoupling Feature Learning Network for UAV\n  RGB-Infrared Vehicle Re-Identification","summary":"  Owing to the capacity of performing full-time target search, cross-modality\nvehicle re-identification (Re-ID) based on unmanned aerial vehicle (UAV) is\ngaining more attention in both video surveillance and public security. However,\nthis promising and innovative research has not been studied sufficiently due to\nthe data inadequacy issue. Meanwhile, the cross-modality discrepancy and\norientation discrepancy challenges further aggravate the difficulty of this\ntask. To this end, we pioneer a cross-modality vehicle Re-ID benchmark named\nUAV Cross-Modality Vehicle Re-ID (UCM-VeID), containing 753 identities with\n16015 RGB and 13913 infrared images. Moreover, to meet cross-modality\ndiscrepancy and orientation discrepancy challenges, we present a hybrid weights\ndecoupling network (HWDNet) to learn the shared discriminative\norientation-invariant features. For the first challenge, we proposed a hybrid\nweights siamese network with a well-designed weight restrainer and its\ncorresponding objective function to learn both modality-specific and modality\nshared information. In terms of the second challenge, three effective\ndecoupling structures with two pretext tasks are investigated to learn\norientation-invariant feature. Comprehensive experiments are carried out to\nvalidate the effectiveness of the proposed method. The dataset and codes will\nbe released at https://github.com/moonstarL/UAV-CM-VeID.\n","authors":["Xingyue Liu","Jiahao Qi","Chen Chen","Kangcheng Bin","Ping Zhong"],"pdf_url":"https://arxiv.org/pdf/2310.08026v1.pdf","comment":"13 pages, 10 figures, 64 citations, submitted to TMM"},{"id":"http://arxiv.org/abs/2210.15889v4","updated":"2023-10-12T04:05:41Z","published":"2022-10-28T04:38:10Z","title":"Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on\n  Neuro-Symbolic Computing","summary":"  Neural-symbolic computing (NeSy), which pursues the integration of the\nsymbolic and statistical paradigms of cognition, has been an active research\narea of Artificial Intelligence (AI) for many years. As NeSy shows promise of\nreconciling the advantages of reasoning and interpretability of symbolic\nrepresentation and robust learning in neural networks, it may serve as a\ncatalyst for the next generation of AI. In the present paper, we provide a\nsystematic overview of the recent developments and important contributions of\nNeSy research. Firstly, we introduce study history of this area, covering early\nwork and foundations. We further discuss background concepts and identify key\ndriving factors behind the development of NeSy. Afterward, we categorize recent\nlandmark approaches along several main characteristics that underline this\nresearch paradigm, including neural-symbolic integration, knowledge\nrepresentation, knowledge embedding, and functionality. Next, we briefly\ndiscuss the successful application of modern NeSy approaches in several\ndomains. Then, we benchmark several NeSy methods on three representative\napplication tasks. Finally, we identify the open problems together with\npotential future research directions. This survey is expected to help new\nresearchers enter this rapidly evolving field and accelerate the progress\ntowards data-and knowledge-driven AI.\n","authors":["Wenguan Wang","Yi Yang","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2210.15889v4.pdf","comment":"Ongoing project"},{"id":"http://arxiv.org/abs/2308.03807v2","updated":"2023-10-12T03:36:17Z","published":"2023-08-06T15:47:03Z","title":"Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS\n  Image Reconstruction","summary":"  Proximal gradient-based optimization is one of the most common strategies to\nsolve inverse problem of images, and it is easy to implement. However, these\ntechniques often generate heavy artifacts in image reconstruction. One of the\nmost popular refinement methods is to fine-tune the regularization parameter to\nalleviate such artifacts, but it may not always be sufficient or applicable due\nto increased computational costs. In this work, we propose a deep geometric\nincremental learning framework based on the second Nesterov proximal gradient\noptimization. The proposed end-to-end network not only has the powerful\nlearning ability for high-/low-frequency image features, but also can\ntheoretically guarantee that geometric texture details will be reconstructed\nfrom preliminary linear reconstruction. Furthermore, it can avoid the risk of\nintermediate reconstruction results falling outside the geometric decomposition\ndomains and achieve fast convergence. Our reconstruction framework is\ndecomposed into four modules including general linear reconstruction, cascade\ngeometric incremental restoration, Nesterov acceleration, and post-processing.\nIn the image restoration step, a cascade geometric incremental learning module\nis designed to compensate for missing texture information from different\ngeometric spectral decomposition domains. Inspired by the overlap-tile\nstrategy, we also develop a post-processing module to remove the block effect\nin patch-wise-based natural image reconstruction. All parameters in the\nproposed model are learnable, an adaptive initialization technique of physical\nparameters is also employed to make model flexibility and ensure converging\nsmoothly. We compare the reconstruction performance of the proposed method with\nexisting state-of-the-art methods to demonstrate its superiority. Our source\ncodes are available at https://github.com/fanxiaohong/Nest-DGIL.\n","authors":["Xiaohong Fan","Yin Yang","Ke Chen","Yujie Feng","Jianping Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.03807v2.pdf","comment":"15 pages,our source codes are available at\n  https://github.com/fanxiaohong/Nest-DGIL"},{"id":"http://arxiv.org/abs/2310.06488v2","updated":"2023-10-12T03:23:40Z","published":"2023-10-10T09:57:17Z","title":"SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural\n  Network","summary":"  Spiking neural networks (SNNs) have demonstrated the capability to achieve\ncomparable performance to deep neural networks (DNNs) in both visual and\nlinguistic domains while offering the advantages of improved energy efficiency\nand adherence to biological plausibility. However, the extension of such\nsingle-modality SNNs into the realm of multimodal scenarios remains an\nunexplored territory. Drawing inspiration from the concept of contrastive\nlanguage-image pre-training (CLIP), we introduce a novel framework, named\nSpikeCLIP, to address the gap between two modalities within the context of\nspike-based computing through a two-step recipe involving ``Alignment\nPre-training + Dual-Loss Fine-tuning\". Extensive experiments demonstrate that\nSNNs achieve comparable results to their DNN counterparts while significantly\nreducing energy consumption across a variety of datasets commonly used for\nmultimodal model evaluation. Furthermore, SpikeCLIP maintains robust\nperformance in image classification tasks that involve class labels not\npredefined within specific categories.\n","authors":["Tianlong Li","Wenhao Liu","Changze Lv","Jianhan Xu","Cenyuan Zhang","Muling Wu","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08009v1","updated":"2023-10-12T03:21:12Z","published":"2023-10-12T03:21:12Z","title":"Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video\n  Retrieval","summary":"  Unsupervised video hashing usually optimizes binary codes by learning to\nreconstruct input videos. Such reconstruction constraint spends much effort on\nframe-level temporal context changes without focusing on video-level global\nsemantics that are more useful for retrieval. Hence, we address this problem by\ndecomposing video information into reconstruction-dependent and\nsemantic-dependent information, which disentangles the semantic extraction from\nreconstruction constraint. Specifically, we first design a simple dual-stream\nstructure, including a temporal layer and a hash layer. Then, with the help of\nsemantic similarity knowledge obtained from self-supervision, the hash layer\nlearns to capture information for semantic retrieval, while the temporal layer\nlearns to capture the information for reconstruction. In this way, the model\nnaturally preserves the disentangled semantics into binary codes. Validated by\ncomprehensive experiments, our method consistently outperforms the\nstate-of-the-arts on three video benchmarks.\n","authors":["Pandeng Li","Hongtao Xie","Jiannan Ge","Lei Zhang","Shaobo Min","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.08009v1.pdf","comment":"17 pages, 8 figures, ECCV 2022"},{"id":"http://arxiv.org/abs/2310.08002v1","updated":"2023-10-12T03:14:02Z","published":"2023-10-12T03:14:02Z","title":"MLP-AMDC: An MLP Architecture for Adaptive-Mask-based Dual-Camera\n  snapshot hyperspectral imaging","summary":"  Coded Aperture Snapshot Spectral Imaging (CASSI) system has great advantages\nover traditional methods in dynamically acquiring Hyper-Spectral Image (HSI),\nbut there are the following problems. 1) Traditional mask relies on random\npatterns or analytical design, both of which limit the performance improvement\nof CASSI. 2) Existing high-quality reconstruction algorithms are slow in\nreconstruction and can only reconstruct scene information offline. To address\nthe above two problems, this paper designs the AMDC-CASSI system, introducing\nRGB camera with CASSI based on Adaptive-Mask as multimodal input to improve the\nreconstruction quality. The existing SOTA reconstruction schemes are based on\ntransformer, but the operation of self-attention pulls down the operation\nefficiency of the network. In order to improve the inference speed of the\nreconstruction network, this paper proposes An MLP Architecture for\nAdaptive-Mask-based Dual-Camera (MLP-AMDC) to replace the transformer structure\nof the network. Numerous experiments have shown that MLP performs no less well\nthan transformer-based structures for HSI reconstruction, while MLP greatly\nimproves the network inference speed and has less number of parameters and\noperations, our method has a 8 db improvement over SOTA and at least a 5-fold\nimprovement in reconstruction speed. (https://github.com/caizeyu1992/MLP-AMDC.)\n","authors":["Zeyu Cai","Can Zhang","Xunhao Chen","Shanghuan Liu","Chengqian Jin","Feipeng Da"],"pdf_url":"https://arxiv.org/pdf/2310.08002v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2308.01541"},{"id":"http://arxiv.org/abs/2209.05167v3","updated":"2023-10-12T03:09:25Z","published":"2022-09-12T11:51:20Z","title":"LF-VISLAM: A SLAM Framework for Large Field-of-View Cameras with\n  Negative Imaging Plane on Mobile Agents","summary":"  Simultaneous Localization And Mapping (SLAM) has become a crucial aspect in\nthe fields of autonomous driving and robotics. One crucial component of visual\nSLAM is the Field-of-View (FoV) of the camera, as a larger FoV allows for a\nwider range of surrounding elements and features to be perceived. However, when\nthe FoV of the camera reaches the negative half-plane, traditional methods for\nrepresenting image feature points using [u,v,1]^T become ineffective. While the\npanoramic FoV is advantageous for loop closure, its benefits are not easily\nrealized under large-attitude-angle differences where loop-closure frames\ncannot be easily matched by existing methods. As loop closure on wide-FoV\npanoramic data further comes with a large number of outliers, traditional\noutlier rejection methods are not directly applicable. To address these issues,\nwe propose LF-VISLAM, a Visual Inertial SLAM framework for cameras with\nextremely Large FoV with loop closure. A three-dimensional vector with unit\nlength is introduced to effectively represent feature points even on the\nnegative half-plane. The attitude information of the SLAM system is leveraged\nto guide the feature point detection of the loop closure. Additionally, a new\noutlier rejection method based on the unit length representation is integrated\ninto the loop closure module. We collect the PALVIO dataset using a Panoramic\nAnnular Lens (PAL) system with an entire FoV of 360{\\deg}x(40{\\deg}~120{\\deg})\nand an Inertial Measurement Unit (IMU) for Visual Inertial Odometry (VIO) to\naddress the lack of panoramic SLAM datasets. Experiments on the established\nPALVIO and public datasets show that the proposed LF-VISLAM outperforms\nstate-of-the-art SLAM methods. Our code will be open-sourced at\nhttps://github.com/flysoaryun/LF-VISLAM.\n","authors":["Ze Wang","Kailun Yang","Hao Shi","Peng Li","Fei Gao","Jian Bai","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2209.05167v3.pdf","comment":"Accepted to IEEE Transactions on Automation Science and Engineering\n  (T-ASE). Extended version of IROS2022 paper arXiv:2202.12613. Code and\n  dataset will be open-sourced at https://github.com/flysoaryun/LF-SLAM"},{"id":"http://arxiv.org/abs/2310.07997v1","updated":"2023-10-12T02:52:33Z","published":"2023-10-12T02:52:33Z","title":"Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by\n  Volume Rendering","summary":"  Recently, learning neural implicit surface by volume rendering has been a\npromising way for multi-view reconstruction. However, limited accuracy and\nexcessive time complexity remain bottlenecks that current methods urgently need\nto overcome. To address these challenges, we propose a new method called\nPoint-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient\nreconstruction. Point modeling is organically embedded into the volume\nrendering to enhance and regularize the representation of implicit surface.\nSpecifically, to achieve precise point guidance and noise robustness, aleatoric\nuncertainty of the point cloud is modeled to capture the distribution of noise\nand estimate the reliability of points. Additionally, a Neural Projection\nmodule connecting points and images is introduced to add geometric constraints\nto the Signed Distance Function (SDF). To better compensate for geometric bias\nbetween volume rendering and point modeling, high-fidelity points are filtered\ninto an Implicit Displacement Network to improve the representation of SDF.\nBenefiting from our effective point guidance, lightweight networks are employed\nto achieve an impressive 11x speedup compared to NeuS. Extensive experiments\nshow that our method yields high-quality surfaces, especially for fine-grained\ndetails and smooth regions. Moreover, it exhibits strong robustness to both\nnoisy and sparse data.\n","authors":["Chen Zhang","Wanjuan Su","Wenbing Tao"],"pdf_url":"https://arxiv.org/pdf/2310.07997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07996v1","updated":"2023-10-12T02:52:14Z","published":"2023-10-12T02:52:14Z","title":"Reset It and Forget It: Relearning Last-Layer Weights Improves Continual\n  and Transfer Learning","summary":"  This work identifies a simple pre-training mechanism that leads to\nrepresentations exhibiting better continual and transfer learning. This\nmechanism -- the repeated resetting of weights in the last layer, which we\nnickname \"zapping\" -- was originally designed for a meta-continual-learning\nprocedure, yet we show it is surprisingly applicable in many settings beyond\nboth meta-learning and continual learning. In our experiments, we wish to\ntransfer a pre-trained image classifier to a new set of classes, in a few\nshots. We show that our zapping procedure results in improved transfer accuracy\nand/or more rapid adaptation in both standard fine-tuning and continual\nlearning settings, while being simple to implement and computationally\nefficient. In many cases, we achieve performance on par with state of the art\nmeta-learning without needing the expensive higher-order gradients, by using a\ncombination of zapping and sequential learning. An intuitive explanation for\nthe effectiveness of this zapping procedure is that representations trained\nwith repeated zapping learn features that are capable of rapidly adapting to\nnewly initialized classifiers. Such an approach may be considered a\ncomputationally cheaper type of, or alternative to, meta-learning rapidly\nadaptable features with higher-order gradients. This adds to recent work on the\nusefulness of resetting neural network parameters during training, and invites\nfurther investigation of this mechanism.\n","authors":["Lapo Frati","Neil Traft","Jeff Clune","Nick Cheney"],"pdf_url":"https://arxiv.org/pdf/2310.07996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07995v1","updated":"2023-10-12T02:49:00Z","published":"2023-10-12T02:49:00Z","title":"HeightFormer: A Multilevel Interaction and Image-adaptive\n  Classification-regression Network for Monocular Height Estimation with Aerial\n  Images","summary":"  Height estimation has long been a pivotal topic within measurement and remote\nsensing disciplines, proving critical for endeavours such as 3D urban\nmodelling, MR and autonomous driving. Traditional methods utilise stereo\nmatching or multisensor fusion, both well-established techniques that typically\nnecessitate multiple images from varying perspectives and adjunct sensors like\nSAR, leading to substantial deployment costs. Single image height estimation\nhas emerged as an attractive alternative, boasting a larger data source variety\nand simpler deployment. However, current methods suffer from limitations such\nas fixed receptive fields, a lack of global information interaction, leading to\nnoticeable instance-level height deviations. The inherent complexity of height\nprediction can result in a blurry estimation of object edge depth when using\nmainstream regression methods based on fixed height division. This paper\npresents a comprehensive solution for monocular height estimation in remote\nsensing, termed HeightFormer, combining multilevel interactions and\nimage-adaptive classification-regression. It features the Multilevel\nInteraction Backbone (MIB) and Image-adaptive Classification-regression Height\nGenerator (ICG). MIB supplements the fixed sample grid in CNN of the\nconventional backbone network with tokens of different interaction ranges. It\nis complemented by a pixel-, patch-, and feature map-level hierarchical\ninteraction mechanism, designed to relay spatial geometry information across\ndifferent scales and introducing a global receptive field to enhance the\nquality of instance-level height estimation. The ICG dynamically generates\nheight partition for each image and reframes the traditional regression task,\nusing a refinement from coarse to fine classification-regression that\nsignificantly mitigates the innate ill-posedness issue and drastically improves\nedge sharpness.\n","authors":["Zhan Chen","Yidan Zhang","Xiyu Qi","Yongqiang Mao","Xin Zhou","Lulu Niu","Hui Wu","Lei Wang","Yunping Ge"],"pdf_url":"https://arxiv.org/pdf/2310.07995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12535v2","updated":"2023-10-12T02:38:50Z","published":"2023-03-21T17:28:44Z","title":"An Effective Motion-Centric Paradigm for 3D Single Object Tracking in\n  Point Clouds","summary":"  3D single object tracking in LiDAR point clouds (LiDAR SOT) plays a crucial\nrole in autonomous driving. Current approaches all follow the Siamese paradigm\nbased on appearance matching. However, LiDAR point clouds are usually\ntextureless and incomplete, which hinders effective appearance matching.\nBesides, previous methods greatly overlook the critical motion clues among\ntargets. In this work, beyond 3D Siamese tracking, we introduce a\nmotion-centric paradigm to handle LiDAR SOT from a new perspective. Following\nthis paradigm, we propose a matching-free two-stage tracker M^2-Track. At the\n1st-stage, M^2-Track localizes the target within successive frames via motion\ntransformation. Then it refines the target box through motion-assisted shape\ncompletion at the 2nd-stage. Due to the motion-centric nature, our method shows\nits impressive generalizability with limited training labels and provides good\ndifferentiability for end-to-end cycle training. This inspires us to explore\nsemi-supervised LiDAR SOT by incorporating a pseudo-label-based motion\naugmentation and a self-supervised loss term. Under the fully-supervised\nsetting, extensive experiments confirm that M^2-Track significantly outperforms\nprevious state-of-the-arts on three large-scale datasets while running at 57FPS\n(~3%, ~11% and ~22% precision gains on KITTI, NuScenes, and Waymo Open Dataset\nrespectively). While under the semi-supervised setting, our method performs on\npar with or even surpasses its fully-supervised counterpart using fewer than\nhalf of the labels from KITTI. Further analysis verifies each component's\neffectiveness and shows the motion-centric paradigm's promising potential for\nauto-labeling and unsupervised domain adaptation.\n","authors":["Chaoda Zheng","Xu Yan","Haiming Zhang","Baoyuan Wang","Shenghui Cheng","Shuguang Cui","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2303.12535v2.pdf","comment":"Accepted version of the journal extension of M^2-Track. Accepted by\n  TPAMI. arXiv admin note: substantial text overlap with arXiv:2203.01730"},{"id":"http://arxiv.org/abs/2207.12389v2","updated":"2023-10-12T02:01:50Z","published":"2022-07-25T17:55:28Z","title":"MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised\n  Domain Adaptation","summary":"  Practical real world datasets with plentiful categories introduce new\nchallenges for unsupervised domain adaptation like small inter-class\ndiscriminability, that existing approaches relying on domain invariance alone\ncannot handle sufficiently well. In this work we propose MemSAC, which exploits\nsample level similarity across source and target domains to achieve\ndiscriminative transfer, along with architectures that scale to a large number\nof categories. For this purpose, we first introduce a memory augmented approach\nto efficiently extract pairwise similarity relations between labeled source and\nunlabeled target domain instances, suited to handle an arbitrary number of\nclasses. Next, we propose and theoretically justify a novel variant of the\ncontrastive loss to promote local consistency among within-class cross domain\nsamples while enforcing separation between classes, thus preserving\ndiscriminative transfer from source to target. We validate the advantages of\nMemSAC with significant improvements over previous state-of-the-art on multiple\nchallenging transfer tasks designed for large-scale adaptation, such as\nDomainNet with 345 classes and fine-grained adaptation on Caltech-UCSD birds\ndataset with 200 classes. We also provide in-depth analysis and insights into\nthe effectiveness of MemSAC.\n","authors":["Tarun Kalluri","Astuti Sharma","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2207.12389v2.pdf","comment":"Accepted at ECCV 2022. Project Webpage:\n  https://tarun005.github.io/MemSAC/"},{"id":"http://arxiv.org/abs/2310.07975v1","updated":"2023-10-12T01:47:55Z","published":"2023-10-12T01:47:55Z","title":"Self-supervised visual learning for analyzing firearms trafficking\n  activities on the Web","summary":"  Automated visual firearms classification from RGB images is an important\nreal-world task with applications in public space security, intelligence\ngathering and law enforcement investigations. When applied to images massively\ncrawled from the World Wide Web (including social media and dark Web sites), it\ncan serve as an important component of systems that attempt to identify\ncriminal firearms trafficking networks, by analyzing Big Data from open-source\nintelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology\nfor achieving this, with Convolutional Neural Networks (CNN) being typically\nemployed. The common transfer learning approach consists of pretraining on a\nlarge-scale, generic annotated dataset for whole-image classification, such as\nImageNet-1k, and then finetuning the DNN on a smaller, annotated,\ntask-specific, downstream dataset for visual firearms classification. Neither\nVisual Transformer (ViT) neural architectures nor Self-Supervised Learning\n(SSL) approaches have been so far evaluated on this critical task. SSL\nessentially consists of replacing the traditional supervised pretraining\nobjective with an unsupervised pretext task that does not require ground-truth\nlabels..\n","authors":["Sotirios Konstantakos","Despina Ioanna Chalkiadaki","Ioannis Mademlis","Adamantia Anna Rebolledo Chrysochoou","Georgios Th. Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2310.07975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01244v2","updated":"2023-10-12T01:44:33Z","published":"2022-10-03T21:50:14Z","title":"Event-based Temporally Dense Optical Flow Estimation with Sequential\n  Learning","summary":"  Event cameras provide an advantage over traditional frame-based cameras when\ncapturing fast-moving objects without a motion blur. They achieve this by\nrecording changes in light intensity (known as events), thus allowing them to\noperate at a much higher frequency and making them suitable for capturing\nmotions in a highly dynamic scene. Many recent studies have proposed methods to\ntrain neural networks (NNs) for predicting optical flow from events. However,\nthey often rely on a spatio-temporal representation constructed from events\nover a fixed interval, such as 10Hz used in training on the DSEC dataset. This\nlimitation restricts the flow prediction to the same interval (10Hz) whereas\nthe fast speed of event cameras, which can operate up to 3kHz, has not been\neffectively utilized. In this work, we show that a temporally dense flow\nestimation at 100Hz can be achieved by treating the flow estimation as a\nsequential problem using two different variants of recurrent networks -\nLong-short term memory (LSTM) and spiking neural network (SNN). First, We\nutilize the NN model constructed similar to the popular EV-FlowNet but with\nLSTM layers to demonstrate the efficiency of our training method. The model not\nonly produces 10x more frequent optical flow than the existing ones, but the\nestimated flows also have 13% lower errors than predictions from the baseline\nEV-FlowNet. Second, we construct an EV-FlowNet SNN but with leaky integrate and\nfire neurons to efficiently capture the temporal dynamics. We found that simple\ninherent recurrent dynamics of SNN lead to significant parameter reduction\ncompared to the LSTM model. In addition, because of its event-driven\ncomputation, the spiking model is estimated to consume only 1.5% energy of the\nLSTM model, highlighting the efficiency of SNN in processing events and the\npotential for achieving temporally dense flow.\n","authors":["Wachirawit Ponghiran","Chamika Mihiranga Liyanagedera","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2210.01244v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07969v1","updated":"2023-10-12T01:25:21Z","published":"2023-10-12T01:25:21Z","title":"CleftGAN: Adapting A Style-Based Generative Adversarial Network To\n  Create Images Depicting Cleft Lip Deformity","summary":"  A major obstacle when attempting to train a machine learning system to\nevaluate facial clefts is the scarcity of large datasets of high-quality,\nethics board-approved patient images. In response, we have built a deep\nlearning-based cleft lip generator designed to produce an almost unlimited\nnumber of artificial images exhibiting high-fidelity facsimiles of cleft lip\nwith wide variation. We undertook a transfer learning protocol testing\ndifferent versions of StyleGAN-ADA (a generative adversarial network image\ngenerator incorporating adaptive data augmentation (ADA)) as the base model.\nTraining images depicting a variety of cleft deformities were pre-processed to\nadjust for rotation, scaling, color adjustment and background blurring. The ADA\nmodification of the primary algorithm permitted construction of our new\ngenerative model while requiring input of a relatively small number of training\nimages. Adversarial training was carried out using 514 unique frontal\nphotographs of cleft-affected faces to adapt a pre-trained model based on\n70,000 normal faces. The Frechet Inception Distance (FID) was used to measure\nthe similarity of the newly generated facial images to the cleft training\ndataset, while Perceptual Path Length (PPL) and the novel Divergence Index of\nSeverity Histograms (DISH) measures were also used to assess the performance of\nthe image generator that we dub CleftGAN. We found that StyleGAN3 with\ntranslation invariance (StyleGAN3-t) performed optimally as a base model.\nGenerated images achieved a low FID reflecting a close similarity to our\ntraining input dataset of genuine cleft images. Low PPL and DISH measures\nreflected a smooth and semantically valid interpolation of images through the\ntransfer learning process and a similar distribution of severity in the\ntraining and generated images, respectively.\n","authors":["Abdullah Hayajneh","Erchin Serpedin","Mohammad Shaqfeh","Graeme Glass","Mitchell A. Stotland"],"pdf_url":"https://arxiv.org/pdf/2310.07969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03270v3","updated":"2023-10-12T01:13:41Z","published":"2023-10-05T02:51:53Z","title":"EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit\n  Diffusion Models","summary":"  Diffusion models have demonstrated remarkable capabilities in image synthesis\nand related generative tasks. Nevertheless, their practicality for low-latency\nreal-world applications is constrained by substantial computational costs and\nlatency issues. Quantization is a dominant way to compress and accelerate\ndiffusion models, where post-training quantization (PTQ) and quantization-aware\ntraining (QAT) are two main approaches, each bearing its own properties. While\nPTQ exhibits efficiency in terms of both time and data usage, it may lead to\ndiminished performance in low bit-width. On the other hand, QAT can alleviate\nperformance degradation but comes with substantial demands on computational and\ndata resources. To capitalize on the advantages while avoiding their respective\ndrawbacks, we introduce a data-free and parameter-efficient fine-tuning\nframework for low-bit diffusion models, dubbed EfficientDM, to achieve\nQAT-level performance with PTQ-like efficiency. Specifically, we propose a\nquantization-aware variant of the low-rank adapter (QALoRA) that can be merged\nwith model weights and jointly quantized to low bit-width. The fine-tuning\nprocess distills the denoising capabilities of the full-precision model into\nits quantized counterpart, eliminating the requirement for training data. We\nalso introduce scale-aware optimization and employ temporal learned step-size\nquantization to further enhance performance. Extensive experimental results\ndemonstrate that our method significantly outperforms previous PTQ-based\ndiffusion models while maintaining similar time and data efficiency.\nSpecifically, there is only a marginal 0.05 sFID increase when quantizing both\nweights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to\nQAT-based methods, our EfficientDM also boasts a 16.2x faster quantization\nspeed with comparable generation quality.\n","authors":["Yefei He","Jing Liu","Weijia Wu","Hong Zhou","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2310.03270v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00613v3","updated":"2023-10-12T00:27:09Z","published":"2022-12-01T16:09:54Z","title":"NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and\n  Animation","summary":"  The capture and animation of human hair are two of the major challenges in\nthe creation of realistic avatars for the virtual reality. Both problems are\nhighly challenging, because hair has complex geometry and appearance, as well\nas exhibits challenging motion. In this paper, we present a two-stage approach\nthat models hair independently from the head to address these challenges in a\ndata-driven manner. The first stage, state compression, learns a\nlow-dimensional latent space of 3D hair states containing motion and\nappearance, via a novel autoencoder-as-a-tracker strategy. To better\ndisentangle the hair and head in appearance learning, we employ multi-view hair\nsegmentation masks in combination with a differentiable volumetric renderer.\nThe second stage learns a novel hair dynamics model that performs temporal hair\ntransfer based on the discovered latent codes. To enforce higher stability\nwhile driving our dynamics model, we employ the 3D point-cloud autoencoder from\nthe compression stage for de-noising of the hair state. Our model outperforms\nthe state of the art in novel view synthesis and is capable of creating novel\nhair animations without having to rely on hair observations as a driving\nsignal. Project page is here https://ziyanw1.github.io/neuwigs/.\n","authors":["Ziyan Wang","Giljoo Nam","Tuur Stuyck","Stephen Lombardi","Chen Cao","Jason Saragih","Michael Zollhoefer","Jessica Hodgins","Christoph Lassner"],"pdf_url":"https://arxiv.org/pdf/2212.00613v3.pdf","comment":null}]}}